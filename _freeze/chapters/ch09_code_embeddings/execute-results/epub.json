{
  "hash": "7f329088657d38287e3df406eb5831ea",
  "result": {
    "engine": "jupyter",
    "markdown": "# Code Embeddings {#sec-code-embeddings}\n\n::: callout-note\n## Chapter Overview\n\nThis chapter covers code embeddings—representations that convert source code into vectors capturing program semantics. We explore how these embeddings understand what code does, not just how it's written, enabling applications from semantic code search to vulnerability detection.\n:::\n\n## What Are Code Embeddings?\n\nCode embeddings convert source code into vectors that capture program semantics—what the code does, not just how it's written. Two functions that sum a list of numbers should have similar embeddings whether implemented with a loop or the built-in `sum()` function.\n\nThe challenge: syntax varies widely while functionality remains the same. Variable names, formatting, and implementation choices differ between programmers, but the underlying logic may be identical. Code embeddings must see through surface differences to capture semantic similarity.\n\n## Creating Code Embeddings\n\n::: {#c2b6fd14 .cell execution_count=1}\n``` {.python .cell-code code-fold=\"false\"}\n\"\"\"\nCode Embeddings: Source Code as Vectors\n\"\"\"\n\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# General text model for demo (production: use CodeBERT, StarCoder, etc.)\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\n\n# Same functionality, different implementations\ncode_snippets = {\n    'sum_loop': '''\ndef sum_numbers(nums):\n    total = 0\n    for n in nums:\n        total += n\n    return total\n''',\n    'sum_builtin': '''\ndef sum_numbers(numbers):\n    return sum(numbers)\n''',\n    'reverse_loop': '''\ndef reverse_list(lst):\n    result = []\n    for i in range(len(lst)-1, -1, -1):\n        result.append(lst[i])\n    return result\n''',\n    'reverse_slice': '''\ndef reverse_list(items):\n    return items[::-1]\n''',\n}\n\nembeddings = {name: model.encode(code) for name, code in code_snippets.items()}\n\nprint(f\"Embedding dimension: {len(embeddings['sum_loop'])}\\n\")\nprint(\"Code embedding similarities:\\n\")\nprint(\"Same functionality, different implementation:\")\nsum_sim = cosine_similarity([embeddings['sum_loop']], [embeddings['sum_builtin']])[0][0]\nrev_sim = cosine_similarity([embeddings['reverse_loop']], [embeddings['reverse_slice']])[0][0]\nprint(f\"  sum (loop) ↔ sum (builtin):       {sum_sim:.3f}\")\nprint(f\"  reverse (loop) ↔ reverse (slice): {rev_sim:.3f}\")\n\nprint(\"\\nDifferent functionality:\")\ncross_sim = cosine_similarity([embeddings['sum_loop']], [embeddings['reverse_loop']])[0][0]\nprint(f\"  sum ↔ reverse:                    {cross_sim:.3f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nEmbedding dimension: 384\n\nCode embedding similarities:\n\nSame functionality, different implementation:\n  sum (loop) ↔ sum (builtin):       0.809\n  reverse (loop) ↔ reverse (slice): 0.822\n\nDifferent functionality:\n  sum ↔ reverse:                    0.272\n```\n:::\n:::\n\n\nFunctions with the same purpose cluster together even with different implementations. The two sum functions are more similar to each other than to the reverse functions, and vice versa.\n\n## When to Use Code Embeddings\n\n- **Semantic code search**—find code by what it does, not just keywords\n- **Code clone detection**—identify copied or similar code across repositories\n- **Vulnerability detection**—find code patterns similar to known vulnerabilities\n- **Code recommendation**—suggest similar functions or implementations\n- **Repository organization**—automatically categorize and tag code\n- **License compliance**—detect potentially copied code for legal review\n\n## Popular Code Architectures\n\n| Architecture | Type | Strengths | Use Cases |\n|-------------|------|-----------|-----------|\n| [CodeBERT](https://github.com/microsoft/CodeBERT) | BERT-style | Multi-language | Search, clone detection |\n| [GraphCodeBERT](https://github.com/microsoft/CodeBERT) | Graph-enhanced | Data flow awareness | Bug detection |\n| [StarCoder](https://github.com/bigcode-project/starcoder) | Large model | 80+ languages | Code generation |\n| [CodeT5](https://github.com/salesforce/CodeT5) | Encoder-decoder | Understanding + generation | Code summarization |\n\n: Code embedding architectures {.striped}\n\n## Advanced: How Code Models Learn {.unnumbered}\n\n::: {.callout-note}\n## Optional Section\nThis section explains how code embedding models capture program semantics. Skip if you just need to use pre-built embeddings.\n:::\n\n### Code as Natural Language\n\nThe simplest approach treats code as text and applies standard NLP techniques. This works surprisingly well because code has structure, naming conventions, and patterns that convey meaning.\n\n```python\n# These look different but serve the same purpose\n# A text model picks up on shared vocabulary: \"sum\", \"numbers\", \"return\"\n\ndef sum_numbers_v1(nums):\n    return sum(nums)\n\ndef sum_numbers_v2(numbers):\n    total = 0\n    for n in numbers:\n        total = total + n\n    return total\n```\n\n### Abstract Syntax Trees (AST)\n\nMore sophisticated models parse code into its structural representation:\n\n```python\nimport ast\n\ncode = \"def add(a, b): return a + b\"\ntree = ast.parse(code)\n\n# The AST captures structure:\n# FunctionDef(name='add', args=['a', 'b'], body=[Return(BinOp(...))])\n```\n\nTraining on ASTs helps models understand that variable names don't change program behavior.\n\n### Data Flow Graphs\n\nGraphCodeBERT goes further by modeling how data flows through programs:\n\n```python\n# Data flow: x → y → z\nx = input()\ny = x.upper()\nz = len(y)\n```\n\nUnderstanding data flow helps detect bugs where data is used before initialization or after it's freed.\n\n### Contrastive Training\n\nCodeBERT and similar models are trained with contrastive objectives:\n\n1. **Natural Language → Code**: Match documentation to the code it describes\n2. **Code → Code**: Match semantically equivalent implementations\n3. **Negative sampling**: Push apart unrelated code pairs\n\n```python\n# Positive pair: documentation matches code\ndoc = \"Returns the sum of all numbers in the list\"\ncode = \"def sum_list(nums): return sum(nums)\"\n\n# Negative pair: documentation doesn't match\ndoc = \"Returns the sum of all numbers in the list\"\ncode = \"def reverse_list(lst): return lst[::-1]\"\n```\n\n## Practical Considerations\n\n### Embedding Granularity\n\nDecide what to embed based on your use case:\n\n- **Function-level**: Best for code search and clone detection\n- **File-level**: Good for repository organization\n- **Line/block-level**: Useful for vulnerability detection\n\n### Multi-Language Support\n\nModels like StarCoder support 80+ programming languages. For cross-language search:\n\n```python\n# A universal code model embeds these similarly\n# because they both sort a list\n\n# Python\nsorted_list = sorted(items)\n\n# JavaScript\nconst sortedList = items.sort();\n```\n\n### Handling Long Code\n\nCode often exceeds model context limits. Strategies include:\n\n1. **Chunking**: Split into functions/classes\n2. **Hierarchical encoding**: Embed chunks, then combine\n3. **Summarization**: Use docstrings/comments plus key lines\n\n## Key Takeaways\n\n- **Code embeddings** capture what code does, not just how it's written—semantically equivalent implementations cluster together\n\n- **General text models** work for basic tasks, but **specialized models** (CodeBERT, StarCoder) understand programming language structure\n\n- **Training approaches** range from treating code as text to parsing ASTs and data flow graphs\n\n- **Applications** include semantic search, clone detection, vulnerability finding, and code recommendation\n\n- **Granularity matters**: embed functions for search, files for organization, blocks for vulnerability detection\n\n## Looking Ahead\n\nThis completes Part II on embedding types. @sec-advanced-embedding-patterns explores advanced patterns like hybrid embeddings, multi-vector representations, and quantized embeddings that extend these foundational types.\n\n## Further Reading\n\n- Feng, Z., et al. (2020). \"CodeBERT: A Pre-Trained Model for Programming and Natural Languages.\" *EMNLP Findings*\n- Guo, D., et al. (2021). \"GraphCodeBERT: Pre-training Code Representations with Data Flow.\" *ICLR*\n- Li, R., et al. (2023). \"StarCoder: may the source be with you!\" *arXiv:2305.06161*\n\n",
    "supporting": [
      "ch09_code_embeddings_files/figure-epub"
    ],
    "filters": [],
    "engineDependencies": {
      "jupyter": [
        {
          "jsWidgets": false,
          "jupyterWidgets": false,
          "htmlLibraries": []
        }
      ]
    }
  }
}