{
  "hash": "2da425c640ad2a7d27c1a7a2515b7cb7",
  "result": {
    "engine": "jupyter",
    "markdown": "# Advanced Embedding Patterns {#sec-advanced-embedding-patterns}\n\n::: callout-note\n## Chapter Overview\n\nProduction embedding systems rarely use single, off-the-shelf embeddings. This chapter covers the advanced patterns that power real-world systems: hybrid vectors combining multiple feature types, multi-vector representations for fine-grained matching, learned sparse embeddings for interpretability, and domain-specific patterns for security, time-series, and structured data. These patterns build on the foundational types from @sec-foundational-embedding-types.\n:::\n\n## Beyond Single Embeddings\n\nThe foundational embedding types—text, image, audio, and others—serve as building blocks. Production systems combine, extend, and specialize these foundations in sophisticated ways:\n\n- **Hybrid embeddings** combine semantic, categorical, numerical, and domain-specific features\n- **Multi-vector representations** use multiple embeddings per item for fine-grained matching\n- **Learned sparse embeddings** balance dense semantics with interpretable sparse features\n- **Specialized architectures** optimize for specific retrieval patterns\n\nUnderstanding these patterns is essential for building embedding systems that perform well on real-world data.\n\n## Hybrid and Composite Embeddings {#sec-hybrid-embeddings}\n\nReal-world entities have multiple facets that single embeddings can't capture. A security log has semantic content (message text), categorical features (event type, severity), numerical features (byte counts, durations), and domain-specific features (IP addresses). Hybrid embeddings combine all of these.\n\n### The Naive Approach Fails\n\nSimple concatenation doesn't work:\n\n::: {#793f3428 .cell execution_count=1}\n``` {.python .cell-code code-fold=\"false\"}\n\"\"\"\nWhy Naive Concatenation Fails\n\nWhen combining embeddings of different dimensions, larger vectors\ndominate similarity calculations, drowning out smaller features.\n\"\"\"\n\nimport numpy as np\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nnp.random.seed(42)\n\n# Simulate: 384-dim text embedding + 10-dim numerical features\ntext_embedding = np.random.randn(384)\nnumerical_features = np.array([0.5, 0.8, 0.2, 0.1, 0.9, 0.3, 0.7, 0.4, 0.6, 0.5])\n\n# Naive concatenation\nnaive_hybrid = np.concatenate([text_embedding, numerical_features])\n\n# The problem: text embedding dominates\ntext_magnitude = np.linalg.norm(text_embedding)\nnum_magnitude = np.linalg.norm(numerical_features)\n\nprint(\"Magnitude comparison:\")\nprint(f\"  Text embedding (384 dims):     {text_magnitude:.2f}\")\nprint(f\"  Numerical features (10 dims):  {num_magnitude:.2f}\")\nprint(f\"  Ratio: {text_magnitude/num_magnitude:.1f}x\")\nprint(\"\\nThe text embedding will dominate similarity calculations!\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMagnitude comparison:\n  Text embedding (384 dims):     18.67\n  Numerical features (10 dims):  1.76\n  Ratio: 10.6x\n\nThe text embedding will dominate similarity calculations!\n```\n:::\n:::\n\n\n### Weighted Normalized Concatenation\n\nThe solution: normalize each component, then apply importance weights:\n\n::: {#301f8128 .cell execution_count=2}\n``` {.python .cell-code code-fold=\"false\"}\n\"\"\"\nWeighted Normalized Concatenation\n\nProperly combines multiple feature types by:\n1. L2-normalizing each component independently\n2. Applying learned or tuned weights\n3. Concatenating the weighted, normalized components\n\"\"\"\n\nimport numpy as np\nfrom sklearn.preprocessing import normalize\n\nnp.random.seed(42)\n\ndef create_hybrid_embedding(\n    text_embedding: np.ndarray,\n    categorical_embedding: np.ndarray,\n    numerical_features: np.ndarray,\n    domain_features: np.ndarray,\n    weights: dict\n) -> np.ndarray:\n    \"\"\"\n    Create a hybrid embedding from multiple feature types.\n\n    Args:\n        text_embedding: Semantic embedding from text encoder (e.g., 384 dims)\n        categorical_embedding: Learned embeddings for categorical features\n        numerical_features: Scaled numerical features\n        domain_features: Domain-specific features (e.g., IP encoding)\n        weights: Importance weights for each component (should sum to 1.0)\n\n    Returns:\n        Hybrid embedding vector\n    \"\"\"\n    # L2-normalize each component\n    text_norm = normalize(text_embedding.reshape(1, -1))[0]\n    cat_norm = normalize(categorical_embedding.reshape(1, -1))[0]\n    num_norm = normalize(numerical_features.reshape(1, -1))[0]\n    domain_norm = normalize(domain_features.reshape(1, -1))[0]\n\n    # Apply weights and concatenate\n    hybrid = np.concatenate([\n        text_norm * weights['text'],\n        cat_norm * weights['categorical'],\n        num_norm * weights['numerical'],\n        domain_norm * weights['domain']\n    ])\n\n    return hybrid\n\n# Example: Security log embedding\ntext_emb = np.random.randn(384)  # From sentence transformer\ncat_emb = np.random.randn(32)    # Learned embeddings for event_type, severity\nnum_feat = np.random.randn(10)   # Scaled: bytes_in, bytes_out, duration\ndomain_feat = np.array([0.75, 0.65, 0.003, 0.039, 1.0])  # IP octets + is_private\n\n# Weights are hyperparameters to tune\nweights = {\n    'text': 0.50,        # Semantic content is most important\n    'categorical': 0.20, # Event type matters\n    'numerical': 0.15,   # Metrics provide context\n    'domain': 0.15       # IP information for security\n}\n\nhybrid = create_hybrid_embedding(\n    text_emb, cat_emb, num_feat, domain_feat, weights\n)\n\nprint(f\"Hybrid embedding dimension: {len(hybrid)}\")\nprint(f\"  Text component: 384 dims × {weights['text']} weight\")\nprint(f\"  Categorical: 32 dims × {weights['categorical']} weight\")\nprint(f\"  Numerical: 10 dims × {weights['numerical']} weight\")\nprint(f\"  Domain: 5 dims × {weights['domain']} weight\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nHybrid embedding dimension: 431\n  Text component: 384 dims × 0.5 weight\n  Categorical: 32 dims × 0.2 weight\n  Numerical: 10 dims × 0.15 weight\n  Domain: 5 dims × 0.15 weight\n```\n:::\n:::\n\n\n### Entity Embeddings for Categorical Features\n\nDon't one-hot encode categorical features—learn embeddings for them:\n\n::: {#e8aaf81d .cell execution_count=3}\n``` {.python .cell-code code-fold=\"false\"}\n\"\"\"\nEntity Embeddings for Categorical Features\n\nLearn dense representations for categorical values instead of sparse one-hot.\nThis captures relationships between categories (e.g., similar event types).\n\"\"\"\n\nimport numpy as np\n\n# Simulated learned embeddings for categorical features\n# In practice, use nn.Embedding in PyTorch/TensorFlow\n\nclass CategoryEmbedder:\n    \"\"\"Simple category embedder (production would use nn.Embedding).\"\"\"\n\n    def __init__(self, categories: list, embedding_dim: int = 8):\n        self.categories = {cat: i for i, cat in enumerate(categories)}\n        self.embedding_dim = embedding_dim\n        # Initialize random embeddings (would be learned in practice)\n        np.random.seed(42)\n        self.embeddings = np.random.randn(len(categories), embedding_dim) * 0.1\n\n    def embed(self, category: str) -> np.ndarray:\n        idx = self.categories.get(category, 0)\n        return self.embeddings[idx]\n\n# Example: Event type embeddings for security logs\nevent_types = ['login', 'logout', 'file_access', 'network_connection',\n               'process_start', 'process_end', 'privilege_escalation']\nseverity_levels = ['info', 'warning', 'error', 'critical']\n\nevent_embedder = CategoryEmbedder(event_types, embedding_dim=8)\nseverity_embedder = CategoryEmbedder(severity_levels, embedding_dim=4)\n\n# Embed categorical features\nevent_emb = event_embedder.embed('login')\nseverity_emb = severity_embedder.embed('warning')\n\n# Combine into categorical embedding\ncategorical_embedding = np.concatenate([event_emb, severity_emb])\n\nprint(f\"Event embedding shape: {event_emb.shape}\")\nprint(f\"Severity embedding shape: {severity_emb.shape}\")\nprint(f\"Combined categorical embedding: {categorical_embedding.shape}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nEvent embedding shape: (8,)\nSeverity embedding shape: (4,)\nCombined categorical embedding: (12,)\n```\n:::\n:::\n\n\n### Numerical Feature Preprocessing\n\nNumerical features need careful preprocessing before embedding:\n\n::: {#9f280607 .cell execution_count=4}\n``` {.python .cell-code code-fold=\"false\"}\n\"\"\"\nNumerical Feature Preprocessing Pipeline\n\nProper preprocessing for numerical features:\n1. Handle missing values\n2. Apply log transform for long-tail distributions\n3. Standardize to zero mean, unit variance\n4. L2-normalize the result\n\"\"\"\n\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\n\nclass NumericalPreprocessor:\n    \"\"\"Preprocess numerical features for embedding.\"\"\"\n\n    def __init__(self, feature_names: list):\n        self.feature_names = feature_names\n        self.scaler = StandardScaler()\n        self.fitted = False\n\n    def fit(self, data: np.ndarray):\n        \"\"\"Fit the scaler on training data.\"\"\"\n        # Apply log1p for long-tail features (bytes, counts)\n        log_data = np.log1p(np.clip(data, 0, None))\n        self.scaler.fit(log_data)\n        self.fitted = True\n        return self\n\n    def transform(self, data: np.ndarray) -> np.ndarray:\n        \"\"\"Transform and normalize numerical features.\"\"\"\n        # Handle missing values\n        data = np.nan_to_num(data, nan=0.0)\n\n        # Log transform for long-tail distributions\n        log_data = np.log1p(np.clip(data, 0, None))\n\n        # Standardize\n        if self.fitted:\n            scaled = self.scaler.transform(log_data.reshape(1, -1))[0]\n        else:\n            scaled = log_data\n\n        return scaled\n\n# Example: Network metrics\nfeature_names = ['bytes_in', 'bytes_out', 'duration_ms', 'packet_count']\npreprocessor = NumericalPreprocessor(feature_names)\n\n# Simulate training data for fitting\ntrain_data = np.array([\n    [1024, 2048, 150, 10],\n    [1000000, 500000, 5000, 1000],  # Long-tail values\n    [512, 1024, 50, 5],\n])\npreprocessor.fit(train_data)\n\n# Transform new data point\nnew_data = np.array([50000, 25000, 200, 50])\nprocessed = preprocessor.transform(new_data)\n\nprint(\"Original features:\", new_data)\nprint(\"Processed features:\", np.round(processed, 3))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nOriginal features: [50000 25000   200    50]\nProcessed features: [ 0.533  0.325 -0.265  0.102]\n```\n:::\n:::\n\n\n## Multi-Vector Representations {#sec-multi-vector-representations}\n\nSingle vectors compress all information into one point. Multi-vector representations preserve more detail by using multiple vectors per item.\n\n### ColBERT-Style Late Interaction\n\nColBERT represents documents with one vector per token, enabling fine-grained matching:\n\n::: {#48aaf062 .cell execution_count=5}\n``` {.python .cell-code code-fold=\"false\"}\n\"\"\"\nColBERT-Style Multi-Vector Representation\n\nInstead of one vector per document, use one vector per token.\nMatching happens at the token level (late interaction).\n\"\"\"\n\nimport numpy as np\n\ndef simulate_colbert_encoding(text: str, dim: int = 128) -> np.ndarray:\n    \"\"\"\n    Simulate ColBERT token-level encoding.\n\n    Returns: Matrix of shape (num_tokens, dim)\n    \"\"\"\n    tokens = text.lower().split()\n    np.random.seed(hash(text) % 2**32)\n    # Each token gets its own embedding\n    return np.random.randn(len(tokens), dim)\n\ndef colbert_similarity(query_vecs: np.ndarray, doc_vecs: np.ndarray) -> float:\n    \"\"\"\n    ColBERT MaxSim: For each query token, find max similarity to any doc token.\n    Sum these max similarities.\n    \"\"\"\n    # Compute all pairwise similarities\n    similarities = query_vecs @ doc_vecs.T  # (q_tokens, d_tokens)\n\n    # MaxSim: max over document tokens for each query token\n    max_sims = similarities.max(axis=1)\n\n    return max_sims.sum()\n\n# Example\nquery = \"machine learning models\"\ndoc1 = \"deep learning neural network models for prediction\"\ndoc2 = \"cooking recipes and kitchen equipment\"\n\nquery_vecs = simulate_colbert_encoding(query)\ndoc1_vecs = simulate_colbert_encoding(doc1)\ndoc2_vecs = simulate_colbert_encoding(doc2)\n\nsim1 = colbert_similarity(query_vecs, doc1_vecs)\nsim2 = colbert_similarity(query_vecs, doc2_vecs)\n\nprint(f\"Query: '{query}'\")\nprint(f\"Query vectors shape: {query_vecs.shape}\")\nprint(f\"\\nDoc 1: '{doc1}'\")\nprint(f\"Doc 1 vectors shape: {doc1_vecs.shape}\")\nprint(f\"Similarity: {sim1:.2f}\")\nprint(f\"\\nDoc 2: '{doc2}'\")\nprint(f\"Similarity: {sim2:.2f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nQuery: 'machine learning models'\nQuery vectors shape: (3, 128)\n\nDoc 1: 'deep learning neural network models for prediction'\nDoc 1 vectors shape: (7, 128)\nSimilarity: 52.05\n\nDoc 2: 'cooking recipes and kitchen equipment'\nSimilarity: 38.68\n```\n:::\n:::\n\n\n**When to use multi-vector:**\n- Fine-grained matching matters (exact phrase matching)\n- Documents are long and diverse\n- You can afford 10-100x storage overhead\n\n## Matryoshka Embeddings {#sec-matryoshka-embeddings}\n\nMatryoshka (nested doll) embeddings encode information hierarchically—the first N dimensions are a valid embedding on their own:\n\n::: {#e60b5f0d .cell execution_count=6}\n``` {.python .cell-code code-fold=\"false\"}\n\"\"\"\nMatryoshka Embeddings: Variable-Length Representations\n\nThe first N dimensions form a valid embedding for any N.\nTrade off quality vs. efficiency at query time.\n\"\"\"\n\nimport numpy as np\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Simulate Matryoshka embeddings (trained to work at multiple dimensions)\nnp.random.seed(42)\n\ndef simulate_matryoshka_embedding(text: str, full_dim: int = 768) -> np.ndarray:\n    \"\"\"\n    Simulate a Matryoshka embedding where prefixes are valid embeddings.\n    Real models are trained with a special loss to ensure this property.\n    \"\"\"\n    np.random.seed(hash(text) % 2**32)\n    return np.random.randn(full_dim)\n\ntexts = [\n    \"machine learning for natural language processing\",\n    \"deep learning NLP models\",\n    \"cooking italian pasta recipes\",\n]\n\nembeddings = [simulate_matryoshka_embedding(t) for t in texts]\n\n# Compare at different dimension prefixes\nprint(\"Similarity at different dimensions:\\n\")\nfor dim in [64, 128, 256, 768]:\n    truncated = [e[:dim] for e in embeddings]\n    sim_01 = cosine_similarity([truncated[0]], [truncated[1]])[0][0]\n    sim_02 = cosine_similarity([truncated[0]], [truncated[2]])[0][0]\n    print(f\"  {dim} dims: ML↔DL={sim_01:.3f}, ML↔Cooking={sim_02:.3f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSimilarity at different dimensions:\n\n  64 dims: ML↔DL=-0.073, ML↔Cooking=0.096\n  128 dims: ML↔DL=-0.039, ML↔Cooking=-0.020\n  256 dims: ML↔DL=0.076, ML↔Cooking=-0.003\n  768 dims: ML↔DL=0.064, ML↔Cooking=-0.013\n```\n:::\n:::\n\n\n**Benefits of Matryoshka embeddings:**\n- Use short prefixes for fast initial retrieval\n- Use full dimensions for final reranking\n- Adapt to latency/quality requirements at runtime\n- Reduce storage by storing only needed dimensions\n\n## Learned Sparse Embeddings {#sec-learned-sparse-embeddings}\n\nSPLADE and similar models learn sparse representations that combine the best of dense and sparse retrieval:\n\n::: {#5f820e62 .cell execution_count=7}\n``` {.python .cell-code code-fold=\"false\"}\n\"\"\"\nLearned Sparse Embeddings (SPLADE-style)\n\nLearn to predict which vocabulary terms are important for a document.\nResults in sparse vectors with interpretable dimensions (actual words).\n\"\"\"\n\nimport numpy as np\n\ndef simulate_splade_embedding(text: str, vocab_size: int = 30000) -> dict:\n    \"\"\"\n    Simulate SPLADE-style sparse embedding.\n\n    Returns dict mapping vocabulary indices to importance weights.\n    Real SPLADE uses a transformer to predict term importance.\n    \"\"\"\n    words = text.lower().split()\n    sparse = {}\n\n    np.random.seed(hash(text) % 2**32)\n\n    for word in words:\n        # Simulate vocabulary index\n        idx = hash(word) % vocab_size\n        # Simulate learned importance weight\n        weight = np.random.exponential(1.0)\n        sparse[idx] = max(sparse.get(idx, 0), weight)\n\n    # SPLADE also expands to related terms\n    for _ in range(len(words)):\n        expanded_idx = np.random.randint(vocab_size)\n        sparse[expanded_idx] = np.random.exponential(0.5)\n\n    return sparse\n\ndef sparse_dot_product(sparse1: dict, sparse2: dict) -> float:\n    \"\"\"Compute dot product of two sparse vectors.\"\"\"\n    score = 0.0\n    for idx, weight1 in sparse1.items():\n        if idx in sparse2:\n            score += weight1 * sparse2[idx]\n    return score\n\n# Example\nquery = \"machine learning models\"\ndoc1 = \"neural network deep learning\"\ndoc2 = \"kitchen cooking recipes\"\n\nq_sparse = simulate_splade_embedding(query)\nd1_sparse = simulate_splade_embedding(doc1)\nd2_sparse = simulate_splade_embedding(doc2)\n\nprint(f\"Query sparse embedding: {len(q_sparse)} non-zero terms\")\nprint(f\"Doc 1 sparse embedding: {len(d1_sparse)} non-zero terms\")\nprint(f\"Doc 2 sparse embedding: {len(d2_sparse)} non-zero terms\")\nprint(f\"\\nQuery ↔ Doc 1 (related): {sparse_dot_product(q_sparse, d1_sparse):.2f}\")\nprint(f\"Query ↔ Doc 2 (unrelated): {sparse_dot_product(q_sparse, d2_sparse):.2f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nQuery sparse embedding: 6 non-zero terms\nDoc 1 sparse embedding: 8 non-zero terms\nDoc 2 sparse embedding: 6 non-zero terms\n\nQuery ↔ Doc 1 (related): 2.71\nQuery ↔ Doc 2 (unrelated): 0.00\n```\n:::\n:::\n\n\n**Benefits of learned sparse:**\n- Interpretable (dimensions correspond to vocabulary terms)\n- Works with inverted indices (fast exact matching)\n- Captures term expansion (related terms)\n- Combines well with dense embeddings (hybrid search)\n\n## Time-Series Pattern Embeddings {#sec-timeseries-pattern-embeddings}\n\nBeyond basic statistical features, production systems use learned representations for time-series patterns.\n\n### ROCKET: Random Convolutional Kernels\n\nROCKET transforms time-series into features using random convolutional kernels:\n\n::: {#3e7696eb .cell execution_count=8}\n``` {.python .cell-code code-fold=\"false\"}\n\"\"\"\nROCKET-Style Time-Series Embeddings\n\nUses random convolutional kernels to extract features from time-series.\nFast to compute, works well for classification and similarity.\n\"\"\"\n\nimport numpy as np\n\ndef generate_random_kernels(n_kernels: int = 100, max_length: int = 9) -> list:\n    \"\"\"Generate random convolutional kernels.\"\"\"\n    np.random.seed(42)\n    kernels = []\n    for _ in range(n_kernels):\n        length = np.random.choice([3, 5, 7, 9])\n        weights = np.random.randn(length)\n        bias = np.random.randn()\n        dilation = np.random.choice([1, 2, 4])\n        kernels.append((weights, bias, dilation))\n    return kernels\n\ndef apply_kernel(series: np.ndarray, kernel: tuple) -> tuple:\n    \"\"\"Apply a single kernel and extract features (max, ppv).\"\"\"\n    weights, bias, dilation = kernel\n    length = len(weights)\n\n    # Dilated convolution\n    output = []\n    for i in range(len(series) - (length - 1) * dilation):\n        indices = [i + j * dilation for j in range(length)]\n        value = np.dot(series[indices], weights) + bias\n        output.append(value)\n\n    output = np.array(output)\n\n    # ROCKET features: max value and proportion of positive values (PPV)\n    max_val = np.max(output) if len(output) > 0 else 0\n    ppv = np.mean(output > 0) if len(output) > 0 else 0\n\n    return max_val, ppv\n\ndef rocket_embedding(series: np.ndarray, kernels: list) -> np.ndarray:\n    \"\"\"Create ROCKET embedding from time-series.\"\"\"\n    features = []\n    for kernel in kernels:\n        max_val, ppv = apply_kernel(series, kernel)\n        features.extend([max_val, ppv])\n    return np.array(features)\n\n# Generate kernels (done once)\nkernels = generate_random_kernels(n_kernels=50)\n\n# Example time-series patterns\nt = np.linspace(0, 4*np.pi, 100)\npatterns = {\n    'sine': np.sin(t) + np.random.randn(100) * 0.1,\n    'cosine': np.cos(t) + np.random.randn(100) * 0.1,\n    'trend_up': t/10 + np.random.randn(100) * 0.2,\n    'random': np.random.randn(100),\n}\n\n# Create embeddings\nembeddings = {name: rocket_embedding(series, kernels)\n              for name, series in patterns.items()}\n\nprint(f\"ROCKET embedding dimension: {len(embeddings['sine'])}\")\nprint(f\"  ({len(kernels)} kernels × 2 features each)\")\n\n# Compare patterns\nfrom sklearn.metrics.pairwise import cosine_similarity\nprint(\"\\nPattern similarities:\")\nprint(f\"  sine ↔ cosine: {cosine_similarity([embeddings['sine']], [embeddings['cosine']])[0][0]:.3f}\")\nprint(f\"  sine ↔ trend:  {cosine_similarity([embeddings['sine']], [embeddings['trend_up']])[0][0]:.3f}\")\nprint(f\"  sine ↔ random: {cosine_similarity([embeddings['sine']], [embeddings['random']])[0][0]:.3f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nROCKET embedding dimension: 100\n  (50 kernels × 2 features each)\n\nPattern similarities:\n  sine ↔ cosine: 0.998\n  sine ↔ trend:  0.828\n  sine ↔ random: 0.894\n```\n:::\n:::\n\n\n### Learned Temporal Embeddings\n\nFor more complex patterns, use neural networks:\n\n::: {#7078b715 .cell execution_count=9}\n``` {.python .cell-code code-fold=\"false\"}\n\"\"\"\nLearned Temporal Embeddings\n\nUse LSTMs, Transformers, or Temporal CNNs to learn time-series representations.\nThis example shows a simplified LSTM-style encoding.\n\"\"\"\n\nimport numpy as np\n\nclass SimpleTemporalEncoder:\n    \"\"\"\n    Simplified temporal encoder for illustration.\n    Production systems use PyTorch/TensorFlow LSTM or Transformer.\n    \"\"\"\n\n    def __init__(self, hidden_dim: int = 64):\n        self.hidden_dim = hidden_dim\n        np.random.seed(42)\n        # Simplified: project statistics to hidden space\n        self.projection = np.random.randn(10, hidden_dim) * 0.1\n\n    def encode(self, series: np.ndarray) -> np.ndarray:\n        \"\"\"Encode time-series to fixed-length embedding.\"\"\"\n        # Extract temporal features\n        features = np.array([\n            np.mean(series),\n            np.std(series),\n            np.min(series),\n            np.max(series),\n            np.mean(np.diff(series)),  # Trend\n            np.std(np.diff(series)),   # Volatility\n            np.corrcoef(series[:-1], series[1:])[0, 1],  # Autocorrelation\n            len(np.where(np.diff(np.sign(series)))[0]),  # Zero crossings\n            np.percentile(series, 25),\n            np.percentile(series, 75),\n        ])\n        features = np.nan_to_num(features)\n\n        # Project to embedding space\n        embedding = np.tanh(features @ self.projection)\n        return embedding\n\nencoder = SimpleTemporalEncoder(hidden_dim=64)\n\n# Encode different patterns\nt = np.linspace(0, 4*np.pi, 100)\nembeddings = {\n    'periodic': encoder.encode(np.sin(t)),\n    'trending': encoder.encode(t / 10),\n    'volatile': encoder.encode(np.random.randn(100)),\n}\n\nprint(f\"Temporal embedding dimension: {len(embeddings['periodic'])}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTemporal embedding dimension: 64\n```\n:::\n:::\n\n\n## Binary and Quantized Embeddings {#sec-quantized-embeddings}\n\nFor massive scale, compress embeddings to reduce storage and accelerate search:\n\n::: {#a881a3a0 .cell execution_count=10}\n``` {.python .cell-code code-fold=\"false\"}\n\"\"\"\nBinary and Quantized Embeddings\n\nCompress embeddings for efficiency:\n- Binary: Each dimension → 1 bit (32x compression)\n- Product Quantization: Learn codebooks for compression\n\"\"\"\n\nimport numpy as np\n\ndef binarize_embedding(embedding: np.ndarray) -> np.ndarray:\n    \"\"\"Convert to binary embedding (sign of each dimension).\"\"\"\n    return (embedding > 0).astype(np.int8)\n\ndef hamming_distance(bin1: np.ndarray, bin2: np.ndarray) -> int:\n    \"\"\"Hamming distance between binary vectors.\"\"\"\n    return np.sum(bin1 != bin2)\n\ndef hamming_similarity(bin1: np.ndarray, bin2: np.ndarray) -> float:\n    \"\"\"Normalized Hamming similarity (0 to 1).\"\"\"\n    return 1 - hamming_distance(bin1, bin2) / len(bin1)\n\n# Example: Compare binary vs float embeddings\nnp.random.seed(42)\nemb1 = np.random.randn(768)\nemb2 = emb1 + np.random.randn(768) * 0.5  # Similar\nemb3 = np.random.randn(768)  # Different\n\n# Float similarity\nfrom sklearn.metrics.pairwise import cosine_similarity\nfloat_sim_12 = cosine_similarity([emb1], [emb2])[0][0]\nfloat_sim_13 = cosine_similarity([emb1], [emb3])[0][0]\n\n# Binary similarity\nbin1, bin2, bin3 = [binarize_embedding(e) for e in [emb1, emb2, emb3]]\nbin_sim_12 = hamming_similarity(bin1, bin2)\nbin_sim_13 = hamming_similarity(bin1, bin3)\n\nprint(\"Float vs Binary similarity comparison:\")\nprint(f\"\\n  Similar pair:\")\nprint(f\"    Float cosine: {float_sim_12:.3f}\")\nprint(f\"    Binary Hamming: {bin_sim_12:.3f}\")\nprint(f\"\\n  Different pair:\")\nprint(f\"    Float cosine: {float_sim_13:.3f}\")\nprint(f\"    Binary Hamming: {bin_sim_13:.3f}\")\n\nprint(f\"\\nStorage comparison for 768-dim embedding:\")\nprint(f\"  Float32: {768 * 4} bytes\")\nprint(f\"  Binary:  {768 // 8} bytes ({768 * 4 / (768 // 8):.0f}x compression)\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFloat vs Binary similarity comparison:\n\n  Similar pair:\n    Float cosine: 0.894\n    Binary Hamming: 0.859\n\n  Different pair:\n    Float cosine: -0.016\n    Binary Hamming: 0.504\n\nStorage comparison for 768-dim embedding:\n  Float32: 3072 bytes\n  Binary:  96 bytes (32x compression)\n```\n:::\n:::\n\n\n**When to use quantized embeddings:**\n- Billions of vectors (storage constraints)\n- Latency-critical applications\n- First-stage retrieval (rerank with full precision)\n- Edge deployment\n\n## Session and Behavioral Embeddings {#sec-behavioral-embeddings}\n\nEmbed user sessions and behaviors as sequences:\n\n::: {#d724d1fa .cell execution_count=11}\n``` {.python .cell-code code-fold=\"false\"}\n\"\"\"\nSession and Behavioral Embeddings\n\nEmbed sequences of user actions to capture behavioral patterns.\nSimilar sessions (browsing patterns) get similar embeddings.\n\"\"\"\n\nimport numpy as np\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nclass SessionEncoder:\n    \"\"\"Encode user sessions as embeddings.\"\"\"\n\n    def __init__(self, action_vocab: list, embedding_dim: int = 64):\n        self.action_vocab = {a: i for i, a in enumerate(action_vocab)}\n        self.embedding_dim = embedding_dim\n        np.random.seed(42)\n        # Action embeddings (would be learned)\n        self.action_embeddings = np.random.randn(len(action_vocab), embedding_dim) * 0.1\n\n    def encode_session(self, actions: list) -> np.ndarray:\n        \"\"\"Encode a session (sequence of actions) to single embedding.\"\"\"\n        if not actions:\n            return np.zeros(self.embedding_dim)\n\n        # Get embeddings for each action\n        action_embs = []\n        for action in actions:\n            if action in self.action_vocab:\n                idx = self.action_vocab[action]\n                action_embs.append(self.action_embeddings[idx])\n\n        if not action_embs:\n            return np.zeros(self.embedding_dim)\n\n        # Combine with weighted average (recent actions weighted more)\n        weights = np.exp(np.linspace(-1, 0, len(action_embs)))\n        weights /= weights.sum()\n\n        session_emb = np.average(action_embs, axis=0, weights=weights)\n        return session_emb\n\n# Define action vocabulary\nactions = ['view_product', 'add_to_cart', 'remove_from_cart',\n           'view_category', 'search', 'checkout', 'view_reviews']\n\nencoder = SessionEncoder(actions)\n\n# Example sessions\nshopping_session = ['view_category', 'view_product', 'view_reviews',\n                    'add_to_cart', 'view_product', 'add_to_cart', 'checkout']\nbrowsing_session = ['view_category', 'view_product', 'view_category',\n                    'search', 'view_product', 'view_category']\ncart_abandon = ['view_product', 'add_to_cart', 'view_product',\n                'add_to_cart', 'remove_from_cart']\n\nemb_shopping = encoder.encode_session(shopping_session)\nemb_browsing = encoder.encode_session(browsing_session)\nemb_abandon = encoder.encode_session(cart_abandon)\n\nprint(\"Session similarities:\")\nprint(f\"  Shopping ↔ Browsing: {cosine_similarity([emb_shopping], [emb_browsing])[0][0]:.3f}\")\nprint(f\"  Shopping ↔ Cart abandon: {cosine_similarity([emb_shopping], [emb_abandon])[0][0]:.3f}\")\nprint(f\"  Browsing ↔ Cart abandon: {cosine_similarity([emb_browsing], [emb_abandon])[0][0]:.3f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSession similarities:\n  Shopping ↔ Browsing: 0.308\n  Shopping ↔ Cart abandon: 0.747\n  Browsing ↔ Cart abandon: 0.243\n```\n:::\n:::\n\n\n## Domain-Specific Embeddings {#sec-domain-specific-embeddings}\n\nSome domains require specialized embedding approaches.\n\n### Security Log Embeddings\n\nCombining semantic, categorical, numerical, and network features:\n\n::: {#b0de1cbf .cell execution_count=12}\n``` {.python .cell-code code-fold=\"false\"}\n\"\"\"\nSecurity Log Embedding (OCSF-style)\n\nHybrid embedding for security events combining:\n- Semantic: Log message content\n- Categorical: Event type, severity, status\n- Numerical: Byte counts, durations\n- Network: IP address encoding\n\"\"\"\n\nimport numpy as np\nfrom sklearn.preprocessing import normalize\n\ndef encode_ip_address(ip: str) -> np.ndarray:\n    \"\"\"\n    Encode IP address as 5-dim vector:\n    - 4 normalized octets\n    - 1 is_private indicator\n    \"\"\"\n    try:\n        octets = [int(x) for x in ip.split('.')]\n        normalized = [o / 255.0 for o in octets]\n\n        # Check if private IP\n        is_private = (\n            octets[0] == 10 or\n            (octets[0] == 172 and 16 <= octets[1] <= 31) or\n            (octets[0] == 192 and octets[1] == 168)\n        )\n\n        return np.array(normalized + [float(is_private)])\n    except:\n        return np.zeros(5)\n\nclass SecurityLogEmbedder:\n    \"\"\"Create hybrid embeddings for security logs.\"\"\"\n\n    def __init__(self):\n        np.random.seed(42)\n        # Simulated text encoder (would use sentence-transformers)\n        self.text_dim = 384\n        # Category embeddings\n        self.event_types = ['login', 'logout', 'file_access', 'network', 'process']\n        self.event_embeddings = np.random.randn(len(self.event_types), 8) * 0.1\n        self.severities = ['info', 'warning', 'error', 'critical']\n        self.severity_embeddings = np.random.randn(len(self.severities), 4) * 0.1\n\n        # Weights for combining\n        self.weights = {\n            'text': 0.50,\n            'categorical': 0.20,\n            'numerical': 0.15,\n            'network': 0.15\n        }\n\n    def embed(self, log: dict) -> np.ndarray:\n        \"\"\"Create hybrid embedding for a security log.\"\"\"\n        # Text embedding (simulated)\n        np.random.seed(hash(log.get('message', '')) % 2**32)\n        text_emb = np.random.randn(self.text_dim)\n\n        # Categorical embeddings\n        event_idx = self.event_types.index(log.get('event_type', 'network'))\n        severity_idx = self.severities.index(log.get('severity', 'info'))\n        cat_emb = np.concatenate([\n            self.event_embeddings[event_idx],\n            self.severity_embeddings[severity_idx]\n        ])\n\n        # Numerical features\n        num_features = np.array([\n            np.log1p(log.get('bytes_in', 0)),\n            np.log1p(log.get('bytes_out', 0)),\n            np.log1p(log.get('duration_ms', 0)),\n        ])\n\n        # Network features\n        ip_emb = encode_ip_address(log.get('src_ip', '0.0.0.0'))\n\n        # Normalize and weight\n        text_norm = normalize(text_emb.reshape(1, -1))[0] * self.weights['text']\n        cat_norm = normalize(cat_emb.reshape(1, -1))[0] * self.weights['categorical']\n        num_norm = normalize(num_features.reshape(1, -1))[0] * self.weights['numerical']\n        ip_norm = normalize(ip_emb.reshape(1, -1))[0] * self.weights['network']\n\n        return np.concatenate([text_norm, cat_norm, num_norm, ip_norm])\n\n# Example\nembedder = SecurityLogEmbedder()\n\nlog1 = {\n    'message': 'Failed login attempt from external IP',\n    'event_type': 'login',\n    'severity': 'warning',\n    'bytes_in': 1024,\n    'bytes_out': 512,\n    'duration_ms': 150,\n    'src_ip': '203.0.113.50'\n}\n\nlog2 = {\n    'message': 'Successful login from internal network',\n    'event_type': 'login',\n    'severity': 'info',\n    'bytes_in': 2048,\n    'bytes_out': 1024,\n    'duration_ms': 100,\n    'src_ip': '192.168.1.50'\n}\n\nemb1 = embedder.embed(log1)\nemb2 = embedder.embed(log2)\n\nprint(f\"Security log embedding dimension: {len(emb1)}\")\nprint(f\"  Text: 384, Categorical: 12, Numerical: 3, Network: 5\")\nprint(f\"\\nLog similarity: {cosine_similarity([emb1], [emb2])[0][0]:.3f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSecurity log embedding dimension: 404\n  Text: 384, Categorical: 12, Numerical: 3, Network: 5\n\nLog similarity: 0.157\n```\n:::\n:::\n\n\n## Choosing the Right Pattern\n\n| Pattern | Best For | Trade-offs |\n|---------|----------|------------|\n| **Hybrid vectors** | Multi-faceted entities (logs, products) | Requires weight tuning |\n| **Multi-vector (ColBERT)** | Fine-grained matching | 10-100x storage |\n| **Matryoshka** | Variable quality/latency needs | Requires special training |\n| **Learned sparse (SPLADE)** | Interpretability + performance | More complex indexing |\n| **ROCKET time-series** | Pattern similarity | Fixed representation |\n| **Binary/quantized** | Massive scale | Quality loss |\n| **Session embeddings** | Behavioral patterns | Requires sequence modeling |\n\n: Advanced embedding pattern selection guide {.striped}\n\n## Key Takeaways\n\n- **Naive concatenation fails** when combining embeddings of different sizes—use weighted, normalized concatenation\n\n- **Entity embeddings** for categorical features outperform one-hot encoding by learning relationships between categories\n\n- **Multi-vector representations** (ColBERT) provide fine-grained matching at the cost of storage\n\n- **Matryoshka embeddings** enable quality/latency trade-offs at query time\n\n- **Learned sparse embeddings** (SPLADE) combine interpretability with semantic matching\n\n- **Time-series patterns** can be captured with ROCKET (fast, simple) or learned encoders (more expressive)\n\n- **Domain-specific embeddings** like security logs require thoughtful combination of semantic, categorical, numerical, and specialized features\n\n## Looking Ahead\n\nWith both foundational types and advanced patterns understood, @sec-strategic-architecture covers how to architect systems that deploy these embeddings at scale. For training custom embeddings with these patterns, @sec-custom-embedding-strategies provides guidance on when to build versus fine-tune.\n\n## Further Reading\n\n- Khattab, O. & Zaharia, M. (2020). \"ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT.\" *SIGIR*\n- Kusupati, A., et al. (2022). \"Matryoshka Representation Learning.\" *NeurIPS*\n- Formal, T., et al. (2021). \"SPLADE: Sparse Lexical and Expansion Model for First Stage Ranking.\" *SIGIR*\n- Dempster, A., et al. (2020). \"ROCKET: Exceptionally Fast and Accurate Time Series Classification Using Random Convolutional Kernels.\" *Data Mining and Knowledge Discovery*\n- Guo, C., et al. (2016). \"Entity Embeddings of Categorical Variables.\" *arXiv:1604.06737*\n\n",
    "supporting": [
      "ch03_advanced_embedding_patterns_files"
    ],
    "filters": [],
    "includes": {}
  }
}