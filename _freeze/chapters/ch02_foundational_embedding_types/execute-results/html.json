{
  "hash": "845241389cc5c4d23851ec6a76162831",
  "result": {
    "engine": "jupyter",
    "markdown": "# Foundational Embedding Types {#sec-foundational-embedding-types}\n\n::: callout-note\n## Chapter Overview\n\nThis chapter provides a comprehensive tour of foundational embedding types across different data modalities. We examine text, image, audio, video, multi-modal, graph, time-series, and code embeddings—understanding what makes each unique, when to use them, and how they're created. These foundational types form the building blocks that production systems combine and extend using the advanced patterns covered in @sec-advanced-embedding-patterns.\n:::\n\n## The Embedding Landscape\n\nEvery type of data can be converted to embeddings, but different data types require different approaches. The key insight is that **the embedding architecture must match the structure of your data**:\n\n- **Text** (words, sentences, documents): Transformer models capture context and semantic meaning\n- **Images**: CNNs and Vision Transformers capture spatial patterns and visual features\n- **Audio** (speech, music, sounds): Spectral models process frequency patterns over time\n- **Video**: Temporal models combine frame-level features with motion understanding\n- **Multi-modal** (text + images): Alignment models map different modalities to a shared space\n- **Graphs** (networks, relationships): Message-passing models aggregate neighborhood information\n- **Time-series** (sensors, sequences): Recurrent and convolutional models capture temporal patterns\n- **Code** (programs, functions): Specialized models understand syntax and program semantics\n\nLet's explore each embedding type in depth.\n\n## Text Embeddings {#sec-text-embedding-types}\n\nText embeddings are the most mature and widely used embedding type. They've evolved through several generations:\n\n### Word Embeddings\n\nThe foundation of modern NLP, word embeddings map individual words to vectors:\n\n::: {#c40b8f06 .cell execution_count=1}\n``` {.python .cell-code code-fold=\"false\"}\n\"\"\"\nWord Embeddings: From Words to Vectors\n\nWord embeddings capture semantic relationships between individual words.\nWords with similar meanings cluster together in the embedding space.\n\"\"\"\n\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport numpy as np\n\n# Use a sentence model to embed individual words\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\n\n# Embed words across different categories\nwords = {\n    'animals': ['cat', 'dog', 'elephant', 'whale'],\n    'vehicles': ['car', 'truck', 'airplane', 'boat'],\n    'colors': ['red', 'blue', 'green', 'yellow'],\n}\n\nall_words = [w for group in words.values() for w in group]\nembeddings = model.encode(all_words)\n\n# Show that words cluster by category\nprint(\"Word similarities (same category = higher similarity):\\n\")\nprint(\"Within categories:\")\nprint(f\"  cat ↔ dog:      {cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]:.3f}\")\nprint(f\"  car ↔ truck:    {cosine_similarity([embeddings[4]], [embeddings[5]])[0][0]:.3f}\")\nprint(f\"  red ↔ blue:     {cosine_similarity([embeddings[8]], [embeddings[9]])[0][0]:.3f}\")\n\nprint(\"\\nAcross categories:\")\nprint(f\"  cat ↔ car:      {cosine_similarity([embeddings[0]], [embeddings[4]])[0][0]:.3f}\")\nprint(f\"  dog ↔ red:      {cosine_similarity([embeddings[1]], [embeddings[8]])[0][0]:.3f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nWord similarities (same category = higher similarity):\n\nWithin categories:\n  cat ↔ dog:      0.661\n  car ↔ truck:    0.689\n  red ↔ blue:     0.729\n\nAcross categories:\n  cat ↔ car:      0.463\n  dog ↔ red:      0.377\n```\n:::\n:::\n\n\n**Key characteristics:**\n\n- One vector per word (static, context-independent in classic models)\n- Typically 100-300 dimensions\n- Captures synonyms, analogies, and semantic relationships\n\n### Sentence and Document Embeddings\n\nModern applications need to embed entire sentences or documents:\n\n::: {#afd8ff82 .cell execution_count=2}\n``` {.python .cell-code code-fold=\"false\"}\n\"\"\"\nSentence Embeddings: Capturing Complete Thoughts\n\nSentence embeddings represent the meaning of entire sentences,\nenabling semantic search and similarity comparison.\n\"\"\"\n\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\n\n# Sentences with similar meaning but different words\nsentences = [\n    \"The quick brown fox jumps over the lazy dog\",\n    \"A fast auburn fox leaps above a sleepy canine\",\n    \"Machine learning models require lots of training data\",\n    \"AI systems need substantial amounts of examples to learn\",\n]\n\nembeddings = model.encode(sentences)\n\nprint(\"Sentence similarities:\\n\")\nprint(\"Similar meaning (paraphrases):\")\nprint(f\"  Sentence 1 ↔ 2: {cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]:.3f}\")\nprint(f\"  Sentence 3 ↔ 4: {cosine_similarity([embeddings[2]], [embeddings[3]])[0][0]:.3f}\")\n\nprint(\"\\nDifferent topics:\")\nprint(f\"  Sentence 1 ↔ 3: {cosine_similarity([embeddings[0]], [embeddings[2]])[0][0]:.3f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSentence similarities:\n\nSimilar meaning (paraphrases):\n  Sentence 1 ↔ 2: 0.733\n  Sentence 3 ↔ 4: 0.525\n\nDifferent topics:\n  Sentence 1 ↔ 3: -0.024\n```\n:::\n:::\n\n\n**When to use text embeddings:**\n\n- Text classification, clustering, and sentiment analysis (see @sec-text-classification-clustering)\n- Question answering and RAG systems (see @sec-rag-at-scale)\n- Chatbots and conversational AI—intent matching, response selection (see @sec-conversational-ai)\n- Summarization—finding representative sentences (see @sec-embedding-summarization)\n- Semantic search—finding documents by meaning (see @sec-semantic-search)\n- Recommendation systems—content-based filtering (see @sec-recommendation-systems)\n- Customer support—ticket routing, finding similar issues (see @sec-cross-industry-patterns)\n- Content moderation—detecting similar problematic content (see @sec-content-moderation)\n- Duplicate and near-duplicate detection (see @sec-entity-resolution)\n- Entity resolution—matching names and descriptions (see @sec-entity-resolution)\n- Machine translation—cross-lingual embeddings (see @sec-defense-intelligence)\n\n**Popular models:**\n\n| Model | Dimensions | Speed | Quality | Best For |\n|-------|-----------|-------|---------|----------|\n| [all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) | 384 | Fast | Good | General purpose |\n| [all-mpnet-base-v2](https://huggingface.co/sentence-transformers/all-mpnet-base-v2) | 768 | Medium | Better | Higher quality needs |\n| [text-embedding-3-small](https://platform.openai.com/docs/guides/embeddings) | 1536 | API | Excellent | Production systems |\n| [text-embedding-3-large](https://platform.openai.com/docs/guides/embeddings) | 3072 | API | Best | Maximum quality |\n\n: Popular text embedding models {.striped}\n\nSee @sec-sentence-transformers for details on how these models are trained.\n\n### Classification, Clustering, and Sentiment Analysis {#sec-text-classification-clustering}\n\nOnce you have text embeddings, three foundational tasks become straightforward: **classification** (assigning labels), **clustering** (discovering groups), and **sentiment analysis** (a special case of classification). All three leverage the same principle—similar texts have similar embeddings.\n\n**Classification with embeddings:**\n\nTrain a simple classifier on top of frozen embeddings, or use nearest-neighbor approaches. The k-NN (k-nearest neighbors) method shown below works as follows: during training, embed each text and store the embedding alongside its label. To predict a new text's label, embed it, find the k training embeddings most similar to it (using cosine similarity), and return the most common label among those neighbors.\n\n::: {#871932fb .cell execution_count=3}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show Text Classifier\"}\nimport numpy as np\nfrom collections import Counter\n\n\nclass EmbeddingClassifier:\n    \"\"\"Simple k-NN classifier using embeddings.\"\"\"\n\n    def __init__(self, encoder, k: int = 5):\n        self.encoder = encoder\n        self.k = k\n        self.embeddings = []\n        self.labels = []\n\n    def fit(self, texts: list, labels: list):\n        \"\"\"Embed texts and store with their labels.\"\"\"\n        self.embeddings = [self.encoder.encode(text) for text in texts]\n        self.labels = labels\n\n    def predict(self, text: str) -> str:\n        \"\"\"Predict label using k-NN.\"\"\"\n        query_emb = self.encoder.encode(text)\n\n        # Cosine similarity: (A · B) / (||A|| × ||B||)\n        distances = []\n        for i, emb in enumerate(self.embeddings):\n            dist = np.dot(query_emb, emb) / (np.linalg.norm(query_emb) * np.linalg.norm(emb))\n            distances.append((dist, self.labels[i]))\n\n        # Get k nearest neighbors\n        distances.sort(reverse=True)\n        k_nearest = [label for _, label in distances[: self.k]]\n\n        # Return most common label\n        return Counter(k_nearest).most_common(1)[0][0]\n\n\n# Example: Sentiment classification\nfrom sentence_transformers import SentenceTransformer\nencoder = SentenceTransformer('all-MiniLM-L6-v2')\n\nclassifier = EmbeddingClassifier(encoder, k=3)\nclassifier.fit(\n    texts=[\"Great product!\", \"Loved it\", \"Terrible\", \"Waste of money\", \"Amazing quality\"],\n    labels=[\"positive\", \"positive\", \"negative\", \"negative\", \"positive\"],\n)\nprint(f\"Prediction: {classifier.predict('This is wonderful!')}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPrediction: positive\n```\n:::\n:::\n\n\n**Clustering with embeddings:**\n\nClustering discovers natural groups in your data without predefined labels. Since similar texts have similar embeddings, texts on the same topic will cluster together in embedding space.\n\nK-means is a popular clustering algorithm. You specify k (the number of clusters), and the algorithm finds k groups by positioning a *centroid* (center point) for each cluster. Each text belongs to the cluster whose centroid is closest to its embedding.\n\nThe algorithm works as follows: first, embed all texts. Then pick k random embeddings as initial centroids—the algorithm needs starting points before it can begin refining. Next, iterate until convergence: (1) assign each embedding to its nearest centroid (measured by Euclidean distance), and (2) update each centroid to be the mean of its assigned embeddings. The algorithm converges when assignments stop changing.\n\n::: {#cell-fig-kmeans-clustering .cell execution_count=4}\n\n::: {.cell-output .cell-output-display}\n![K-means clustering in 2D embedding space. Points are colored by cluster assignment, with centroids marked as X.](ch02_foundational_embedding_types_files/figure-html/fig-kmeans-clustering-output-1.png){#fig-kmeans-clustering width=566 height=470}\n:::\n:::\n\n\n::: {#f602cb3c .cell execution_count=5}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show Text Clustering\"}\nimport numpy as np\nfrom typing import List, Dict\n\n\nclass EmbeddingClusterer:\n    \"\"\"K-means clustering on text embeddings.\"\"\"\n\n    def __init__(self, encoder, n_clusters: int = 3):\n        self.encoder = encoder\n        self.n_clusters = n_clusters\n        self.centroids = None\n\n    def fit(self, texts: List[str], max_iters: int = 100):\n        \"\"\"Cluster texts and return assignments.\"\"\"\n        embeddings = np.array([self.encoder.encode(text) for text in texts])\n\n        # Initialize centroids by picking k random embeddings as starting points\n        indices = np.random.choice(len(embeddings), self.n_clusters, replace=False)\n        self.centroids = embeddings[indices].copy()\n\n        for _ in range(max_iters):\n            # Assign points to nearest centroid\n            assignments = []\n            for emb in embeddings:\n                distances = [np.linalg.norm(emb - c) for c in self.centroids]\n                assignments.append(np.argmin(distances))\n\n            # Update centroids\n            new_centroids = []\n            for i in range(self.n_clusters):\n                cluster_points = embeddings[np.array(assignments) == i]\n                if len(cluster_points) > 0:\n                    new_centroids.append(cluster_points.mean(axis=0))\n                else:\n                    new_centroids.append(self.centroids[i])\n\n            self.centroids = np.array(new_centroids)\n\n        return assignments\n\n    def get_cluster_examples(self, texts: List[str], assignments: List[int]) -> Dict[int, List[str]]:\n        \"\"\"Group texts by cluster.\"\"\"\n        clusters = {i: [] for i in range(self.n_clusters)}\n        for text, cluster_id in zip(texts, assignments):\n            clusters[cluster_id].append(text)\n        return clusters\n\n\n# Example: Topic discovery\ntexts = [\n    # Cooking\n    \"Chop the onions and garlic finely\",\n    \"Simmer the sauce for twenty minutes\",\n    \"Season with salt and pepper to taste\",\n    \"Preheat the oven to 350 degrees\",\n    # Space\n    \"The telescope discovered a new exoplanet\",\n    \"Astronauts completed their spacewalk today\",\n    \"The Mars rover collected soil samples\",\n    \"A new comet is visible this month\",\n    # Weather\n    \"Heavy rain expected throughout the weekend\",\n    \"Temperatures will drop below freezing tonight\",\n    \"A warm front is moving in from the south\",\n    \"Clear skies and sunshine forecast for Monday\",\n]\n\nnp.random.seed(42)  # For reproducible results\nclusterer = EmbeddingClusterer(encoder, n_clusters=3)\nassignments = clusterer.fit(texts)\nclusters = clusterer.get_cluster_examples(texts, assignments)\nfor cluster_id, examples in clusters.items():\n    print(f\"\\nCluster {cluster_id}:\")\n    for text in examples:\n        print(f\"  - {text}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCluster 0:\n  - The Mars rover collected soil samples\n  - A new comet is visible this month\n  - A warm front is moving in from the south\n\nCluster 1:\n  - Preheat the oven to 350 degrees\n  - Astronauts completed their spacewalk today\n  - Heavy rain expected throughout the weekend\n  - Temperatures will drop below freezing tonight\n  - Clear skies and sunshine forecast for Monday\n\nCluster 2:\n  - Chop the onions and garlic finely\n  - Simmer the sauce for twenty minutes\n  - Season with salt and pepper to taste\n  - The telescope discovered a new exoplanet\n```\n:::\n:::\n\n\nNotice that some items may appear in unexpected clusters. Embeddings capture semantic similarity that doesn't always match our intuitive topic categories—\"Preheat the oven to 350 degrees\" mentions temperature, which may pull it toward weather texts, while \"A warm front is moving in\" uses directional language similar to space descriptions. This is a feature, not a bug: embeddings capture meaning patterns that humans might overlook.\n\n**Sentiment analysis:**\n\nSentiment analysis determines whether text expresses positive, negative, or neutral opinions. While you could treat this as classification (train on labeled examples), an elegant alternative uses *anchor texts*—words or phrases with known sentiment.\n\nThe approach works as follows: embed a set of clearly positive words (\"excellent\", \"amazing\", \"love it\") and compute their centroid. Do the same for negative words. To analyze new text, embed it and measure which centroid it's closer to. The difference in distances gives both a label and a confidence score—text much closer to the positive centroid is strongly positive.\n\n::: {#cell-fig-sentiment-anchors .cell execution_count=6}\n\n::: {.cell-output .cell-output-display}\n![Sentiment analysis using anchor texts. Positive and negative anchor words form centroids. New text is classified by which centroid it's closer to.](ch02_foundational_embedding_types_files/figure-html/fig-sentiment-anchors-output-1.png){#fig-sentiment-anchors width=758 height=470}\n:::\n:::\n\n\n::: {#47f584e5 .cell execution_count=7}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show Sentiment Analyzer\"}\nimport numpy as np\nfrom typing import Tuple\n\n\nclass SentimentAnalyzer:\n    \"\"\"Embedding-based sentiment analysis using anchor texts.\"\"\"\n\n    def __init__(self, encoder):\n        self.encoder = encoder\n        # Anchor texts define the sentiment space\n        positive_anchors = [\"excellent\", \"amazing\", \"wonderful\", \"fantastic\", \"love it\"]\n        negative_anchors = [\"terrible\", \"awful\", \"horrible\", \"hate it\", \"worst ever\"]\n\n        # Compute anchor centroids: average the embeddings of all anchor words\n        # to find the \"center\" of positive/negative regions in embedding space.\n        # Using multiple anchors makes the centroid more robust than any single word.\n        self.positive_centroid = np.mean(\n            [encoder.encode(t) for t in positive_anchors], axis=0\n        )\n        self.negative_centroid = np.mean(\n            [encoder.encode(t) for t in negative_anchors], axis=0\n        )\n\n    def analyze(self, text: str) -> Tuple[str, float]:\n        \"\"\"\n        Return sentiment label and confidence score.\n        Score ranges from -1 (negative) to +1 (positive).\n        \"\"\"\n        emb = self.encoder.encode(text)\n\n        # Cosine similarity: (A · B) / (||A|| × ||B||)\n        pos_sim = np.dot(emb, self.positive_centroid) / (\n            np.linalg.norm(emb) * np.linalg.norm(self.positive_centroid)\n        )\n        neg_sim = np.dot(emb, self.negative_centroid) / (\n            np.linalg.norm(emb) * np.linalg.norm(self.negative_centroid)\n        )\n\n        # Score: positive if closer to positive centroid\n        score = pos_sim - neg_sim\n        label = \"positive\" if score > 0 else \"negative\"\n        confidence = abs(score)\n\n        return label, confidence\n\n\n# Example usage\nnp.random.seed(42)\nanalyzer = SentimentAnalyzer(encoder)\nfor text in [\"This product exceeded expectations!\", \"Complete waste of money\"]:\n    label, conf = analyzer.analyze(text)\n    print(f\"'{text[:30]}...' -> {label} ({conf:.2f})\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n'This product exceeded expectat...' -> positive (0.02)\n'Complete waste of money...' -> negative (0.07)\n```\n:::\n:::\n\n\nThe number in parentheses is the confidence score—the difference between cosine similarities to the positive and negative centroids. These values appear low because general-purpose embeddings capture broad semantics, not just sentiment. The key insight is that the *relative* scores still correctly distinguish positive from negative text, even when absolute differences are small. For production sentiment analysis, you'd typically fine-tune embeddings on sentiment-labeled data (see @sec-custom-embedding-strategies).\n\n:::{.callout-tip}\n## Classification and Clustering Best Practices\n\n**For classification:**\n\n- **Few-shot is often enough**: With good embeddings, 10-50 examples per class often suffices (see @sec-siamese-networks for few-shot techniques)\n- **k-NN for simplicity**: No training required, just store examples\n- **Logistic regression for speed**: Train a simple linear classifier on embeddings\n- **Fine-tune for best quality**: When you have thousands of examples, fine-tune the embedding model itself (see @sec-custom-embedding-strategies)\n\n**For clustering:**\n\n- **Choose k carefully**: Use elbow method or silhouette scores to find optimal cluster count\n- **[HDBSCAN](https://hdbscan.readthedocs.io/) for unknown k**: Unlike k-means, HDBSCAN doesn't require specifying cluster count upfront—it discovers clusters based on density and labels sparse points as outliers rather than forcing them into clusters\n- **Reduce dimensions first**: For visualization, use UMAP or t-SNE on embeddings\n- **Label clusters post-hoc**: Examine cluster members to assign meaningful names\n\n**For sentiment:**\n\n- **Domain matters**: Financial sentiment differs from product reviews—use domain-specific anchors (see @sec-financial-services for financial sentiment)\n- **Beyond binary**: Instead of just positive/negative centroids, create centroids for multiple emotions (joy, anger, sadness, fear, surprise). Measure distance to each and return the closest emotion, or return a distribution across all emotions for nuanced analysis.\n- **Aspect-based**: Reviews often mix sentiment across topics (\"great battery, terrible screen\"). First extract aspects (product features, service elements), then run sentiment analysis on each aspect separately to understand what users love and hate.\n:::\n\n## Image Embeddings {#sec-image-embedding-types}\n\nImage embeddings convert visual content into vectors that capture visual semantics—shapes, colors, textures, objects, and spatial relationships. Unlike pixel-by-pixel comparison, embeddings understand that two photos of the same cat are similar even if taken from different angles or lighting conditions.\n\nThe example below uses ResNet50 [@he2016resnet], a CNN pre-trained on ImageNet's 1.4 million images. ResNet learns hierarchical visual features—early layers detect edges and textures, middle layers recognize shapes and parts, and deep layers understand objects and scenes. We remove the final classification layer to extract the 2048-dimensional feature vector as our embedding.\n\n::: {#f260b0df .cell execution_count=8}\n``` {.python .cell-code code-fold=\"false\"}\n\"\"\"\nImage Embeddings: Visual Content as Vectors\n\"\"\"\n\nimport torch\nimport numpy as np\nfrom PIL import Image\nfrom torchvision import models, transforms\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Suppress download messages\nimport logging\nlogging.getLogger('torch').setLevel(logging.ERROR)\n\nfrom torchvision.models import ResNet50_Weights\n\n# Load pretrained ResNet50 as feature extractor\nweights = ResNet50_Weights.IMAGENET1K_V1\nmodel = models.resnet50(weights=None)\nmodel.load_state_dict(weights.get_state_dict(progress=False))\nmodel.eval()\n\n# Remove classification head to get embeddings\nfeature_extractor = torch.nn.Sequential(*list(model.children())[:-1])\n\n# Standard ImageNet preprocessing\npreprocess = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\ndef get_image_embedding(image):\n    \"\"\"Extract embedding from PIL Image.\"\"\"\n    tensor = preprocess(image).unsqueeze(0)\n    with torch.no_grad():\n        embedding = feature_extractor(tensor)\n    return embedding.squeeze().numpy()\n\n# Create synthetic test images with different color patterns\nnp.random.seed(42)\nimages = {\n    'red_pattern': Image.fromarray(\n        np.random.randint([180, 0, 0], [255, 80, 80], (224, 224, 3), dtype=np.uint8)\n    ),\n    'blue_pattern': Image.fromarray(\n        np.random.randint([0, 0, 180], [80, 80, 255], (224, 224, 3), dtype=np.uint8)\n    ),\n    'orange_pattern': Image.fromarray(\n        np.random.randint([200, 100, 0], [255, 150, 50], (224, 224, 3), dtype=np.uint8)\n    ),\n}\n\n# Get embeddings\nembeddings = {name: get_image_embedding(img) for name, img in images.items()}\n\nprint(\"Image embedding similarities:\\n\")\nprint(\"Red and orange (similar warm colors) should be more similar than red and blue:\")\nred_orange = cosine_similarity([embeddings['red_pattern']], [embeddings['orange_pattern']])[0][0]\nred_blue = cosine_similarity([embeddings['red_pattern']], [embeddings['blue_pattern']])[0][0]\nprint(f\"  red ↔ orange: {red_orange:.3f}\")\nprint(f\"  red ↔ blue:   {red_blue:.3f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nImage embedding similarities:\n\nRed and orange (similar warm colors) should be more similar than red and blue:\n  red ↔ orange: 0.898\n  red ↔ blue:   0.717\n```\n:::\n:::\n\n\nWhen comparing image embeddings, we use cosine similarity just like with text. Images with similar visual features—colors, textures, shapes, or objects—will have embeddings that point in similar directions, yielding high similarity scores. The red and orange patterns share warm color features, so their embeddings are closer together than red and blue. In practice, this means a photo of a red dress will be more similar to an orange dress than a blue one, even though all three are \"dresses.\"\n\nHow does the model \"understand\" colors? Images are input as RGB values (three numbers per pixel). Early CNN layers learn filters that activate for specific color combinations—some filters respond to warm tones (high red, medium green, low blue), others to cool tones. As layers get deeper, the network combines color with texture and shape information. By training on millions of labeled images, the model learns that red and orange often appear together (sunsets, autumn leaves, fire) more frequently than red and blue, encoding this statistical relationship in the embedding space.\n\n::: {#cell-fig-image-embedding-space .cell execution_count=9}\n\n::: {.cell-output .cell-output-display}\n![Image embeddings in 2D space. Similar colors cluster together—red and orange are close, while blue is distant.](ch02_foundational_embedding_types_files/figure-html/fig-image-embedding-space-output-1.png){#fig-image-embedding-space width=662 height=470}\n:::\n:::\n\n\nBeyond colors, early CNN layers also learn edge detectors—filters that respond to boundaries between light and dark regions. For a hands-on introduction to how a single neuron learns to detect edges, see [How Neural Networks Learn to See](https://snowch.github.io/nn_edge_detector_blog.html).\n\n**When to use image embeddings:**\n\n- Visual recommendation systems—suggest visually similar items (see @sec-recommendation-systems)\n- Content moderation—detect variations of prohibited images (see @sec-content-moderation)\n- Forensic video search—find specific people or objects in footage (see @sec-video-surveillance) *(reverse image lookup)*\n- Duplicate and near-duplicate detection—identify copied or modified images (see @sec-entity-resolution) *(reverse image lookup)*\n- Medical imaging—find similar X-rays, scans, or pathology slides (see @sec-healthcare-life-sciences)\n- Visual product search—find products similar to a photo (see @sec-retail-ecommerce) *(reverse image lookup)*\n- Quality control—detect defects by comparing to reference images (see @sec-manufacturing-industry40)\n- Face recognition—identify or verify individuals from photos (see @sec-video-surveillance)\n\nThere are dozens of other applications including art style matching, stock photo search, image classification, trademark and logo detection, scene recognition, wildlife identification, satellite imagery analysis, document scanning, and autonomous vehicle perception.\n\n**Popular architectures:**\n\n| Architecture | Type | Strengths | Use Cases |\n|-------------|------|-----------|-----------|\n| [ResNet](https://pytorch.org/vision/main/models/resnet.html) | CNN | Fast, proven | General visual search |\n| [EfficientNet](https://pytorch.org/vision/main/models/efficientnet.html) | CNN | Efficient, accurate | Mobile/edge deployment |\n| [ViT](https://pytorch.org/vision/main/models/vision_transformer.html) | Transformer | Best accuracy | High-quality requirements |\n| [CLIP](https://github.com/openai/CLIP) | Multi-modal | Text-image alignment | Zero-shot classification |\n\n: Image embedding architectures {.striped}\n\nSee @sec-image-embedding-models for details on how these architectures work.\n\n## Audio Embeddings {#sec-audio-embedding-types}\n\nAudio embeddings capture acoustic features from speech, music, and environmental sounds. Let's create embeddings from real audio using MFCC (Mel-frequency cepstral coefficients) features:\n\n::: {#0507d388 .cell execution_count=10}\n``` {.python .cell-code code-fold=\"false\"}\n\"\"\"\nAudio Embeddings: Real Audio to Vectors\n\nWe'll use librosa to extract MFCC features from actual audio,\ncreating embeddings that capture timbral characteristics.\n\"\"\"\n\nimport numpy as np\nimport librosa\nfrom sklearn.metrics.pairwise import cosine_similarity\n\ndef audio_to_embedding(audio, sr, n_mfcc=20):\n    \"\"\"Convert audio waveform to a fixed-size embedding using MFCCs.\"\"\"\n    mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=n_mfcc)\n    # Aggregate over time: mean and std of each coefficient\n    return np.concatenate([mfccs.mean(axis=1), mfccs.std(axis=1)])\n\n# Load librosa's built-in trumpet sample\naudio, sr = librosa.load(librosa.ex('trumpet'))\ntrumpet_embedding = audio_to_embedding(audio, sr)\n\n# Create variations to demonstrate similarity\ntrumpet_slow = librosa.effects.time_stretch(audio, rate=0.8)\ntrumpet_pitch_up = librosa.effects.pitch_shift(audio, sr=sr, n_steps=4)\n\n# Generate embeddings for each variation\nembeddings = {\n    'trumpet_original': trumpet_embedding,\n    'trumpet_slower': audio_to_embedding(trumpet_slow, sr),\n    'trumpet_higher_pitch': audio_to_embedding(trumpet_pitch_up, sr),\n}\n\nprint(f\"Embedding dimension: {len(trumpet_embedding)} (20 MFCCs × 2 stats)\\n\")\n\n# Compare similarities\nprint(\"Audio embedding similarities:\")\nsim_slow = cosine_similarity(\n    [embeddings['trumpet_original']], [embeddings['trumpet_slower']]\n)[0][0]\nprint(f\"  Original ↔ Slower tempo:   {sim_slow:.3f}\")\n\nsim_pitch = cosine_similarity(\n    [embeddings['trumpet_original']], [embeddings['trumpet_higher_pitch']]\n)[0][0]\nprint(f\"  Original ↔ Higher pitch:   {sim_pitch:.3f}\")\n\nsim_variations = cosine_similarity(\n    [embeddings['trumpet_slower']], [embeddings['trumpet_higher_pitch']]\n)[0][0]\nprint(f\"  Slower ↔ Higher pitch:     {sim_variations:.3f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nEmbedding dimension: 40 (20 MFCCs × 2 stats)\n\nAudio embedding similarities:\n  Original ↔ Slower tempo:   1.000\n  Original ↔ Higher pitch:   0.996\n  Slower ↔ Higher pitch:     0.996\n```\n:::\n:::\n\n\nThe tempo change preserves timbre (high similarity), while pitch shifting alters the spectral characteristics more significantly.\n\n**Popular audio embedding models:**\n\n- **Wav2Vec2**: Self-supervised speech representations\n- **Whisper**: Multi-task speech model (transcription + embeddings)\n- **CLAP**: Audio-text alignment (like CLIP for audio)\n- **VGGish**: Audio event classification embeddings\n\n**When to use audio embeddings:**\n\n- Voice search and speaker identification\n- Music recommendation and similarity\n- Sound event detection and classification\n- Audio fingerprinting\n- Podcast/video content search\n\n## Video Embeddings {#sec-video-embedding-types}\n\nVideo embeddings must capture both spatial (visual) and temporal (motion) information:\n\n::: {#b9f3ef07 .cell execution_count=11}\n``` {.python .cell-code code-fold=\"false\"}\n\"\"\"\nVideo Embeddings: Motion and Time\n\nVideo embeddings extend image embeddings to capture temporal dynamics:\nactions, transitions, and narrative flow.\n\nApproaches:\n1. Frame-level: Embed individual frames, aggregate (mean, max, attention)\n2. Clip-level: Models like X3D, SlowFast process multiple frames together\n3. Two-stream: Separate spatial (RGB) and temporal (optical flow) processing\n\"\"\"\n\nimport numpy as np\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Simulated video embeddings capturing action types\nnp.random.seed(42)\n\nvideo_embeddings = {\n    'running_outdoor': np.random.randn(512) + np.array([1, 0, 0] + [0]*509),\n    'jogging_park': np.random.randn(512) + np.array([0.9, 0.1, 0] + [0]*509),\n    'cooking_kitchen': np.random.randn(512) + np.array([0, 1, 0] + [0]*509),\n    'baking_cake': np.random.randn(512) + np.array([0.1, 0.9, 0.2] + [0]*509),\n    'cat_playing': np.random.randn(512) + np.array([0, 0, 1] + [0]*509),\n}\n\nprint(\"Simulated video embedding similarities:\\n\")\nprint(\"Similar actions cluster together:\")\nrun_jog = cosine_similarity(\n    [video_embeddings['running_outdoor']],\n    [video_embeddings['jogging_park']]\n)[0][0]\ncook_bake = cosine_similarity(\n    [video_embeddings['cooking_kitchen']],\n    [video_embeddings['baking_cake']]\n)[0][0]\nrun_cook = cosine_similarity(\n    [video_embeddings['running_outdoor']],\n    [video_embeddings['cooking_kitchen']]\n)[0][0]\n\nprint(f\"  Running ↔ Jogging: {run_jog:.3f}\")\nprint(f\"  Cooking ↔ Baking:  {cook_bake:.3f}\")\nprint(f\"  Running ↔ Cooking: {run_cook:.3f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSimulated video embedding similarities:\n\nSimilar actions cluster together:\n  Running ↔ Jogging: -0.008\n  Cooking ↔ Baking:  -0.091\n  Running ↔ Cooking: 0.074\n```\n:::\n:::\n\n\n**Video embedding strategies:**\n\n| Approach | Description | Pros | Cons |\n|----------|-------------|------|------|\n| Frame sampling | Embed N frames, average | Simple, fast | Misses motion |\n| 3D CNNs | Process frame volumes | Captures motion | Computationally heavy |\n| Two-stream | RGB + optical flow | Best accuracy | Complex pipeline |\n| Video transformers | Attention over frames | State-of-the-art | Very expensive |\n\n: Video embedding approaches {.striped}\n\n**When to use video embeddings:**\n\n- Action recognition and search\n- Video recommendation\n- Content moderation\n- Surveillance and anomaly detection\n- Video summarization\n\n## Multi-Modal Embeddings {#sec-multimodal-embedding-types}\n\nMulti-modal embeddings align different data types in a shared space:\n\n::: {#4cb62dde .cell execution_count=12}\n``` {.python .cell-code code-fold=\"false\"}\n\"\"\"\nMulti-Modal Embeddings: Bridging Modalities\n\nMulti-modal models like CLIP learn a shared embedding space where\ntext and images can be directly compared. This enables:\n- Text-to-image search (\"find images of cats\")\n- Image-to-text search (find descriptions for an image)\n- Zero-shot classification (classify without training examples)\n\"\"\"\n\nimport numpy as np\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Simulated CLIP-style embeddings where text and images share a space\nnp.random.seed(42)\n\n# In a real CLIP model, these would be 512-768 dimensional\n# and text/image of the same concept would have similar vectors\nembeddings = {\n    # Text embeddings\n    'text: a photo of a cat': np.array([0.8, 0.1, 0.2, 0.1]),\n    'text: a picture of a dog': np.array([0.7, 0.3, 0.2, 0.1]),\n    'text: a red sports car': np.array([0.1, 0.1, 0.9, 0.3]),\n    # Image embeddings (would come from image encoder)\n    'image: cat_photo.jpg': np.array([0.75, 0.15, 0.25, 0.1]),\n    'image: dog_photo.jpg': np.array([0.65, 0.35, 0.15, 0.15]),\n    'image: car_photo.jpg': np.array([0.15, 0.1, 0.85, 0.35]),\n}\n\nprint(\"Multi-modal embedding alignment:\\n\")\nprint(\"Text queries matched to images:\")\n\ntext_cat = embeddings['text: a photo of a cat']\nfor name, emb in embeddings.items():\n    if name.startswith('image:'):\n        sim = cosine_similarity([text_cat], [emb])[0][0]\n        print(f\"  'a photo of a cat' ↔ {name}: {sim:.3f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMulti-modal embedding alignment:\n\nText queries matched to images:\n  'a photo of a cat' ↔ image: cat_photo.jpg: 0.995\n  'a photo of a cat' ↔ image: dog_photo.jpg: 0.934\n  'a photo of a cat' ↔ image: car_photo.jpg: 0.427\n```\n:::\n:::\n\n\n**Multi-modal embedding models:**\n\n- **CLIP** (OpenAI): Text-image alignment, 400M image-text pairs\n- **BLIP/BLIP-2**: Image captioning + retrieval\n- **ImageBind** (Meta): Aligns 6 modalities (image, text, audio, depth, thermal, IMU)\n- **LLaVA**: Large language model with vision\n\n**When to use multi-modal embeddings:**\n\n- Cross-modal search (text→image, image→text)\n- Zero-shot image classification\n- Image captioning\n- Visual question answering\n- Product search with text and images\n\n## Graph Embeddings {#sec-graph-embedding-types}\n\nGraph embeddings represent nodes, edges, and subgraphs in vector space:\n\n::: {#2d3c08c1 .cell execution_count=13}\n``` {.python .cell-code code-fold=\"false\"}\n\"\"\"\nGraph Embeddings: Relationships as Vectors\n\nGraph embeddings capture structural relationships between entities.\nNodes that are connected or share neighbors get similar embeddings.\n\nApplications: Knowledge graphs, social networks, molecule structures\n\"\"\"\n\nimport numpy as np\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Simulated node embeddings from a social network\n# In practice, use models like Node2Vec, GraphSAGE, or GNN-based approaches\nnp.random.seed(42)\n\n# Imagine a graph where:\n# - Alice, Bob, Carol are in the same friend group\n# - Xavier, Yuki, Zara are in a different friend group\n# - Nodes in the same community should have similar embeddings\n\nnode_embeddings = {\n    'Alice': np.random.randn(64) + np.array([1, 0] + [0]*62),\n    'Bob': np.random.randn(64) + np.array([0.9, 0.1] + [0]*62),\n    'Carol': np.random.randn(64) + np.array([0.8, 0.2] + [0]*62),\n    'Xavier': np.random.randn(64) + np.array([0, 1] + [0]*62),\n    'Yuki': np.random.randn(64) + np.array([0.1, 0.9] + [0]*62),\n    'Zara': np.random.randn(64) + np.array([0.2, 0.8] + [0]*62),\n}\n\nprint(\"Graph embedding similarities (same community = higher):\\n\")\nprint(\"Within community 1:\")\nab = cosine_similarity([node_embeddings['Alice']], [node_embeddings['Bob']])[0][0]\nac = cosine_similarity([node_embeddings['Alice']], [node_embeddings['Carol']])[0][0]\nprint(f\"  Alice ↔ Bob:   {ab:.3f}\")\nprint(f\"  Alice ↔ Carol: {ac:.3f}\")\n\nprint(\"\\nAcross communities:\")\nax = cosine_similarity([node_embeddings['Alice']], [node_embeddings['Xavier']])[0][0]\nprint(f\"  Alice ↔ Xavier: {ax:.3f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nGraph embedding similarities (same community = higher):\n\nWithin community 1:\n  Alice ↔ Bob:   0.056\n  Alice ↔ Carol: 0.000\n\nAcross communities:\n  Alice ↔ Xavier: -0.035\n```\n:::\n:::\n\n\n**Graph embedding methods:**\n\n| Method | Approach | Best For |\n|--------|----------|----------|\n| Node2Vec | Random walks + Word2Vec | Homogeneous graphs |\n| GraphSAGE | Neighborhood aggregation | Inductive learning |\n| GAT | Attention over neighbors | Weighted importance |\n| Knowledge Graph Embeddings | TransE, RotatE, etc. | Link prediction |\n\n: Graph embedding methods {.striped}\n\n**When to use graph embeddings:**\n\n- Social network analysis\n- Recommendation systems (user-item graphs)\n- Knowledge graph completion\n- Fraud detection (transaction graphs)\n- Drug discovery (molecular graphs)\n\n## Time-Series Embeddings {#sec-timeseries-embedding-types}\n\nTime-series embeddings capture temporal patterns and dynamics:\n\n::: {#8b209598 .cell execution_count=14}\n``` {.python .cell-code code-fold=\"false\"}\n\"\"\"\nTime-Series Embeddings: Patterns Over Time\n\nTime-series embeddings capture temporal dynamics, trends, and patterns.\nSimilar time-series (e.g., similar sensor readings) cluster together.\n\"\"\"\n\nimport numpy as np\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nnp.random.seed(42)\n\n# Simulate different time-series patterns\ndef generate_pattern(pattern_type, length=100):\n    t = np.linspace(0, 4*np.pi, length)\n    if pattern_type == 'sine':\n        return np.sin(t) + np.random.randn(length) * 0.1\n    elif pattern_type == 'cosine':\n        return np.cos(t) + np.random.randn(length) * 0.1\n    elif pattern_type == 'increasing':\n        return t/10 + np.random.randn(length) * 0.2\n    elif pattern_type == 'decreasing':\n        return -t/10 + 5 + np.random.randn(length) * 0.2\n    else:\n        return np.random.randn(length)\n\n# Simple embedding: statistical features (real systems use learned embeddings)\ndef simple_ts_embedding(series):\n    return np.array([\n        np.mean(series),\n        np.std(series),\n        np.max(series) - np.min(series),\n        np.mean(np.diff(series)),  # trend\n        np.corrcoef(series[:-1], series[1:])[0,1],  # autocorrelation\n    ])\n\n# Generate and embed time series\npatterns = {\n    'sine_wave_1': generate_pattern('sine'),\n    'sine_wave_2': generate_pattern('sine'),\n    'trend_up_1': generate_pattern('increasing'),\n    'trend_up_2': generate_pattern('increasing'),\n    'random_1': generate_pattern('random'),\n}\n\nembeddings = {name: simple_ts_embedding(ts) for name, ts in patterns.items()}\n\nprint(\"Time-series embedding similarities:\\n\")\nprint(\"Similar patterns cluster together:\")\nsine_sim = cosine_similarity([embeddings['sine_wave_1']], [embeddings['sine_wave_2']])[0][0]\ntrend_sim = cosine_similarity([embeddings['trend_up_1']], [embeddings['trend_up_2']])[0][0]\ncross_sim = cosine_similarity([embeddings['sine_wave_1']], [embeddings['trend_up_1']])[0][0]\n\nprint(f\"  Sine wave 1 ↔ Sine wave 2:  {sine_sim:.3f}\")\nprint(f\"  Trend up 1 ↔ Trend up 2:    {trend_sim:.3f}\")\nprint(f\"  Sine wave ↔ Trend up:       {cross_sim:.3f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTime-series embedding similarities:\n\nSimilar patterns cluster together:\n  Sine wave 1 ↔ Sine wave 2:  1.000\n  Trend up 1 ↔ Trend up 2:    0.998\n  Sine wave ↔ Trend up:       0.943\n```\n:::\n:::\n\n\n**Time-series embedding approaches:**\n\n- **Learned features**: LSTMs, Transformers, TCNs trained on time-series\n- **Self-supervised**: Contrastive learning on augmented time-series\n- **Statistical**: Hand-crafted features (mean, variance, entropy, etc.)\n- **Frequency domain**: FFT-based representations\n\n**When to use time-series embeddings:**\n\n- Anomaly detection in sensor data\n- Predictive maintenance\n- Financial pattern recognition\n- Health monitoring (ECG, EEG)\n- IoT device fingerprinting\n\n## Code Embeddings {#sec-code-embedding-types}\n\nCode embeddings represent programs, functions, and code snippets:\n\n::: {#507de3a0 .cell execution_count=15}\n``` {.python .cell-code code-fold=\"false\"}\n\"\"\"\nCode Embeddings: Programs as Vectors\n\nCode embeddings capture the semantics of source code, enabling:\n- Code search (find code by description)\n- Clone detection (find duplicate/similar code)\n- Bug detection (identify anomalous patterns)\n\"\"\"\n\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Use a code-aware model (or general sentence transformer for demo)\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\n\n# Code snippets with similar functionality but different implementations\ncode_snippets = {\n    'sum_loop': '''\ndef sum_numbers(nums):\n    total = 0\n    for n in nums:\n        total += n\n    return total\n''',\n    'sum_builtin': '''\ndef sum_numbers(numbers):\n    return sum(numbers)\n''',\n    'reverse_loop': '''\ndef reverse_list(lst):\n    result = []\n    for i in range(len(lst)-1, -1, -1):\n        result.append(lst[i])\n    return result\n''',\n    'reverse_slice': '''\ndef reverse_list(items):\n    return items[::-1]\n''',\n}\n\nembeddings = {name: model.encode(code) for name, code in code_snippets.items()}\n\nprint(\"Code embedding similarities:\\n\")\nprint(\"Functionally similar code clusters together:\")\nsum_sim = cosine_similarity([embeddings['sum_loop']], [embeddings['sum_builtin']])[0][0]\nrev_sim = cosine_similarity([embeddings['reverse_loop']], [embeddings['reverse_slice']])[0][0]\ncross_sim = cosine_similarity([embeddings['sum_loop']], [embeddings['reverse_loop']])[0][0]\n\nprint(f\"  sum (loop) ↔ sum (builtin):     {sum_sim:.3f}\")\nprint(f\"  reverse (loop) ↔ reverse (slice): {rev_sim:.3f}\")\nprint(f\"  sum ↔ reverse:                   {cross_sim:.3f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCode embedding similarities:\n\nFunctionally similar code clusters together:\n  sum (loop) ↔ sum (builtin):     0.809\n  reverse (loop) ↔ reverse (slice): 0.822\n  sum ↔ reverse:                   0.272\n```\n:::\n:::\n\n\n**Code embedding models:**\n\n- **CodeBERT**: BERT-style model trained on code\n- **GraphCodeBERT**: Incorporates data flow graphs\n- **StarCoder**: Large code model with embeddings\n- **Codex/GPT-4**: Can generate embeddings via API\n\n**When to use code embeddings:**\n\n- Semantic code search\n- Code clone detection\n- Vulnerability detection\n- Code recommendation\n- Repository organization\n\n## Choosing the Right Embedding Type\n\nHere's a decision framework for selecting embedding types:\n\n```\n                    What is your primary data type?\n                              │\n        ┌──────────┬──────────┼──────────┬──────────┐\n        │          │          │          │          │\n      Text      Images     Audio/    Multiple   Structured/\n        │          │       Video     Modalities   Relational\n        │          │          │          │          │\n        ▼          ▼          ▼          ▼          ▼\n   Sentence    CNN/ViT    Domain-    CLIP/      Graph\n  Transformers  CLIP     specific  ImageBind  Embeddings\n```\n\n| Scenario | Recommended Approach |\n|----------|---------------------|\n| Search documents by meaning | Text embeddings (sentence transformers) |\n| Find visually similar images | Image embeddings (ResNet, CLIP) |\n| Match images to text queries | Multi-modal (CLIP) |\n| Find similar entities in a network | Graph embeddings (GraphSAGE) |\n| Detect anomalies in sensor data | Time-series embeddings |\n| Search code by functionality | Code embeddings (CodeBERT) |\n| Match voice to speaker | Audio embeddings (speaker verification) |\n\n: Embedding type selection guide {.striped}\n\n## Key Takeaways\n\n- **Different data types require different embedding architectures** that match the structure of the data\n\n- **Text embeddings** are the most mature, with sentence transformers providing excellent quality for most applications\n\n- **Image embeddings** use CNNs or Vision Transformers to capture visual semantics\n\n- **Multi-modal embeddings** like CLIP enable cross-modal search and zero-shot classification\n\n- **Graph embeddings** capture relational structure, essential for social networks and knowledge graphs\n\n- **Time-series embeddings** encode temporal patterns for anomaly detection and similarity search\n\n- **Code embeddings** understand program semantics, enabling semantic code search\n\n## Looking Ahead\n\nNow that you understand the foundational embedding types, @sec-advanced-embedding-patterns covers advanced patterns used in production systems—hybrid vectors that combine multiple feature types, multi-vector representations, learned sparse embeddings, and more. For deep dives into how specific models work, @sec-embedding-model-fundamentals explains the underlying architectures. Then @sec-strategic-architecture covers how to architect embedding systems that can handle multiple modalities at scale.\n\n## Further Reading\n\n- Reimers, N. & Gurevych, I. (2019). \"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks.\" *arXiv:1908.10084*\n- Radford, A., et al. (2021). \"Learning Transferable Visual Models From Natural Language Supervision.\" *arXiv:2103.00020* (CLIP)\n- Grover, A. & Leskovec, J. (2016). \"node2vec: Scalable Feature Learning for Networks.\" *KDD*\n- Baevski, A., et al. (2020). \"wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations.\" *NeurIPS*\n- Feng, Z., et al. (2020). \"CodeBERT: A Pre-Trained Model for Programming and Natural Languages.\" *EMNLP*\n\n",
    "supporting": [
      "ch02_foundational_embedding_types_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}