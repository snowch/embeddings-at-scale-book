{
  "hash": "d736d6ef9b61b096d82d397592f5f6f3",
  "result": {
    "engine": "jupyter",
    "markdown": "# Contrastive Learning for Enterprise Embeddings {#sec-contrastive-learning}\n\n:::{.callout-note}\n## Chapter Overview\nContrastive learning has emerged as the dominant paradigm for training state-of-the-art embeddings without labeled data. This chapter explores how to leverage contrastive learning at enterprise scale—from fundamental principles through production architectures that handle trillion-row training. We cover SimCLR, MoCo, hard negative mining strategies, batch optimization techniques, and distributed training patterns that power modern embedding systems.\n:::\n\n## Contrastive Learning Fundamentals\n\nContrastive learning transforms the embedding problem from \"predict labels\" to \"distinguish similar from dissimilar.\" This shift unlocks massive unlabeled datasets and produces embeddings that capture nuanced semantic relationships beyond what supervised learning achieves.\n\n### The Core Principle\n\nThe fundamental insight: **embeddings should place similar items close together and dissimilar items far apart**. Simple in concept, revolutionary in practice.\n\nTraditional supervised learning requires:\n\n- Expensive labeled data (millions of examples)\n- Fixed label space (categories defined upfront)\n- Limited to explicit labels (can't capture unlabeled nuances)\n\nContrastive learning requires only:\n\n- Pairs or triplets indicating similarity\n- Any method to generate positive pairs (augmentation, co-occurrence, etc.)\n- Scales to billions of unlabeled examples\n\n### The Contrastive Loss Landscape\n\n**InfoNCE Loss: The Foundation**\n\nInfoNCE (Noise Contrastive Estimation with Information theory) is the most widely used contrastive loss:\n\n::: {#1e18bc52 .cell execution_count=1}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show InfoNCE Loss Implementation\"}\nimport torch\nimport torch.nn.functional as F\n\n\nclass InfoNCELoss:\n    \"\"\"\n    InfoNCE loss for contrastive learning.\n\n    Core idea: Given an anchor and one positive example, distinguish the\n    positive from N-1 negative examples drawn from the distribution.\n    \"\"\"\n\n    def __init__(self, temperature=0.07):\n        self.temperature = temperature\n\n    def compute_loss(self, anchor_embeddings, positive_embeddings, all_embeddings):\n        batch_size = anchor_embeddings.shape[0]\n\n        # Normalize embeddings (critical for stable training)\n        anchor_norm = F.normalize(anchor_embeddings, p=2, dim=1)\n        positive_norm = F.normalize(positive_embeddings, p=2, dim=1)\n        all_norm = F.normalize(all_embeddings, p=2, dim=1)\n\n        # Positive similarities\n        positive_sim = torch.sum(anchor_norm * positive_norm, dim=1) / self.temperature\n\n        # Similarity matrix: anchor × all\n        similarity_matrix = torch.matmul(anchor_norm, all_norm.T) / self.temperature\n\n        # Labels: positive is at index i for anchor i\n        labels = torch.arange(batch_size, device=anchor_embeddings.device)\n\n        # Cross-entropy loss\n        loss = F.cross_entropy(similarity_matrix, labels)\n\n        # Metrics\n        with torch.no_grad():\n            predictions = similarity_matrix.argmax(dim=1)\n            accuracy = (predictions == labels).float().mean()\n            positive_sim_mean = positive_sim.mean()\n\n            mask = torch.ones_like(similarity_matrix, dtype=torch.bool)\n            mask[torch.arange(batch_size), labels] = False\n            negative_sim_mean = similarity_matrix[mask].mean()\n\n        return loss, {\n            \"accuracy\": accuracy.item(),\n            \"positive_similarity\": positive_sim_mean.item(),\n            \"negative_similarity\": negative_sim_mean.item(),\n        }\n\n\n# Example usage\ntorch.manual_seed(42)\nencoder = torch.nn.Sequential(\n    torch.nn.Linear(512, 256), torch.nn.ReLU(), torch.nn.Linear(256, 128)\n)\n\nanchors = torch.randn(64, 512)\npositives = torch.randn(64, 512)\nall_batch = torch.cat([anchors, positives], dim=0)\n\nanchor_emb = encoder(anchors)\npositive_emb = encoder(positives)\nall_emb = encoder(all_batch)\n\nloss_fn = InfoNCELoss(temperature=0.07)\nloss, metrics = loss_fn.compute_loss(anchor_emb, positive_emb, all_emb)\n\nprint(f\"InfoNCE Loss: {loss.item():.4f}\")\nprint(f\"Accuracy: {metrics['accuracy']:.2%}\")\nprint(f\"Positive similarity: {metrics['positive_similarity']:.4f}\")\nprint(f\"Negative similarity: {metrics['negative_similarity']:.4f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nInfoNCE Loss: 0.0124\nAccuracy: 100.00%\nPositive similarity: 4.1863\nNegative similarity: 4.3139\n```\n:::\n:::\n\n\n**The Temperature Parameter: Critical but Often Misunderstood**\n\nTemperature τ controls the \"softness\" of the distribution:\n\n- **Low temperature (0.01-0.05)**: Sharp distribution, focuses on hardest negatives\n  - Pro: Faster convergence, better final performance\n  - Con: Numerical instability, requires careful tuning\n  - Use when: Large batches (1024+), well-curated negatives\n\n- **Medium temperature (0.07-0.1)**: Balanced (most common)\n  - Pro: Stable training, good performance\n  - Con: May not fully utilize hard negatives\n  - Use when: Standard training, batch size 256-1024\n\n- **High temperature (0.2-0.5)**: Soft distribution, considers all negatives\n  - Pro: Very stable, handles noisy negatives well\n  - Con: Slower convergence, potentially lower final performance\n  - Use when: Small batches, noisy data, initial training phase\n\n::: {#d82313a1 .cell execution_count=2}\n``` {.python .cell-code}\nclass TemperatureAnalysis:\n    \"\"\"Analyze impact of temperature on contrastive learning.\"\"\"\n\n    def recommend_temperature(self, batch_size, data_quality=\"high\"):\n        if batch_size >= 4096:\n            if data_quality == \"high\":\n                return 0.03, \"Large batch + high quality -> very low temperature\"\n            return 0.05, \"Large batch but lower quality -> slightly higher\"\n        elif batch_size >= 1024:\n            if data_quality == \"high\":\n                return 0.05, \"Large batch + high quality -> low temperature\"\n            return 0.07, \"Standard setting for large batches\"\n        elif batch_size >= 256:\n            return 0.07, \"Standard temperature for medium batches\"\n        elif batch_size >= 64:\n            if data_quality == \"low\":\n                return 0.15, \"Small batch + noisy data -> higher temperature\"\n            return 0.1, \"Small batch -> moderately high temperature\"\n        return 0.2, \"Very small batch -> high temperature\"\n\n\n# Example: Get recommendations for different setups\nanalyzer = TemperatureAnalysis()\n\nprint(\"Temperature Recommendations:\")\nprint(\"-\" * 50)\nfor batch_size, quality in [(4096, \"high\"), (512, \"medium\"), (64, \"low\")]:\n    temp, reasoning = analyzer.recommend_temperature(batch_size, quality)\n    print(f\"Batch {batch_size:4d}, {quality:6s} quality: τ={temp:.2f}\")\n    print(f\"  {reasoning}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTemperature Recommendations:\n--------------------------------------------------\nBatch 4096, high   quality: τ=0.03\n  Large batch + high quality -> very low temperature\nBatch  512, medium quality: τ=0.07\n  Standard temperature for medium batches\nBatch   64, low    quality: τ=0.15\n  Small batch + noisy data -> higher temperature\n```\n:::\n:::\n\n\n### Alternative Contrastive Losses\n\n**Triplet Loss: The Classic Approach**\n\n::: {#08d1a441 .cell execution_count=3}\n``` {.python .cell-code}\nimport torch\nimport torch.nn.functional as F\n\n\nclass TripletLoss:\n    \"\"\"Triplet loss with margin.\"\"\"\n\n    def __init__(self, margin=1.0):\n        self.margin = margin\n\n    def compute_loss(self, anchor, positive, negative):\n        pos_dist = 1 - F.cosine_similarity(anchor, positive, dim=-1)\n        neg_dist = 1 - F.cosine_similarity(anchor, negative, dim=-1)\n        loss = F.relu(pos_dist - neg_dist + self.margin)\n        return loss.mean()\n\n\n# Example\ntorch.manual_seed(42)\nanchor = torch.randn(32, 128)\npositive = anchor + torch.randn(32, 128) * 0.1  # Similar\nnegative = torch.randn(32, 128)  # Random\n\ntriplet_loss = TripletLoss(margin=0.5)\nloss = triplet_loss.compute_loss(anchor, positive, negative)\nprint(f\"Triplet Loss: {loss.item():.4f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTriplet Loss: 0.0000\n```\n:::\n:::\n\n\n**NTXentLoss (Normalized Temperature-scaled Cross Entropy)**\n\nThe loss used in SimCLR, a normalized variant of InfoNCE:\n\n::: {#76ed7fc3 .cell execution_count=4}\n``` {.python .cell-code}\nimport torch\nimport torch.nn.functional as F\n\n\nclass NTXentLoss:\n    \"\"\"NT-Xent loss from SimCLR paper.\"\"\"\n\n    def __init__(self, temperature=0.5):\n        self.temperature = temperature\n\n    def compute_loss(self, embeddings):\n        batch_size = embeddings.shape[0] // 2\n        embeddings = F.normalize(embeddings, p=2, dim=1)\n\n        similarity_matrix = torch.matmul(embeddings, embeddings.T) / self.temperature\n\n        mask = torch.eye(2 * batch_size, dtype=torch.bool, device=embeddings.device)\n        similarity_matrix.masked_fill_(mask, -9e15)\n\n        labels = torch.cat([\n            torch.arange(batch_size, 2 * batch_size),\n            torch.arange(0, batch_size),\n        ]).to(embeddings.device)\n\n        return F.cross_entropy(similarity_matrix, labels)\n\n\n# Example\ntorch.manual_seed(42)\nembeddings = torch.randn(64, 128)  # 32 pairs\n\nnt_xent = NTXentLoss(temperature=0.5)\nloss = nt_xent.compute_loss(embeddings)\nprint(f\"NT-Xent Loss: {loss.item():.4f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNT-Xent Loss: 4.1953\n```\n:::\n:::\n\n\n### Why Contrastive Learning Works: The Theoretical Foundation\n\n**Mutual Information Maximization**\n\nContrastive learning maximizes the mutual information between different views of the same data:\n\n```\nI(x; x̃) = H(x) - H(x|x̃)\n```\n\nInfoNCE provides a lower bound on mutual information:\n\n```\nI(x; x̃) ≥ log(K) - L_InfoNCE\n```\n\nWhere K is the number of negatives. **Larger batches (more negatives) provide a tighter bound, explaining why contrastive learning benefits dramatically from large batch sizes.**\n\n**Alignment and Uniformity**\n\nRecent work decomposes contrastive learning success into two properties:\n\n1. **Alignment**: Positive pairs should be close\n2. **Uniformity**: Embeddings should be uniformly distributed on unit hypersphere\n\n::: {#067be012 .cell execution_count=5}\n``` {.python .cell-code}\nimport torch\nimport torch.nn.functional as F\n\n\nclass AlignmentUniformityAnalysis:\n    \"\"\"Analyze embedding quality via alignment and uniformity.\"\"\"\n\n    def compute_alignment(self, emb1, emb2):\n        \"\"\"Lower is better (closer pairs).\"\"\"\n        emb1 = F.normalize(emb1, p=2, dim=1)\n        emb2 = F.normalize(emb2, p=2, dim=1)\n        return torch.norm(emb1 - emb2, p=2, dim=1).pow(2).mean().item()\n\n    def compute_uniformity(self, embeddings, t=2):\n        \"\"\"Lower is better (more uniform distribution).\"\"\"\n        emb = F.normalize(embeddings, p=2, dim=1)\n        sim_matrix = torch.matmul(emb, emb.T)\n        mask = ~torch.eye(len(emb), dtype=torch.bool, device=emb.device)\n        similarities = sim_matrix[mask]\n        squared_distances = 2 * (1 - similarities)\n        return torch.log(torch.exp(-t * squared_distances).mean()).item()\n\n\n# Example\ntorch.manual_seed(42)\nanalyzer = AlignmentUniformityAnalysis()\n\n# Good embeddings\ngood_emb1 = torch.randn(100, 64)\ngood_emb2 = good_emb1 + torch.randn(100, 64) * 0.1\n\n# Collapsed embeddings (bad)\nbad_emb = torch.randn(1, 64).expand(100, -1) + torch.randn(100, 64) * 0.01\n\nprint(\"Good Embeddings:\")\nprint(f\"  Alignment: {analyzer.compute_alignment(good_emb1, good_emb2):.4f}\")\nprint(f\"  Uniformity: {analyzer.compute_uniformity(good_emb1):.4f}\")\nprint(\"\\nCollapsed Embeddings (BAD):\")\nprint(f\"  Uniformity: {analyzer.compute_uniformity(bad_emb):.4f} <- higher = collapsed!\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nGood Embeddings:\n  Alignment: 0.0100\n  Uniformity: -3.8725\n\nCollapsed Embeddings (BAD):\n  Uniformity: -0.0005 <- higher = collapsed!\n```\n:::\n:::\n\n\n## SimCLR, MoCo, and Enterprise Adaptations\n\n### SimCLR: Simple Framework, Powerful Results\n\nSimCLR achieves remarkable results with a straightforward recipe:\n\n1. **Data augmentation pipeline**: Generate two views of each example\n2. **Encoder network**: Extract embeddings\n3. **Projection head**: Non-linear MLP (critical for performance)\n4. **NT-Xent loss**: Normalized temperature-scaled cross entropy\n5. **Large batch training**: 4096+ examples per batch\n\n::: {#c35527b6 .cell execution_count=6}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show SimCLR Implementation\"}\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass SimCLRTextEmbedding(nn.Module):\n    \"\"\"SimCLR adapted for text embeddings.\"\"\"\n\n    def __init__(self, vocab_size=10000, embed_dim=256, projection_dim=128):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, embed_dim)\n        self.encoder = nn.Sequential(\n            nn.Linear(embed_dim, embed_dim), nn.ReLU(), nn.Linear(embed_dim, embed_dim)\n        )\n        self.projection_head = nn.Sequential(\n            nn.Linear(embed_dim, embed_dim), nn.ReLU(), nn.Linear(embed_dim, projection_dim)\n        )\n        self.temperature = 0.07\n\n    def forward(self, input_ids):\n        x = self.embedding(input_ids).mean(dim=1)\n        representations = self.encoder(x)\n        embeddings = self.projection_head(representations)\n        return embeddings, representations\n\n    def compute_loss(self, embeddings):\n        batch_size = embeddings.shape[0] // 2\n        embeddings = F.normalize(embeddings, p=2, dim=1)\n\n        sim_matrix = torch.matmul(embeddings, embeddings.T) / self.temperature\n        mask = torch.eye(2 * batch_size, dtype=torch.bool, device=embeddings.device)\n        sim_matrix.masked_fill_(mask, -9e15)\n\n        labels = torch.cat([\n            torch.arange(batch_size, 2 * batch_size),\n            torch.arange(0, batch_size),\n        ]).to(embeddings.device)\n\n        loss = F.cross_entropy(sim_matrix, labels)\n\n        with torch.no_grad():\n            accuracy = (sim_matrix.argmax(dim=1) == labels).float().mean()\n\n        return loss, {\"accuracy\": accuracy.item()}\n\n\n# Example\ntorch.manual_seed(42)\nmodel = SimCLRTextEmbedding(vocab_size=1000, embed_dim=128, projection_dim=64)\n\ninput_ids = torch.randint(0, 1000, (32, 20))  # 16 pairs\nembeddings, _ = model(input_ids)\nloss, metrics = model.compute_loss(embeddings)\n\nprint(f\"SimCLR Loss: {loss.item():.4f}\")\nprint(f\"Accuracy: {metrics['accuracy']:.2%}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSimCLR Loss: 3.4567\nAccuracy: 9.38%\n```\n:::\n:::\n\n\n### MoCo: Memory-Efficient Contrastive Learning\n\nMoCo solves a critical problem: **SimCLR requires enormous batch sizes (4096+) for good negatives, which demands massive GPU memory.**\n\nMoCo's solution: **maintain a queue of negative examples across batches**.\n\n::: {#71cd5f55 .cell execution_count=7}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show MoCo Implementation\"}\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass MoCoTextEmbedding(nn.Module):\n    \"\"\"MoCo for text embeddings - works with small batches!\"\"\"\n\n    def __init__(self, vocab_size=10000, embed_dim=256, projection_dim=128,\n                 queue_size=4096, momentum=0.999):\n        super().__init__()\n        self.queue_size = queue_size\n        self.momentum = momentum\n        self.temperature = 0.07\n\n        # Query encoder\n        self.encoder_q = nn.Sequential(\n            nn.Embedding(vocab_size, embed_dim),\n            nn.Flatten(1), nn.Linear(embed_dim * 20, projection_dim)\n        )\n        # Key encoder (momentum updated)\n        self.encoder_k = nn.Sequential(\n            nn.Embedding(vocab_size, embed_dim),\n            nn.Flatten(1), nn.Linear(embed_dim * 20, projection_dim)\n        )\n\n        for p_q, p_k in zip(self.encoder_q.parameters(), self.encoder_k.parameters()):\n            p_k.data.copy_(p_q.data)\n            p_k.requires_grad = False\n\n        self.register_buffer(\"queue\", F.normalize(torch.randn(projection_dim, queue_size), dim=0))\n        self.register_buffer(\"queue_ptr\", torch.zeros(1, dtype=torch.long))\n\n    @torch.no_grad()\n    def _momentum_update(self):\n        for p_q, p_k in zip(self.encoder_q.parameters(), self.encoder_k.parameters()):\n            p_k.data = p_k.data * self.momentum + p_q.data * (1 - self.momentum)\n\n    @torch.no_grad()\n    def _update_queue(self, keys):\n        batch_size = keys.shape[0]\n        ptr = int(self.queue_ptr)\n        self.queue[:, ptr:ptr + batch_size] = keys.T\n        self.queue_ptr[0] = (ptr + batch_size) % self.queue_size\n\n    def forward(self, query_ids, key_ids):\n        q = F.normalize(self.encoder_q(query_ids), dim=1)\n\n        with torch.no_grad():\n            self._momentum_update()\n            k = F.normalize(self.encoder_k(key_ids), dim=1)\n\n        l_pos = torch.einsum(\"nc,nc->n\", q, k).unsqueeze(-1)\n        l_neg = torch.einsum(\"nc,ck->nk\", q, self.queue.clone().detach())\n\n        logits = torch.cat([l_pos, l_neg], dim=1) / self.temperature\n        labels = torch.zeros(logits.shape[0], dtype=torch.long, device=q.device)\n\n        loss = F.cross_entropy(logits, labels)\n        self._update_queue(k)\n\n        with torch.no_grad():\n            accuracy = (logits.argmax(dim=1) == labels).float().mean()\n\n        return loss, {\"accuracy\": accuracy.item(), \"queue_ptr\": int(self.queue_ptr)}\n\n\n# Example: MoCo works with small batches!\ntorch.manual_seed(42)\nmodel = MoCoTextEmbedding(vocab_size=1000, embed_dim=64, projection_dim=32, queue_size=256)\n\nfor i in range(5):\n    query = torch.randint(0, 1000, (16, 20))\n    key = torch.randint(0, 1000, (16, 20))\n    loss, metrics = model(query, key)\n\nprint(f\"MoCo Loss: {loss.item():.4f}\")\nprint(f\"Accuracy: {metrics['accuracy']:.2%}\")\nprint(f\"Queue filled: {metrics['queue_ptr']}/256\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMoCo Loss: 7.8156\nAccuracy: 0.00%\nQueue filled: 80/256\n```\n:::\n:::\n\n\n### Enterprise Adaptations\n\n**Multi-Modal Contrastive Learning**\n\n::: {#d980c6da .cell execution_count=8}\n``` {.python .cell-code}\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass MultiModalContrastive(nn.Module):\n    \"\"\"Contrastive learning for text + image.\"\"\"\n\n    def __init__(self, text_dim=256, image_dim=512, projection_dim=128):\n        super().__init__()\n        self.text_proj = nn.Linear(text_dim, projection_dim)\n        self.image_proj = nn.Linear(image_dim, projection_dim)\n        self.temperature = 0.07\n\n    def forward(self, text_features, image_features):\n        text_emb = F.normalize(self.text_proj(text_features), dim=1)\n        image_emb = F.normalize(self.image_proj(image_features), dim=1)\n\n        logits = torch.matmul(text_emb, image_emb.T) / self.temperature\n        labels = torch.arange(text_emb.shape[0], device=text_emb.device)\n\n        loss = (F.cross_entropy(logits, labels) + F.cross_entropy(logits.T, labels)) / 2\n        return loss\n\n\n# Example\ntorch.manual_seed(42)\nmodel = MultiModalContrastive()\nloss = model(torch.randn(32, 256), torch.randn(32, 512))\nprint(f\"Multi-modal Contrastive Loss: {loss.item():.4f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMulti-modal Contrastive Loss: 4.5515\n```\n:::\n:::\n\n\n## Hard Negative Mining at Scale\n\nThe quality of negative examples determines contrastive learning success.\n\n### The Hard Negative Spectrum\n\n- **Easy Negatives**: Too different; model learns nothing useful\n- **Medium Negatives**: Provide useful signal\n- **Hard Negatives**: Force fine-grained learning (best!)\n- **False Negatives**: Actually positive; hurt training (avoid!)\n\n### Hard Negative Mining Strategies\n\n**Strategy 1: In-Batch Hard Negative Mining**\n\n::: {#c5a5a337 .cell execution_count=9}\n``` {.python .cell-code}\nimport torch\nimport torch.nn.functional as F\n\n\nclass InBatchHardNegativeMining:\n    \"\"\"Mine hard negatives from within batch (zero overhead).\"\"\"\n\n    def __init__(self, temperature=0.07, num_hard=5):\n        self.temperature = temperature\n        self.num_hard = num_hard\n\n    def compute_loss(self, anchor_emb, positive_emb):\n        anchor = F.normalize(anchor_emb, dim=1)\n        positive = F.normalize(positive_emb, dim=1)\n\n        all_emb = torch.cat([anchor, positive], dim=0)\n        sim_matrix = torch.matmul(anchor, all_emb.T)\n\n        losses = []\n        for i in range(len(anchor)):\n            pos_sim = F.cosine_similarity(anchor[i:i+1], positive[i:i+1])\n            neg_sims = torch.cat([sim_matrix[i, :i], sim_matrix[i, i+1:]])\n            hard_negs = neg_sims.topk(min(self.num_hard, len(neg_sims)))[0]\n\n            pos_exp = torch.exp(pos_sim / self.temperature)\n            neg_exp = torch.exp(hard_negs / self.temperature).sum()\n            losses.append(-torch.log(pos_exp / (pos_exp + neg_exp)))\n\n        return torch.stack(losses).mean()\n\n\n# Example\ntorch.manual_seed(42)\nminer = InBatchHardNegativeMining()\nloss = miner.compute_loss(torch.randn(32, 128), torch.randn(32, 128))\nprint(f\"In-batch hard negative loss: {loss.item():.4f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nIn-batch hard negative loss: 4.1985\n```\n:::\n:::\n\n\n**Strategy 2: Queue-Based Hard Negative Mining**\n\n::: {#0c8efb2b .cell execution_count=10}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show Queue-Based Mining\"}\nimport torch\nimport torch.nn.functional as F\n\n\nclass QueueBasedMining:\n    \"\"\"Maintain queue for larger negative pool.\"\"\"\n\n    def __init__(self, dim, queue_size=4096):\n        self.queue = F.normalize(torch.randn(queue_size, dim), dim=1)\n        self.ptr = 0\n        self.queue_size = queue_size\n        self.filled = 0\n\n    def update(self, embeddings):\n        n = embeddings.shape[0]\n        self.queue[self.ptr:self.ptr + n] = F.normalize(embeddings.detach(), dim=1)\n        self.ptr = (self.ptr + n) % self.queue_size\n        self.filled = min(self.filled + n, self.queue_size)\n\n    def get_hard_negatives(self, anchors, k=10):\n        anchors = F.normalize(anchors, dim=1)\n        sims = torch.matmul(anchors, self.queue[:self.filled].T)\n        return sims.topk(min(k, self.filled), dim=1)[0]\n\n\n# Example\ntorch.manual_seed(42)\nminer = QueueBasedMining(dim=128, queue_size=512)\n\nfor _ in range(5):\n    miner.update(torch.randn(32, 128))\n\nhard_neg_sims = miner.get_hard_negatives(torch.randn(16, 128), k=10)\nprint(f\"Queue filled: {miner.filled}/512\")\nprint(f\"Hard negative similarities shape: {hard_neg_sims.shape}\")\nprint(f\"Average hard negative sim: {hard_neg_sims.mean().item():.4f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nQueue filled: 160/512\nHard negative similarities shape: torch.Size([16, 10])\nAverage hard negative sim: 0.1701\n```\n:::\n:::\n\n\n**Strategy 3: Debiased Hard Negative Mining**\n\n::: {#c05adc6d .cell execution_count=11}\n``` {.python .cell-code}\nimport torch\nimport torch.nn.functional as F\n\n\nclass DebiasedMining:\n    \"\"\"Filter false negatives from hard negative candidates.\"\"\"\n\n    def filter_by_margin(self, anchor, positive, candidates, margin=0.1):\n        \"\"\"Keep negatives with sufficient margin from positive.\"\"\"\n        anchor = F.normalize(anchor, dim=1)\n        positive = F.normalize(positive, dim=1)\n\n        pos_sim = F.cosine_similarity(anchor, positive, dim=1)\n\n        filtered = []\n        for i in range(len(anchor)):\n            neg_sims = F.cosine_similarity(anchor[i:i+1], candidates[i], dim=1)\n            valid = neg_sims < (pos_sim[i] - margin)\n            filtered.append(valid.sum().item())\n\n        return filtered\n\n\n# Example\ntorch.manual_seed(42)\ndebiaser = DebiasedMining()\n\nanchor = torch.randn(4, 64)\npositive = anchor + torch.randn(4, 64) * 0.1\ncandidates = torch.randn(4, 10, 64)\ncandidates[:, :2] = anchor.unsqueeze(1) + torch.randn(4, 2, 64) * 0.05  # False negatives\n\nkept = debiaser.filter_by_margin(anchor, positive, candidates, margin=0.1)\nprint(\"Negatives kept after debiasing:\")\nfor i, k in enumerate(kept):\n    print(f\"  Example {i}: {k}/10 negatives kept\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNegatives kept after debiasing:\n  Example 0: 8/10 negatives kept\n  Example 1: 8/10 negatives kept\n  Example 2: 8/10 negatives kept\n  Example 3: 8/10 negatives kept\n```\n:::\n:::\n\n\n## Batch Optimization for Trillion-Row Training\n\n### Why Large Batches Matter\n\n| Batch Size | Relative Performance | Memory (A100) |\n|------------|---------------------|---------------|\n| 256        | 0.85                | 12 GB         |\n| 1024       | 0.94                | 45 GB         |\n| 4096       | 1.00                | OOM           |\n\n### Gradient Accumulation\n\n::: {#8b491f9a .cell execution_count=12}\n``` {.python .cell-code}\nimport torch\n\n\nclass GradientAccumulation:\n    \"\"\"Simulate large batches through accumulation.\"\"\"\n\n    def __init__(self, micro_batch=256, effective_batch=2048):\n        self.steps = effective_batch // micro_batch\n        print(f\"Accumulating {self.steps} steps: {micro_batch} × {self.steps} = {effective_batch}\")\n\n\ntrainer = GradientAccumulation(micro_batch=256, effective_batch=2048)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAccumulating 8 steps: 256 × 8 = 2048\n```\n:::\n:::\n\n\n**Note**: Gradient accumulation has a flaw for contrastive learning—each micro-batch only sees its own negatives. Use distributed training for truly large batches.\n\n### Distributed Contrastive Learning\n\n::: {#4772a84c .cell execution_count=13}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show Distributed Training\"}\nimport torch\nimport torch.nn.functional as F\n\n\nclass DistributedContrastive:\n    \"\"\"Distributed contrastive learning across GPUs.\"\"\"\n\n    def __init__(self, world_size, rank):\n        self.world_size = world_size\n        self.rank = rank\n\n    def simulate_gather(self, local_emb):\n        \"\"\"Simulate all-gather across GPUs.\"\"\"\n        return torch.cat([local_emb + torch.randn_like(local_emb) * 0.01\n                          for _ in range(self.world_size)], dim=0)\n\n    def compute_loss(self, anchor, positive, temperature=0.07):\n        local_batch = anchor.shape[0]\n\n        all_anchor = self.simulate_gather(anchor)\n        all_positive = self.simulate_gather(positive)\n        global_batch = all_anchor.shape[0]\n\n        all_anchor = F.normalize(all_anchor, dim=1)\n        all_positive = F.normalize(all_positive, dim=1)\n        local_anchor = F.normalize(anchor, dim=1)\n\n        all_emb = torch.cat([all_anchor, all_positive], dim=0)\n        sim_matrix = torch.matmul(local_anchor, all_emb.T) / temperature\n\n        labels = torch.arange(self.rank * local_batch, (self.rank + 1) * local_batch) + global_batch\n\n        return F.cross_entropy(sim_matrix, labels), global_batch\n\n\n# Example: 4 GPU simulation\ntorch.manual_seed(42)\ntrainer = DistributedContrastive(world_size=4, rank=0)\n\nloss, global_batch = trainer.compute_loss(torch.randn(64, 128), torch.randn(64, 128))\nprint(f\"Distributed Loss: {loss.item():.4f}\")\nprint(f\"Effective batch: 4 GPUs × 64 = {global_batch}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDistributed Loss: 15.9274\nEffective batch: 4 GPUs × 64 = 256\n```\n:::\n:::\n\n\n### Mixed Precision for Larger Batches\n\n::: {#561ec93a .cell execution_count=14}\n``` {.python .cell-code}\nimport torch\nimport torch.nn.functional as F\n\n\nclass StableInfoNCE:\n    \"\"\"Numerically stable loss for FP16 training.\"\"\"\n\n    def __init__(self, temperature=0.07):\n        self.temperature = temperature\n\n    def compute_loss(self, anchor, all_emb):\n        # Normalize in FP32 for stability\n        anchor = F.normalize(anchor.float(), dim=1)\n        all_emb = F.normalize(all_emb.float(), dim=1)\n\n        sim = torch.matmul(anchor, all_emb.T) / self.temperature\n        labels = torch.arange(len(anchor), device=anchor.device)\n\n        # Log-sum-exp trick for stability\n        log_denom = torch.logsumexp(sim, dim=1)\n        pos_logits = sim[torch.arange(len(anchor)), labels]\n\n        return (log_denom - pos_logits).mean()\n\n\n# Example with FP16 inputs\ntorch.manual_seed(42)\nloss_fn = StableInfoNCE()\nloss = loss_fn.compute_loss(\n    torch.randn(32, 128, dtype=torch.float16),\n    torch.randn(64, 128, dtype=torch.float16)\n)\nprint(f\"Stable InfoNCE (from FP16): {loss.item():.4f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nStable InfoNCE (from FP16): 5.3713\n```\n:::\n:::\n\n\n## Multi-Node Distributed Architectures\n\n::: {#9ddfdd53 .cell execution_count=15}\n``` {.python .cell-code}\nimport torch\n\n\nclass MultiNodeTraining:\n    \"\"\"Multi-node distributed contrastive learning.\"\"\"\n\n    def __init__(self, nodes, gpus_per_node, local_batch):\n        self.total_gpus = nodes * gpus_per_node\n        self.global_batch = self.total_gpus * local_batch\n\n    def info(self):\n        return {\n            \"total_gpus\": self.total_gpus,\n            \"global_batch\": self.global_batch\n        }\n\n\n# Example: 16 nodes × 8 GPUs\ntrainer = MultiNodeTraining(nodes=16, gpus_per_node=8, local_batch=256)\ninfo = trainer.info()\nprint(f\"Multi-Node Setup:\")\nprint(f\"  Total GPUs: {info['total_gpus']}\")\nprint(f\"  Global batch: {info['global_batch']:,}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMulti-Node Setup:\n  Total GPUs: 128\n  Global batch: 32,768\n```\n:::\n:::\n\n\n### Memory Optimization with Gradient Checkpointing\n\n::: {#90b03955 .cell execution_count=16}\n``` {.python .cell-code}\nimport torch.nn as nn\nfrom torch.utils.checkpoint import checkpoint\n\n\nclass MemoryEfficientModel(nn.Module):\n    \"\"\"Trade compute for memory with checkpointing.\"\"\"\n\n    def __init__(self, dim=768, proj_dim=128):\n        super().__init__()\n        self.projection = nn.Sequential(\n            nn.Linear(dim, 512), nn.ReLU(), nn.Linear(512, proj_dim)\n        )\n\n    def forward(self, x, use_checkpoint=True):\n        if use_checkpoint and self.training:\n            return checkpoint(self.projection, x, use_reentrant=False)\n        return self.projection(x)\n\n\nmodel = MemoryEfficientModel()\nx = torch.randn(32, 768, requires_grad=True)\nout = model(x, use_checkpoint=True)\nprint(f\"Output shape: {out.shape}\")\nprint(\"Memory saved: ~50% with gradient checkpointing\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nOutput shape: torch.Size([32, 128])\nMemory saved: ~50% with gradient checkpointing\n```\n:::\n:::\n\n\n## Key Takeaways\n\n- **Contrastive learning transforms embeddings into a similarity learning problem** requiring only pairs/triplets instead of expensive labels\n\n- **InfoNCE loss** treats contrastive learning as classification: identify the positive from K negatives (larger batches → better embeddings)\n\n- **Temperature critically affects training**: low (0.01-0.05) for large batches, medium (0.07-0.1) for standard training, high (0.2-0.5) for noisy data\n\n- **SimCLR vs MoCo trade-offs**: SimCLR needs 4096+ batches; MoCo works with 256 using a momentum encoder and queue\n\n- **Hard negative mining** dramatically improves quality: in-batch (zero overhead), queue-based (larger pool), offline (global negatives)\n\n- **Debiased mining prevents false negatives** from hurting training through margin-based filtering\n\n- **Distributed training enables truly large batches**: 8 GPUs × 512 = 4096 effective batch size\n\n- **Memory optimization**: gradient checkpointing trades 20-30% compute for 50% memory savings\n\n## Looking Ahead\n\n@sec-siamese-networks explores Siamese Networks, a specialized architecture for one-shot and few-shot learning—critical for applications with limited labeled data.\n\n## Further Reading\n\n- Chen, T., et al. (2020). \"A Simple Framework for Contrastive Learning of Visual Representations.\" *ICML 2020* (SimCLR)\n- He, K., et al. (2020). \"Momentum Contrast for Unsupervised Visual Representation Learning.\" *CVPR 2020* (MoCo)\n- Oord, A., et al. (2018). \"Representation Learning with Contrastive Predictive Coding.\" *arXiv:1807.03748* (InfoNCE)\n- Wang, T., & Isola, P. (2020). \"Understanding Contrastive Representation Learning through Alignment and Uniformity on the Hypersphere.\" *ICML 2020*\n- Gao, T., et al. (2021). \"SimCSE: Simple Contrastive Learning of Sentence Embeddings.\" *EMNLP 2021*\n- Robinson, J., et al. (2021). \"Contrastive Learning with Hard Negative Samples.\" *ICLR 2021*\n- Chuang, C., et al. (2020). \"Debiased Contrastive Learning.\" *NeurIPS 2020*\n\n",
    "supporting": [
      "ch09_contrastive_learning_files"
    ],
    "filters": [],
    "includes": {}
  }
}