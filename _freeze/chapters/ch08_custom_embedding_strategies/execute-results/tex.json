{
  "hash": "af6a84ab31d947e26493875a63eb2de0",
  "result": {
    "engine": "jupyter",
    "markdown": "# Beyond Pre-trained: Custom Embedding Strategies {#sec-custom-embedding-strategies}\n\n:::{.callout-note}\n## Chapter Overview\nThis chapter bridges strategic planning and implementation by answering a critical question: when should you build custom embeddings versus fine-tuning existing models? We explore domain-specific requirements, multi-objective design, dimensionality optimization, and cost-performance trade-offs that determine success at scale.\n:::\n\n## When to Build Custom Embeddings vs. Fine-Tune\n\nThe decision to build custom embeddings from scratch versus fine-tuning pre-trained models is one of the most consequential choices in your embedding strategy. Make the wrong choice and you'll either waste months building unnecessary infrastructure or deploy suboptimal models that never reach competitive performance.\n\n### The Custom vs. Fine-Tune Spectrum\n\nMost discussions frame this as a binary choice. In reality, it's a spectrum with five distinct approaches:\n\n:::{.callout-note}\nThe following cost and quality estimates are rough guidelines based on typical projects. Actual results vary significantly based on domain, data quality, team expertise, and specific requirements.\n:::\n\n**Level 0: Use Pre-trained, Frozen**\n\n- **Description**: Use off-the-shelf embeddings (OpenAI, Sentence-BERT) without modification\n- **Effort**: Hours\n- **Cost**: $0-$1K/month\n- **Quality**: 60-70% of optimal for your domain\n- **Best for**: Proof-of-concepts, generic use cases, rapid prototyping\n\n**Level 1: Prompt Engineering**\n\n- **Description**: Optimize prompts for pre-trained models to better capture domain nuances\n- **Effort**: Days to weeks\n- **Cost**: $1K-$5K/month\n- **Quality**: 70-80% of optimal\n- **Best for**: Specific queries, instruction-based models, low-budget projects\n\n**Level 2: Fine-Tune Last Layers**\n\n- **Description**: Fine-tune final layers of pre-trained model on your domain data\n- **Effort**: Weeks\n- **Cost**: $5K-$25K one-time + ongoing inference\n- **Quality**: 80-90% of optimal\n- **Best for**: Domain adaptation with limited data (10K-100K examples)\n\n**Level 3: Full Model Fine-Tuning**\n\n- **Description**: Fine-tune entire pre-trained model on your data\n- **Effort**: 1-3 months\n- **Cost**: $25K-$150K one-time + ongoing\n- **Quality**: 85-95% of optimal\n- **Best for**: Substantial domain data (100K-10M examples), clear performance gaps\n\n**Level 4: Train From Scratch**\n\n- **Description**: Design and train custom architecture for your specific requirements\n- **Effort**: 6-18 months\n- **Cost**: $500K-$5M+ one-time + ongoing\n- **Quality**: 95-100% optimal (when done right)\n- **Best for**: Highly specialized domains, massive data (10M+ examples), competitive moat\n\n:::{.callout-tip}\n## The 80/20 Rule\nFor most organizations, **Level 3 (Full Model Fine-Tuning)** delivers 95% of the benefit at 20% of the cost compared to training from scratch. Only pursue Level 4 if embeddings are core to your competitive advantage.\n:::\n\n### Decision Framework: When to Build Custom\n\nUse this framework to determine your approach. For each factor, assess whether your situation favors fine-tuning an existing model or building custom embeddings from scratch:\n\n| Factor | Favors Fine-Tuning | Favors Custom |\n|--------|-------------------|---------------|\n| **Training data** | <1M labeled examples | >10M labeled examples |\n| **Domain gap** | Low/medium (medical, financial) | High (genomics, specialized legal, non-text) |\n| **Performance requirement** | \"Good enough\" for business needs | World-class, no compromises |\n| **Specialized requirements** | Standard text/image | Multi-modal without pre-trained options, tiny models for edge, interpretability |\n| **Budget** | <$150K | >$500K |\n| **Timeline** | <6 months | >12 months |\n| **Team capability** | Limited ML expertise | Published researchers, prior large model experience |\n| **Competitive advantage** | Embeddings support product | Embeddings ARE the product/moat |\n\n**How to interpret**: If most factors point toward fine-tuning, start with Level 2 or 3. If several factors strongly favor custom (especially domain gap and competitive advantage), consider Level 4.\n\n**The hybrid path**: When factors are mixed, start with fine-tuning to establish a baseline and prove business value. This de-risks the investment before committing to custom development. Many successful systems follow this pattern—ship a fine-tuned model in months, then build custom after validating the opportunity.\n\n### Illustrative Case Studies\n\n:::{.callout-note}\nThe following case studies are hypothetical examples designed to illustrate decision-making patterns. While based on realistic scenarios and typical project parameters, they are not descriptions of specific real-world implementations.\n:::\n\n**Case Study 1: Medical Literature Search (Fine-Tuning Win)**\n\nConsider a medical research platform that might initially consider training custom embeddings for biomedical literature. They might have:\n\n- 500K labeled medical article pairs\n- Medium domain gap (medical terminology specialized but well-covered in pre-training)\n- 3-month timeline\n- $100K budget\n\n**Potential Decision**: Fine-tune BioBERT (domain-specific BERT variant already pre-trained on PubMed)\n\n**Potential Outcome**:\n\n- Could achieve ~91% of custom model performance at ~10% of cost\n- Could launch in ~2 months vs. 12+ months for custom\n- Fine-tuning cost: ~$40K one-time\n- Performance: ~0.847 MRR (Mean Reciprocal Rank) vs. ~0.812 for frozen BioBERT\n\n**Case Study 2: Genomics Sequence Embeddings (Custom Win)**\n\nConsider a genomics company that might need embeddings for DNA/protein sequences. They might have:\n\n- 50M protein sequences with structural/functional annotations\n- Extreme domain gap (genomic sequences fundamentally different from text)\n- 18-month timeline\n- $2M budget\n- World-class performance requirement (competitive moat)\n\n**Potential Decision**: Build custom transformer architecture designed specifically for sequences\n\n**Potential Outcome**:\n\n- Custom architecture could outperform adapted text models by ~34%\n- Could enable novel capabilities (structure prediction, functional annotation)\n- Development cost: ~$1.8M over ~16 months\n- Result: Potential industry-leading model, published research, patent applications\n\n**Key Lesson**: Domain gap is often the decisive factor. Natural language pre-training provides limited transfer to genomic sequences.\n\n**Case Study 3: E-commerce Search (Hybrid Approach)**\n\nConsider an e-commerce platform with 100M products that might need multi-modal (text + image) embeddings:\n\n**Phase 1 (Months 1-3)**: Could fine-tune CLIP on ~2M product images + descriptions\n\n- Cost: ~$50K\n- Result: Could achieve ~28% improvement over generic CLIP\n- Launch to production, validate business impact\n\n**Phase 2 (Months 4-12)**: Could build custom architecture incorporating product catalog structure\n\n- Cost: ~$400K\n- Result: Could achieve additional ~15% improvement over fine-tuned CLIP\n- Could enable category-aware search, better handling of attributes\n\n**Key Lesson**: A hybrid approach can de-risk investment. Fine-tuning provides fast wins; custom models deliver competitive advantage after proving value.\n\n### The Fine-Tuning Recipe\n\nWhen fine-tuning is the right choice, follow this battle-tested recipe:\n\n::: {.cell execution_count=1}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show embedding fine-tuner implementation\"}\nfrom sentence_transformers import InputExample, SentenceTransformer, losses\nfrom torch.utils.data import DataLoader\n\nclass EmbeddingFineTuner:\n    \"\"\"Production-ready fine-tuning for sentence embeddings\"\"\"\n\n    def __init__(self, base_model_name=\"all-mpnet-base-v2\"):\n        self.model = SentenceTransformer(base_model_name)\n        self.base_model_name = base_model_name\n\n    def prepare_training_data(self, examples):\n        \"\"\"Prepare training data (query, positive, optional negative)\"\"\"\n        train_examples = []\n        for ex in examples:\n            if \"negative\" in ex:\n                train_examples.append(InputExample(texts=[ex[\"query\"], ex[\"positive\"], ex[\"negative\"]]))\n            else:\n                train_examples.append(InputExample(texts=[ex[\"query\"], ex[\"positive\"]], label=1.0))\n        return DataLoader(train_examples, shuffle=True, batch_size=16)\n\n    def fine_tune(self, train_dataloader, num_epochs=3, loss_function=\"cosine\", warmup_steps=100):\n        \"\"\"Fine-tune with cosine, triplet, or contrastive loss\"\"\"\n        if loss_function == \"cosine\":\n            train_loss = losses.CosineSimilarityLoss(self.model)\n        elif loss_function == \"triplet\":\n            train_loss = losses.TripletLoss(model=self.model, triplet_margin=0.5)\n        elif loss_function == \"contrastive\":\n            train_loss = losses.ContrastiveLoss(self.model)\n\n        self.model.fit(\n            train_objectives=[(train_dataloader, train_loss)],\n            epochs=num_epochs, warmup_steps=warmup_steps,\n            optimizer_params={\"lr\": 2e-5}, show_progress_bar=True\n        )\n\n    def save_model(self, output_path):\n        self.model.save(output_path)\n\n# Usage example\ntraining_data = [\n    {\"query\": \"comfortable running shoes\", \"positive\": \"Nike Air Zoom - cushioning for running\",\n     \"negative\": \"Nike Basketball Shoes - high-top for court\"},\n]\nfinetuner = EmbeddingFineTuner(base_model_name=\"all-mpnet-base-v2\")\nprint(f\"Fine-tuner initialized with model: {finetuner.base_model_name}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFine-tuner initialized with model: all-mpnet-base-v2\n```\n:::\n:::\n\n\n:::{.callout-important}\n## Fine-Tuning Pitfalls\nCommon mistakes that tank fine-tuning performance:\n1. **Insufficient data**: Need 10K+ examples minimum, 100K+ for best results\n2. **Poor negative sampling**: Random negatives too easy; model doesn't learn distinction\n3. **Catastrophic forgetting**: Fine-tuning destroys general capabilities; use lower learning rates\n4. **Overfitting to training distribution**: Test on out-of-distribution examples\n:::\n\n## Domain-Specific Embedding Requirements\n\nGeneric embeddings optimize for average performance across diverse tasks. Domain-specific embeddings optimize for your specific requirements. Understanding and articulating these requirements is critical for successful custom embedding development.\n\n### Taxonomy of Domain-Specific Requirements\n\n**1. Semantic Granularity**\n\nHow fine-grained must similarity be?\n\n```python\nclass SemanticGranularity:\n    \"\"\"\n    Examples of semantic granularity requirements across domains\n    \"\"\"\n\n    COARSE = {\n        'name': 'Coarse-grained',\n        'example': 'News article categorization',\n        'requirement': 'Distinguish broad topics (sports vs. politics vs. technology)',\n        'embedding_dim': '128-256 sufficient',\n        'training_data': '10K-100K examples'\n    }\n\n    MEDIUM = {\n        'name': 'Medium-grained',\n        'example': 'E-commerce product search',\n        'requirement': 'Distinguish product types and attributes (running shoes vs. hiking boots)',\n        'embedding_dim': '256-512 recommended',\n        'training_data': '100K-1M examples'\n    }\n\n    FINE = {\n        'name': 'Fine-grained',\n        'example': 'Legal document retrieval',\n        'requirement': 'Distinguish subtle legal distinctions (contract types, precedent applicability)',\n        'embedding_dim': '512-768 recommended',\n        'training_data': '1M-10M examples'\n    }\n\n    ULTRA_FINE = {\n        'name': 'Ultra-fine',\n        'example': 'Molecular drug discovery',\n        'requirement': 'Distinguish molecules with minor structural differences that dramatically affect properties',\n        'embedding_dim': '768-1024+ required',\n        'training_data': '10M+ examples or sophisticated augmentation'\n    }\n```\n\n**The Granularity-Dimension Relationship**: Finer semantic distinctions require higher-dimensional embeddings. You cannot reliably distinguish 10,000 fine-grained categories in 128 dimensions—the information simply doesn't fit.\n\n**2. Asymmetric Similarity**\n\nAre similarities symmetric or asymmetric?\n\n```python\nclass AsymmetricSimilarity:\n    \"\"\"\n    Handle asymmetric similarity (query → document differs from document → query)\n    \"\"\"\n\n    def __init__(self, embedding_dim=512):\n        self.query_encoder = QueryEncoder(embedding_dim)\n        self.document_encoder = DocumentEncoder(embedding_dim)\n\n    def encode_query(self, query_text):\n        \"\"\"\n        Encode query with query-specific model\n        Queries are typically short, focused, and incomplete\n        \"\"\"\n        return self.query_encoder.encode(query_text)\n\n    def encode_document(self, document_text):\n        \"\"\"\n        Encode document with document-specific model\n        Documents are longer, complete, and information-rich\n        \"\"\"\n        return self.document_encoder.encode(document_text)\n\n    def similarity(self, query_embedding, document_embedding):\n        \"\"\"\n        Asymmetric similarity: query → document\n        \"\"\"\n        # In asymmetric setup, similarity is directional\n        # \"running shoes\" → \"Nike Air Zoom Pegasus...\" (HIGH similarity)\n        # \"Nike Air Zoom Pegasus...\" → \"running shoes\" (LOWER similarity - too specific)\n\n        return cosine_similarity(query_embedding, document_embedding)\n\n\n# Use cases requiring asymmetric similarity:\nasymmetric_use_cases = [\n    {\n        'domain': 'Question Answering',\n        'query': 'Short question',\n        'target': 'Long passage with answer',\n        'asymmetry': 'Question seeks answer; answer does not seek question'\n    },\n    {\n        'domain': 'Web Search',\n        'query': '2-5 keywords',\n        'target': 'Full web page content',\n        'asymmetry': 'Query is intent; document is content'\n    },\n    {\n        'domain': 'Image Search',\n        'query': 'Text description',\n        'target': 'Image',\n        'asymmetry': 'Cross-modal: text → image different from image → text'\n    },\n    {\n        'domain': 'Recommendation',\n        'query': 'User behavior history',\n        'target': 'Product catalog',\n        'asymmetry': 'User history implies preferences; products have features'\n    }\n]\n```\n\n**Why Asymmetric Matters**: Using symmetric embeddings (same encoder for queries and documents) for asymmetric tasks leaves performance on the table. Specialized encoders can optimize for each side's characteristics.\n\n**3. Multi-Faceted Similarity**\n\nDo items have multiple aspects of similarity?\n\n```python\nclass MultiFacetedEmbeddings:\n    \"\"\"\n    Represent multiple facets of similarity in separate embedding spaces\n    \"\"\"\n\n    def __init__(self):\n        # E-commerce example: products similar in different ways\n        self.visual_encoder = VisualEncoder()  # Visual appearance\n        self.functional_encoder = FunctionalEncoder()  # Use case/function\n        self.attribute_encoder = AttributeEncoder()  # Specific attributes (brand, price, etc.)\n\n    def encode_product(self, product):\n        \"\"\"\n        Encode product with multiple faceted embeddings\n        \"\"\"\n        return {\n            'visual': self.visual_encoder.encode(product.images),\n            'functional': self.functional_encoder.encode(product.description),\n            'attributes': self.attribute_encoder.encode({\n                'brand': product.brand,\n                'price_tier': self.discretize_price(product.price),\n                'category': product.category\n            })\n        }\n\n    def multi_faceted_search(self, query, facet_weights=None):\n        \"\"\"\n        Search using multiple facets with different weights\n        \"\"\"\n        if facet_weights is None:\n            facet_weights = {'visual': 0.4, 'functional': 0.4, 'attributes': 0.2}\n\n        # Encode query (may not have all facets)\n        query_embs = self.encode_query(query)\n\n        # Search each facet independently\n        results_by_facet = {}\n        for facet in query_embs:\n            results_by_facet[facet] = self.search_facet(\n                query_embs[facet],\n                facet_index=getattr(self, f'{facet}_index')\n            )\n\n        # Combine results with weighted fusion\n        final_results = self.fuse_facet_results(\n            results_by_facet,\n            weights=facet_weights\n        )\n\n        return final_results\n```\n\n**Multi-Faceted Use Cases**:\n\n- **E-commerce**: Visual similarity (looks like), functional similarity (used for same purpose), price similarity\n- **Movies**: Genre similarity, cast similarity, theme similarity, visual style similarity\n- **Scientific papers**: Topic similarity, methodology similarity, citation network similarity\n- **Recipes**: Ingredient similarity, cuisine similarity, difficulty similarity, taste profile similarity\n\n**4. Temporal Dynamics**\n\nDoes similarity change over time?\n\n```python\nclass TemporalEmbeddings:\n    \"\"\"\n    Handle time-varying embeddings\n    \"\"\"\n\n    def __init__(self, embedding_dim=512, time_encoding_dim=64):\n        self.static_encoder = StaticEncoder(embedding_dim - time_encoding_dim)\n        self.time_encoder = TimeEncoder(time_encoding_dim)\n        self.embedding_dim = embedding_dim\n\n    def encode_with_time(self, content, timestamp):\n        \"\"\"\n        Encode content with temporal context\n        \"\"\"\n        # Static content embedding\n        static_emb = self.static_encoder.encode(content)\n\n        # Time encoding (positional encoding or learned)\n        time_emb = self.time_encoder.encode(timestamp)\n\n        # Concatenate\n        temporal_emb = torch.cat([static_emb, time_emb], dim=-1)\n\n        return temporal_emb\n\n    def time_decayed_similarity(self, query_time, document_time, document_emb):\n        \"\"\"\n        Adjust similarity based on temporal distance\n        \"\"\"\n        time_diff_days = abs((query_time - document_time).days)\n\n        # Exponential decay: more recent = more relevant\n        decay_factor = np.exp(-time_diff_days / 180)  # 180-day half-life\n\n        return document_emb * decay_factor\n\n\n# Domains requiring temporal awareness:\ntemporal_use_cases = [\n    {\n        'domain': 'News Search',\n        'requirement': 'Recent articles more relevant for most queries',\n        'approach': 'Time decay on similarity scores'\n    },\n    {\n        'domain': 'Social Media',\n        'requirement': 'Trending topics change rapidly',\n        'approach': 'Short-window embeddings, frequent retraining'\n    },\n    {\n        'domain': 'Fashion/Trends',\n        'requirement': 'Style similarity depends on current trends',\n        'approach': 'Time-conditioned embeddings, seasonal retraining'\n    },\n    {\n        'domain': 'Scientific Research',\n        'requirement': 'Paradigm shifts change what\\'s similar',\n        'approach': 'Period-specific embeddings (pre/post major discoveries)'\n    }\n]\n```\n\n**5. Hierarchical Structure**\n\nDo your items have natural hierarchies?\n\n```python\nclass HierarchicalEmbeddings:\n    \"\"\"\n    Preserve hierarchical structure in embedding space\n    \"\"\"\n\n    def __init__(self):\n        self.level_encoders = {\n            'category': Encoder(dim=256),    # Coarse level\n            'subcategory': Encoder(dim=512),  # Medium level\n            'product': Encoder(dim=768)       # Fine level\n        }\n\n    def encode_hierarchical(self, item, level='product'):\n        \"\"\"\n        Encode at different hierarchy levels\n\n        Example:\n          Category: \"Electronics\"\n          Subcategory: \"Smartphones\"\n          Product: \"iPhone 15 Pro Max 256GB\"\n        \"\"\"\n        embeddings = {}\n\n        # Encode at each level in hierarchy\n        for level_name in ['category', 'subcategory', 'product']:\n            if level_name in item:\n                embeddings[level_name] = self.level_encoders[level_name].encode(\n                    item[level_name]\n                )\n\n            # Stop at requested level\n            if level_name == level:\n                break\n\n        return embeddings\n\n    def hierarchical_search(self, query, level='product'):\n        \"\"\"\n        Search at appropriate hierarchy level\n\n        Coarse queries (\"electronics\") match at category level\n        Fine queries (\"iphone 15 pro max\") match at product level\n        \"\"\"\n        # Classify query specificity\n        query_level = self.infer_query_level(query)\n\n        # Encode at appropriate level\n        query_emb = self.level_encoders[query_level].encode(query)\n\n        # Search at that level\n        results = self.search_at_level(query_emb, level=query_level)\n\n        return results\n```\n\n### Domain-Specific Training Objectives\n\nDifferent domains require different training objectives:\n\n::: {.cell execution_count=2}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show domain-specific training objectives\"}\nimport torch\nimport torch.nn.functional as F\n\nclass DomainSpecificObjectives:\n    \"\"\"Domain-specific training objectives beyond standard contrastive learning\"\"\"\n\n    def ranking_loss(self, query_emb, doc_embs, relevance_labels):\n        \"\"\"Ranking loss: Learn to order documents by relevance\"\"\"\n        scores = torch.matmul(query_emb, doc_embs.T)\n        loss = 0\n        for i in range(len(doc_embs)):\n            for j in range(i + 1, len(doc_embs)):\n                if relevance_labels[i] > relevance_labels[j]:\n                    loss += torch.clamp(1.0 - (scores[i] - scores[j]), min=0.0)\n        return loss / (len(doc_embs) * (len(doc_embs) - 1) / 2)\n\n    def attribute_preservation_loss(self, embedding, attributes):\n        \"\"\"Ensure embeddings preserve important attributes (category, brand, price)\"\"\"\n        losses = []\n        for attr_name, attr_value in attributes.items():\n            attr_classifier = self.attribute_classifiers[attr_name]\n            pred = attr_classifier(embedding)\n            loss = F.cross_entropy(pred, attr_value)\n            losses.append(loss)\n        return sum(losses)\n\n    def diversity_loss(self, embeddings):\n        \"\"\"Encourage embedding diversity (avoid collapse)\"\"\"\n        pairwise_sim = torch.matmul(embeddings, embeddings.T)\n        mask = ~torch.eye(len(embeddings), dtype=torch.bool)\n        return pairwise_sim[mask].mean()\n\n# Usage example\nobjectives = DomainSpecificObjectives()\nprint(\"Domain objectives: ranking, attribute preservation, diversity, cross-domain alignment\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDomain objectives: ranking, attribute preservation, diversity, cross-domain alignment\n```\n:::\n:::\n\n\n## Multi-Objective Embedding Design\n\nMost real-world embedding systems must optimize for multiple objectives simultaneously. Single-objective optimization leaves performance on the table.\n\n### The Multi-Objective Challenge\n\nConsider an e-commerce search system. The embedding should:\n1. **Semantic relevance**: Match customer intent\n2. **Attribute accuracy**: Preserve product attributes (category, brand, price)\n3. **Personalization**: Adapt to user preferences\n4. **Business metrics**: Optimize for conversion, revenue, not just clicks\n5. **Diversity**: Avoid filter bubbles, show variety\n\nOptimizing for one objective often degrades others. Multi-objective design balances these trade-offs.\n\n### Multi-Objective Architecture Patterns\n\n**Pattern 1: Multi-Task Learning**\n\nTrain single model with multiple heads:\n\n```python\nimport torch\nimport torch.nn as nn\n\nclass MultiTaskEmbeddingModel(nn.Module):\n    \"\"\"\n    Single encoder with multiple task-specific heads\n    \"\"\"\n\n    def __init__(self, embedding_dim=512, num_categories=1000, num_brands=5000):\n        super().__init__()\n\n        # Shared encoder (e.g., transformer)\n        self.shared_encoder = TransformerEncoder(\n            dim=embedding_dim,\n            depth=6,\n            heads=8\n        )\n\n        # Task-specific heads\n        self.similarity_head = nn.Linear(embedding_dim, embedding_dim)  # For similarity search\n        self.category_head = nn.Linear(embedding_dim, num_categories)   # Category classification\n        self.brand_head = nn.Linear(embedding_dim, num_brands)          # Brand classification\n        self.price_head = nn.Linear(embedding_dim, 1)                   # Price regression\n\n    def forward(self, input_ids, attention_mask):\n        \"\"\"\n        Forward pass through shared encoder\n        \"\"\"\n        # Shared representation\n        hidden_state = self.shared_encoder(input_ids, attention_mask)\n        pooled = hidden_state.mean(dim=1)  # Average pooling\n\n        # Task-specific outputs\n        outputs = {\n            'embedding': self.similarity_head(pooled),\n            'category_logits': self.category_head(pooled),\n            'brand_logits': self.brand_head(pooled),\n            'price_pred': self.price_head(pooled)\n        }\n\n        return outputs\n\n    def compute_loss(self, outputs, targets, task_weights):\n        \"\"\"\n        Weighted multi-task loss\n        \"\"\"\n        losses = {}\n\n        # Similarity loss (contrastive or triplet)\n        if 'positive' in targets and 'negative' in targets:\n            pos_sim = F.cosine_similarity(outputs['embedding'], targets['positive'])\n            neg_sim = F.cosine_similarity(outputs['embedding'], targets['negative'])\n            losses['similarity'] = torch.clamp(1.0 - pos_sim + neg_sim, min=0.0).mean()\n\n        # Category classification loss\n        if 'category' in targets:\n            losses['category'] = F.cross_entropy(\n                outputs['category_logits'],\n                targets['category']\n            )\n\n        # Brand classification loss\n        if 'brand' in targets:\n            losses['brand'] = F.cross_entropy(\n                outputs['brand_logits'],\n                targets['brand']\n            )\n\n        # Price regression loss\n        if 'price' in targets:\n            losses['price'] = F.mse_loss(\n                outputs['price_pred'].squeeze(),\n                targets['price']\n            )\n\n        # Weighted combination\n        total_loss = sum(\n            task_weights.get(task, 1.0) * loss\n            for task, loss in losses.items()\n        )\n\n        return total_loss, losses\n\n\n# Training with multi-task learning\nmodel = MultiTaskEmbeddingModel(embedding_dim=512)\n\n# Task weights (tune based on importance)\ntask_weights = {\n    'similarity': 1.0,   # Core task\n    'category': 0.3,     # Help preserve category info\n    'brand': 0.2,        # Help preserve brand info\n    'price': 0.1         # Weak signal for price tier\n}\n\n# Training loop\noptimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n\nfor batch in train_loader:\n    outputs = model(batch['input_ids'], batch['attention_mask'])\n\n    loss, task_losses = model.compute_loss(\n        outputs,\n        targets=batch['targets'],\n        task_weights=task_weights\n    )\n\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n```\n\n**Pattern 2: Multi-Vector Representations**\n\nUse separate embeddings for different objectives:\n\n```python\nclass MultiVectorEmbedding:\n    \"\"\"\n    Represent items with multiple specialized embeddings\n    \"\"\"\n\n    def __init__(self):\n        # Different encoders for different aspects\n        self.semantic_encoder = SemanticEncoder(dim=512)     # Semantic meaning\n        self.structural_encoder = StructuralEncoder(dim=256)  # Structured attributes\n        self.behavioral_encoder = BehavioralEncoder(dim=256)  # User interaction patterns\n\n    def encode(self, item, user_context=None):\n        \"\"\"\n        Create multi-vector representation\n        \"\"\"\n        vectors = {}\n\n        # Semantic vector: text content\n        vectors['semantic'] = self.semantic_encoder.encode(\n            item['title'] + ' ' + item['description']\n        )\n\n        # Structural vector: categorical attributes\n        vectors['structural'] = self.structural_encoder.encode({\n            'category': item['category'],\n            'brand': item['brand'],\n            'price_tier': self.discretize_price(item['price']),\n            'rating': item['avg_rating']\n        })\n\n        # Behavioral vector: how users interact with this item\n        if 'user_interactions' in item:\n            vectors['behavioral'] = self.behavioral_encoder.encode(\n                item['user_interactions']\n            )\n\n        return vectors\n\n    def search(self, query, user_context=None, objective='balanced'):\n        \"\"\"\n        Search with different objectives\n        \"\"\"\n        # Encode query with multiple vectors\n        query_vectors = self.encode_query(query, user_context)\n\n        # Different objectives use different vector combinations\n        if objective == 'relevance':\n            # Focus on semantic similarity\n            weights = {'semantic': 1.0, 'structural': 0.2, 'behavioral': 0.1}\n        elif objective == 'personalization':\n            # Focus on behavioral patterns\n            weights = {'semantic': 0.3, 'structural': 0.2, 'behavioral': 1.0}\n        elif objective == 'balanced':\n            # Balance all factors\n            weights = {'semantic': 0.5, 'structural': 0.3, 'behavioral': 0.2}\n        elif objective == 'exploration':\n            # Emphasize diversity (structural differences)\n            weights = {'semantic': 0.3, 'structural': 0.7, 'behavioral': 0.1}\n\n        # Search each vector space\n        results_by_vector = {}\n        for vector_type, query_vec in query_vectors.items():\n            results_by_vector[vector_type] = self.search_vector_space(\n                query_vec,\n                vector_space=vector_type\n            )\n\n        # Combine results with objective-specific weights\n        final_results = self.weighted_fusion(results_by_vector, weights)\n\n        return final_results\n```\n\n**Pattern 3: Composite Objectives with Constraints**\n\nOptimize primary objective subject to constraints:\n\n::: {.cell execution_count=3}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show constrained embedding objective\"}\nclass ConstrainedEmbeddingObjective:\n    \"\"\"Optimize embeddings with hard constraints\"\"\"\n\n    def __init__(self):\n        self.primary_objective = \"relevance\"\n        self.constraints = [\n            {\"type\": \"diversity\", \"threshold\": 0.3},   # Min 30% diversity\n            {\"type\": \"freshness\", \"threshold\": 0.5},   # Min 50% from last 30 days\n            {\"type\": \"price_range\", \"threshold\": 0.2}, # Min 20% price range coverage\n        ]\n\n    def search_with_constraints(self, query, k=20):\n        \"\"\"Retrieve results satisfying constraints\"\"\"\n        candidates = self.retrieve_candidates(query, k=k * 10)  # 10x oversampling\n        return self.constrained_reranking(candidates, self.constraints, k)\n\n    def constrained_reranking(self, candidates, constraints, k):\n        \"\"\"Rerank candidates to satisfy constraints while maximizing relevance\"\"\"\n        selected, remaining = [], candidates.copy()\n        while len(selected) < k and remaining:\n            best_candidate, best_score = None, -float(\"inf\")\n            for candidate in remaining:\n                temp_selected = selected + [candidate]\n                if self.satisfies_constraints(temp_selected, constraints):\n                    if candidate[\"relevance_score\"] > best_score:\n                        best_candidate, best_score = candidate, candidate[\"relevance_score\"]\n            if best_candidate:\n                selected.append(best_candidate)\n                remaining.remove(best_candidate)\n            else:\n                break\n        return selected\n\n    def satisfies_constraints(self, selected, constraints):\n        \"\"\"Check if selected results satisfy all constraints\"\"\"\n        for c in constraints:\n            if c[\"type\"] == \"diversity\" and self.compute_diversity(selected) < c[\"threshold\"]:\n                return False\n        return True\n\n# Usage example\nconstrained = ConstrainedEmbeddingObjective()\nprint(f\"Constraints: {[c['type'] for c in constrained.constraints]}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nConstraints: ['diversity', 'freshness', 'price_range']\n```\n:::\n:::\n\n\n### Balancing Trade-offs: The Pareto Frontier\n\nMulti-objective optimization involves trade-offs. Visualize and navigate the Pareto frontier:\n\n::: {.cell execution_count=4}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show multi-objective optimization\"}\nclass MultiObjectiveOptimization:\n    \"\"\"Navigate trade-offs between multiple objectives\"\"\"\n\n    def compute_pareto_frontier(self, models, test_data):\n        \"\"\"Compute Pareto frontier across objectives\"\"\"\n        evaluations = []\n        for model in models:\n            metrics = {\n                \"model\": model,\n                \"relevance\": self.evaluate_relevance(model, test_data),\n                \"diversity\": self.evaluate_diversity(model, test_data),\n                \"personalization\": self.evaluate_personalization(model, test_data),\n                \"business_metrics\": self.evaluate_business(model, test_data),\n            }\n            evaluations.append(metrics)\n\n        # Find Pareto-optimal models (not dominated by any other)\n        pareto_optimal = []\n        for eval_i in evaluations:\n            dominated = False\n            for eval_j in evaluations:\n                if eval_i != eval_j and self.dominates(eval_j, eval_i):\n                    dominated = True\n                    break\n            if not dominated:\n                pareto_optimal.append(eval_i)\n        return pareto_optimal\n\n    def dominates(self, eval_a, eval_b):\n        \"\"\"Check if eval_a dominates eval_b (better on all objectives)\"\"\"\n        objectives = [\"relevance\", \"diversity\", \"personalization\", \"business_metrics\"]\n        better_on_at_least_one = False\n        for obj in objectives:\n            if eval_a[obj] < eval_b[obj]:\n                return False\n            if eval_a[obj] > eval_b[obj]:\n                better_on_at_least_one = True\n        return better_on_at_least_one\n\n    def select_operating_point(self, pareto_frontier, business_priorities):\n        \"\"\"Select model from Pareto frontier based on business priorities\"\"\"\n        best_model, best_score = None, -float(\"inf\")\n        for eval_point in pareto_frontier:\n            weighted_score = sum(\n                business_priorities.get(obj, 0) * eval_point[obj]\n                for obj in [\"relevance\", \"diversity\", \"personalization\", \"business_metrics\"]\n            )\n            if weighted_score > best_score:\n                best_score, best_model = weighted_score, eval_point[\"model\"]\n        return best_model\n\n# Usage example\noptimizer = MultiObjectiveOptimization()\nprint(\"Multi-objective: relevance, diversity, personalization, business metrics\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMulti-objective: relevance, diversity, personalization, business metrics\n```\n:::\n:::\n\n\n## Embedding Dimensionality Optimization\n\nEmbedding dimensionality has profound impacts on performance, cost, and latency. Too low: information loss. Too high: computational waste and overfitting. Finding the optimal dimensionality is critical for production systems.\n\n### The Dimensionality Trade-off\n\n| Dimension | Storage (100B embeddings) | QPS (single server) | Pros | Cons |\n|-----------|---------------------------|---------------------|------|------|\n| 128 | 48 TB | 50,000 | Extremely fast, cheap | Limited capacity |\n| 256 | 96 TB | 35,000 | Good balance | May lose fine-grained information |\n| 512 | 192 TB | 18,000 | High capacity | 2x cost vs. 256 |\n| 768 | 288 TB | 12,000 | BERT standard | 3x cost vs. 256 |\n| 1024 | 384 TB | 9,000 | Maximum capacity | 4x cost, often overkill |\n\n### Determining Optimal Dimensionality\n\n**Method 1: Empirical Evaluation**\n\n::: {.cell execution_count=5}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show dimensionality experiment\"}\nimport pandas as pd\n\nclass DimensionalityExperiment:\n    \"\"\"Systematically evaluate different embedding dimensions\"\"\"\n\n    def run_dimensionality_sweep(self, train_data, test_data, dimensions=None):\n        \"\"\"Train models at different dimensions and evaluate\"\"\"\n        if dimensions is None:\n            dimensions = [128, 256, 384, 512, 768]\n        results = []\n\n        for dim in dimensions:\n            model = self.train_model(train_data, embedding_dim=dim)\n            metrics = self.evaluate_model(model, test_data)\n            storage_gb = self.estimate_storage(dim, num_embeddings=100_000_000)\n            latency_ms = self.measure_latency(model)\n\n            results.append({\n                \"dimension\": dim, \"recall@10\": metrics[\"recall@10\"], \"mrr\": metrics[\"mrr\"],\n                \"storage_gb\": storage_gb, \"p99_latency_ms\": latency_ms,\n            })\n        return pd.DataFrame(results)\n\n    def find_optimal_dimension(self, results, quality_threshold=0.95):\n        \"\"\"Find smallest dimension meeting quality threshold\"\"\"\n        max_recall = results[\"recall@10\"].max()\n        results[\"normalized_quality\"] = results[\"recall@10\"] / max_recall\n        acceptable = results[results[\"normalized_quality\"] >= quality_threshold]\n        if acceptable.empty:\n            return results.loc[results[\"recall@10\"].idxmax(), \"dimension\"]\n        return acceptable.loc[acceptable[\"dimension\"].idxmin(), \"dimension\"]\n\n# Example results:\n# | Dim  | Recall@10 | Storage | Quality |\n# |------|-----------|---------|---------|\n# | 128  | 0.834     | 48 GB   | 0.909   |\n# | 256  | 0.891     | 96 GB   | 0.972   |\n# | 384  | 0.908     | 144 GB  | 0.991   | ← Optimal (99.1% quality, 50% cheaper than 768)\n# | 512  | 0.915     | 192 GB  | 0.998   |\n# | 768  | 0.917     | 288 GB  | 1.000   |\nexperiment = DimensionalityExperiment()\nprint(\"Dimensions to test: [128, 256, 384, 512, 768]\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDimensions to test: [128, 256, 384, 512, 768]\n```\n:::\n:::\n\n\n**Method 2: Intrinsic Dimensionality Estimation**\n\nEstimate the intrinsic dimensionality of your data:\n\n::: {.cell execution_count=6}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show intrinsic dimensionality estimation\"}\nimport numpy as np\nfrom sklearn.decomposition import PCA\nfrom sklearn.neighbors import NearestNeighbors\n\nclass IntrinsicDimensionality:\n    \"\"\"Estimate intrinsic dimensionality of embedding space\"\"\"\n\n    def estimate_via_pca(self, embeddings, variance_threshold=0.95):\n        \"\"\"Use PCA to find dimensions capturing X% of variance\"\"\"\n        pca = PCA()\n        pca.fit(embeddings)\n        cumsum_variance = np.cumsum(pca.explained_variance_ratio_)\n        n_components = np.argmax(cumsum_variance >= variance_threshold) + 1\n        return {\"intrinsic_dimension\": n_components, \"variance_captured\": cumsum_variance[n_components - 1]}\n\n    def estimate_via_mle(self, embeddings, k=10):\n        \"\"\"MLE estimation (Levina & Bickel 2004)\"\"\"\n        nbrs = NearestNeighbors(n_neighbors=k + 1).fit(embeddings)\n        distances, _ = nbrs.kneighbors(embeddings)\n        distances = distances[:, 1:]  # Remove self\n        dimensions = []\n        for dist_vec in distances:\n            r_k = dist_vec[-1]\n            if r_k > 0:\n                log_ratios = np.log(r_k / dist_vec[:-1])\n                if log_ratios.sum() > 0:\n                    dimensions.append((k - 1) / log_ratios.sum())\n        return {\"intrinsic_dimension\": int(np.median(dimensions))}\n\n# Usage example\nembeddings = np.random.randn(1000, 768).astype(np.float32)\nestimator = IntrinsicDimensionality()\npca_result = estimator.estimate_via_pca(embeddings, variance_threshold=0.95)\nprint(f\"PCA estimate: {pca_result['intrinsic_dimension']} dims capture 95% variance\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPCA estimate: 526 dims capture 95% variance\n```\n:::\n:::\n\n\n**Method 3: Progressive Dimensionality Reduction**\n\nTrain high-dimensional model, then compress:\n\n::: {.cell execution_count=7}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show progressive dimension reduction\"}\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass ProgressiveDimensionReduction:\n    \"\"\"Start with high dimensions, progressively reduce while monitoring quality\"\"\"\n\n    def __init__(self, base_model, original_dim=768):\n        self.base_model = base_model\n        self.original_dim = original_dim\n\n    def train_projection(self, embeddings, target_dim):\n        \"\"\"Learn projection from high-dim to low-dim\"\"\"\n        projection_net = nn.Linear(self.original_dim, target_dim)\n        optimizer = torch.optim.Adam(projection_net.parameters(), lr=1e-3)\n\n        for _epoch in range(10):\n            idx1 = torch.randint(0, len(embeddings), (1000,))\n            idx2 = torch.randint(0, len(embeddings), (1000,))\n            orig_sim = F.cosine_similarity(embeddings[idx1], embeddings[idx2])\n            proj_sim = F.cosine_similarity(projection_net(embeddings[idx1]), projection_net(embeddings[idx2]))\n            loss = F.mse_loss(proj_sim, orig_sim)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n        return projection_net\n\n    def find_minimal_dimension(self, embeddings, test_data, quality_threshold=0.95):\n        \"\"\"Binary search for minimal dimension meeting quality threshold\"\"\"\n        original_quality = self.evaluate(self.base_model, test_data)\n        target_quality = original_quality * quality_threshold\n        low, high, best_dim = 64, self.original_dim, self.original_dim\n\n        while low <= high:\n            mid = (low + high) // 2\n            projection = self.train_projection(embeddings, target_dim=mid)\n            quality = self.evaluate_with_projection(self.base_model, projection, test_data)\n            if quality >= target_quality:\n                best_dim, high = mid, mid - 1\n            else:\n                low = mid + 1\n        return best_dim\n\n# Usage example\nprint(\"Progressive reduction: 768 → find minimal dim maintaining 95% quality\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nProgressive reduction: 768 → find minimal dim maintaining 95% quality\n```\n:::\n:::\n\n\n### Dimension-Specific Optimizations\n\nDifferent dimensions enable different optimizations:\n\n**Ultra-Low Dimensions (64-128): Binary/Hamming Embeddings**\n\n::: {.cell execution_count=8}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show binary embeddings for ultra-compression\"}\nimport numpy as np\n\nclass BinaryEmbeddings:\n    \"\"\"Ultra-compressed binary embeddings for massive scale\"\"\"\n\n    def binarize(self, embeddings):\n        \"\"\"\n        Convert float embeddings to binary\n        768-dim float32 → 96 bytes\n        768-dim binary → 96 bits = 12 bytes (8x compression)\n        \"\"\"\n        binary = (embeddings > 0).astype(np.uint8)\n        return np.packbits(binary, axis=1)\n\n    def hamming_similarity(self, binary1, binary2):\n        \"\"\"Ultra-fast similarity using Hamming distance\"\"\"\n        xor = np.bitwise_xor(binary1, binary2)\n        hamming_dist = np.unpackbits(xor).sum()\n        max_dist = len(binary1) * 8\n        return 1 - (hamming_dist / max_dist)\n\n# Usage example\nembeddings = np.random.randn(100, 768).astype(np.float32)\nbinary_emb = BinaryEmbeddings()\npacked = binary_emb.binarize(embeddings)\nprint(f\"Original: {embeddings.nbytes:,} bytes → Binary: {packed.nbytes:,} bytes ({embeddings.nbytes/packed.nbytes:.0f}x compression)\")\n# Binary enables: 8x compression, 10-100x faster search via POPCOUNT\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nOriginal: 307,200 bytes → Binary: 9,600 bytes (32x compression)\n```\n:::\n:::\n\n\n## Cost-Performance Trade-offs at Scale\n\nAt trillion-row scale, the cost-performance trade-off becomes the dominant factor in embedding design. This section provides frameworks for optimizing this trade-off.\n\n### Total Cost of Ownership (TCO) Model\n\n```python\nclass EmbeddingTCO:\n    \"\"\"\n    Comprehensive TCO model for embedding systems\n    \"\"\"\n\n    def __init__(self):\n        # Cloud pricing (approximate, as of 2024)\n        self.storage_cost_per_gb_month = 0.023  # S3 standard\n        self.compute_cost_per_hour = 3.0  # A100 GPU\n        self.inference_cost_per_million = 10.0  # Vector DB queries\n\n    def calculate_tco(self, config, duration_years=3):\n        \"\"\"\n        Calculate total cost of ownership\n\n        Args:\n            config: {\n                'num_embeddings': 100_000_000_000,\n                'embedding_dim': 768,\n                'qps': 10_000,\n                'training_frequency_per_year': 4,\n                'team_size': 10\n            }\n        \"\"\"\n\n        # Component 1: Storage\n        storage_cost = self.compute_storage_cost(\n            config['num_embeddings'],\n            config['embedding_dim'],\n            duration_years\n        )\n\n        # Component 2: Training\n        training_cost = self.compute_training_cost(\n            config['num_embeddings'],\n            config['training_frequency_per_year'],\n            duration_years\n        )\n\n        # Component 3: Inference\n        inference_cost = self.compute_inference_cost(\n            config['qps'],\n            duration_years\n        )\n\n        # Component 4: Engineering team\n        team_cost = self.compute_team_cost(\n            config['team_size'],\n            duration_years\n        )\n\n        # Total\n        total_cost = (\n            storage_cost +\n            training_cost +\n            inference_cost +\n            team_cost\n        )\n\n        return {\n            'total_cost_3_years': total_cost,\n            'annual_cost': total_cost / duration_years,\n            'breakdown': {\n                'storage': storage_cost,\n                'training': training_cost,\n                'inference': inference_cost,\n                'team': team_cost\n            },\n            'cost_per_embedding': total_cost / config['num_embeddings'],\n            'cost_per_million_queries': inference_cost / (\n                config['qps'] * 60 * 60 * 24 * 365 * duration_years / 1_000_000\n            )\n        }\n\n    def compute_storage_cost(self, num_embeddings, dim, duration_years):\n        \"\"\"Storage cost with replication and indexing overhead\"\"\"\n        bytes_per_embedding = dim * 4  # float32\n        total_bytes = num_embeddings * bytes_per_embedding\n\n        # Index overhead (HNSW adds ~50%)\n        indexed_bytes = total_bytes * 1.5\n\n        # Replication (3x for availability)\n        replicated_bytes = indexed_bytes * 3\n\n        # Convert to GB\n        total_gb = replicated_bytes / (1024 ** 3)\n\n        # Monthly cost\n        monthly_cost = total_gb * self.storage_cost_per_gb_month\n\n        # Total over duration\n        return monthly_cost * 12 * duration_years\n\n    def optimize_for_budget(self, requirements, budget_annual):\n        \"\"\"\n        Given requirements and budget, find optimal configuration\n        \"\"\"\n        # Requirements: {'num_embeddings', 'qps', 'min_quality'}\n        # Budget: annual spending limit\n\n        # Explore dimension options\n        dimensions = [128, 256, 384, 512, 768]\n        configs = []\n\n        for dim in dimensions:\n            config = {\n                'num_embeddings': requirements['num_embeddings'],\n                'embedding_dim': dim,\n                'qps': requirements['qps'],\n                'training_frequency_per_year': 4,\n                'team_size': 10\n            }\n\n            tco = self.calculate_tco(config, duration_years=1)\n\n            # Estimate quality (simplified)\n            quality_score = self.estimate_quality(dim, requirements)\n\n            configs.append({\n                'dimension': dim,\n                'annual_cost': tco['annual_cost'],\n                'quality_score': quality_score,\n                'within_budget': tco['annual_cost'] <= budget_annual\n            })\n\n        # Filter to budget\n        viable = [c for c in configs if c['within_budget']]\n\n        if not viable:\n            return {\n                'recommendation': 'INSUFFICIENT_BUDGET',\n                'message': f\"Minimum cost: ${min(c['annual_cost'] for c in configs):,.0f}/year\"\n            }\n\n        # Choose highest quality within budget\n        best = max(viable, key=lambda c: c['quality_score'])\n\n        return {\n            'recommendation': 'OPTIMAL_CONFIG',\n            'dimension': best['dimension'],\n            'annual_cost': best['annual_cost'],\n            'quality_score': best['quality_score'],\n            'configurations_evaluated': configs\n        }\n```\n\n### Performance-Cost Pareto Frontier\n\nNavigate the trade-off space:\n\n::: {.cell execution_count=9}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show cost-performance frontier analysis\"}\nclass CostPerformanceFrontier:\n    \"\"\"Explore cost-performance trade-offs\"\"\"\n\n    def generate_configuration_space(self, requirements):\n        \"\"\"Generate configurations spanning cost-performance space\"\"\"\n        configs = []\n        dimensions = [128, 256, 384, 512, 768, 1024]\n        quantizations = [\"float32\", \"float16\", \"int8\", \"binary\"]\n        index_types = [\"flat\", \"ivf\", \"hnsw\", \"pq\"]\n\n        for dim in dimensions:\n            for quant in quantizations:\n                for index in index_types:\n                    config = {\n                        \"dimension\": dim, \"quantization\": quant, \"index_type\": index,\n                        \"num_embeddings\": requirements[\"num_embeddings\"],\n                    }\n                    cost = self.estimate_cost(config)\n                    performance = self.estimate_performance(config)\n                    configs.append({\n                        **config, \"annual_cost\": cost,\n                        \"p99_latency_ms\": performance[\"latency\"], \"recall@10\": performance[\"recall\"],\n                    })\n        return configs\n\n    def find_pareto_optimal(self, configs):\n        \"\"\"Find Pareto-optimal configurations\"\"\"\n        pareto = []\n        for c in configs:\n            dominated = False\n            for other in configs:\n                if (other[\"recall@10\"] >= c[\"recall@10\"] and\n                    other[\"annual_cost\"] <= c[\"annual_cost\"] and\n                    other[\"p99_latency_ms\"] <= c[\"p99_latency_ms\"] and\n                    (other[\"recall@10\"] > c[\"recall@10\"] or\n                     other[\"annual_cost\"] < c[\"annual_cost\"] or\n                     other[\"p99_latency_ms\"] < c[\"p99_latency_ms\"])):\n                    dominated = True\n                    break\n            if not dominated:\n                pareto.append(c)\n        return pareto\n\n# Usage example\nfrontier = CostPerformanceFrontier()\nprint(\"Configuration space: 6 dims × 4 quantizations × 4 indices = 96 configs\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nConfiguration space: 6 dims × 4 quantizations × 4 indices = 96 configs\n```\n:::\n:::\n\n\n### Cost Optimization Strategies\n\n**Strategy 1: Tiered Embeddings**\n\nUse different dimensions for different data tiers:\n\n```python\nclass TieredEmbeddings:\n    \"\"\"\n    Different embedding dimensions for different data tiers\n    \"\"\"\n\n    def __init__(self):\n        self.hot_encoder = HighDimEncoder(dim=768)   # Frequent queries\n        self.warm_encoder = MediumDimEncoder(dim=384)  # Moderate queries\n        self.cold_encoder = LowDimEncoder(dim=128)    # Rare queries\n\n    def encode_with_tier(self, item, access_frequency):\n        \"\"\"\n        Encode with appropriate dimension based on access frequency\n        \"\"\"\n        if access_frequency > 1000:  # >1000 queries/day\n            # Hot tier: high quality, high cost justified\n            return self.hot_encoder.encode(item), 'hot'\n        elif access_frequency > 10:\n            # Warm tier: good quality, moderate cost\n            return self.warm_encoder.encode(item), 'warm'\n        else:\n            # Cold tier: acceptable quality, low cost\n            return self.cold_encoder.encode(item), 'cold'\n\n\n# Cost savings:\n# - 90% of embeddings in cold tier (128-dim): 83% storage savings\n# - 9% in warm tier (384-dim): 50% savings\n# - 1% in hot tier (768-dim): full quality\n# - Overall: ~80% storage cost reduction\n```\n\n## Key Takeaways\n\n- **The build vs. fine-tune decision follows a spectrum** from using frozen pre-trained models (Level 0) to training custom architectures from scratch (Level 4)—most organizations should target Level 3 (full fine-tuning) which delivers 95% of benefits at 20% of cost\n\n- **Domain-specific requirements shape embedding design** across five dimensions: semantic granularity (coarse to ultra-fine), asymmetry (query vs. document), multi-faceted similarity (multiple aspects), temporal dynamics (time-varying relevance), and hierarchical structure\n\n- **Multi-objective embedding design balances competing goals** through multi-task learning (shared encoder with task-specific heads), multi-vector representations (separate embeddings per objective), or constrained optimization (optimize primary objective subject to constraints)\n\n- **Optimal embedding dimensionality balances capacity and cost**—empirical evaluation across dimensions (128-1024) reveals diminishing returns beyond intrinsic dimensionality, with most domains achieving 95%+ quality at 256-512 dimensions vs. 768+ standard models\n\n- **Dimensionality reduction techniques** including PCA-based compression, learned projections, and binary embeddings enable 8-10x cost savings while maintaining acceptable quality for many use cases\n\n- **Total cost of ownership spans storage, training, inference, and team costs**—using the TCO model above, 100B embeddings at 768 dimensions would have annual costs around $47M, but optimization through dimension reduction (768→256), quantization (float32→int8), and tiered storage can achieve 90%+ cost savings\n\n- **Cost-performance trade-offs navigate the Pareto frontier** where different configurations offer optimal points—no single configuration dominates all objectives, requiring explicit business priority weighting to select operating points\n\n## Looking Ahead\n\n@sec-contrastive-learning dives deep into contrastive learning—one of the most powerful techniques for training custom embeddings that achieve state-of-the-art performance across diverse domains.\n\n## Further Reading\n\n- Devlin, J., et al. (2018). \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.\" *arXiv:1810.04805*\n- Reimers, N., & Gurevych, I. (2019). \"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks.\" *arXiv:1908.10084*\n- Muennighoff, N., et al. (2022). \"SGPT: GPT Sentence Embeddings for Semantic Search.\" *arXiv:2202.08904*\n- Radford, A., et al. (2021). \"Learning Transferable Visual Models From Natural Language Supervision.\" *arXiv:2103.00020* (CLIP)\n- Chen, T., et al. (2020). \"A Simple Framework for Contrastive Learning of Visual Representations.\" *arXiv:2002.05709* (SimCLR)\n- Levina, E., & Bickel, P. (2004). \"Maximum Likelihood Estimation of Intrinsic Dimension.\" *NIPS 2004*\n- Jégou, H., et al. (2011). \"Product Quantization for Nearest Neighbor Search.\" *IEEE TPAMI*\n- Gong, Y., et al. (2020). \"Quantization based Fast Inner Product Search.\" *AISTATS*\n- Ruder, S. (2017). \"An Overview of Multi-Task Learning in Deep Neural Networks.\" *arXiv:1706.05098*\n- Caruana, R. (1997). \"Multitask Learning.\" *Machine Learning* 28, 41–75\n\n",
    "supporting": [
      "ch08_custom_embedding_strategies_files/figure-pdf"
    ],
    "filters": []
  }
}