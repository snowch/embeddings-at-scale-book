{
  "hash": "5728fe8bee1d0be9cf8b762fc9fd8c24",
  "result": {
    "engine": "jupyter",
    "markdown": "# Text Chunking for Embeddings {#sec-text-chunking}\n\n:::{.callout-note}\n## Chapter Overview\nDocument RAG systems don't embed individual words—they embed chunks of text that capture semantic meaning in context. This chapter explores why chunking matters, how different strategies affect retrieval quality, and practical techniques for preparing text documents for embedding systems. You'll learn fixed-size, sentence-based, semantic, and hierarchical chunking approaches, with code examples and guidance for choosing the right strategy for your use case.\n:::\n\nA common misconception about embedding-based retrieval systems is that they embed every word individually. In reality, **RAG systems embed chunks of text**—larger semantic units that preserve context and meaning. Understanding chunking is essential because it directly impacts retrieval quality: poor chunking leads to poor results, regardless of how sophisticated your embedding model or vector database might be.\n\n## Why Chunking Matters\n\nWhen building a RAG system, you face a fundamental question: what unit of text should receive its own embedding? The answer is almost never \"individual words\" and rarely \"entire documents.\"\n\n### The Problem with Word-Level Embeddings\n\nWord embeddings (like Word2Vec or GloVe) represent individual words as vectors. While valuable for understanding vocabulary relationships, they're insufficient for retrieval:\n\n```python\n# Word embeddings: one vector per word\nword_embeddings = {\n    'bank': [0.2, 0.8, 0.1, ...],  # But which meaning? Financial? River?\n    'river': [0.1, 0.3, 0.9, ...],\n    'money': [0.8, 0.2, 0.1, ...],\n}\n\n# The word 'bank' has the same embedding regardless of context\n# \"I went to the bank to deposit money\" vs \"I sat on the river bank\"\n# Same vector, completely different meanings!\n```\n\nModern embedding models solve this by processing entire passages, producing a single vector that captures the **contextual meaning** of the whole chunk:\n\n```python\n# Chunk embeddings: one vector per passage\nchunk_embedding = encoder.encode(\n    \"I went to the bank to deposit my paycheck into savings.\"\n)\n# This single 1024-dim vector captures: financial institution,\n# personal finance, banking transaction, savings context\n```\n\n### The Problem with Document-Level Embeddings\n\nAt the other extreme, embedding entire documents creates different problems:\n\n1. **Diluted semantics**: A 50-page document covers many topics. Its embedding becomes a vague average, matching poorly with specific queries.\n\n2. **Context window limits**: LLMs have finite context windows (4K-128K tokens). Retrieved chunks must fit within these limits alongside the query and system prompt.\n\n3. **Retrieval granularity**: Users ask specific questions. Returning entire documents forces them to hunt for the relevant paragraph.\n\n```python\n# Document-level embedding: too coarse\ndoc_embedding = encoder.encode(entire_50_page_document)\n# This vector represents the \"average\" meaning of 50 pages\n# Query: \"What is the return policy for electronics?\"\n# Result: Entire product manual returned, user must find the relevant section\n```\n\n### The Chunking Sweet Spot\n\nChunking finds the middle ground: units large enough to preserve context but small enough for precise retrieval and LLM consumption.\n\n| Embedding Level | Typical Size | Context Preservation | Retrieval Precision | LLM Friendly |\n|----------------|--------------|---------------------|--------------------| -------------|\n| Word | 1 token | None | N/A | N/A |\n| Sentence | 10-30 tokens | Low | High | Yes |\n| Paragraph | 50-200 tokens | Medium | Medium | Yes |\n| Chunk | 100-500 tokens | High | High | Yes |\n| Document | 1000+ tokens | Complete but diluted | Low | Often too large |\n\n: Embedding granularity trade-offs {.striped}\n\n## Chunk Embeddings vs Word Embeddings\n\nLet's clarify the distinction that confuses many practitioners:\n\n### Word Embeddings (Historical Context)\n\nWord embeddings like Word2Vec (2013) revolutionized NLP by learning dense vector representations for individual words:\n\n```python\n# Word2Vec: learns one vector per vocabulary word\n# Training: predict surrounding words from center word (or vice versa)\n\nfrom gensim.models import Word2Vec\n\nsentences = [[\"the\", \"cat\", \"sat\", \"on\", \"mat\"], ...]\nmodel = Word2Vec(sentences, vector_size=300)\n\n# Each word gets exactly one 300-dim vector\ncat_vector = model.wv['cat']  # Always the same vector for 'cat'\n```\n\n**Key limitation**: No context sensitivity. \"Bank\" has one vector whether discussing finance or rivers.\n\n### Chunk Embeddings (Modern RAG)\n\nModern embedding models (Sentence-BERT, OpenAI embeddings, Cohere, etc.) process entire text passages:\n\n```python\n# Modern embeddings: one vector per input passage\nfrom sentence_transformers import SentenceTransformer\n\nencoder = SentenceTransformer('all-MiniLM-L6-v2')\n\n# Same word, different contexts → different chunk embeddings\nchunk1 = \"The bank approved my mortgage application yesterday.\"\nchunk2 = \"We had a picnic on the grassy bank beside the river.\"\n\nemb1 = encoder.encode(chunk1)  # Financial context captured\nemb2 = encoder.encode(chunk2)  # Nature context captured\n\n# These embeddings are very different despite both containing 'bank'\n```\n\n### The Transformation Process\n\nHere's what happens when you embed a chunk:\n\n```\nInput: \"The quarterly financial report shows revenue increased\n        by 15% compared to last year, driven primarily by\n        strong performance in the cloud services division.\"\n\n                    ↓ Embedding Model (e.g., all-MiniLM-L6-v2)\n\nOutput: [0.023, -0.156, 0.089, ..., 0.042]  # 384 dimensions\n\nThis single vector encodes:\n- Topic: Financial/business reporting\n- Sentiment: Positive (increased, strong)\n- Entities: Cloud services, quarterly reports\n- Relationships: Revenue growth, divisional performance\n- Context: Corporate earnings, year-over-year comparison\n```\n\nThe embedding model—typically a transformer—processes the entire chunk through attention layers that let every word influence every other word's representation. The final vector is a learned compression of this contextual understanding.\n\n## Chunking Strategies\n\nDifferent chunking strategies suit different use cases. Here's a comprehensive overview:\n\n### Fixed-Size Chunking\n\nThe simplest approach: split text into chunks of N characters or tokens.\n\n::: {#fc1a0a98 .cell execution_count=1}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show Fixed-Size Chunking Implementation\"}\nfrom typing import List\n\ndef chunk_by_characters(text: str, chunk_size: int = 500, overlap: int = 50) -> List[str]:\n    \"\"\"Split text into fixed-size character chunks with overlap.\"\"\"\n    chunks = []\n    start = 0\n    while start < len(text):\n        end = start + chunk_size\n        chunk = text[start:end]\n        if chunk.strip():\n            chunks.append(chunk)\n        start = end - overlap if overlap < chunk_size else end\n    return chunks\n\ndef chunk_by_tokens(text: str, chunk_size: int = 256, overlap: int = 25) -> List[str]:\n    \"\"\"Split text into fixed-size token chunks using simple whitespace tokenization.\"\"\"\n    tokens = text.split()  # Simple word-based tokenization\n    chunks = []\n    start = 0\n    while start < len(tokens):\n        end = min(start + chunk_size, len(tokens))\n        chunk_tokens = tokens[start:end]\n        chunk_text = ' '.join(chunk_tokens)\n        if chunk_text.strip():\n            chunks.append(chunk_text)\n        # Ensure start always advances to avoid infinite loop at end of token list\n        new_start = end - overlap if overlap < chunk_size else end\n        start = new_start if new_start > start else end\n    return chunks\n\n# Usage example\ntext = \"Machine learning transforms data processing. \" * 50\nchar_chunks = chunk_by_characters(text, chunk_size=200, overlap=20)\ntoken_chunks = chunk_by_tokens(text, chunk_size=50, overlap=5)\nprint(f\"Character chunking: {len(char_chunks)} chunks\")\nprint(f\"Token chunking: {len(token_chunks)} chunks\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCharacter chunking: 13 chunks\nToken chunking: 7 chunks\n```\n:::\n:::\n\n\n**Pros:**\n\n- Simple to implement and understand\n- Predictable chunk sizes for capacity planning\n- Works for any text without structural assumptions\n\n**Cons:**\n\n- Breaks mid-sentence, mid-paragraph, even mid-word\n- No respect for semantic boundaries\n- May split critical information across chunks\n\n**When to use:** Homogeneous text without clear structure, or as a baseline to compare against smarter strategies.\n\n### Sentence-Based Chunking\n\nSplit on sentence boundaries, grouping sentences to reach target size.\n\n::: {#242d639e .cell execution_count=2}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show Sentence-Based Chunking\"}\nimport re\nfrom typing import List\n\ndef chunk_by_sentences(text: str, target_size: int = 256, max_size: int = 512) -> List[str]:\n    \"\"\"Group sentences into chunks of approximately target_size words.\"\"\"\n    # Split into sentences\n    sentences = re.split(r'(?<=[.!?])\\s+(?=[A-Z])', text)\n    sentences = [s.strip() for s in sentences if s.strip()]\n\n    chunks = []\n    current_chunk = []\n    current_words = 0\n\n    for sentence in sentences:\n        sentence_words = len(sentence.split())  # Simple word count\n        if current_words + sentence_words > target_size and current_chunk:\n            chunks.append(\" \".join(current_chunk))\n            current_chunk = []\n            current_words = 0\n        current_chunk.append(sentence)\n        current_words += sentence_words\n\n    if current_chunk:\n        chunks.append(\" \".join(current_chunk))\n    return chunks\n\n# Usage example\ntext = \"ML transforms data. Neural networks learn patterns. Deep learning uses layers. Transformers power NLP.\"\nchunks = chunk_by_sentences(text, target_size=10)\nprint(f\"Created {len(chunks)} sentence-based chunks\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCreated 2 sentence-based chunks\n```\n:::\n:::\n\n\n**Pros:**\n\n- Preserves complete thoughts\n- Natural linguistic boundaries\n- Better semantic coherence than fixed-size\n\n**Cons:**\n\n- Sentence detection can fail on abbreviations, URLs, code\n- Variable chunk sizes\n- May still split related sentences\n\n**When to use:** Well-formed prose like articles, documentation, or reports.\n\n### Paragraph-Based Chunking\n\nUse paragraph breaks as natural semantic boundaries.\n\n::: {#1edc8aca .cell execution_count=3}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show Paragraph-Based Chunking\"}\nimport re\nfrom typing import List\n\ndef chunk_by_paragraphs(text: str, min_chunk_size: int = 100, max_chunk_size: int = 500) -> List[str]:\n    \"\"\"Split text on paragraph boundaries, combining short paragraphs.\"\"\"\n    paragraphs = re.split(r'\\n\\s*\\n', text)\n    paragraphs = [p.strip() for p in paragraphs if p.strip()]\n\n    chunks = []\n    current_chunk = []\n    current_size = 0\n\n    for para in paragraphs:\n        para_size = len(para)\n        if current_size + para_size <= max_chunk_size or not current_chunk:\n            current_chunk.append(para)\n            current_size += para_size\n        else:\n            chunks.append('\\n\\n'.join(current_chunk))\n            current_chunk = [para]\n            current_size = para_size\n\n    if current_chunk:\n        chunks.append('\\n\\n'.join(current_chunk))\n    return chunks\n\n# Usage example\ntext = \"First paragraph here.\\n\\nSecond paragraph here.\\n\\nThird paragraph text.\"\nchunks = chunk_by_paragraphs(text)\nprint(f\"Created {len(chunks)} paragraph-based chunks\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCreated 1 paragraph-based chunks\n```\n:::\n:::\n\n\n**Pros:**\n\n- Authors create paragraphs around coherent ideas\n- Strongest natural semantic boundaries\n- Often ideal chunk size naturally\n\n**Cons:**\n\n- Paragraph length varies wildly\n- Some documents lack clear paragraphs\n- Very short paragraphs may lack context\n\n**When to use:** Well-structured documents with clear paragraph formatting.\n\n### Semantic Chunking\n\nSplit based on topic shifts detected by embedding similarity.\n\n::: {#e2a6fcc9 .cell execution_count=4}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show Semantic Chunking\"}\nfrom typing import List\nimport re\n\n# Check for optional dependencies\ntry:\n    import numpy as np\n    from sentence_transformers import SentenceTransformer\n    HAS_SEMANTIC_DEPS = True\nexcept ImportError:\n    HAS_SEMANTIC_DEPS = False\n\ndef semantic_chunk(text: str, similarity_threshold: float = 0.5) -> List[str]:\n    \"\"\"Split text at semantic boundaries using embedding similarity.\"\"\"\n    # Split into sentences first\n    sentences = re.split(r'(?<=[.!?])\\s+(?=[A-Z])', text)\n    sentences = [s.strip() for s in sentences if s.strip()]\n    if len(sentences) < 2:\n        return [text]\n\n    if not HAS_SEMANTIC_DEPS:\n        # Fallback: simple sentence grouping without embeddings\n        chunks = []\n        for i in range(0, len(sentences), 2):\n            chunk = \" \".join(sentences[i:i+2])\n            chunks.append(chunk)\n        return chunks\n\n    # Embed sentences\n    model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n    embeddings = model.encode(sentences)\n\n    # Calculate similarities between consecutive sentences\n    similarities = []\n    for i in range(len(embeddings) - 1):\n        sim = np.dot(embeddings[i], embeddings[i + 1]) / (\n            np.linalg.norm(embeddings[i]) * np.linalg.norm(embeddings[i + 1])\n        )\n        similarities.append(sim)\n\n    # Split where similarity drops\n    chunks = []\n    current_chunk = [sentences[0]]\n    for sentence, sim in zip(sentences[1:], similarities):\n        if sim < similarity_threshold:\n            chunks.append(\" \".join(current_chunk))\n            current_chunk = [sentence]\n        else:\n            current_chunk.append(sentence)\n    if current_chunk:\n        chunks.append(\" \".join(current_chunk))\n    return chunks\n\n# Usage example\ntext = \"ML enables learning. AI powers systems. Dogs are animals. Cats like milk.\"\nchunks = semantic_chunk(text, similarity_threshold=0.6)\nprint(f\"Created {len(chunks)} semantic chunks\")\nif not HAS_SEMANTIC_DEPS:\n    print(\"(Using fallback mode - install sentence-transformers for full functionality)\")\n```\n:::\n\n\n**Pros:**\n\n- Chunks align with actual topic boundaries\n- Captures semantic coherence directly\n- Adapts to content structure\n\n**Cons:**\n\n- Computationally expensive (requires embedding each sentence)\n- Threshold tuning required\n- May create very uneven chunk sizes\n\n**When to use:** Documents with multiple topics, transcripts, or content without clear structural markers.\n\n### Recursive/Hierarchical Chunking\n\nTry multiple splitters in order of preference, falling back as needed.\n\n::: {#280e6e3f .cell execution_count=5}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show Recursive/Hierarchical Chunking\"}\nfrom typing import List, Optional\n\nclass RecursiveChunker:\n    \"\"\"Recursively split text using a hierarchy of separators.\"\"\"\n    def __init__(self, chunk_size: int = 500, chunk_overlap: int = 50, separators: Optional[List[str]] = None):\n        self.chunk_size = chunk_size\n        self.chunk_overlap = chunk_overlap\n        self.separators = separators or [\"\\n\\n\", \"\\n\", \". \", \", \", \" \"]\n\n    def chunk(self, text: str) -> List[str]:\n        \"\"\"Split text recursively using separator hierarchy.\"\"\"\n        return self._recursive_split(text, self.separators)\n\n    def _recursive_split(self, text: str, separators: List[str]) -> List[str]:\n        \"\"\"Recursively split text, trying separators in order.\"\"\"\n        if len(text) <= self.chunk_size:\n            return [text] if text.strip() else []\n        if not separators:\n            return self._force_split(text)\n\n        current_sep = separators[0]\n        remaining_seps = separators[1:]\n        splits = text.split(current_sep)\n\n        if len(splits) == 1:\n            return self._recursive_split(text, remaining_seps)\n\n        chunks = []\n        current_chunk = []\n        current_length = 0\n\n        for split in splits:\n            split_length = len(split) + len(current_sep)\n            if current_length + split_length > self.chunk_size and current_chunk:\n                chunks.append(current_sep.join(current_chunk))\n                overlap_text = self._get_overlap(current_chunk, current_sep)\n                current_chunk = [overlap_text] if overlap_text else []\n                current_length = len(overlap_text) if overlap_text else 0\n            current_chunk.append(split)\n            current_length += split_length\n\n        if current_chunk:\n            remaining = current_sep.join(current_chunk)\n            if len(remaining) > self.chunk_size:\n                chunks.extend(self._recursive_split(remaining, remaining_seps))\n            elif remaining.strip():\n                chunks.append(remaining)\n        return chunks\n\n    def _get_overlap(self, parts: List[str], sep: str) -> str:\n        \"\"\"Get overlap text from the end of current chunk.\"\"\"\n        if not self.chunk_overlap or not parts:\n            return \"\"\n        overlap_parts = []\n        overlap_length = 0\n        for part in reversed(parts):\n            if overlap_length + len(part) > self.chunk_overlap:\n                break\n            overlap_parts.insert(0, part)\n            overlap_length += len(part) + len(sep)\n        return sep.join(overlap_parts)\n\n    def _force_split(self, text: str) -> List[str]:\n        \"\"\"Force split text at chunk_size boundaries.\"\"\"\n        chunks = []\n        start = 0\n        while start < len(text):\n            end = start + self.chunk_size\n            chunk = text[start:end]\n            if chunk.strip():\n                chunks.append(chunk)\n            start = end - self.chunk_overlap\n        return chunks\n\n# Usage example\ntext = \"ML enables learning. AI powers systems. \" * 20\nchunker = RecursiveChunker(chunk_size=200, chunk_overlap=20)\nchunks = chunker.chunk(text)\nprint(f\"Created {len(chunks)} chunks from text\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCreated 5 chunks from text\n```\n:::\n:::\n\n\n**Pros:**\n\n- Respects document hierarchy (sections → paragraphs → sentences)\n- Graceful degradation for messy documents\n- Flexible target sizes\n\n**Cons:**\n\n- More complex implementation\n- Order of separators matters\n- May still produce uneven chunks\n\n**When to use:** Documents with mixed structure, or when you need consistent chunk sizes with best-effort boundary respect.\n\n### Sliding Window with Overlap\n\nCreate overlapping chunks to preserve context at boundaries.\n\n::: {#39d12139 .cell execution_count=6}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show Sliding Window Chunking\"}\nfrom dataclasses import dataclass\nfrom typing import List\n\n@dataclass\nclass ChunkWithMetadata:\n    \"\"\"A chunk with position metadata for deduplication.\"\"\"\n    text: str\n    start_char: int\n    end_char: int\n    chunk_index: int\n\ndef sliding_window_chunks(text: str, window_size: int = 500, stride: int = 400) -> List[ChunkWithMetadata]:\n    \"\"\"\n    Create overlapping chunks using a sliding window.\n\n    Args:\n        window_size: Size of each chunk in characters\n        stride: How far to move the window (overlap = window_size - stride)\n    \"\"\"\n    if stride > window_size:\n        stride = window_size\n\n    chunks = []\n    start = 0\n    chunk_index = 0\n\n    while start < len(text):\n        end = min(start + window_size, len(text))\n        chunk_text = text[start:end].strip()\n\n        if chunk_text:\n            chunks.append(ChunkWithMetadata(\n                text=chunk_text, start_char=start, end_char=end, chunk_index=chunk_index\n            ))\n            chunk_index += 1\n        start += stride\n\n    return chunks\n\ndef calculate_overlap(start1: int, end1: int, start2: int, end2: int) -> float:\n    \"\"\"Calculate overlap ratio between two ranges.\"\"\"\n    overlap_start = max(start1, start2)\n    overlap_end = min(end1, end2)\n    if overlap_start >= overlap_end:\n        return 0.0\n    overlap_length = overlap_end - overlap_start\n    min_length = min(end1 - start1, end2 - start2)\n    return overlap_length / min_length\n\n# Usage example\ntext = \"Machine learning transforms data. Neural networks learn patterns. \" * 10\nchunks = sliding_window_chunks(text, window_size=200, stride=150)\nprint(f\"Created {len(chunks)} overlapping chunks\")\nif len(chunks) > 1:\n    overlap = calculate_overlap(chunks[0].start_char, chunks[0].end_char,\n                                chunks[1].start_char, chunks[1].end_char)\n    print(f\"Overlap between chunks: {overlap:.1%}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCreated 5 overlapping chunks\nOverlap between chunks: 25.0%\n```\n:::\n:::\n\n\n**Pros:**\n\n- Information at chunk boundaries appears in multiple chunks\n- Reduces risk of splitting critical context\n- Better recall for boundary-spanning queries\n\n**Cons:**\n\n- Increases storage requirements (overlap percentage)\n- May retrieve duplicate information\n- Requires deduplication in results\n\n**When to use:** When retrieval quality matters more than storage efficiency, especially for dense technical content.\n\n## Document-Type Specific Strategies\n\nDifferent document types require different chunking approaches:\n\n### PDF Documents\n\nPDFs present unique challenges: headers/footers on every page, multi-column layouts, embedded tables, and inconsistent text extraction.\n\n::: {#d7f3a925 .cell execution_count=7}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show PDF Chunking\"}\nimport re\nfrom dataclasses import dataclass, field\nfrom typing import Dict, List, Optional\n\n# Check for optional PDF dependency\ntry:\n    import fitz  # PyMuPDF\n    HAS_PYMUPDF = True\nexcept ImportError:\n    HAS_PYMUPDF = False\n\n@dataclass\nclass PDFChunk:\n    \"\"\"A chunk extracted from a PDF with metadata.\"\"\"\n    text: str\n    page_numbers: List[int]\n    section_title: Optional[str] = None\n    chunk_type: str = \"text\"\n    metadata: Dict = field(default_factory=dict)\n\nclass PDFChunker:\n    \"\"\"Extract and chunk text from PDFs while preserving structure.\"\"\"\n    def __init__(self, chunk_size: int = 500, chunk_overlap: int = 50,\n                 remove_headers_footers: bool = True, detect_sections: bool = True):\n        self.chunk_size = chunk_size\n        self.chunk_overlap = chunk_overlap\n        self.remove_headers_footers = remove_headers_footers\n        self.detect_sections = detect_sections\n\n    def chunk_pdf(self, pdf_path: str) -> List[PDFChunk]:\n        \"\"\"Extract and chunk a PDF document.\"\"\"\n        if not HAS_PYMUPDF:\n            raise ImportError(\"PyMuPDF (fitz) is required for PDF processing. \"\n                            \"Install with: pip install PyMuPDF\")\n        doc = fitz.open(pdf_path)\n        chunks = []\n        current_section = None\n\n        for page_num, page in enumerate(doc):\n            text = page.get_text(\"text\")\n            if self.remove_headers_footers:\n                text = self._remove_headers_footers(text, page_num, len(doc))\n            if self.detect_sections:\n                sections = self._detect_sections(text)\n                for section_title, section_text in sections:\n                    current_section = section_title or current_section\n                    page_chunks = self._chunk_text(section_text, page_num + 1, current_section)\n                    chunks.extend(page_chunks)\n            else:\n                page_chunks = self._chunk_text(text, page_num + 1, current_section)\n                chunks.extend(page_chunks)\n        doc.close()\n        return self._merge_small_chunks(chunks)\n\n    def _detect_sections(self, text: str) -> List[tuple]:\n        \"\"\"Detect section headers and split text accordingly.\"\"\"\n        header_patterns = [\n            r\"^(?:Chapter\\s+)?(\\d+\\.?\\s+[A-Z][^\\n]+)$\",\n            r\"^([A-Z][A-Z\\s]+)$\",\n        ]\n        sections = []\n        current_title = None\n        current_text = []\n        for line in text.split(\"\\n\"):\n            is_header = False\n            for pattern in header_patterns:\n                match = re.match(pattern, line.strip())\n                if match:\n                    if current_text:\n                        sections.append((current_title, \"\\n\".join(current_text)))\n                    current_title = match.group(1)\n                    current_text = []\n                    is_header = True\n                    break\n            if not is_header:\n                current_text.append(line)\n        if current_text:\n            sections.append((current_title, \"\\n\".join(current_text)))\n        return sections if sections else [(None, text)]\n\n    def _remove_headers_footers(self, text: str, page_num: int, total_pages: int) -> str:\n        \"\"\"Remove common header/footer patterns.\"\"\"\n        lines = text.split(\"\\n\")\n        filtered = []\n        for i, line in enumerate(lines):\n            if i < 3 and len(line.strip()) < 50 and not any(c.islower() for c in line):\n                continue\n            filtered.append(line)\n        return \"\\n\".join(filtered)\n\n    def _chunk_text(self, text: str, page_num: int, section_title: Optional[str]) -> List[PDFChunk]:\n        \"\"\"Chunk text into appropriately sized pieces.\"\"\"\n        # Simple chunking for example\n        chunks = []\n        words = text.split()\n        current_chunk = []\n        for word in words:\n            current_chunk.append(word)\n            if len(\" \".join(current_chunk)) >= self.chunk_size:\n                chunks.append(PDFChunk(\n                    text=\" \".join(current_chunk),\n                    page_numbers=[page_num],\n                    section_title=section_title\n                ))\n                current_chunk = []\n        if current_chunk:\n            chunks.append(PDFChunk(\n                text=\" \".join(current_chunk),\n                page_numbers=[page_num],\n                section_title=section_title\n            ))\n        return chunks\n\n    def _merge_small_chunks(self, chunks: List[PDFChunk]) -> List[PDFChunk]:\n        \"\"\"Merge chunks that are too small.\"\"\"\n        if not chunks:\n            return chunks\n        merged = []\n        current = chunks[0]\n        for next_chunk in chunks[1:]:\n            can_merge = (current.section_title == next_chunk.section_title and\n                        len(current.text) + len(next_chunk.text) < self.chunk_size)\n            if can_merge:\n                current = PDFChunk(\n                    text=current.text + \"\\n\\n\" + next_chunk.text,\n                    page_numbers=list(set(current.page_numbers + next_chunk.page_numbers)),\n                    section_title=current.section_title\n                )\n            else:\n                merged.append(current)\n                current = next_chunk\n        merged.append(current)\n        return merged\n\n# Usage example\nif HAS_PYMUPDF:\n    print(\"PDFChunker ready for processing PDF documents with structure preservation\")\nelse:\n    print(\"PDFChunker defined (install PyMuPDF for PDF processing: pip install PyMuPDF)\")\n```\n:::\n\n\n### HTML Documents\n\nHTML carries structural information that aids chunking:\n\n::: {#118aa7db .cell execution_count=8}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show HTML Chunking\"}\nimport re\nfrom dataclasses import dataclass\nfrom typing import List, Optional\n\n# Check for optional HTML parsing dependency\ntry:\n    from bs4 import BeautifulSoup\n    HAS_BS4 = True\nexcept ImportError:\n    HAS_BS4 = False\n\n@dataclass\nclass HTMLChunk:\n    \"\"\"A chunk extracted from HTML with metadata.\"\"\"\n    text: str\n    tag_path: str\n    heading: Optional[str] = None\n\nclass HTMLChunker:\n    \"\"\"Extract and chunk text from HTML while preserving semantic structure.\"\"\"\n    BLOCK_TAGS = {\"article\", \"section\", \"div\", \"p\", \"blockquote\", \"li\"}\n    SECTION_TAGS = {\"article\", \"section\", \"main\", \"aside\"}\n    HEADING_TAGS = [\"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\"]\n\n    def __init__(self, chunk_size: int = 500, chunk_overlap: int = 50,\n                 preserve_structure: bool = True, include_headings: bool = True):\n        self.chunk_size = chunk_size\n        self.chunk_overlap = chunk_overlap\n        self.preserve_structure = preserve_structure\n        self.include_headings = include_headings\n\n    def chunk_html(self, html: str) -> List[HTMLChunk]:\n        \"\"\"Extract and chunk HTML content.\"\"\"\n        if not HAS_BS4:\n            raise ImportError(\"BeautifulSoup is required for HTML processing. \"\n                            \"Install with: pip install beautifulsoup4\")\n        soup = BeautifulSoup(html, \"html.parser\")\n\n        # Remove script and style elements\n        for element in soup([\"script\", \"style\", \"nav\", \"footer\", \"header\"]):\n            element.decompose()\n\n        if self.preserve_structure:\n            return self._chunk_by_structure(soup)\n        else:\n            return self._chunk_flat(soup)\n\n    def _chunk_by_structure(self, soup) -> List[HTMLChunk]:\n        \"\"\"Chunk based on HTML structure (sections, articles, etc.).\"\"\"\n        chunks = []\n        current_heading = None\n\n        main_content = (soup.find(\"main\") or soup.find(\"article\") or\n                       soup.find(\"div\", class_=re.compile(r\"content|main|article\")) or\n                       soup.body or soup)\n\n        for section in self._find_sections(main_content):\n            section_heading = self._extract_heading(section)\n            if section_heading:\n                current_heading = section_heading\n\n            section_text = self._extract_text(section)\n            if not section_text.strip():\n                continue\n\n            section_chunks = self._split_text(section_text, current_heading,\n                                              self._get_tag_path(section))\n            chunks.extend(section_chunks)\n\n        return chunks\n\n    def _chunk_flat(self, soup) -> List[HTMLChunk]:\n        \"\"\"Simple flat chunking of all text content.\"\"\"\n        text = soup.get_text(separator=\"\\n\", strip=True)\n        return self._split_text(text, None, \"body\")\n\n    def _find_sections(self, element):\n        \"\"\"Find content sections in the HTML.\"\"\"\n        sections = element.find_all(self.SECTION_TAGS)\n        if sections:\n            yield from sections\n        else:\n            for child in element.children:\n                if hasattr(child, \"name\") and child.name in self.BLOCK_TAGS:\n                    yield child\n\n    def _extract_heading(self, element) -> Optional[str]:\n        \"\"\"Extract the heading for a section.\"\"\"\n        for tag in self.HEADING_TAGS:\n            heading = element.find(tag)\n            if heading:\n                return heading.get_text(strip=True)\n        return None\n\n    def _extract_text(self, element) -> str:\n        \"\"\"Extract clean text from an element.\"\"\"\n        text = element.get_text(separator=\"\\n\", strip=True)\n        return re.sub(r\"\\n{3,}\", \"\\n\\n\", text)\n\n    def _get_tag_path(self, element) -> str:\n        \"\"\"Get the tag path to an element.\"\"\"\n        path = []\n        current = element\n        while current and hasattr(current, \"name\") and current.name:\n            tag_info = current.name\n            if current.get(\"id\"):\n                tag_info += f\"#{current['id']}\"\n            path.insert(0, tag_info)\n            current = current.parent\n        return \" > \".join(path[-4:])\n\n    def _split_text(self, text: str, heading: Optional[str], tag_path: str) -> List[HTMLChunk]:\n        \"\"\"Split text into chunks.\"\"\"\n        if self.include_headings and heading:\n            text = f\"## {heading}\\n\\n{text}\"\n\n        chunks = []\n        words = text.split()\n        current_chunk = []\n        for word in words:\n            current_chunk.append(word)\n            if len(\" \".join(current_chunk)) >= self.chunk_size:\n                chunks.append(HTMLChunk(\n                    text=\" \".join(current_chunk),\n                    tag_path=tag_path,\n                    heading=heading\n                ))\n                current_chunk = []\n        if current_chunk:\n            chunks.append(HTMLChunk(\n                text=\" \".join(current_chunk),\n                tag_path=tag_path,\n                heading=heading\n            ))\n        return chunks\n\n# Usage example\nif HAS_BS4:\n    print(\"HTMLChunker ready for processing HTML documents\")\nelse:\n    print(\"HTMLChunker defined (install beautifulsoup4 for HTML processing: pip install beautifulsoup4)\")\n```\n:::\n\n\n### Markdown Documents\n\nMarkdown headers provide explicit hierarchy:\n\n::: {#914954fe .cell execution_count=9}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show Markdown Chunking\"}\nimport re\nfrom dataclasses import dataclass\nfrom typing import List\n\n@dataclass\nclass MarkdownChunk:\n    \"\"\"A chunk from a Markdown document with context.\"\"\"\n    text: str\n    header_hierarchy: List[str]\n    header_level: int\n    start_line: int\n    end_line: int\n\nclass MarkdownChunker:\n    \"\"\"Chunk Markdown documents while preserving header hierarchy.\"\"\"\n    def __init__(self, chunk_size: int = 500, chunk_overlap: int = 50,\n                 include_header_context: bool = True, min_chunk_size: int = 100):\n        self.chunk_size = chunk_size\n        self.chunk_overlap = chunk_overlap\n        self.include_header_context = include_header_context\n        self.min_chunk_size = min_chunk_size\n\n    def chunk_markdown(self, markdown: str) -> List[MarkdownChunk]:\n        \"\"\"Chunk a Markdown document.\"\"\"\n        lines = markdown.split(\"\\n\")\n        sections = self._parse_sections(lines)\n        chunks = self._chunk_sections(sections)\n        return chunks\n\n    def _parse_sections(self, lines: List[str]) -> List[dict]:\n        \"\"\"Parse Markdown into sections based on headers.\"\"\"\n        sections = []\n        current_section = {\"headers\": [], \"content\": [], \"start_line\": 0, \"level\": 0}\n        header_stack = []\n\n        for i, line in enumerate(lines):\n            header_match = re.match(r\"^(#{1,6})\\s+(.+)$\", line)\n\n            if header_match:\n                if current_section[\"content\"]:\n                    current_section[\"end_line\"] = i - 1\n                    sections.append(current_section)\n\n                level = len(header_match.group(1))\n                title = header_match.group(2).strip()\n\n                while header_stack and header_stack[-1][0] >= level:\n                    header_stack.pop()\n\n                header_stack.append((level, title))\n\n                current_section = {\n                    \"headers\": [h[1] for h in header_stack],\n                    \"content\": [line],\n                    \"start_line\": i,\n                    \"level\": level,\n                }\n            else:\n                current_section[\"content\"].append(line)\n\n        if current_section[\"content\"]:\n            current_section[\"end_line\"] = len(lines) - 1\n            sections.append(current_section)\n\n        return sections\n\n    def _chunk_sections(self, sections: List[dict]) -> List[MarkdownChunk]:\n        \"\"\"Chunk each section into appropriately sized pieces.\"\"\"\n        chunks = []\n\n        for section in sections:\n            content = \"\\n\".join(section[\"content\"])\n\n            if self.include_header_context and section[\"headers\"]:\n                header_context = \" > \".join(section[\"headers\"]) + \"\\n\\n\"\n            else:\n                header_context = \"\"\n\n            if len(content) <= self.chunk_size:\n                if content.strip():\n                    chunks.append(MarkdownChunk(\n                        text=header_context + content if header_context else content,\n                        header_hierarchy=section[\"headers\"],\n                        header_level=section[\"level\"],\n                        start_line=section[\"start_line\"],\n                        end_line=section.get(\"end_line\", section[\"start_line\"]),\n                    ))\n            else:\n                sub_chunks = self._split_section(content, header_context)\n                for sub_text in sub_chunks:\n                    chunks.append(MarkdownChunk(\n                        text=sub_text,\n                        header_hierarchy=section[\"headers\"],\n                        header_level=section[\"level\"],\n                        start_line=section[\"start_line\"],\n                        end_line=section.get(\"end_line\", section[\"start_line\"]),\n                    ))\n\n        return chunks\n\n    def _split_section(self, content: str, header_context: str) -> List[str]:\n        \"\"\"Split a large section into smaller chunks.\"\"\"\n        effective_chunk_size = self.chunk_size - len(header_context)\n        chunks = []\n        words = content.split()\n        current_chunk = []\n\n        for word in words:\n            current_chunk.append(word)\n            if len(\" \".join(current_chunk)) >= effective_chunk_size:\n                chunk_text = \" \".join(current_chunk)\n                chunks.append(header_context + chunk_text if header_context else chunk_text)\n                current_chunk = []\n\n        if current_chunk:\n            chunk_text = \" \".join(current_chunk)\n            chunks.append(header_context + chunk_text if header_context else chunk_text)\n\n        return chunks\n\n# Usage example\nprint(\"MarkdownChunker ready for processing Markdown documents\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMarkdownChunker ready for processing Markdown documents\n```\n:::\n:::\n\n\n### Source Code\n\nCode requires special handling to preserve syntactic units:\n\n::: {#0167b97b .cell execution_count=10}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show Code Chunking\"}\nimport ast\nfrom dataclasses import dataclass\nfrom typing import List, Optional\n\n@dataclass\nclass CodeChunk:\n    \"\"\"A chunk of source code with metadata.\"\"\"\n    code: str\n    language: str\n    chunk_type: str  # function, class, module, block\n    name: Optional[str] = None\n    docstring: Optional[str] = None\n\nclass CodeChunker:\n    \"\"\"Chunk source code while preserving syntactic structure.\"\"\"\n    def __init__(self, chunk_size: int = 1000, include_docstrings: bool = True,\n                 include_imports: bool = True):\n        self.chunk_size = chunk_size\n        self.include_docstrings = include_docstrings\n        self.include_imports = include_imports\n\n    def chunk_python(self, code: str) -> List[CodeChunk]:\n        \"\"\"Chunk Python code using AST parsing.\"\"\"\n        try:\n            tree = ast.parse(code)\n            return self._chunk_python_ast(code, tree)\n        except SyntaxError:\n            return []\n\n    def _chunk_python_ast(self, code: str, tree: ast.Module) -> List[CodeChunk]:\n        \"\"\"Extract chunks from Python AST.\"\"\"\n        chunks = []\n        lines = code.split(\"\\n\")\n\n        # Extract imports as a single chunk\n        if self.include_imports:\n            imports = self._extract_imports(tree, lines)\n            if imports:\n                chunks.append(CodeChunk(\n                    code=imports, language=\"python\", chunk_type=\"imports\", name=\"imports\"\n                ))\n\n        # Extract classes and functions\n        for node in ast.walk(tree):\n            if isinstance(node, ast.ClassDef):\n                chunk = self._extract_class(node, lines)\n                if chunk:\n                    chunks.append(chunk)\n            elif isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):\n                if not self._is_method(node, tree):\n                    chunk = self._extract_function(node, lines)\n                    if chunk:\n                        chunks.append(chunk)\n\n        return chunks\n\n    def _extract_imports(self, tree: ast.Module, lines: List[str]) -> str:\n        \"\"\"Extract all import statements.\"\"\"\n        import_lines = []\n        for node in ast.iter_child_nodes(tree):\n            if isinstance(node, (ast.Import, ast.ImportFrom)):\n                start = node.lineno - 1\n                end = node.end_lineno if hasattr(node, \"end_lineno\") else start + 1\n                import_lines.extend(lines[start:end])\n        return \"\\n\".join(import_lines)\n\n    def _extract_class(self, node: ast.ClassDef, lines: List[str]) -> CodeChunk:\n        \"\"\"Extract a class definition.\"\"\"\n        start = node.lineno - 1\n        end = node.end_lineno if hasattr(node, \"end_lineno\") else len(lines)\n        code = \"\\n\".join(lines[start:end])\n        docstring = ast.get_docstring(node) if self.include_docstrings else None\n        return CodeChunk(\n            code=code, language=\"python\", chunk_type=\"class\",\n            name=node.name, docstring=docstring\n        )\n\n    def _extract_function(self, node, lines: List[str]) -> CodeChunk:\n        \"\"\"Extract a function definition.\"\"\"\n        start = node.lineno - 1\n        end = node.end_lineno if hasattr(node, \"end_lineno\") else len(lines)\n        code = \"\\n\".join(lines[start:end])\n        docstring = ast.get_docstring(node) if self.include_docstrings else None\n        return CodeChunk(\n            code=code, language=\"python\", chunk_type=\"function\",\n            name=node.name, docstring=docstring\n        )\n\n    def _is_method(self, node, tree: ast.Module) -> bool:\n        \"\"\"Check if a function is a method inside a class.\"\"\"\n        for parent in ast.walk(tree):\n            if isinstance(parent, ast.ClassDef):\n                for child in ast.iter_child_nodes(parent):\n                    if child is node:\n                        return True\n        return False\n\n# Usage example\nsample_code = '''\nimport numpy as np\n\nclass DataProcessor:\n    \"\"\"Process data.\"\"\"\n    def __init__(self):\n        self.data = []\n\ndef calculate(x, y):\n    \"\"\"Calculate something.\"\"\"\n    return x + y\n'''\n\nchunker = CodeChunker()\nchunks = chunker.chunk_python(sample_code)\nprint(f\"Found {len(chunks)} code chunks\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFound 3 code chunks\n```\n:::\n:::\n\n\n## Chunk Size Optimization\n\nChoosing optimal chunk size involves balancing competing concerns:\n\n### The Trade-off Triangle\n\n```\n                    CONTEXT\n                      /\\\n                     /  \\\n                    /    \\\n                   /      \\\n                  /        \\\n                 /__________\\\n           PRECISION      EFFICIENCY\n\nLarger chunks → More context, less precision, fewer chunks\nSmaller chunks → Less context, more precision, more chunks\n```\n\n### Empirical Sizing Guidelines\n\nBased on production experience across different use cases:\n\n| Use Case | Recommended Size | Overlap | Rationale |\n|----------|-----------------|---------|-----------|\n| Q&A over documentation | 256-512 tokens | 10-20% | Balance context with precision |\n| Legal document search | 512-1024 tokens | 20-30% | Preserve legal context and cross-references |\n| Customer support | 128-256 tokens | 10% | Short, focused answers needed |\n| Academic papers | 512-768 tokens | 15% | Preserve argument flow |\n| Code documentation | 256-512 tokens | 0% | Function/class boundaries are natural |\n| Chat/transcript search | 128-256 tokens | 20% | Conversational turns are short |\n\n: Chunk size recommendations by use case {.striped}\n\n### Finding Your Optimal Size\n\n::: {#9f319060 .cell execution_count=11}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show Chunk Size Evaluation\"}\nfrom dataclasses import dataclass\nfrom typing import Dict, List\n\n# Check for numpy\ntry:\n    import numpy as np\n    HAS_NUMPY = True\nexcept ImportError:\n    HAS_NUMPY = False\n\n@dataclass\nclass EvaluationResult:\n    \"\"\"Results from chunk size evaluation.\"\"\"\n    chunk_size: int\n    num_chunks: int\n    avg_chunk_length: float\n    retrieval_precision: float\n    retrieval_recall: float\n    retrieval_f1: float\n\ndef evaluate_chunk_sizes(documents: List[str], queries: List[str],\n                        ground_truth: List[List[int]],\n                        chunk_sizes: List[int] = None, top_k: int = 5) -> List[EvaluationResult]:\n    \"\"\"\n    Evaluate retrieval quality across different chunk sizes.\n\n    Args:\n        documents: List of documents to chunk and index\n        queries: List of test queries\n        ground_truth: For each query, indices of relevant documents\n        chunk_sizes: List of chunk sizes to evaluate\n        top_k: Number of results to retrieve\n    \"\"\"\n    if chunk_sizes is None:\n        chunk_sizes = [128, 256, 512, 1024]\n\n    results = []\n    for chunk_size in chunk_sizes:\n        # Simplified evaluation for demo\n        result = EvaluationResult(\n            chunk_size=chunk_size,\n            num_chunks=len(documents) * (1000 // chunk_size),\n            avg_chunk_length=chunk_size * 0.9,\n            retrieval_precision=0.75,\n            retrieval_recall=0.68,\n            retrieval_f1=0.71\n        )\n        results.append(result)\n    return results\n\ndef analyze_chunk_statistics(chunks: List[str]) -> Dict:\n    \"\"\"Analyze statistics of a chunking result.\"\"\"\n    lengths = [len(c) for c in chunks]\n    if HAS_NUMPY:\n        return {\n            \"num_chunks\": len(chunks),\n            \"avg_length\": np.mean(lengths),\n            \"std_length\": np.std(lengths),\n            \"min_length\": min(lengths) if lengths else 0,\n            \"max_length\": max(lengths) if lengths else 0,\n        }\n    else:\n        # Fallback without numpy\n        avg = sum(lengths) / len(lengths) if lengths else 0\n        variance = sum((x - avg) ** 2 for x in lengths) / len(lengths) if lengths else 0\n        std = variance ** 0.5\n        return {\n            \"num_chunks\": len(chunks),\n            \"avg_length\": avg,\n            \"std_length\": std,\n            \"min_length\": min(lengths) if lengths else 0,\n            \"max_length\": max(lengths) if lengths else 0,\n        }\n\n# Usage example\nsample_docs = [\"ML transforms data. \" * 50] * 3\nsample_queries = [\"What is machine learning?\"]\nground_truth = [[0]]\n\nresults = evaluate_chunk_sizes(sample_docs, sample_queries, ground_truth,\n                               chunk_sizes=[128, 256, 512])\nprint(f\"Evaluated {len(results)} chunk sizes\")\nfor r in results:\n    print(f\"Size {r.chunk_size}: F1={r.retrieval_f1:.3f}\")\n```\n:::\n\n\n## Metadata Preservation\n\nChunks without context are less useful. Preserve metadata for filtering and context:\n\n::: {#6f0a90ab .cell execution_count=12}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show Metadata Preservation\"}\nimport hashlib\nfrom dataclasses import dataclass, field\nfrom datetime import datetime\nfrom typing import Any, Dict, List, Optional\n\n@dataclass\nclass ChunkMetadata:\n    \"\"\"Comprehensive metadata for a text chunk.\"\"\"\n    source_id: str\n    source_type: str  # pdf, html, markdown, etc.\n    source_url: Optional[str] = None\n    page_number: Optional[int] = None\n    section_title: Optional[str] = None\n    section_hierarchy: List[str] = field(default_factory=list)\n    start_char: int = 0\n    end_char: int = 0\n    language: str = \"en\"\n    content_type: str = \"text\"\n    word_count: int = 0\n    indexed_at: datetime = field(default_factory=datetime.now)\n    custom: Dict[str, Any] = field(default_factory=dict)\n\n    def to_dict(self) -> Dict:\n        \"\"\"Convert to dictionary for storage.\"\"\"\n        return {\n            \"source_id\": self.source_id,\n            \"source_type\": self.source_type,\n            \"source_url\": self.source_url,\n            \"page_number\": self.page_number,\n            \"section_title\": self.section_title,\n            \"section_hierarchy\": self.section_hierarchy,\n            \"start_char\": self.start_char,\n            \"end_char\": self.end_char,\n            \"language\": self.language,\n            \"content_type\": self.content_type,\n            \"word_count\": self.word_count,\n            \"indexed_at\": self.indexed_at.isoformat(),\n            \"custom\": self.custom,\n        }\n\n@dataclass\nclass EnrichedChunk:\n    \"\"\"A chunk with its text, embedding, and metadata.\"\"\"\n    chunk_id: str\n    text: str\n    metadata: ChunkMetadata\n    embedding: Optional[List[float]] = None\n\n    def __post_init__(self):\n        if not self.chunk_id:\n            self.chunk_id = self._generate_id()\n\n    def _generate_id(self) -> str:\n        \"\"\"Generate a unique ID based on content and source.\"\"\"\n        content = f\"{self.metadata.source_id}:{self.metadata.start_char}:{self.text[:100]}\"\n        return hashlib.sha256(content.encode()).hexdigest()[:16]\n\n# Usage example\nmetadata = ChunkMetadata(\n    source_id=\"doc_001\",\n    source_type=\"markdown\",\n    section_title=\"Introduction\",\n    word_count=42\n)\nchunk = EnrichedChunk(\n    chunk_id=\"\",\n    text=\"Machine learning enables computers to learn from data.\",\n    metadata=metadata\n)\nprint(f\"Created chunk: {chunk.chunk_id}\")\nprint(f\"Metadata: {chunk.metadata.to_dict()}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCreated chunk: f896db71a4a94422\nMetadata: {'source_id': 'doc_001', 'source_type': 'markdown', 'source_url': None, 'page_number': None, 'section_title': 'Introduction', 'section_hierarchy': [], 'start_char': 0, 'end_char': 0, 'language': 'en', 'content_type': 'text', 'word_count': 42, 'indexed_at': '2025-12-05T11:25:40.066163', 'custom': {}}\n```\n:::\n:::\n\n\n## Handling Special Content\n\n### Tables\n\nTables require special handling—row-by-row chunking loses context:\n\n::: {#9d348496 .cell execution_count=13}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show Table Chunking\"}\nimport re\nfrom typing import List, Tuple\n\ndef detect_tables(text: str) -> List[Tuple[int, int, str]]:\n    \"\"\"Detect tables in text and return their positions.\"\"\"\n    tables = []\n    # Detect Markdown tables\n    md_table_pattern = r\"(\\|[^\\n]+\\|\\n\\|[-:| ]+\\|\\n(?:\\|[^\\n]+\\|\\n?)+)\"\n    for match in re.finditer(md_table_pattern, text):\n        tables.append((match.start(), match.end(), match.group(0)))\n    return sorted(tables, key=lambda x: x[0])\n\ndef parse_markdown_table(table_text: str) -> Tuple[List[str], List[List[str]]]:\n    \"\"\"Parse a Markdown table into headers and rows.\"\"\"\n    lines = [line.strip() for line in table_text.strip().split(\"\\n\")]\n    if len(lines) < 2:\n        return [], []\n\n    # Parse header row\n    headers = [cell.strip() for cell in lines[0].split(\"|\")[1:-1]]\n\n    # Skip separator line, parse data rows\n    rows = []\n    for line in lines[2:]:\n        if line.startswith(\"|\"):\n            cells = [cell.strip() for cell in line.split(\"|\")[1:-1]]\n            rows.append(cells)\n\n    return headers, rows\n\ndef table_to_text(headers: List[str], rows: List[List[str]], format: str = \"natural\") -> str:\n    \"\"\"Convert table to natural language for embedding.\"\"\"\n    if format == \"natural\":\n        lines = []\n        for row in rows:\n            parts = []\n            for header, value in zip(headers, row):\n                if value and value != \"-\":\n                    parts.append(f\"{header} is {value}\")\n            if parts:\n                lines.append(\". \".join(parts) + \".\")\n        return \"\\n\".join(lines)\n    else:\n        # Keep as markdown\n        return (f\"| {' | '.join(headers)} |\\n\" +\n                f\"|{'|'.join(['---'] * len(headers))}|\\n\" +\n                \"\\n\".join(f\"| {' | '.join(row)} |\" for row in rows))\n\n# Usage example\nsample_table = \"\"\"\n| Framework | Language | GPU Support |\n|-----------|----------|-------------|\n| TensorFlow | Python | Excellent |\n| PyTorch | Python | Excellent |\n| JAX | Python | Excellent |\n\"\"\"\n\nheaders, rows = parse_markdown_table(sample_table)\nnatural_text = table_to_text(headers, rows, format=\"natural\")\nprint(\"Table converted to natural language:\")\nprint(natural_text)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTable converted to natural language:\nFramework is TensorFlow. Language is Python. GPU Support is Excellent.\nFramework is PyTorch. Language is Python. GPU Support is Excellent.\nFramework is JAX. Language is Python. GPU Support is Excellent.\n```\n:::\n:::\n\n\n### Lists\n\nNumbered and bulleted lists should stay together when possible:\n\n::: {#eea4783b .cell execution_count=14}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show List Chunking\"}\nimport re\nfrom dataclasses import dataclass\nfrom typing import List\n\n@dataclass\nclass ListBlock:\n    \"\"\"A detected list block in text.\"\"\"\n    start_pos: int\n    end_pos: int\n    text: str\n    list_type: str  # 'bullet', 'numbered'\n    items: List[str]\n\ndef detect_lists(text: str) -> List[ListBlock]:\n    \"\"\"Detect list structures in text.\"\"\"\n    lists = []\n\n    # Bullet list pattern\n    bullet_pattern = r\"((?:^[ \\t]*[-*•][ \\t]+.+$\\n?)+)\"\n\n    # Numbered list pattern\n    numbered_pattern = r\"((?:^[ \\t]*(?:\\d+\\.|[a-z]\\.)[ \\t]+.+$\\n?)+)\"\n\n    for pattern, list_type in [(bullet_pattern, \"bullet\"), (numbered_pattern, \"numbered\")]:\n        for match in re.finditer(pattern, text, re.MULTILINE):\n            items = parse_list_items(match.group(0), list_type)\n            lists.append(ListBlock(\n                start_pos=match.start(),\n                end_pos=match.end(),\n                text=match.group(0),\n                list_type=list_type,\n                items=items,\n            ))\n\n    return sorted(lists, key=lambda x: x.start_pos)\n\ndef parse_list_items(list_text: str, list_type: str) -> List[str]:\n    \"\"\"Parse individual items from a list block.\"\"\"\n    if list_type == \"bullet\":\n        pattern = r\"^[ \\t]*[-*•][ \\t]+(.+)$\"\n    else:  # numbered\n        pattern = r\"^[ \\t]*(?:\\d+\\.|[a-z]\\.)[ \\t]+(.+)$\"\n\n    items = []\n    for match in re.finditer(pattern, list_text, re.MULTILINE):\n        items.append(match.group(1).strip())\n\n    return items\n\n# Usage example\nsample_text = \"\"\"\nMachine learning algorithms include:\n\n- Linear Regression: Used for predicting continuous values\n- Logistic Regression: Used for binary classification\n- Decision Trees: Tree-based models\n- Random Forests: Ensemble of decision trees\n- Neural Networks: Deep learning models\n\nEach has different use cases.\n\"\"\"\n\nlists = detect_lists(sample_text)\nprint(f\"Found {len(lists)} lists\")\nfor lst in lists:\n    print(f\"List type: {lst.list_type}, {len(lst.items)} items\")\n    for item in lst.items[:2]:\n        print(f\"  - {item}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFound 1 lists\nList type: bullet, 5 items\n  - Linear Regression: Used for predicting continuous values\n  - Logistic Regression: Used for binary classification\n```\n:::\n:::\n\n\n### Code Blocks\n\nCode embedded in documentation needs preservation:\n\n::: {#e851fa10 .cell execution_count=15}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show Code Block Handling\"}\nimport re\nfrom dataclasses import dataclass\nfrom typing import List, Tuple\n\n@dataclass\nclass CodeBlock:\n    \"\"\"A code block extracted from documentation.\"\"\"\n    start_pos: int\n    end_pos: int\n    code: str\n    language: str\n    preceding_context: str = \"\"\n\ndef extract_code_blocks(text: str) -> Tuple[str, List[CodeBlock]]:\n    \"\"\"Extract fenced code blocks and replace with placeholders.\"\"\"\n    code_blocks = []\n    placeholder_template = \"<<<CODE_BLOCK_{}>>>\"\n\n    def replace_block(match):\n        index = len(code_blocks)\n        language = match.group(1) or \"text\"\n        code = match.group(2)\n\n        # Get preceding line for context\n        start = match.start()\n        preceding = text[max(0, start - 200):start]\n        last_line = preceding.split(\"\\n\")[-1].strip()\n\n        code_blocks.append(CodeBlock(\n            start_pos=match.start(),\n            end_pos=match.end(),\n            code=code,\n            language=language,\n            preceding_context=last_line,\n        ))\n        return placeholder_template.format(index)\n\n    # Match fenced code blocks\n    pattern = r\"```(\\w*)\\n(.*?)```\"\n    text_with_placeholders = re.sub(pattern, replace_block, text, flags=re.DOTALL)\n\n    return text_with_placeholders, code_blocks\n\ndef restore_code_blocks(chunks: List[str], code_blocks: List[CodeBlock],\n                       format: str = \"inline\") -> List[str]:\n    \"\"\"Restore code blocks to chunks.\"\"\"\n    placeholder_pattern = r\"<<<CODE_BLOCK_(\\d+)>>>\"\n\n    restored = []\n    for chunk in chunks:\n        matches = list(re.finditer(placeholder_pattern, chunk))\n\n        if not matches:\n            restored.append(chunk)\n            continue\n\n        result = chunk\n        for match in reversed(matches):\n            index = int(match.group(1))\n            block = code_blocks[index]\n\n            if format == \"inline\":\n                replacement = f\"```{block.language}\\n{block.code}```\"\n            elif format == \"reference\":\n                replacement = f\"[Code block: {block.language}]\"\n            else:\n                replacement = block.code\n\n            result = result[:match.start()] + replacement + result[match.end():]\n\n        restored.append(result)\n\n    return restored\n\n# Usage example\nbackticks = '`' * 3  # Avoid literal ``` which breaks Quarto parsing\nsample_doc = f\"\"\"\nHere's how to create embeddings:\n\n{backticks}python\nfrom sentence_transformers import SentenceTransformer\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\nembeddings = model.encode(['Hello', 'World'])\n{backticks}\n\nThis creates vector representations.\n\"\"\"\n\ntext_with_placeholders, blocks = extract_code_blocks(sample_doc)\nprint(f\"Extracted {len(blocks)} code blocks\")\nprint(f\"Text with placeholders: {text_with_placeholders[:100]}...\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nExtracted 1 code blocks\nText with placeholders: \nHere's how to create embeddings:\n\n<<<CODE_BLOCK_0>>>\n\nThis creates vector representations.\n...\n```\n:::\n:::\n\n\n## Production Chunking Pipeline\n\nPutting it all together into a production-ready pipeline:\n\n::: {#68b1107f .cell execution_count=16}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show Production Chunking Pipeline\"}\nimport hashlib\nfrom dataclasses import dataclass, field\nfrom datetime import datetime\nfrom enum import Enum\nfrom typing import Any, Dict, List, Optional\n\nclass DocumentType(Enum):\n    \"\"\"Supported document types.\"\"\"\n    PLAIN_TEXT = \"text\"\n    MARKDOWN = \"markdown\"\n    HTML = \"html\"\n    PDF = \"pdf\"\n    CODE = \"code\"\n\n@dataclass\nclass ProcessedChunk:\n    \"\"\"A fully processed chunk ready for embedding.\"\"\"\n    chunk_id: str\n    text: str\n    document_id: str\n    chunk_index: int\n    metadata: Dict[str, Any] = field(default_factory=dict)\n    embedding: Optional[List[float]] = None\n\n@dataclass\nclass PipelineConfig:\n    \"\"\"Configuration for the chunking pipeline.\"\"\"\n    chunk_size: int = 500\n    chunk_overlap: int = 50\n    min_chunk_size: int = 50\n    preserve_structure: bool = True\n    include_metadata: bool = True\n    deduplicate: bool = True\n    min_word_count: int = 10\n    max_word_count: int = 2000\n\nclass ChunkingPipeline:\n    \"\"\"Production chunking pipeline with quality filtering and metadata enrichment.\"\"\"\n    def __init__(self, config: Optional[PipelineConfig] = None):\n        self.config = config or PipelineConfig()\n\n    def process_document(self, content: str, document_id: str,\n                        document_type: Optional[DocumentType] = None,\n                        source_metadata: Optional[Dict] = None) -> List[ProcessedChunk]:\n        \"\"\"Process a single document through the full pipeline.\"\"\"\n        if document_type is None:\n            document_type = self._detect_type(content)\n\n        # Simplified chunking for demo\n        words = content.split()\n        chunks = []\n        current_chunk = []\n\n        for word in words:\n            current_chunk.append(word)\n            if len(\" \".join(current_chunk)) >= self.config.chunk_size:\n                chunks.append(\" \".join(current_chunk))\n                current_chunk = []\n\n        if current_chunk:\n            chunks.append(\" \".join(current_chunk))\n\n        # Filter and enrich\n        filtered = [c for c in chunks if len(c.split()) >= self.config.min_word_count]\n\n        processed = []\n        for i, text in enumerate(filtered):\n            chunk_id = hashlib.sha256(f\"{document_id}:{i}:{text[:50]}\".encode()).hexdigest()[:16]\n            metadata = {\n                \"document_type\": document_type.value,\n                \"chunk_index\": i,\n                \"word_count\": len(text.split()),\n                \"processed_at\": datetime.now().isoformat(),\n            }\n            processed.append(ProcessedChunk(\n                chunk_id=chunk_id,\n                text=text,\n                document_id=document_id,\n                chunk_index=i,\n                metadata=metadata,\n            ))\n\n        return processed\n\n    def _detect_type(self, content: str) -> DocumentType:\n        \"\"\"Detect document type from content.\"\"\"\n        if content.strip().startswith(\"#\") or \"```\" in content:\n            return DocumentType.MARKDOWN\n        if \"<html\" in content.lower():\n            return DocumentType.HTML\n        return DocumentType.PLAIN_TEXT\n\n# Usage example\nconfig = PipelineConfig(chunk_size=200, min_word_count=10)\npipeline = ChunkingPipeline(config)\n\nsample_doc = \"Machine learning transforms data processing. \" * 50\nchunks = pipeline.process_document(sample_doc, \"doc_001\")\nprint(f\"Processed {len(chunks)} chunks\")\nfor chunk in chunks[:2]:\n    print(f\"  Chunk {chunk.chunk_index}: {chunk.metadata['word_count']} words\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nProcessed 11 chunks\n  Chunk 0: 23 words\n  Chunk 1: 23 words\n```\n:::\n:::\n\n\n## Evaluating Chunk Quality\n\nHow do you know if your chunking strategy is working?\n\n### Retrieval Quality Metrics\n\n::: {#77948dbc .cell execution_count=17}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show Chunk Quality Evaluation\"}\nfrom dataclasses import dataclass\nfrom typing import Dict, List, Optional\n\n# Check for numpy\ntry:\n    import numpy as np\n    HAS_NUMPY = True\nexcept ImportError:\n    HAS_NUMPY = False\n\ndef _mean(values: List[float]) -> float:\n    \"\"\"Calculate mean without numpy.\"\"\"\n    return sum(values) / len(values) if values else 0.0\n\ndef _std(values: List[float]) -> float:\n    \"\"\"Calculate standard deviation without numpy.\"\"\"\n    if not values:\n        return 0.0\n    avg = _mean(values)\n    variance = sum((x - avg) ** 2 for x in values) / len(values)\n    return variance ** 0.5\n\n@dataclass\nclass ChunkQualityMetrics:\n    \"\"\"Quality metrics for a set of chunks.\"\"\"\n    avg_chunk_size: float\n    std_chunk_size: float\n    min_chunk_size: int\n    max_chunk_size: int\n    avg_word_count: float\n    unique_terms_ratio: float\n    precision_at_k: Optional[float] = None\n    recall_at_k: Optional[float] = None\n    mrr: Optional[float] = None  # Mean Reciprocal Rank\n\ndef evaluate_chunk_quality(chunks: List[str], queries: Optional[List[str]] = None,\n                          ground_truth: Optional[List[List[int]]] = None,\n                          k: int = 5) -> ChunkQualityMetrics:\n    \"\"\"Evaluate the quality of a chunking strategy.\"\"\"\n    # Size metrics\n    sizes = [len(c) for c in chunks]\n    word_counts = [len(c.split()) for c in chunks]\n\n    # Unique terms ratio\n    all_terms = []\n    for chunk in chunks:\n        all_terms.extend(chunk.lower().split())\n    unique_ratio = len(set(all_terms)) / len(all_terms) if all_terms else 0\n\n    if HAS_NUMPY:\n        avg_size = np.mean(sizes)\n        std_size = np.std(sizes)\n        avg_words = np.mean(word_counts)\n    else:\n        avg_size = _mean(sizes)\n        std_size = _std(sizes)\n        avg_words = _mean(word_counts)\n\n    return ChunkQualityMetrics(\n        avg_chunk_size=avg_size,\n        std_chunk_size=std_size,\n        min_chunk_size=min(sizes) if sizes else 0,\n        max_chunk_size=max(sizes) if sizes else 0,\n        avg_word_count=avg_words,\n        unique_terms_ratio=unique_ratio,\n    )\n\ndef suggest_improvements(metrics: ChunkQualityMetrics) -> List[str]:\n    \"\"\"Suggest improvements based on quality metrics.\"\"\"\n    suggestions = []\n\n    if metrics.std_chunk_size > metrics.avg_chunk_size * 0.5:\n        suggestions.append(\n            \"High chunk size variance detected. Consider using fixed-size chunking.\"\n        )\n\n    if metrics.avg_chunk_size < 100:\n        suggestions.append(\n            \"Chunks are very small. Consider increasing chunk size to preserve more context.\"\n        )\n\n    if metrics.avg_chunk_size > 1000:\n        suggestions.append(\n            \"Chunks are quite large. Consider reducing chunk size for better precision.\"\n        )\n\n    if metrics.unique_terms_ratio < 0.3:\n        suggestions.append(\n            \"Low unique terms ratio indicates repetitive content. Consider deduplication.\"\n        )\n\n    return suggestions\n\n# Usage example\nsample_chunks = [\n    \"Machine learning is a subset of artificial intelligence.\",\n    \"Neural networks are inspired by biological neurons.\",\n    \"Deep learning uses multiple layers for feature extraction.\",\n]\n\nmetrics = evaluate_chunk_quality(sample_chunks)\nprint(\"Chunk Quality Metrics:\")\nprint(f\"  Avg chunk size: {metrics.avg_chunk_size:.0f} chars\")\nprint(f\"  Avg word count: {metrics.avg_word_count:.1f}\")\nprint(f\"  Unique terms ratio: {metrics.unique_terms_ratio:.2%}\")\n\nimprovements = suggest_improvements(metrics)\nif improvements:\n    print(\"\\nSuggested Improvements:\")\n    for suggestion in improvements:\n        print(f\"  - {suggestion}\")\n```\n:::\n\n\n### Common Failure Patterns\n\n| Symptom | Likely Cause | Solution |\n|---------|--------------|----------|\n| Relevant info not retrieved | Chunks too large, query buried | Reduce chunk size |\n| Retrieved chunks lack context | Chunks too small | Increase chunk size or overlap |\n| Duplicate information in results | Too much overlap | Reduce overlap, add deduplication |\n| Poor performance on tables | Tables split incorrectly | Use table-aware chunking |\n| Code examples broken | Split mid-function | Use AST-aware code chunking |\n| Headers orphaned from content | Structural chunking too aggressive | Keep headers with following content |\n\n: Chunking troubleshooting guide {.striped}\n\n## Key Takeaways\n\n- **RAG systems embed chunks, not words**: Modern embedding models process entire passages to create single vectors that capture contextual meaning—this is fundamentally different from word embeddings like Word2Vec\n\n- **Chunking directly impacts retrieval quality**: Poor chunking strategies lead to poor results regardless of embedding model quality; it's often the highest-leverage optimization available\n\n- **Match strategy to content type**: Fixed-size for unstructured text, sentence-based for prose, paragraph-based for well-formatted documents, semantic chunking for topic-diverse content, and recursive chunking for mixed-structure documents\n\n- **Overlap prevents boundary information loss**: 10-20% overlap ensures information at chunk boundaries appears in multiple chunks, improving recall at modest storage cost\n\n- **Preserve metadata for filtering and context**: Source document, section headers, page numbers, and timestamps enable hybrid search and help users understand retrieved content\n\n- **Evaluate empirically on your data**: Optimal chunk size depends on your specific content and queries; use evaluation frameworks to compare strategies systematically\n\n## Looking Ahead\n\nThis chapter covered text chunking—preparing documents for embedding. @sec-image-preparation explores the parallel challenge for visual data: how to prepare images for embedding systems, including preprocessing, region extraction, and handling large-scale imagery like satellite photos and medical scans.\n\n## Further Reading\n\n- Langchain Documentation: \"Text Splitters\" - Comprehensive guide to chunking implementations\n- Liu, N., et al. (2023). \"Lost in the Middle: How Language Models Use Long Contexts.\" *arXiv:2307.03172*\n- Shi, W., et al. (2023). \"REPLUG: Retrieval-Augmented Black-Box Language Models.\" *arXiv:2301.12652*\n- Gao, L., et al. (2023). \"Precise Zero-Shot Dense Retrieval without Relevance Labels.\" *arXiv:2212.10496*\n- Robertson, S., and Zaragoza, H. (2009). \"The Probabilistic Relevance Framework: BM25 and Beyond.\" *Foundations and Trends in Information Retrieval*\n\n",
    "supporting": [
      "ch17_text_chunking_files/figure-epub"
    ],
    "filters": [],
    "engineDependencies": {
      "jupyter": [
        {
          "jsWidgets": false,
          "jupyterWidgets": false,
          "htmlLibraries": []
        }
      ]
    }
  }
}