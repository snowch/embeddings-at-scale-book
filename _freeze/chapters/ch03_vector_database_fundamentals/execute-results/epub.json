{
  "hash": "fd988d18be2109ea2aac9b8a9c5fa9aa",
  "result": {
    "engine": "jupyter",
    "markdown": "# Vector Database Fundamentals for Scale {#sec-vector-database-fundamentals}\n\n:::{.callout-important}\n## Chapter Update In Progress\nThis chapter is being updated to reflect the VAST Data Platform Vector DB architecture. Sections on sharding, replication, and distribution patterns will be revised to cover VAST-specific approaches. The foundational concepts (indexing algorithms, SLA design, benchmarking) remain applicable.\n:::\n\n:::{.callout-note}\n## Chapter Overview\nThis chapter covers vector database architecture principles, indexing strategies for 256+ trillion rows, distributed systems considerations, performance benchmarking and SLA design, and data locality patterns for global-scale embedding deployments.\n:::\n\n## Vector Database Architecture Principles\n\nTraditional databases were designed for exact matches: \"Find customer with ID=12345\" or \"Return all orders where status='shipped'\". Vector databases serve a fundamentally different purpose: finding semantic similarity in high-dimensional space. This section explores the architectural principles that make trillion-row vector search possible.\n\n### Why Traditional Databases Fail for Embeddings\n\nThe scale mismatch becomes clear with a simple calculation:\n\n```python\n# Traditional database query\ndef find_customer(database, customer_id):\n    \"\"\"O(log N) with B-tree index\"\"\"\n    return database.index['customer_id'].lookup(customer_id)\n    # 256 trillion rows: ~48 comparisons\n\n# Naive embedding search\ndef find_similar_naive(query_embedding, all_embeddings):\n    \"\"\"O(N * D) where N=rows, D=dimensions\"\"\"\n    similarities = []\n    for embedding in all_embeddings:  # 256 trillion iterations\n        similarity = cosine_similarity(query_embedding, embedding)  # 768 multiplications\n        similarities.append(similarity)\n    return top_k(similarities, k=10)\n\n# Cost calculation:\n# 256 trillion rows × 768 dimensions = 196 quadrillion operations\n# At 1 billion ops/second: 6 years per query\n```\n\nTraditional databases optimize for exact lookups and range scans. Vector databases optimize for approximate nearest neighbor (ANN) search in high-dimensional space. These are fundamentally different problems requiring different architectures.\n\n### The Core Architectural Principles\n\n**Principle 1: Approximate is Sufficient**\n\nUnlike financial transactions where precision is mandatory, embedding similarity is inherently approximate. Whether an item is the 47th or 48th most similar out of 256 trillion doesn't matter—both are highly relevant.\n\nThis insight unlocks massive performance gains:\n\n| Aspect | Traditional DB | Vector DB |\n|--------|---------------|-----------|\n| **Correctness** | 100% exact | 95-99% approximate |\n| **Performance** | O(log N) with index | O(log N) even without perfect accuracy |\n| **Use Case** | Exact match, transactions | Semantic similarity, recommendations |\n\nThe key insight: trading a small amount of accuracy for massive speed gains. Finding the top-10 most similar items from 256T vectors via exact search is infeasible—approximate nearest neighbor (ANN) algorithms achieve 95%+ recall in milliseconds.\n\n**Principle 2: Geometry Matters More Than Algebra**\n\nVector databases exploit geometric structure rather than brute-force computation. Similar embeddings cluster together in space, and similarity metrics like cosine and Euclidean distance have properties that enable clever indexing shortcuts. For a detailed comparison of these metrics and when to use each, see @sec-similarity-distance-metrics.\n\n**Principle 3: Index Structure is Everything**\n\nYou don't need to compare against all vectors—the right index structure lets you navigate efficiently. The choice determines performance, accuracy, and scalability:\n\n| Index | Build Time | Query Time | Memory | Accuracy | Max Scale | Use Case |\n|-------|-----------|------------|--------|----------|-----------|----------|\n| **Flat** | O(N) | O(N×D) | O(N×D) | 100% | ~1M | Ground truth |\n| **IVF** | O(N×k) | O((N/k)×D×n_probe) | O(N×D+k×D) | 80-95% | ~1B | Balanced |\n| **HNSW** | O(N×log(N)×M) | O(log(N)×M) | O(N×(D+M)) | 95-99% | 100B+ | Production (best tradeoff) |\n| **LSH** | O(N×L) | O(L×bucket) | O(N×L) | 70-90% | Trillion+ | Ultra-massive scale |\n| **PQ** | O(N×iter) | O(N) compressed | O(N×code) | 85-95% | 10B+ | Memory-constrained |\n\nHNSW is the gold standard for high-performance production systems due to its best accuracy/speed tradeoff at scale.\n\n:::{.callout-note}\n## VAST Data Platform Approach\nThis section will cover VAST Data Platform's approach to data distribution and storage architecture, which differs from traditional sharding patterns. VAST's unified storage architecture eliminates many of the manual sharding decisions required by other vector databases.\n:::\n\n:::{.callout-important}\n## Architecture First, Scaling Later\nThe most expensive mistake: starting with single-node architecture and retrofitting for scale. Design for distribution from day one, even if you start with one machine. The patterns are the same, only the scale changes.\n:::\n\n## Indexing Strategies at Scale\n\nScaling to trillions of embeddings requires sophisticated indexing strategies that balance accuracy, speed, memory, and build time. This section explores battle-tested approaches.\n\n### The Indexing Challenge at Scale\n\nBefore diving into solutions, let's quantify what 1 trillion 768-dimensional vectors actually means:\n\n- **Storage**: Each vector is 768 floats × 4 bytes = 3KB. One trillion vectors = ~2.8 PB raw, plus ~50% overhead for HNSW graph structure.\n- **Memory cost**: If you wanted everything in RAM, you'd need thousands of high-memory machines at tens of millions per month.\n- **Build time**: HNSW takes ~100μs per vector insertion. Single-threaded, that's years. Even with massive parallelism, it's a significant operation.\n- **Query budget**: Users expect results in under 100ms. After network and processing overhead, the index search itself gets maybe 50ms—no room for brute force.\n\n::: {#5f2fbb60 .cell execution_count=1}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show the math\"}\n# Scale parameters\nnum_vectors = 1_000_000_000_000  # 1 trillion\nembedding_dim = 768\nbytes_per_float = 4\n\n# Storage calculation\nraw_petabytes = (num_vectors * embedding_dim * bytes_per_float) / (1024 ** 5)\nwith_index_pb = raw_petabytes * 1.5  # HNSW adds ~50% overhead\n\n# Cost if all in RAM (AWS r6i.32xlarge: 1TB RAM, $8.064/hour)\nmachines_1tb = int(with_index_pb * 1024)\nmonthly_cost = machines_1tb * 8.064 * 24 * 30\n\n# Build time (~100μs per vector for HNSW)\nbuild_seconds = (num_vectors * 100) / 1_000_000\nbuild_years = build_seconds / (60 * 60 * 24 * 365)\nparallel_hours = (build_seconds / 10_000) / (60 * 60)  # With 10K machines\n\nprint(f\"Storage:    {with_index_pb:,.0f} PB (embeddings + HNSW index)\")\nprint(f\"Machines:   {machines_1tb:,} × 1TB RAM instances\")\nprint(f\"Cost:       ${monthly_cost/1e6:,.0f}M/month if all in RAM\")\nprint(f\"Build time: {build_years:,.0f} years single-machine, {parallel_hours:.0f} hours with 10K machines\")\nprint(f\"Query:      Must return results in <50ms (no room for brute force)\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nStorage:    4 PB (embeddings + HNSW index)\nMachines:   4,190 × 1TB RAM instances\nCost:       $24M/month if all in RAM\nBuild time: 3 years single-machine, 3 hours with 10K machines\nQuery:      Must return results in <50ms (no room for brute force)\n```\n:::\n:::\n\n\nClearly, naïve approaches won't work. We need sophisticated indexing strategies.\n\n### Strategy 1: Hierarchical Navigable Small World (HNSW)\n\nHNSW is the gold standard for high-recall, low-latency vector search. Understanding how it works is essential for trillion-scale deployments.\n\n**Core Concept**: HNSW builds a multi-layer graph where each vector is randomly assigned to a level (higher levels are exponentially rarer). Within each layer, vectors connect to their nearest neighbors. The key insight:\n\n- **Upper layers**: Few nodes, spread far apart → each hop covers large distances\n- **Bottom layer**: All nodes, densely packed → each hop is fine-grained\n- **Search**: Enter at top, greedily hop toward query, descend when stuck\n\n::: {#cell-fig-hnsw-navigation .cell execution_count=2}\n\n::: {.cell-output .cell-output-display}\n![HNSW navigation: enter at sparse top layer, greedily hop to neighbors closest to query, descend when you can't improve. With 1T vectors in ~15 layers, this turns O(N) brute force into O(log N) traversal.](ch03_vector_database_fundamentals_files/figure-epub/fig-hnsw-navigation-output-1.png){#fig-hnsw-navigation}\n:::\n:::\n\n\n**Tuning HNSW Parameters**\n\n| Parameter | What it controls | Small (1M) | Medium (100M) | Large (1B+) |\n|-----------|-----------------|------------|---------------|-------------|\n| **M** | Connections per node | 16 | 32 | 48-64 |\n| **ef_construction** | Build quality (candidates considered) | 100 | 200 | 400 |\n| **ef_search** | Query quality vs speed | 50 | 100 | 200 |\n\nHigher M and ef values improve recall but increase memory and latency. At 1B vectors with M=48, expect ~6 layers and ~300 comparisons per query—orders of magnitude faster than brute force.\n\n### Strategy 2: IVF (Inverted File Index) with Product Quantization\n\nWhile HNSW is excellent for recall and latency, IVF-PQ excels at massive scale with memory constraints.\n\n**Core Concept**: IVF-PQ combines two techniques:\n\n- **IVF (Inverted File)**: Partition vectors into clusters using k-means. At query time, only search the nearest clusters instead of all vectors.\n- **Product Quantization (PQ)**: Split each vector into subvectors and quantize each independently. A 768-dim vector becomes ~96 bytes instead of 3KB—32x compression.\n\n::: {#cell-fig-ivf-pq .cell execution_count=3}\n\n::: {.cell-output .cell-output-display}\n![IVF-PQ: (Left) IVF partitions space into clusters—query searches only nearby cells (green), skipping distant ones (gray). (Right) PQ compresses each vector by splitting into subvectors and storing cluster IDs instead of floats.](ch03_vector_database_fundamentals_files/figure-epub/fig-ivf-pq-output-1.png){#fig-ivf-pq}\n:::\n:::\n\n\n**Tuning IVF-PQ Parameters**\n\n| Parameter | What it controls | Small (10M) | Medium (1B) | Large (100B+) |\n|-----------|-----------------|-------------|-------------|---------------|\n| **nlist** | Number of clusters | 1,024 | 16,384 | 65,536 |\n| **nprobe** | Clusters to search | 8 | 64 | 128 |\n| **M (PQ)** | Subquantizers | 16 | 48 | 96 |\n\nHigher nprobe improves recall but increases latency. At 100B vectors with 96 subquantizers, expect 32x compression (286 TB → 9 TB) with 85-90% recall.\n\n**HNSW vs IVF-PQ Trade-offs**\n\n| Dimension | HNSW | IVF-PQ | Winner |\n|-----------|------|--------|--------|\n| **Memory** | 1.5-2x raw data | 0.02-0.05x (20-50x compression) | IVF-PQ |\n| **Recall** | 95-99% | 85-95% | HNSW |\n| **Latency (p99)** | 20-100ms | 50-200ms | HNSW |\n| **Build Time** | Slower (graph construction) | Faster (k-means) | IVF-PQ |\n| **Updates** | Easy incremental | Must reassign centroids | HNSW |\n| **Max Scale** | ~100B vectors | Trillions+ | IVF-PQ |\n\n**When to use each:**\n\n- **HNSW**: High recall (>95%), low latency (<100ms p99), frequent updates, sufficient memory\n- **IVF-PQ**: Memory constrained, can tolerate 85-90% recall, infrequent updates, trillion+ scale\n- **Hybrid IVF-HNSW**: Best of both—IVF for coarse search, HNSW within partitions\n\n### Strategy 3: Data Distribution at Scale\n\nAt trillion scale, efficient data distribution is essential for performance and availability.\n\n:::{.callout-note}\n## VAST Data Platform Approach\nThis section will cover VAST Data Platform's approach to data distribution, which provides automatic scaling and data placement without manual sharding configuration. VAST's architecture handles data distribution transparently, eliminating the complexity of traditional sharding strategies.\n:::\n\n## Distributed Systems Considerations\n\nVector databases at trillion-scale are distributed systems, inheriting all the challenges of distributed computing: consistency, availability, partition tolerance, and coordination.\n\n### The CAP Theorem for Vector Databases\n\nVector databases choose **AP (Availability + Partition Tolerance)** over strong consistency. This is the right tradeoff because embeddings are inherently approximate—if one replica has slightly outdated embeddings, query results are still useful.\n\n**Consistency requirements by operation:**\n\n- **Writes/Inserts**: Eventual consistency. Write to primary, async replicate. New embedding visible within 5 seconds.\n- **Updates/Deletions**: Eventual consistency with tombstones. Deleted items filtered at query time.\n- **Reads/Queries**: Read-your-writes for same session (via session affinity). May see stale data from other users—acceptable.\n- **Metadata filters**: Strong consistency required. Security filters (user access) must be immediate.\n\n**Availability techniques**: 3x replication, read from any replica, automatic failover, circuit breakers. Target: 99.99%.\n\n**Partition tolerance**: Gracefully degrade by serving cached results, partial results from available shards, or falling back to multi-region replicas.\n\n### Replication and Data Protection\n\nData replication ensures availability and durability at scale.\n\n:::{.callout-note}\n## VAST Data Platform Approach\nThis section will cover VAST Data Platform's built-in data protection and failure recovery mechanisms, including:\n\n- **Data Protection**: VAST's approach to erasure coding and data redundancy\n- **Automatic Failover**: How VAST handles node and disk failures transparently\n- **Disaster Recovery**: Multi-site replication and recovery capabilities\n- **Self-Healing**: Automatic detection and recovery from failures\n\nVAST provides enterprise-grade reliability without requiring manual configuration of replication, sharding, or failover logic.\n:::\n\n### Coordination and Cluster Management\n\nDistributed vector databases require coordination for metadata management and cluster operations.\n\n:::{.callout-note}\n## VAST Data Platform Approach\nThis section will cover how VAST Data Platform handles cluster coordination and management. VAST's architecture simplifies operational complexity by providing integrated cluster management without external coordination services like ZooKeeper or etcd.\n:::\n\n## Performance Benchmarking and SLA Design\n\nProduction vector databases require rigorous SLA design and continuous performance monitoring. This section covers benchmarking methodologies and SLA patterns.\n\n### Defining SLA Metrics\n\nCore SLA metrics for vector databases:\n\n| Metric | Typical Target | Business Impact |\n|--------|---------------|-----------------|\n| **Query Latency** | p50 <20ms, p95 <50ms, p99 <100ms | Every 100ms → 1% conversion loss |\n| **Recall@K** | recall@10 >0.95, recall@100 >0.98 | Low recall → users don't find relevant items |\n| **Throughput** | 1K-10K QPS/shard, 100K-1M global | Insufficient → requests queued or dropped |\n| **Availability** | 99.99% (52 min downtime/year) | Downtime → lost revenue |\n| **Index Freshness** | <5 minutes to queryable | Stale data → missing new items |\n| **Resource Utilization** | CPU <70%, Memory <85%, Disk I/O <80% | Over-utilization → latency spikes |\n\n**Availability budget by target:**\n\n| Target | Allowed Downtime |\n|--------|-----------------|\n| 99% | 3.65 days/year |\n| 99.9% | 8.76 hours/year |\n| 99.99% | 52.6 minutes/year |\n| 99.999% | 5.26 minutes/year |\n\n**SLI vs SLO vs SLA:**\n\n- **SLI (Service Level Indicator)**: Quantitative measurement (e.g., \"p99 latency: 78ms\")\n- **SLO (Service Level Objective)**: Internal target (e.g., \"p99 < 100ms\")\n- **SLA (Service Level Agreement)**: Contract with consequences (e.g., \"p99 < 100ms or 10% credit\")\n\n### Benchmarking Methodology\n\nKey benchmark dimensions for vector databases:\n\n| Category | Metrics | Variables |\n|----------|---------|-----------|\n| **Index Build** | Build time, throughput (vec/sec), peak memory, CPU | Dataset size, dimensions, index params (M, ef) |\n| **Query** | p50/p95/p99 latency, QPS, recall@10/100 | K, ef_search, query distribution, concurrency |\n| **Updates** | Insert latency/throughput, recall drift | Insert rate, update fraction |\n| **Scalability** | Latency/memory vs size | Test at 1M, 10M, 100M, 1B, 10B vectors |\n\nStandard benchmark datasets include SIFT-1M (1M vectors, 128 dims), Deep1B (1B vectors, 96 dims), and LAION-5B (5B vectors, 768 dims). However, production data provides the most accurate benchmarks since query distributions differ from academic datasets.\n\n### Load Testing and Capacity Planning\n\nEssential load test scenarios for vector databases:\n\n- **Steady State**: Maintain target QPS (e.g., 100K) for 1 hour. Verify p99 <100ms, no errors, stable resource usage.\n- **Ramp Up**: Gradually increase 0→200K QPS over 30 minutes to find breaking point and verify graceful degradation.\n- **Spike**: Sudden burst (50K→500K QPS for 5 minutes) to test autoscaling—system should scale within 2 minutes.\n- **Sustained Peak**: 150K QPS for 8 hours to detect memory leaks and resource exhaustion.\n- **Thundering Herd**: 1M simultaneous requests to test queue depth control and load shedding.\n- **Geographic**: Multi-region simultaneous load to verify routing and cross-region failover.\n\nFor capacity planning, assume ~10K QPS per shard and maintain 2x headroom for spikes. With 50% YoY growth, plan 3 years ahead: 100K QPS today requires 20 shards with headroom, growing to 68 shards by year 3.\n\n## Data Locality and Global Distribution\n\nFor trillion-row systems serving global users, data locality and geographic distribution are critical for latency and compliance.\n\n:::{.callout-note}\n## VAST Data Platform Approach\nThis section will cover VAST Data Platform's approach to global data distribution, including:\n\n- **Geographic Distribution**: How VAST handles multi-region deployments and data placement\n- **Data Residency**: VAST's capabilities for GDPR, CCPA, and regional compliance requirements\n- **Latency Optimization**: Built-in mechanisms for minimizing query latency across regions\n- **Global Namespace**: VAST's unified approach to accessing data across locations\n\nVAST provides enterprise-grade global distribution without requiring manual configuration of replication patterns or regional sharding.\n:::\n\n## Key Takeaways\n\n- **Vector databases are fundamentally different from traditional databases**—optimized for approximate nearest neighbor search in high-dimensional space rather than exact matches, making approximate results and geometric reasoning core architectural principles\n\n- **HNSW is the gold standard for high-recall, low-latency search** at billion to trillion scale, achieving O(log N) query complexity through hierarchical graph navigation, with typical configurations (M=32-64, ef_construction=200-400) delivering 95-99% recall at <100ms p99\n\n- **IVF-PQ provides extreme memory efficiency** with 20-100x compression through coarse quantization and product quantization, making it the best choice for memory-constrained trillion-scale deployments despite slightly lower recall (85-95%)\n\n- **Data distribution is essential at trillion-scale**—modern platforms like VAST Data Platform handle distribution automatically, while traditional approaches require manual sharding with configurations of 100M-1B vectors per partition\n\n- **Vector databases choose AP over C in the CAP theorem**, prioritizing availability and partition tolerance with eventual consistency for embeddings (acceptable due to inherent approximation) while maintaining strong consistency for critical metadata like access controls\n\n- **SLA design requires percentile-based latency targets** (p99 <100ms is typical), recall guarantees (>95% recall@10), and availability targets (99.99%), measured continuously with public dashboards and automated alerting on violations\n\n- **Global distribution requires geographic strategies**—full replication for lowest latency (5x cost), regional sharding for data sovereignty (lower cost), tiered distribution for balanced cost/latency (60-80% savings), or edge caching for popular queries (85-95% hit rates)\n\n## Looking Ahead\n\nPart II begins with @sec-text-embeddings, exploring text embeddings—the most common type you'll encounter. You'll learn to create embeddings for words, sentences, and documents, with an optional \"Advanced\" section explaining how models like Word2Vec, BERT, and Sentence Transformers work under the hood. Subsequent chapters cover image, multi-modal, graph, time-series, and code embeddings.\n\n## Further Reading\n\n- Malkov, Y. A., & Yashunin, D. A. (2018). \"Efficient and robust approximate nearest neighbor search using Hierarchical Navigable Small World graphs.\" *IEEE Transactions on Pattern Analysis and Machine Intelligence*\n- Jégou, H., Douze, M., & Schmid, C. (2011). \"Product Quantization for Nearest Neighbor Search.\" *IEEE Transactions on Pattern Analysis and Machine Intelligence*\n- Johnson, J., Douze, M., & Jégou, H. (2019). \"Billion-scale similarity search with GPUs.\" *IEEE Transactions on Big Data*\n- Aumüller, M., Bernhardsson, E., & Faithfull, A. (2020). \"ANN-Benchmarks: A Benchmarking Tool for Approximate Nearest Neighbor Algorithms.\" *Information Systems*\n- Brewer, E. A. (2012). \"CAP twelve years later: How the 'rules' have changed.\" *Computer*\n- Gormley, C., & Tong, Z. (2015). *Elasticsearch: The Definitive Guide*. O'Reilly Media\n- Kleppmann, M. (2017). *Designing Data-Intensive Applications*. O'Reilly Media\n- Beyer, B., et al. (2016). *Site Reliability Engineering: How Google Runs Production Systems*. O'Reilly Media\n\n",
    "supporting": [
      "ch03_vector_database_fundamentals_files/figure-epub"
    ],
    "filters": [],
    "engineDependencies": {
      "jupyter": [
        {
          "jsWidgets": false,
          "jupyterWidgets": false,
          "htmlLibraries": []
        }
      ]
    }
  }
}