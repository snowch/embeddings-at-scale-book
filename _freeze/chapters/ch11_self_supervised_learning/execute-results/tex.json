{
  "hash": "d8049984261d21777f5adca4fce194a3",
  "result": {
    "engine": "jupyter",
    "markdown": "# Self-Supervised Learning Pipelines {#sec-self-supervised-learning}\n\n:::{.callout-note}\n## Chapter Overview\nWhile contrastive learning (Chapter 5) and Siamese networks (Chapter 6) require labeled pairs or triplets, self-supervised learning unlocks the ability to learn from unlabeled data at unprecedented scale. This chapter explores self-supervised techniques that leverage the inherent structure of data to create powerful embeddings without manual annotation. We cover masked language modeling for domain-specific text, vision transformers for industrial imagery, time-series forecasting approaches, and multi-modal self-supervision strategies. These techniques enable enterprises to train embeddings on trillions of unlabeled documents, images, and sensor readings—data that already exists but was previously unusable for training.\n:::\n\n## Self-Supervised Learning for Unlabeled Enterprise Data\n\nThe fundamental challenge facing enterprise AI: **you have petabytes of data but almost no labels**. Traditional supervised learning requires expensive manual annotation. Self-supervised learning solves this by turning the data itself into both input and supervision.\n\n### The Self-Supervised Paradigm\n\nSelf-supervised learning creates \"pretext tasks\" where the model must predict part of the input from other parts. The key insight: **by learning to solve these pretext tasks, the model develops representations that capture the underlying structure of the data**.\n\nCommon pretext tasks:\n\n- **Masked prediction**: Predict hidden parts (BERT, MAE)\n- **Next token prediction**: Predict future content (GPT, autoregressive models)\n- **Contrastive prediction**: Distinguish augmented views (SimCLR, MoCo)\n- **Reconstruction**: Rebuild input from transformed version (autoencoders)\n\n::: {.cell execution_count=1}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show Self-Supervised Embedding Framework\"}\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass SelfSupervisedEmbeddingFramework:\n    \"\"\"Framework for self-supervised learning on enterprise data.\n\n    Supports masked prediction, contrastive learning, and reconstruction tasks.\n    \"\"\"\n\n    def __init__(self, encoder_model, pretext_task=\"masked\", embedding_dim=768, mask_probability=0.15):\n        self.encoder = encoder_model\n        self.pretext_task = pretext_task\n        self.embedding_dim = embedding_dim\n        self.mask_probability = mask_probability\n\n        if pretext_task == \"masked\":\n            self.prediction_head = nn.Linear(embedding_dim, embedding_dim)\n        elif pretext_task == \"contrastive\":\n            self.projection_head = nn.Sequential(\n                nn.Linear(embedding_dim, embedding_dim), nn.ReLU(), nn.Linear(embedding_dim, 128)\n            )\n        elif pretext_task == \"reconstruction\":\n            self.decoder = self._build_decoder(embedding_dim)\n\n    def _build_decoder(self, embedding_dim):\n        return nn.Sequential(\n            nn.Linear(embedding_dim, embedding_dim * 2), nn.ReLU(),\n            nn.Linear(embedding_dim * 2, embedding_dim * 4), nn.ReLU(),\n            nn.Linear(embedding_dim * 4, embedding_dim)\n        )\n\n    def create_pretext_task(self, batch):\n        \"\"\"Create pretext task from unlabeled batch.\"\"\"\n        if self.pretext_task == \"masked\":\n            return self._create_masked_task(batch)\n        elif self.pretext_task == \"contrastive\":\n            return self._create_contrastive_task(batch)\n        elif self.pretext_task == \"reconstruction\":\n            return self._create_reconstruction_task(batch)\n\n    def _create_masked_task(self, batch):\n        batch_size, seq_len, features = batch.shape\n        mask = torch.rand(batch_size, seq_len) < self.mask_probability\n        inputs = batch.clone()\n        inputs[mask] = 0\n        return inputs, batch.clone(), mask\n\n    def _create_contrastive_task(self, batch):\n        view1 = self._augment(batch)\n        view2 = self._augment(batch)\n        return (view1, view2), None, None\n\n    def _create_reconstruction_task(self, batch):\n        noise = torch.randn_like(batch) * 0.1\n        return batch + noise, batch, None\n\n    def _augment(self, batch):\n        noise = torch.randn_like(batch) * 0.05\n        return batch + noise\n\n    def forward(self, inputs):\n        return self.encoder(inputs)\n\n    def compute_loss(self, inputs, targets, mask=None):\n        \"\"\"Compute loss for pretext task.\"\"\"\n        if self.pretext_task == \"masked\":\n            embeddings = self.encoder(inputs)\n            predictions = self.prediction_head(embeddings)\n            loss = F.mse_loss(predictions[mask], targets[mask])\n            with torch.no_grad():\n                accuracy = ((predictions[mask] - targets[mask]).abs() < 0.1).float().mean()\n            return loss, {\"loss\": loss.item(), \"accuracy\": accuracy.item()}\n        # Similar for other tasks...\n\n\n# Usage example\nencoder = nn.Sequential(nn.Linear(768, 768), nn.ReLU())\nframework = SelfSupervisedEmbeddingFramework(encoder, pretext_task=\"masked\")\nbatch = torch.randn(32, 512, 768)\ninputs, targets, mask = framework.create_pretext_task(batch)\nloss, metrics = framework.compute_loss(inputs, targets, mask)\nprint(f\"Loss: {metrics['loss']:.4f}, Accuracy: {metrics['accuracy']:.4f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLoss: 1.0021, Accuracy: 0.0796\n```\n:::\n:::\n\n\n:::{.callout-tip}\n## Choosing the Right Pretext Task\n\n**Masked prediction**: Best for structured data with natural ordering (text, sequences, time-series). Captures bidirectional context.\n\n**Contrastive learning**: Best when you can define meaningful augmentations. Works well for images, audio, multimodal data.\n\n**Reconstruction**: Best for high-dimensional data where reconstruction is meaningful. Good for images, sensor data.\n\n**Rule of thumb**: If your data has natural ordering, use masked prediction. If augmentations preserve semantics, use contrastive. If neither, try reconstruction.\n:::\n\n### Enterprise Self-Supervised Pipeline\n\nProduction self-supervised learning requires careful data management and training infrastructure:\n\n::: {.cell execution_count=2}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show Enterprise Self-Supervised Pipeline\"}\nimport torch\nimport torch.distributed as dist\nfrom torch.nn.parallel import DistributedDataParallel\nfrom torch.utils.data import DistributedSampler\n\n\nclass EnterpriseSelfsupervisedPipeline:\n    \"\"\"Production self-supervised learning pipeline with distributed training,\n    checkpointing, and monitoring.\n    \"\"\"\n\n    def __init__(self, model, data_source, batch_size=256, num_workers=8,\n                 checkpoint_dir=\"./checkpoints\", log_dir=\"./logs\"):\n        self.model = model\n        self.data_source = data_source\n        self.batch_size = batch_size\n        self.num_workers = num_workers\n        self.checkpoint_dir = checkpoint_dir\n        self.log_dir = log_dir\n        self.world_size = torch.cuda.device_count()\n        self.is_distributed = self.world_size > 1\n\n    def setup_distributed(self):\n        \"\"\"Initialize distributed training.\"\"\"\n        if self.is_distributed:\n            dist.init_process_group(backend=\"nccl\")\n            local_rank = dist.get_rank()\n            torch.cuda.set_device(local_rank)\n            self.model = DistributedDataParallel(self.model, device_ids=[local_rank])\n\n    def train(self, num_epochs=100, learning_rate=1e-4):\n        \"\"\"Train self-supervised model.\"\"\"\n        self.setup_distributed()\n        optimizer = torch.optim.AdamW(self.model.parameters(), lr=learning_rate, weight_decay=0.01)\n        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n\n        for epoch in range(num_epochs):\n            self.model.train()\n            epoch_loss = 0\n            # Training loop implementation...\n            scheduler.step()\n            if epoch % 10 == 0:\n                self.save_checkpoint(epoch, epoch_loss / 1000)\n\n    def save_checkpoint(self, epoch, loss):\n        \"\"\"Save model checkpoint.\"\"\"\n        checkpoint = {\"epoch\": epoch, \"model_state_dict\": self.model.state_dict(), \"loss\": loss}\n        path = f\"{self.checkpoint_dir}/checkpoint_epoch_{epoch}.pt\"\n        torch.save(checkpoint, path)\n        print(f\"Checkpoint saved: {path}\")\n\n\n# Usage example\nmodel = torch.nn.Sequential(torch.nn.Linear(512, 768), torch.nn.ReLU())\npipeline = EnterpriseSelfsupervisedPipeline(model, data_source=\"s3://bucket/data\")\n# pipeline.train(num_epochs=10, learning_rate=1e-4)\nprint(\"Pipeline configured for distributed SSL training\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPipeline configured for distributed SSL training\n```\n:::\n:::\n\n\n:::{.callout-warning}\n## Production Considerations\n\n**Data Quality**: Self-supervised learning amplifies data quality issues. Bad data → bad embeddings. Filter corrupted samples before training.\n\n**Compute Budget**: Training on billions of samples requires significant compute. For 100M parameters × 1B tokens, expect 100-1000 GPU-hours.\n\n**Checkpoint Frequency**: Save checkpoints every 1-2 hours of training (not epochs). Spot instance interruptions are common.\n\n**Monitoring**: Track loss trends, gradient norms, and embedding quality metrics. Diverging loss indicates instability.\n:::\n\n## Masked Language Modeling for Domain-Specific Text\n\nMasked Language Modeling (MLM), popularized by BERT, is the foundation of modern NLP. For enterprises, the key is adapting MLM to domain-specific vocabulary and writing styles.\n\n### MLM Fundamentals\n\nThe MLM objective: **predict randomly masked tokens from surrounding context**. This forces the model to learn bidirectional representations that capture semantic and syntactic patterns.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show Domain-Specific MLM\"}\nimport torch\nfrom transformers import BertConfig, BertForMaskedLM, BertTokenizer, Trainer, TrainingArguments\n\n\nclass DomainSpecificMLM:\n    \"\"\"Masked Language Modeling for domain-specific text (legal, medical, financial, etc.).\"\"\"\n\n    def __init__(self, domain=\"general\", vocab_size=30000, hidden_size=768, num_layers=12, num_heads=12):\n        self.domain = domain\n        self.config = BertConfig(\n            vocab_size=vocab_size, hidden_size=hidden_size,\n            num_hidden_layers=num_layers, num_attention_heads=num_heads,\n            intermediate_size=hidden_size * 4, max_position_embeddings=512\n        )\n        self.model = BertForMaskedLM(self.config)\n        self.tokenizer = None\n\n    def train_tokenizer(self, text_corpus, save_path=\"./tokenizer\"):\n        \"\"\"Train domain-specific tokenizer - critical for specialized domains.\"\"\"\n        from tokenizers import Tokenizer\n        from tokenizers.models import BPE\n        from tokenizers.trainers import BpeTrainer\n\n        tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n        trainer = BpeTrainer(\n            vocab_size=self.config.vocab_size,\n            special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n        )\n        tokenizer.train_from_iterator(text_corpus, trainer=trainer)\n        tokenizer.save(f\"{save_path}/tokenizer.json\")\n        self.tokenizer = BertTokenizer.from_pretrained(save_path)\n        print(f\"Tokenizer trained and saved to {save_path}\")\n\n    def get_embeddings(self, texts, layer=-1):\n        \"\"\"Extract embeddings from trained model.\"\"\"\n        self.model.eval()\n        inputs = self.tokenizer(texts, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n        with torch.no_grad():\n            outputs = self.model.bert(**inputs, output_hidden_states=True)\n        embeddings = outputs.hidden_states[layer].mean(dim=1)\n        return embeddings.numpy()\n\n\n# Usage example\nlegal_mlm = DomainSpecificMLM(domain=\"legal\", vocab_size=32000)\nlegal_corpus = [\"The plaintiff filed a motion...\", \"Under tort law, negligence requires...\"]\n# legal_mlm.train_tokenizer(legal_corpus)\nprint(f\"Domain: {legal_mlm.domain}, Vocab size: {legal_mlm.config.vocab_size}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDomain: legal, Vocab size: 32000\n```\n:::\n:::\n\n\n### Advanced MLM Techniques\n\nFor production deployments, basic MLM can be enhanced with several techniques:\n\n::: {.cell execution_count=4}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show Advanced MLM Techniques\"}\nimport numpy as np\nimport torch\nfrom transformers import BertForMaskedLM, BertTokenizer\n\n\nclass AdvancedMLM:\n    \"\"\"Advanced MLM with whole word masking, span masking, and entity-aware masking.\"\"\"\n\n    def __init__(self, base_model, tokenizer):\n        self.model = base_model\n        self.tokenizer = tokenizer\n\n    def whole_word_masking(self, input_ids, mlm_probability=0.15):\n        \"\"\"Mask entire words instead of subword tokens for better semantics.\"\"\"\n        words = []\n        current_word = []\n\n        for idx, token_id in enumerate(input_ids):\n            token = self.tokenizer.decode([token_id])\n            if token.startswith(\"##\"):\n                current_word.append(idx)\n            else:\n                if current_word:\n                    words.append(current_word)\n                current_word = [idx]\n        if current_word:\n            words.append(current_word)\n\n        num_words_to_mask = max(1, int(len(words) * mlm_probability))\n        words_to_mask = np.random.choice(len(words), size=num_words_to_mask, replace=False)\n\n        mask = torch.zeros_like(input_ids, dtype=torch.bool)\n        for word_idx in words_to_mask:\n            for token_idx in words[word_idx]:\n                mask[token_idx] = True\n        return mask\n\n    def span_masking(self, input_ids, span_length=3, mlm_probability=0.15):\n        \"\"\"Mask contiguous spans of tokens for longer-range dependencies (SpanBERT).\"\"\"\n        seq_len = len(input_ids)\n        num_masks = int(seq_len * mlm_probability / span_length)\n        mask = torch.zeros_like(input_ids, dtype=torch.bool)\n\n        for _ in range(num_masks):\n            start = np.random.randint(0, max(1, seq_len - span_length))\n            mask[start:start + span_length] = True\n        return mask\n\n\n# Usage example\nmodel = BertForMaskedLM.from_pretrained(\"bert-base-uncased\")\ntokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\nadvanced_mlm = AdvancedMLM(model, tokenizer)\ntext = \"The quick brown fox jumps over the lazy dog\"\ninput_ids = tokenizer.encode(text, return_tensors=\"pt\")[0]\nmask = advanced_mlm.whole_word_masking(input_ids)\nprint(f\"Masked {mask.sum().item()} tokens out of {len(input_ids)}\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nSome weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nMasked 1 tokens out of 11\n```\n:::\n:::\n\n\n:::{.callout-tip}\n## MLM Training Best Practices\n\n**Tokenizer First**: Always train a domain-specific tokenizer before MLM. Generic tokenizers fragment domain terms.\n\n**Masking Strategy**: Use whole-word masking for semantic learning, span masking for longer dependencies.\n\n**Adaptation vs. Scratch**: If you have < 100M tokens, adapt pre-trained model. If > 1B tokens and very specialized domain, train from scratch.\n\n**Hyperparameters**: Standard BERT hyperparameters (lr=5e-5, batch=32, warmup=10%) work well. For adaptation, use lr=2e-5.\n\n**Compute Budget**: 100M parameters × 1B tokens ≈ 500 GPU-hours. Use mixed precision (fp16) to reduce by 2x.\n:::\n\n## Vision Transformers for Industrial Imagery\n\nVision Transformers (ViTs) combined with self-supervised learning enable training on unlabeled industrial imagery—manufacturing defects, medical scans, satellite images, security footage.\n\n### Self-Supervised Vision Transformers\n\n::: {.cell execution_count=5}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show Masked Autoencoder for Vision\"}\nimport torch\nimport torch.nn as nn\nfrom einops import rearrange\n\n\nclass MaskedAutoencoderViT(nn.Module):\n    \"\"\"Masked Autoencoder (MAE) for vision transformers - self-supervised image embeddings.\"\"\"\n\n    def __init__(self, img_size=224, patch_size=16, in_channels=3, embed_dim=768,\n                 depth=12, num_heads=12, decoder_embed_dim=512, decoder_num_heads=8, decoder_depth=8, mask_ratio=0.75):\n        super().__init__()\n        self.patch_size = patch_size\n        self.mask_ratio = mask_ratio\n        num_patches = (img_size // patch_size) ** 2\n\n        self.patch_embed = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n        self.pos_embed = nn.Parameter(torch.randn(1, num_patches, embed_dim) * 0.02)\n        self.encoder = nn.TransformerEncoder(\n            nn.TransformerEncoderLayer(embed_dim, num_heads, dim_feedforward=embed_dim * 4, batch_first=True),\n            num_layers=depth\n        )\n        self.decoder_embed = nn.Linear(embed_dim, decoder_embed_dim)\n        self.decoder = nn.TransformerDecoder(\n            nn.TransformerDecoderLayer(decoder_embed_dim, decoder_num_heads, dim_feedforward=decoder_embed_dim * 4, batch_first=True),\n            num_layers=decoder_depth\n        )\n        self.decoder_pred = nn.Linear(decoder_embed_dim, patch_size ** 2 * in_channels)\n\n    def forward(self, x):\n        # Patchify and embed\n        x = self.patch_embed(x)\n        x = rearrange(x, 'b c h w -> b (h w) c')\n        x = x + self.pos_embed\n\n        # Random masking\n        B, N, D = x.shape\n        len_keep = int(N * (1 - self.mask_ratio))\n        noise = torch.rand(B, N, device=x.device)\n        ids_shuffle = torch.argsort(noise, dim=1)\n        ids_restore = torch.argsort(ids_shuffle, dim=1)\n        ids_keep = ids_shuffle[:, :len_keep]\n        x_masked = torch.gather(x, dim=1, index=ids_keep.unsqueeze(-1).repeat(1, 1, D))\n\n        # Encode visible patches\n        latent = self.encoder(x_masked)\n\n        # Decode and reconstruct\n        latent_full = torch.zeros(B, N, D, device=x.device)\n        latent_full = torch.scatter(latent_full, 1, ids_keep.unsqueeze(-1).repeat(1, 1, D), latent)\n        decoded = self.decoder_embed(latent_full)\n        reconstructed = self.decoder_pred(self.decoder(decoded, decoded))\n\n        return reconstructed, ids_restore\n\n\n# Usage example\nmae = MaskedAutoencoderViT(img_size=224, patch_size=16, mask_ratio=0.75)\nimages = torch.randn(4, 3, 224, 224)\nreconstructed, ids = mae(images)\nprint(f\"Input: {images.shape}, Reconstructed: {reconstructed.shape}\")\nprint(f\"Masked {mae.mask_ratio * 100}% of patches during training\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nInput: torch.Size([4, 3, 224, 224]), Reconstructed: torch.Size([4, 196, 768])\nMasked 75.0% of patches during training\n```\n:::\n:::\n\n\n:::{.callout-tip}\n## ViT Self-Supervision Best Practices\n\n**Mask Ratio**: MAE uses 75% masking (aggressive!). This works because images have high redundancy. For specialized imagery (e.g., X-rays), try 50-60%.\n\n**Patch Size**: Standard is 16x16 for 224x224 images. For higher resolution (512x512+), use 32x32 patches.\n\n**Augmentation**: Strong augmentations (color jitter, blur) improve robustness. But avoid augmentations that change semantics (e.g., don't flip medical images if orientation matters).\n\n**Compute**: ViT-Base with MAE requires ~100 GPU-hours for 1M images. Use ViT-Small (5.7M params) for faster prototyping.\n:::\n\n### Industrial Vision Applications\n\n::: {.cell execution_count=6}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show Industrial Defect Detection\"}\nimport torch\nimport torch.nn as nn\n\n\nclass IndustrialDefectDetection:\n    \"\"\"Self-supervised defect detection for manufacturing using image reconstruction.\"\"\"\n\n    def __init__(self, encoder_model, image_size=256, embedding_dim=512):\n        self.encoder = encoder_model\n        self.image_size = image_size\n        self.embedding_dim = embedding_dim\n        self.decoder = self._build_decoder()\n        self.threshold = None\n\n    def _build_decoder(self):\n        return nn.Sequential(\n            nn.Linear(self.embedding_dim, 1024), nn.ReLU(),\n            nn.Linear(1024, 2048), nn.ReLU(),\n            nn.Linear(2048, self.image_size * self.image_size * 3), nn.Sigmoid()\n        )\n\n    def train_on_normal_samples(self, normal_images, epochs=50, batch_size=32):\n        \"\"\"Train on defect-free samples to learn normal patterns.\"\"\"\n        optimizer = torch.optim.Adam(list(self.encoder.parameters()) + list(self.decoder.parameters()), lr=1e-4)\n        criterion = nn.MSELoss()\n\n        for epoch in range(epochs):\n            total_loss = 0\n            for i in range(0, len(normal_images), batch_size):\n                batch = normal_images[i:i + batch_size]\n                embeddings = self.encoder(batch)\n                reconstructed = self.decoder(embeddings).view(-1, 3, self.image_size, self.image_size)\n                loss = criterion(reconstructed, batch)\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n                total_loss += loss.item()\n            if epoch % 10 == 0:\n                print(f\"Epoch {epoch}: Loss = {total_loss:.4f}\")\n\n        self._calibrate_threshold(normal_images)\n\n    def _calibrate_threshold(self, normal_images):\n        \"\"\"Set anomaly threshold based on reconstruction errors of normal samples.\"\"\"\n        self.encoder.eval()\n        self.decoder.eval()\n        errors = []\n        with torch.no_grad():\n            for img in normal_images:\n                emb = self.encoder(img.unsqueeze(0))\n                recon = self.decoder(emb).view(-1, 3, self.image_size, self.image_size)\n                error = ((recon - img.unsqueeze(0)) ** 2).mean().item()\n                errors.append(error)\n        self.threshold = torch.tensor(errors).mean() + 3 * torch.tensor(errors).std()\n\n    def detect_defects(self, test_image):\n        \"\"\"Detect defects by comparing reconstruction error to threshold.\"\"\"\n        self.encoder.eval()\n        self.decoder.eval()\n        with torch.no_grad():\n            emb = self.encoder(test_image.unsqueeze(0))\n            recon = self.decoder(emb).view(-1, 3, self.image_size, self.image_size)\n            error = ((recon - test_image.unsqueeze(0)) ** 2).mean().item()\n        is_defect = error > self.threshold\n        return is_defect, error\n\n\n# Usage example\nencoder = nn.Sequential(nn.Flatten(), nn.Linear(256 * 256 * 3, 512), nn.ReLU())\ndetector = IndustrialDefectDetection(encoder, image_size=256)\nnormal_samples = torch.randn(100, 3, 256, 256)\n# detector.train_on_normal_samples(normal_samples, epochs=10)\nprint(\"Defect detector trained on normal samples only\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDefect detector trained on normal samples only\n```\n:::\n:::\n\n\n## Time-Series Self-Supervision\n\nTime-series data (sensor readings, financial data, user activity logs) presents unique self-supervision opportunities due to temporal structure.\n\n### Time-Series Pretext Tasks\n\n::: {.cell execution_count=7}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show Time Series Self-Supervised Learning\"}\nimport torch\nimport torch.nn as nn\n\n\nclass TimeSeriesSelfSupervised:\n    \"\"\"Self-supervised learning for time series: masking, forecasting, contrastive learning.\"\"\"\n\n    def __init__(self, input_dim, hidden_dim=256, num_layers=4, task=\"forecasting\"):\n        self.input_dim = input_dim\n        self.hidden_dim = hidden_dim\n        self.task = task\n        self.encoder = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n\n        if task == \"forecasting\":\n            self.predictor = nn.Linear(hidden_dim, input_dim)\n        elif task == \"masking\":\n            self.predictor = nn.Linear(hidden_dim, input_dim)\n        elif task == \"contrastive\":\n            self.projector = nn.Sequential(nn.Linear(hidden_dim, 128), nn.ReLU(), nn.Linear(128, 64))\n\n    def create_forecasting_task(self, timeseries, forecast_horizon=10):\n        \"\"\"Predict future values from past context.\"\"\"\n        context = timeseries[:, :-forecast_horizon, :]\n        target = timeseries[:, -forecast_horizon:, :]\n        return context, target\n\n    def create_masking_task(self, timeseries, mask_ratio=0.15):\n        \"\"\"Mask random timesteps and predict them.\"\"\"\n        B, T, D = timeseries.shape\n        mask = torch.rand(B, T, 1) < mask_ratio\n        masked_series = timeseries.clone()\n        masked_series[mask.expand_as(timeseries)] = 0\n        return masked_series, timeseries, mask\n\n    def forward(self, x):\n        \"\"\"Encode time series to embeddings.\"\"\"\n        _, (h_n, _) = self.encoder(x)\n        return h_n[-1]\n\n    def train_step(self, batch, optimizer):\n        \"\"\"Single training step for chosen task.\"\"\"\n        if self.task == \"forecasting\":\n            context, target = self.create_forecasting_task(batch)\n            embedding = self.forward(context)\n            predictions = self.predictor(embedding).unsqueeze(1).repeat(1, target.size(1), 1)\n            loss = nn.MSELoss()(predictions, target)\n        elif self.task == \"masking\":\n            masked, original, mask = self.create_masking_task(batch)\n            output, _ = self.encoder(masked)\n            predictions = self.predictor(output)\n            loss = nn.MSELoss()(predictions[mask.expand_as(predictions)], original[mask.expand_as(original)])\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        return loss.item()\n\n\n# Usage example\nts_model = TimeSeriesSelfSupervised(input_dim=10, hidden_dim=128, task=\"forecasting\")\ntimeseries_data = torch.randn(32, 100, 10)\noptimizer = torch.optim.Adam(ts_model.encoder.parameters(), lr=1e-3)\nloss = ts_model.train_step(timeseries_data, optimizer)\nprint(f\"Time series SSL loss: {loss:.4f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTime series SSL loss: 0.9971\n```\n:::\n:::\n\n\n:::{.callout-tip}\n## Time-Series SSL Best Practices\n\n**Forecasting Horizon**: For high-frequency data (milliseconds), predict 5-10 steps ahead. For slow-varying data (daily), predict 1-2 steps.\n\n**Masking Strategy**: For bursty data (event logs), use random masking. For smooth data (temperature), use contiguous span masking.\n\n**Augmentations**: Test augmentations carefully. Ensure they preserve semantic meaning (e.g., don't shift phase of financial data).\n\n**Architecture**: Transformers work well for long sequences (> 100 steps). For shorter sequences or limited compute, use LSTM/GRU.\n:::\n\n## Multi-Modal Self-Supervised Approaches\n\nMulti-modal self-supervision learns from multiple data types simultaneously—text + images, audio + video, sensor + text logs.\n\n### CLIP-Style Multi-Modal Learning\n\n::: {.cell execution_count=8}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show Multimodal Self-Supervised Learning\"}\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass MultimodalSelfSupervised:\n    \"\"\"CLIP-style multimodal self-supervised learning for text-image alignment.\"\"\"\n\n    def __init__(self, text_encoder, image_encoder, embedding_dim=512, temperature=0.07):\n        self.text_encoder = text_encoder\n        self.image_encoder = image_encoder\n        self.temperature = temperature\n        self.text_projection = nn.Linear(embedding_dim, embedding_dim)\n        self.image_projection = nn.Linear(embedding_dim, embedding_dim)\n\n    def forward(self, text_inputs, image_inputs):\n        \"\"\"Compute embeddings for both modalities.\"\"\"\n        text_features = self.text_encoder(text_inputs)\n        if text_features.dim() == 3:  # (batch, seq, dim) -> mean pool to (batch, dim)\n            text_features = text_features.mean(dim=1)\n        text_embeds = self.text_projection(text_features)\n        text_embeds = F.normalize(text_embeds, dim=-1)\n\n        image_features = self.image_encoder(image_inputs)\n        image_embeds = self.image_projection(image_features)\n        image_embeds = F.normalize(image_embeds, dim=-1)\n\n        return text_embeds, image_embeds\n\n    def contrastive_loss(self, text_embeds, image_embeds):\n        \"\"\"Symmetric contrastive loss (text-to-image and image-to-text).\"\"\"\n        logits = torch.matmul(text_embeds, image_embeds.T) / self.temperature\n        labels = torch.arange(len(text_embeds), device=logits.device)\n\n        loss_t2i = F.cross_entropy(logits, labels)\n        loss_i2t = F.cross_entropy(logits.T, labels)\n        return (loss_t2i + loss_i2t) / 2\n\n    def train_step(self, text_batch, image_batch, optimizer):\n        \"\"\"Train on paired text-image data.\"\"\"\n        text_embeds, image_embeds = self.forward(text_batch, image_batch)\n        loss = self.contrastive_loss(text_embeds, image_embeds)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        with torch.no_grad():\n            logits = torch.matmul(text_embeds, image_embeds.T)\n            predictions = logits.argmax(dim=-1)  # For each text, which image is most similar?\n            labels = torch.arange(len(text_embeds), device=logits.device)\n            accuracy = (predictions == labels).float().mean()\n\n        return loss.item(), accuracy.item()\n\n\n# Usage example\ntext_enc = nn.Sequential(nn.Embedding(10000, 512), nn.Linear(512, 512), nn.ReLU())\nimage_enc = nn.Sequential(nn.Flatten(), nn.Linear(224 * 224 * 3, 512), nn.ReLU())\nmultimodal = MultimodalSelfSupervised(text_enc, image_enc, embedding_dim=512)\n\ntext = torch.randint(0, 10000, (32, 50))\nimages = torch.randn(32, 3, 224, 224)\noptimizer = torch.optim.Adam(list(multimodal.text_encoder.parameters()) + list(multimodal.image_encoder.parameters()), lr=1e-4)\nloss, acc = multimodal.train_step(text, images, optimizer)\nprint(f\"Multimodal loss: {loss:.4f}, Alignment accuracy: {acc:.2%}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMultimodal loss: 3.5024, Alignment accuracy: 0.00%\n```\n:::\n:::\n\n\n:::{.callout-tip}\n## Multi-Modal SSL Best Practices\n\n**Pairing Quality**: The quality of modality pairs matters more than quantity. 10M high-quality pairs > 100M noisy pairs.\n\n**Batch Size**: Larger batches provide more negative samples. Use at least 256, ideally 1024+ with gradient accumulation.\n\n**Temperature**: Start with 0.07. Lower (0.01) for fine-grained matching, higher (0.2) for coarse similarity.\n\n**Modality Balance**: If one modality is much noisier, consider weighted loss or filtering poor pairs.\n\n**Compute**: CLIP-scale training (400M pairs) requires thousands of GPU-hours. For enterprise, 1M-10M pairs often sufficient.\n:::\n\n## Key Takeaways\n\n- **Self-supervised learning unlocks unlabeled data** at unprecedented scale. No manual annotation needed—data structure provides supervision through pretext tasks.\n\n- **Masked Language Modeling** is the foundation for domain-specific text embeddings. Always train a domain-specific tokenizer first, then adapt or train MLM on your corpus.\n\n- **Vision Transformers with Masked Autoencoding (MAE)** enable learning from unlabeled images with 75% masking. Ideal for manufacturing defects, medical imaging, and satellite imagery where labels are scarce.\n\n- **Time-series self-supervision** uses forecasting, masked reconstruction, or contrastive tasks. Choose based on data characteristics: forecasting for ordered data, contrastive for augmentable data.\n\n- **Multi-modal self-supervision** creates shared embedding spaces across text, images, audio, and sensors without paired labels. Contrastive learning between modalities is highly effective.\n\n- **Production deployment requires distributed training**, checkpointing, and careful data management. For 100M parameters × 1B samples, expect 500-1000 GPU-hours.\n\n## Looking Ahead\n\nIn @sec-advanced-embedding-techniques, we explore advanced embedding techniques that push beyond standard architectures—hierarchical embeddings for taxonomies, dynamic embeddings that evolve over time, compositional embeddings for combinatorial spaces, and quantum-inspired embeddings for ultra-high-dimensional data. These techniques unlock capabilities impossible with standard approaches.\n\n## Further Reading\n\n- Devlin, J., et al. (2018). \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.\" NAACL.\n- He, K., et al. (2021). \"Masked Autoencoders Are Scalable Vision Learners.\" CVPR.\n- Radford, A., et al. (2021). \"Learning Transferable Visual Models From Natural Language Supervision.\" ICML.\n- Chen, T., et al. (2020). \"A Simple Framework for Contrastive Learning of Visual Representations.\" ICML.\n- Oord, A., Li, Y., & Vinyals, O. (2018). \"Representation Learning with Contrastive Predictive Coding.\" arXiv:1807.03748.\n- Liu, Y., et al. (2019). \"RoBERTa: A Robustly Optimized BERT Pretraining Approach.\" arXiv:1907.11692.\n- Dosovitskiy, A., et al. (2020). \"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.\" ICLR.\n- Franceschi, J.Y., et al. (2019). \"Unsupervised Scalable Representation Learning for Multivariate Time Series.\" NeurIPS.\n\n",
    "supporting": [
      "ch11_self_supervised_learning_files/figure-pdf"
    ],
    "filters": []
  }
}