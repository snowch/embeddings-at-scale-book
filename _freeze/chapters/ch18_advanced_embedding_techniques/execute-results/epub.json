{
  "hash": "5d6e28e63a914a2a9306b0f8ec092b7c",
  "result": {
    "engine": "jupyter",
    "markdown": "# Advanced Embedding Techniques {#sec-advanced-embedding-techniques}\n\n:::{.callout-note}\n## Chapter Overview\nAs embedding systems mature, organizations need techniques that go beyond standard vector representations. This chapter explores five advanced approaches that address complex real-world challenges: hierarchical embeddings that preserve taxonomic structure, dynamic embeddings that capture temporal evolution, compositional embeddings for complex entities, uncertainty quantification for trustworthy predictions, and federated learning for privacy-preserving embedding training. These techniques unlock new possibilities for organizations handling structured knowledge graphs, time-varying data, multi-faceted entities, high-stakes decisions, and distributed sensitive data.\n:::\n\n## Hierarchical Embeddings for Taxonomies\n\nMany enterprise domains have inherent hierarchical structure: product catalogs with categories and subcategories, organizational charts with departments and teams, medical ontologies with disease classifications, and scientific taxonomies. Standard embeddings treat all items as independent points in space, losing this valuable structural information. **Hierarchical embeddings preserve taxonomic relationships while maintaining the benefits of vector representations**.\n\n### The Hierarchical Challenge\n\nConsider an e-commerce product catalog:\n\n```\nElectronics\n├── Computers\n│   ├── Laptops\n│   │   ├── Gaming Laptops\n│   │   └── Business Laptops\n│   └── Desktops\n└── Mobile Devices\n    ├── Smartphones\n    └── Tablets\n```\n\nA standard embedding might place \"Gaming Laptops\" and \"Tablets\" closer than \"Gaming Laptops\" and \"Business Laptops\", even though the latter share more hierarchical structure. Hierarchical embeddings ensure that:\n\n1. **Distance reflects hierarchy**: Items in the same subtree are closer\n2. **Transitivity is preserved**: If A is parent of B and B is parent of C, embeddings reflect this chain\n3. **Level information is encoded**: Embeddings capture depth in the hierarchy\n\n### Hyperbolic Embeddings for Hierarchies\n\nEuclidean space has a fundamental limitation: the number of points at distance $d$ grows polynomially. Tree structures, however, grow exponentially—the number of nodes doubles at each level. **Hyperbolic space has negative curvature, allowing exponential volume growth that naturally matches tree structure**.\n\nThe Poincaré ball model represents hyperbolic space as the unit ball in Euclidean space with a special distance metric:\n\n::: {#2c934575 .cell execution_count=1}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show Hyperbolic Embedding Implementation\"}\nimport torch\nimport torch.nn as nn\n\n\nclass HyperbolicEmbedding(nn.Module):\n    \"\"\"Hyperbolic embeddings in Poincaré ball for hierarchical data.\"\"\"\n\n    def __init__(self, num_items, embedding_dim, curvature=1.0):\n        super().__init__()\n        self.curvature = curvature\n        self.embeddings = nn.Embedding(num_items, embedding_dim)\n        nn.init.uniform_(self.embeddings.weight, -1e-3, 1e-3)\n\n    def poincare_distance(self, u, v):\n        \"\"\"Compute Poincaré distance between points u and v.\"\"\"\n        sqrt_c = self.curvature ** 0.5\n        diff_norm_sq = torch.sum((u - v) ** 2, dim=-1)\n        u_norm_sq = torch.sum(u ** 2, dim=-1)\n        v_norm_sq = torch.sum(v ** 2, dim=-1)\n\n        numerator = 2 * diff_norm_sq\n        denominator = (1 - u_norm_sq) * (1 - v_norm_sq)\n        return torch.acosh(1 + numerator / (denominator + 1e-7)) / sqrt_c\n\n    def project_to_ball(self, x, eps=1e-5):\n        \"\"\"Project points to Poincaré ball (norm < 1).\"\"\"\n        norm = torch.norm(x, p=2, dim=-1, keepdim=True)\n        max_norm = 1 - eps\n        return x / torch.clamp(norm / max_norm, min=1.0)\n\n    def forward(self, indices):\n        \"\"\"Get embeddings and project to Poincaré ball.\"\"\"\n        emb = self.embeddings(indices)\n        return self.project_to_ball(emb)\n\n\n# Usage example\nmodel = HyperbolicEmbedding(num_items=1000, embedding_dim=10, curvature=1.0)\nindices = torch.tensor([0, 1, 10])\nembeddings = model(indices)\ndistance = model.poincare_distance(embeddings[0], embeddings[1])\nprint(f\"Hyperbolic distance: {distance.item():.4f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nHyperbolic distance: 0.0060\n```\n:::\n:::\n\n\n### Enterprise Applications of Hierarchical Embeddings\n\n**1. Product Recommendation with Category Awareness**\n\n::: {#661e2e4a .cell execution_count=2}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show Hierarchical Product Recommender\"}\nimport torch\nimport torch.nn as nn\n\n\nclass HierarchicalProductRecommender:\n    \"\"\"Product recommendation system using hyperbolic embeddings for category-aware recommendations.\"\"\"\n\n    def __init__(self, product_catalog, embedding_dim=10, curvature=1.0):\n        self.catalog = product_catalog\n        self.hyperbolic_model = HyperbolicEmbedding(len(product_catalog), embedding_dim, curvature)\n\n    def recommend(self, product_id, top_k=10, category_weight=0.3):\n        \"\"\"Recommend products based on hyperbolic distance and category structure.\"\"\"\n        query_emb = self.hyperbolic_model(torch.tensor([product_id]))\n\n        distances = []\n        for pid in range(len(self.catalog)):\n            if pid == product_id:\n                continue\n            prod_emb = self.hyperbolic_model(torch.tensor([pid]))\n            dist = self.hyperbolic_model.poincare_distance(query_emb, prod_emb)\n            distances.append((pid, dist.item()))\n\n        distances.sort(key=lambda x: x[1])\n        return [pid for pid, _ in distances[:top_k]]\n\n\n# Usage example\ncatalog = {\"laptop_gaming\": 0, \"laptop_business\": 1, \"phone\": 2}\nrecommender = HierarchicalProductRecommender(catalog, embedding_dim=10)\nrecommendations = recommender.recommend(product_id=0, top_k=5)\nprint(f\"Recommendations for product 0: {recommendations}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRecommendations for product 0: [1, 2]\n```\n:::\n:::\n\n\n**2. Knowledge Graph Embeddings**\n\nMedical ontologies, scientific taxonomies, and corporate knowledge bases benefit enormously from hyperbolic embeddings:\n\n```python\ndef embed_medical_ontology():\n    \"\"\"\n    Medical ontology example: Disease hierarchies\n\n    ICD-10 codes have 14,000+ diseases organized hierarchically\n    Hyperbolic embeddings in 10-20 dimensions outperform\n    Euclidean embeddings in 300-500 dimensions\n    \"\"\"\n    # Example: Simplified disease taxonomy\n    disease_taxonomy = {\n        # Cardiovascular diseases\n        'myocardial_infarction': 'ischemic_heart_disease',\n        'angina': 'ischemic_heart_disease',\n        'ischemic_heart_disease': 'cardiovascular_disease',\n\n        'atrial_fibrillation': 'arrhythmia',\n        'ventricular_tachycardia': 'arrhythmia',\n        'arrhythmia': 'cardiovascular_disease',\n\n        # Respiratory diseases\n        'pneumonia': 'lower_respiratory_infection',\n        'bronchitis': 'lower_respiratory_infection',\n        'lower_respiratory_infection': 'respiratory_disease',\n\n        'asthma': 'chronic_respiratory_disease',\n        'copd': 'chronic_respiratory_disease',\n        'chronic_respiratory_disease': 'respiratory_disease',\n    }\n\n    trainer = HierarchicalEmbeddingTrainer(\n        disease_taxonomy,\n        embedding_dim=10,\n        curvature=1.0\n    )\n\n    trainer.train(num_epochs=2000, verbose=True)\n\n    return trainer\n```\n\n:::{.callout-tip}\n## Dimensionality Advantage\nHyperbolic embeddings typically achieve better hierarchical preservation in 5-20 dimensions than Euclidean embeddings in 100-500 dimensions. This reduces storage by 20-100x and speeds up similarity search by 10-50x.\n:::\n\n:::{.callout-warning}\n## Training Stability\nHyperbolic optimization can be unstable near the boundary of the Poincaré ball. Always use projection after gradient steps and consider adaptive learning rates that decrease when approaching the boundary.\n:::\n\n## Dynamic Embeddings for Temporal Data\n\nMost embedding systems assume data is static: a document has one embedding, a product has one representation. But **real-world entities evolve**: user interests shift, document relevance decays, product popularity cycles, and word meanings drift. Dynamic embeddings capture this temporal dimension.\n\n### The Temporal Challenge\n\nConsider a news article about \"AI\":\n\n- **2015**: \"AI\" meant primarily machine learning and narrow applications\n- **2020**: \"AI\" included transformers, GPT models, and broader capabilities\n- **2025**: \"AI\" encompasses multimodal models, agents, and reasoning systems\n\nA static embedding averages these meanings, losing temporal context. A dynamic embedding maintains separate representations for each time period or evolves continuously.\n\n### Approaches to Dynamic Embeddings\n\n**1. Discrete Time Slices**: Separate embeddings per time window\n**2. Continuous Evolution**: Embeddings as functions of time\n**3. Recurrent Updates**: Update embeddings based on new observations\n\n::: {#8caf2436 .cell execution_count=3}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show Dynamic Embedding\"}\nimport torch\nimport torch.nn as nn\n\n\nclass DynamicEmbedding(nn.Module):\n    \"\"\"Dynamic embeddings that evolve over time based on user interactions.\"\"\"\n\n    def __init__(self, num_items, embedding_dim, num_time_slices=10):\n        super().__init__()\n        self.num_time_slices = num_time_slices\n        self.base_embeddings = nn.Embedding(num_items, embedding_dim)\n        self.temporal_adjustment = nn.Embedding(num_time_slices, embedding_dim)\n\n    def forward(self, item_ids, time_slice_ids):\n        \"\"\"Get time-aware embeddings.\"\"\"\n        base_emb = self.base_embeddings(item_ids)\n        temporal_adj = self.temporal_adjustment(time_slice_ids)\n        return base_emb + 0.1 * temporal_adj\n\n    def update_from_interactions(self, item_id, interaction_embedding, learning_rate=0.01):\n        \"\"\"Incrementally update embeddings based on new interactions.\"\"\"\n        with torch.no_grad():\n            current = self.base_embeddings.weight[item_id]\n            self.base_embeddings.weight[item_id] = current + learning_rate * (interaction_embedding - current)\n\n\n# Usage example\nmodel = DynamicEmbedding(num_items=1000, embedding_dim=128, num_time_slices=24)\nitem_ids = torch.tensor([10, 20, 30])\ntime_ids = torch.tensor([5, 5, 10])\nembeddings = model(item_ids, time_ids)\nprint(f\"Dynamic embeddings shape: {embeddings.shape}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDynamic embeddings shape: torch.Size([3, 128])\n```\n:::\n:::\n\n\n### Production Deployment of Dynamic Embeddings\n\n:::{.callout-tip}\n## Streaming Updates at Scale\nFor systems with millions of users and billions of interactions:\n\n1. **Batch updates**: Accumulate interactions over 5-15 minute windows, update in batch\n2. **Incremental training**: Update only affected embeddings, not full model\n3. **Asynchronous updates**: Background process updates embeddings while serving layer uses stale (but recent) versions\n4. **Versioned embeddings**: Maintain multiple versions (current, 5min old, 1hr old) for consistency\n:::\n\n::: {#4430e68c .cell execution_count=4}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show Streaming Embedding Service\"}\nimport asyncio\nfrom collections import deque\nfrom datetime import datetime\nimport torch\n\n\nclass StreamingEmbeddingService:\n    \"\"\"Real-time embedding service with streaming updates.\"\"\"\n\n    def __init__(self, model, update_interval_seconds=60):\n        self.model = model\n        self.update_interval = update_interval_seconds\n        self.pending_updates = deque()\n        self.last_update = datetime.now()\n\n    async def queue_interaction(self, item_id, interaction_data):\n        \"\"\"Queue interaction for batch update.\"\"\"\n        self.pending_updates.append((item_id, interaction_data))\n        if len(self.pending_updates) >= 100 or (datetime.now() - self.last_update).total_seconds() > self.update_interval:\n            await self.flush_updates()\n\n    async def flush_updates(self):\n        \"\"\"Apply pending updates in batch.\"\"\"\n        if not self.pending_updates:\n            return\n\n        updates = list(self.pending_updates)\n        self.pending_updates.clear()\n\n        for item_id, data in updates:\n            self.model.update_from_interactions(item_id, torch.randn(128), learning_rate=0.01)\n\n        self.last_update = datetime.now()\n        print(f\"Flushed {len(updates)} updates\")\n\n# Usage example\nmodel = DynamicEmbedding(num_items=1000, embedding_dim=128)\nservice = StreamingEmbeddingService(model, update_interval_seconds=60)\nprint(\"Streaming service initialized for real-time updates\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nStreaming service initialized for real-time updates\n```\n:::\n:::\n\n\n:::{.callout-warning}\n## Temporal Leakage\nWhen training dynamic embeddings, **never use future information to create past embeddings**. This temporal leakage leads to unrealistically high accuracy in backtesting but fails in production. Always train with strict time-based splits.\n:::\n\n## Compositional Embeddings for Complex Entities\n\nReal-world entities are rarely atomic—they're compositions of multiple components:\n\n- **Documents**: Title + body + metadata + author + date\n- **Products**: Category + brand + attributes + reviews + images\n- **Users**: Demographics + behavior + preferences + context\n- **Transactions**: Buyer + seller + item + time + location + amount\n\n**Compositional embeddings** explicitly model these structures, learning how to combine component embeddings into coherent entity representations.\n\n### Why Composition Matters\n\nA naive approach: concatenate or average component embeddings. This fails because:\n\n1. **Components have different importance**: Product brand matters more than box color\n2. **Interactions exist**: Laptop + Gaming Category ≠ Laptop + Business Category\n3. **Context varies**: User embedding should weight differently for recommendations vs. fraud detection\n\nCompositional embeddings learn **how to combine** components, not just what the components are.\n\n### Approaches to Composition\n\n::: {#815c221a .cell execution_count=5}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show Compositional Embedding\"}\nimport torch\nimport torch.nn as nn\n\n\nclass CompositionalEmbedding(nn.Module):\n    \"\"\"Learn to compose embeddings from multiple components using attention.\"\"\"\n\n    def __init__(self, component_dims, output_dim=128):\n        super().__init__()\n        self.component_encoders = nn.ModuleList([\n            nn.Linear(dim, output_dim) for dim in component_dims\n        ])\n        self.attention = nn.MultiheadAttention(output_dim, num_heads=4, batch_first=True)\n        self.output_proj = nn.Linear(output_dim, output_dim)\n\n    def forward(self, components, component_mask=None):\n        \"\"\"Compose embeddings from multiple components.\n\n        Args:\n            components: List of tensors, one per component\n            component_mask: Boolean mask for missing components\n        \"\"\"\n        encoded = []\n        for i, comp in enumerate(components):\n            if comp is not None:\n                encoded.append(self.component_encoders[i](comp))\n            else:\n                encoded.append(torch.zeros(comp.size(0), self.component_encoders[i].out_features))\n\n        stacked = torch.stack(encoded, dim=1)\n        attended, _ = self.attention(stacked, stacked, stacked, key_padding_mask=component_mask)\n        return self.output_proj(attended.mean(dim=1))\n\n\n# Usage example\nmodel = CompositionalEmbedding(component_dims=[64, 128, 32], output_dim=128)\ncomponents = [torch.randn(16, 64), torch.randn(16, 128), torch.randn(16, 32)]\ncomposed = model(components)\nprint(f\"Composed embedding shape: {composed.shape}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nComposed embedding shape: torch.Size([16, 128])\n```\n:::\n:::\n\n\n### Task-Specific Composition Weights\n\nA powerful extension: **learn different composition weights for different tasks**.\n\n::: {#99845f5e .cell execution_count=6}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show Task-Adaptive Composition\"}\nimport torch\nimport torch.nn as nn\n\n\nclass TaskAdaptiveComposition(nn.Module):\n    \"\"\"Learn task-specific composition weights for multi-component entities.\"\"\"\n\n    def __init__(self, num_components, embedding_dim, num_tasks=3):\n        super().__init__()\n        self.component_embeddings = nn.ModuleList([\n            nn.Embedding(1000, embedding_dim) for _ in range(num_components)\n        ])\n        self.task_weights = nn.Embedding(num_tasks, num_components)\n        nn.init.uniform_(self.task_weights.weight, 0, 1)\n\n    def forward(self, component_ids, task_id):\n        \"\"\"Compose embeddings with task-specific weights.\"\"\"\n        component_embs = [enc(ids) for enc, ids in zip(self.component_embeddings, component_ids)]\n        stacked = torch.stack(component_embs, dim=1)\n\n        weights = torch.softmax(self.task_weights(task_id), dim=-1)\n        weighted = stacked * weights.unsqueeze(-1)\n        return weighted.sum(dim=1)\n\n\n# Usage example\nmodel = TaskAdaptiveComposition(num_components=3, embedding_dim=64, num_tasks=3)\ncomp_ids = [torch.tensor([10]), torch.tensor([20]), torch.tensor([30])]\ntask_id = torch.tensor([1])\ncomposed = model(comp_ids, task_id)\nprint(f\"Task-adaptive composed embedding: {composed.shape}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTask-adaptive composed embedding: torch.Size([1, 64])\n```\n:::\n:::\n\n\n:::{.callout-tip}\n## Handling Missing Components\nReal-world data often has missing components (products without images, documents without abstracts). Use attention with component masks to handle missing data gracefully—the model automatically re-weights remaining components.\n:::\n\n## Uncertainty Quantification in Embeddings\n\nEmbedding systems make high-stakes decisions: loan approvals, medical diagnoses, autonomous vehicle navigation. **A confidence score is as important as the prediction itself**. Uncertainty quantification tells us when to trust an embedding-based decision and when to defer to human judgment or request more information.\n\n### Sources of Uncertainty\n\n1. **Aleatoric uncertainty**: Inherent noise in data (e.g., blurry images, ambiguous text)\n2. **Epistemic uncertainty**: Model's lack of knowledge (e.g., never seen this type of input before)\n3. **Distribution shift**: Input differs from training distribution\n\nStandard embeddings provide point estimates with no uncertainty. We need probabilistic embeddings that capture confidence.\n\n### Approaches to Uncertainty Quantification\n\n::: {#04dec89b .cell execution_count=7}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show Probabilistic Embedding\"}\nimport torch\nimport torch.nn as nn\n\n\nclass ProbabilisticEmbedding(nn.Module):\n    \"\"\"Embeddings with uncertainty quantification using variational approach.\"\"\"\n\n    def __init__(self, num_items, embedding_dim):\n        super().__init__()\n        self.mean_embeddings = nn.Embedding(num_items, embedding_dim)\n        self.logvar_embeddings = nn.Embedding(num_items, embedding_dim)\n\n    def forward(self, item_ids, num_samples=1):\n        \"\"\"Sample from embedding distribution.\"\"\"\n        mean = self.mean_embeddings(item_ids)\n        logvar = self.logvar_embeddings(item_ids)\n        std = torch.exp(0.5 * logvar)\n\n        if num_samples == 1:\n            eps = torch.randn_like(std)\n            return mean + eps * std, std\n        else:\n            samples = []\n            for _ in range(num_samples):\n                eps = torch.randn_like(std)\n                samples.append(mean + eps * std)\n            return torch.stack(samples), std\n\n    def uncertainty(self, item_ids):\n        \"\"\"Get uncertainty scores.\"\"\"\n        logvar = self.logvar_embeddings(item_ids)\n        return torch.exp(0.5 * logvar).mean(dim=-1)\n\n\n# Usage example\nmodel = ProbabilisticEmbedding(num_items=1000, embedding_dim=128)\nitems = torch.tensor([10, 20, 30])\nembeddings, uncertainty = model(items)\nuncertainty_scores = model.uncertainty(items)\nprint(f\"Embeddings: {embeddings.shape}, Uncertainty: {uncertainty_scores}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nEmbeddings: torch.Size([3, 128]), Uncertainty: tensor([1.0809, 1.1542, 1.1123], grad_fn=<MeanBackward1>)\n```\n:::\n:::\n\n\n:::{.callout-warning}\n## Calibration is Critical\nUncertainty estimates must be **calibrated**: if the model says 80% confidence, it should be correct 80% of the time. Uncalibrated uncertainty is misleading and dangerous. Always validate on held-out test set and use temperature scaling or Platt scaling to calibrate.\n:::\n\n:::{.callout-tip}\n## When to Use Uncertainty Quantification\n\nEssential for:\n\n- **High-stakes decisions**: Healthcare, finance, autonomous systems, legal\n- **Out-of-distribution detection**: Detect when input differs from training data\n- **Active learning**: Select most informative examples to label next\n- **Trustworthy AI**: Provide confidence scores to users\n\nNot necessary for:\n\n- Low-stakes applications (music recommendations, article suggestions)\n- Internal R&D where errors are acceptable\n- Applications with human-in-the-loop review anyway\n:::\n\n## Federated Embedding Learning\n\nMany organizations have valuable data they cannot share: medical records, financial transactions, personal communications. **Federated learning** enables training embeddings across multiple data silos without centralizing the data. Each participant trains locally and shares only model updates, preserving privacy.\n\n### The Federated Learning Paradigm\n\nTraditional centralized training:\n1. Collect all data in one place\n2. Train embedding model\n3. Deploy to all clients\n\n**Problem**: Data cannot be centralized due to privacy, regulations (GDPR, HIPAA), competitive concerns, or data volume.\n\nFederated training:\n1. Each client trains on local data\n2. Clients share model updates (gradients, embeddings)\n3. Central server aggregates updates\n4. Repeat until convergence\n\n::: {#ef5332bb .cell execution_count=8}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show Federated Embedding Server\"}\nimport torch\nimport torch.nn as nn\n\n\nclass FederatedEmbeddingServer:\n    \"\"\"Central server for federated embedding learning.\"\"\"\n\n    def __init__(self, global_model, num_clients=10):\n        self.global_model = global_model\n        self.num_clients = num_clients\n        self.client_weights = [1.0 / num_clients] * num_clients\n\n    def aggregate_updates(self, client_models):\n        \"\"\"Aggregate model updates from clients using weighted average.\"\"\"\n        global_dict = self.global_model.state_dict()\n\n        for key in global_dict.keys():\n            global_dict[key] = torch.zeros_like(global_dict[key])\n            for i, client_model in enumerate(client_models):\n                client_dict = client_model.state_dict()\n                global_dict[key] += self.client_weights[i] * client_dict[key]\n\n        self.global_model.load_state_dict(global_dict)\n\n    def distribute_model(self):\n        \"\"\"Send updated global model to clients.\"\"\"\n        return self.global_model.state_dict()\n\n\n# Usage example\nglobal_model = nn.Embedding(1000, 128)\nserver = FederatedEmbeddingServer(global_model, num_clients=5)\nprint(\"Federated server initialized for distributed training\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFederated server initialized for distributed training\n```\n:::\n:::\n\n\n### Privacy-Preserving Techniques\n\n**1. Differential Privacy**: Add calibrated noise to updates\n\n::: {#11fdff86 .cell execution_count=9}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show Differentially Private Embedding\"}\nimport torch\nimport torch.nn as nn\n\n\nclass DifferentiallyPrivateEmbedding:\n    \"\"\"Add differential privacy noise to embeddings for privacy preservation.\"\"\"\n\n    def __init__(self, model, epsilon=1.0, delta=1e-5):\n        self.model = model\n        self.epsilon = epsilon\n        self.delta = delta\n        self.sensitivity = 1.0\n\n    def add_noise(self, gradients):\n        \"\"\"Add calibrated Gaussian noise for differential privacy.\"\"\"\n        sigma = (self.sensitivity * torch.sqrt(2 * torch.log(torch.tensor(1.25 / self.delta)))) / self.epsilon\n        noisy_gradients = {}\n        for key, grad in gradients.items():\n            noise = torch.randn_like(grad) * sigma\n            noisy_gradients[key] = grad + noise\n        return noisy_gradients\n\n    def private_train_step(self, batch, optimizer):\n        \"\"\"Training step with differential privacy.\"\"\"\n        optimizer.zero_grad()\n        loss = self.model(batch)\n        loss.backward()\n\n        gradients = {name: param.grad.clone() for name, param in self.model.named_parameters() if param.grad is not None}\n        noisy_grads = self.add_noise(gradients)\n\n        for name, param in self.model.named_parameters():\n            if name in noisy_grads:\n                param.grad = noisy_grads[name]\n\n        optimizer.step()\n        return loss.item()\n\n\n# Usage example\nmodel = nn.Embedding(1000, 128)\ndp_trainer = DifferentiallyPrivateEmbedding(model, epsilon=1.0)\nprint(f\"DP training with epsilon={dp_trainer.epsilon}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDP training with epsilon=1.0\n```\n:::\n:::\n\n\n**2. Secure Aggregation**: Encrypt updates before sharing\n\n::: {#45f7f576 .cell execution_count=10}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show Secure Aggregation\"}\nimport torch\n\n\nclass SecureAggregation:\n    \"\"\"Secure aggregation using secret sharing for federated learning.\"\"\"\n\n    def __init__(self, num_clients):\n        self.num_clients = num_clients\n\n    def add_secret_shares(self, model_update):\n        \"\"\"Add secret shares to model update for secure aggregation.\"\"\"\n        shares = []\n        for _ in range(self.num_clients - 1):\n            share = {k: torch.randn_like(v) for k, v in model_update.items()}\n            shares.append(share)\n\n        final_share = {}\n        for key in model_update.keys():\n            final_share[key] = model_update[key] - sum(s[key] for s in shares)\n\n        shares.append(final_share)\n        return shares\n\n    def aggregate_shares(self, client_shares):\n        \"\"\"Aggregate secret shares to recover sum without revealing individual updates.\"\"\"\n        aggregated = {}\n        first_client = client_shares[0]\n\n        for key in first_client.keys():\n            aggregated[key] = sum(client[key] for client in client_shares)\n\n        return aggregated\n\n\n# Usage example\nsecure_agg = SecureAggregation(num_clients=5)\nupdate = {'embeddings': torch.randn(100, 128)}\nshares = secure_agg.add_secret_shares(update)\nreconstructed = secure_agg.aggregate_shares(shares)\nprint(f\"Secure aggregation with {secure_agg.num_clients} clients\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSecure aggregation with 5 clients\n```\n:::\n:::\n\n\n:::{.callout-tip}\n## Federated Learning vs. Centralized\n\n**Use federated learning when:**\n\n- Data cannot be centralized (privacy, regulations, size)\n- Multiple organizations want to collaborate without sharing data\n- Data is naturally distributed (mobile devices, edge servers)\n\n**Use centralized learning when:**\n\n- Data can be legally and practically centralized\n- Single organization owns all data\n- Communication costs are prohibitive\n- Need fastest possible training\n:::\n\n:::{.callout-warning}\n## Communication Bottleneck\nFederated learning requires multiple rounds of communication between clients and server. For large models, this can be slower than centralized training even though computation is distributed. Optimize communication:\n\n1. **Model compression**: Send compressed updates (quantization, sparsification)\n2. **Fewer rounds**: More local epochs per round\n3. **Client sampling**: Not all clients participate each round\n4. **Asynchronous updates**: Don't wait for slowest client\n:::\n\n## Key Takeaways\n\n- **Hierarchical embeddings** in hyperbolic space preserve taxonomic structure with 20-100x lower dimensionality than Euclidean embeddings, essential for product catalogs, knowledge graphs, and organizational structures\n\n- **Dynamic embeddings** capture temporal evolution of entities, critical for user preferences, document relevance, and any domain where meanings shift over time\n\n- **Compositional embeddings** explicitly model multi-component entities (products with categories/brands/reviews, documents with title/body/metadata), learning task-specific combination strategies\n\n- **Uncertainty quantification** provides confidence scores for embedding-based decisions, essential for high-stakes applications in healthcare, finance, and autonomous systems where knowing when not to trust a prediction is as important as the prediction itself\n\n- **Federated learning** enables training embeddings across data silos without centralizing data, crucial for privacy-sensitive domains like healthcare, finance, and cross-organizational collaboration\n\n- Advanced techniques are not always necessary—use them when your application has specific requirements (hierarchy, temporal dynamics, privacy constraints) that standard embeddings cannot address\n\n- Production deployment requires careful engineering: streaming updates for dynamic embeddings, calibration for uncertainty, secure communication for federated learning\n\n## Looking Ahead\n\nThis concludes Part II on Custom Embedding Development. We've progressed from basic custom embeddings (@sec-custom-embedding-strategies) through sophisticated training techniques (contrastive learning, Siamese networks, self-supervised learning) to advanced methods for specialized scenarios.\n\n**Part III begins with @sec-embedding-pipeline-engineering**, shifting focus from developing embeddings to deploying them in production. We'll explore MLOps practices, real-time vs. batch processing, versioning strategies, and monitoring embedding systems at scale.\n\n## Further Reading\n\n### Hierarchical Embeddings\n- Nickel & Kiela (2017). \"Poincaré Embeddings for Learning Hierarchical Representations.\" NeurIPS.\n- Sala et al. (2018). \"Representation Tradeoffs for Hyperbolic Embeddings.\" ICML.\n- Dhingra et al. (2018). \"Embedding Text in Hyperbolic Spaces.\" Workshop on Structured Prediction for NLP.\n\n### Dynamic Embeddings\n- Rudolph & Blei (2018). \"Dynamic Embeddings for Language Evolution.\" WWW.\n- Yao et al. (2018). \"Dynamic Word Embeddings for Evolving Semantic Discovery.\" WSDM.\n- Trivedi et al. (2019). \"DyRep: Learning Representations over Dynamic Graphs.\" ICLR.\n\n### Compositional Embeddings\n- Mitchell & Lapata (2010). \"Composition in Distributional Models of Semantics.\" Cognitive Science.\n- Socher et al. (2013). \"Recursive Deep Models for Semantic Compositionality.\" EMNLP.\n- Yu & Dredze (2015). \"Learning Composition Models for Phrase Embeddings.\" TACL.\n\n### Uncertainty Quantification\n- Kendall & Gal (2017). \"What Uncertainties Do We Need in Bayesian Deep Learning?\" NeurIPS.\n- Lakshminarayanan et al. (2017). \"Simple and Scalable Predictive Uncertainty Estimation.\" NeurIPS.\n- Malinin & Gales (2018). \"Predictive Uncertainty Estimation via Prior Networks.\" NeurIPS.\n\n### Federated Learning\n- McMahan et al. (2017). \"Communication-Efficient Learning of Deep Networks from Decentralized Data.\" AISTATS.\n- Li et al. (2020). \"Federated Optimization in Heterogeneous Networks.\" MLSys.\n- Kairouz et al. (2021). \"Advances and Open Problems in Federated Learning.\" Foundations and Trends in Machine Learning.\n- Abadi et al. (2016). \"Deep Learning with Differential Privacy.\" CCS.\n\n",
    "supporting": [
      "ch18_advanced_embedding_techniques_files/figure-epub"
    ],
    "filters": [],
    "engineDependencies": {
      "jupyter": [
        {
          "jsWidgets": false,
          "jupyterWidgets": false,
          "htmlLibraries": []
        }
      ]
    }
  }
}