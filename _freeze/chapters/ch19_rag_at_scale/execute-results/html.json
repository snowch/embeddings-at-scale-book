{
  "hash": "961f1591c732b3ab75474214e48867cf",
  "result": {
    "engine": "jupyter",
    "markdown": "# Retrieval-Augmented Generation (RAG) at Scale {#sec-rag-at-scale}\n\n:::{.callout-note}\n## Chapter Overview\nRetrieval-Augmented Generation combines the power of embedding-based retrieval with large language model generation, enabling LLMs to answer questions grounded in enterprise knowledge rather than relying solely on parametric memory. This chapter explores production RAG systems at scale: enterprise architecture patterns that handle billion-document corpora, context window optimization strategies that maximize information density while respecting token limits, multi-stage retrieval pipelines that balance recall and precision across filtering and reranking stages, evaluation frameworks that measure end-to-end quality beyond simple metrics, and techniques for handling contradictory information when sources disagree. These patterns enable RAG systems that serve millions of users with accurate, attributable, up-to-date responses.\n:::\n\nWith robust data engineering in place (@sec-data-engineering), the foundation exists to build advanced applications that leverage embeddings at scale. **Retrieval-Augmented Generation (RAG)** has emerged as the dominant pattern for grounding large language models in enterprise knowledge. Rather than fine-tuning models on proprietary data (expensive, slow to update, risk of hallucination), RAG retrieves relevant context from vector databases and includes it in the LLM prompt. This approach enables accurate answers over billion-document corpora, maintains attribution to sources, updates knowledge in real-time, and scales to trillion-row datasets—all critical requirements for enterprise deployment.\n\n## Enterprise RAG Architecture Patterns\n\nProduction RAG systems serve thousands of concurrent users querying billion-document knowledge bases with sub-second latency and high accuracy. **Enterprise RAG architectures** decompose this challenge into specialized components: query understanding, retrieval, reranking, context assembly, generation, and response validation. Each component must scale independently while maintaining end-to-end quality.\n\n### The RAG Pipeline\n\nA complete RAG system comprises six stages:\n\n1. **Query Understanding**: Parse user intent, extract entities, expand with synonyms\n2. **Retrieval**: Vector search for top-k relevant documents (k=100-1000)\n3. **Reranking**: Reorder results by relevance using cross-encoder (reduce to k=5-20)\n4. **Context Assembly**: Fit selected documents into context window\n5. **Generation**: LLM generates response given query + context\n6. **Validation**: Verify response accuracy, check for hallucinations\n\n::: {#4f5d3773 .cell execution_count=1}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show Vector Store Setup\"}\nfrom dataclasses import dataclass\nfrom typing import List, Optional\n\nimport faiss\nimport numpy as np\n\n\n@dataclass\nclass Document:\n    \"\"\"Document with embedding.\"\"\"\n    doc_id: str\n    text: str\n    embedding: Optional[np.ndarray] = None\n    metadata: dict = None\n\n    def __post_init__(self):\n        if self.metadata is None:\n            self.metadata = {}\n\n\nclass VectorStore:\n    \"\"\"FAISS-based vector store for document retrieval.\"\"\"\n    def __init__(self, embedding_dim: int = 768):\n        self.embedding_dim = embedding_dim\n        self.index = faiss.IndexFlatIP(embedding_dim)\n        self.documents: List[Document] = []\n\n    def add_documents(self, documents: List[Document]):\n        \"\"\"Add documents to the vector store.\"\"\"\n        embeddings = np.array([doc.embedding for doc in documents]).astype('float32')\n        faiss.normalize_L2(embeddings)\n        self.index.add(embeddings)\n        self.documents.extend(documents)\n\n    def search(self, query_embedding: np.ndarray, k: int = 5) -> List[Document]:\n        \"\"\"Search for top-k most similar documents.\"\"\"\n        query_embedding = query_embedding.astype('float32').reshape(1, -1)\n        faiss.normalize_L2(query_embedding)\n        distances, indices = self.index.search(query_embedding, k)\n        return [self.documents[i] for i in indices[0]]\n\n# Usage example\nstore = VectorStore(embedding_dim=768)\ndocs = [\n    Document(doc_id=\"1\", text=\"Machine learning basics\", embedding=np.random.rand(768)),\n    Document(doc_id=\"2\", text=\"Deep learning with PyTorch\", embedding=np.random.rand(768))\n]\nstore.add_documents(docs)\nresults = store.search(np.random.rand(768), k=2)\nprint(f\"Found {len(results)} documents\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFound 2 documents\n```\n:::\n:::\n\n\n:::{.callout-tip}\n## Enterprise RAG Best Practices\n\n**Architecture:**\n\n- Decouple components (retrieval, reranking, generation)\n- Use async/parallel processing where possible\n- Implement circuit breakers for each component\n- Cache frequent queries and intermediate results\n\n**Query Processing:**\n\n- Always classify intent (different strategies per type)\n- Extract and normalize entities\n- Use query expansion for better recall\n- Parse metadata filters from natural language\n\n**Retrieval:**\n\n- Start with high k (100-1000) for recall\n- Use multiple retrieval strategies (vector + keyword)\n- Apply metadata filters early (before reranking)\n- Log retrieval metrics for continuous improvement\n\n**Reranking:**\n\n- Essential for production accuracy (10-20% improvement)\n- Use cross-encoder models (more accurate than bi-encoders)\n- Batch reranking requests for efficiency\n- Consider two-stage reranking (coarse then fine)\n:::\n\n## Context Window Optimization\n\nLLMs have fixed context windows (4K-128K tokens), but enterprise knowledge bases contain millions of documents. **Context window optimization** maximizes information density: selecting the most relevant passages, removing redundancy, compressing verbose content, and structuring information for LLM comprehension.\n\n### The Context Window Challenge\n\n**Problem**: Retrieved documents often exceed context limits\n- 10 documents × 1000 tokens each = 10K tokens\n- Typical LLM limit: 4K-8K tokens\n- Need to reduce 10K → 4K while preserving key information\n\n**Naive approach**: Truncate each document\n- **Problem**: May cut off critical information, often removes conclusions\n\n**Better approach**: Extract relevant passages, deduplicate, compress\n\n::: {#44aae381 .cell execution_count=2}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show Passage Extractor\"}\nfrom typing import List, Tuple\nimport re\n\n\nclass PassageExtractor:\n    \"\"\"Extract relevant passages from long documents.\"\"\"\n    def __init__(self, max_passage_length: int = 512, overlap: int = 50):\n        self.max_passage_length = max_passage_length\n        self.overlap = overlap\n\n    def extract_passages(self, text: str) -> List[Tuple[str, int, int]]:\n        \"\"\"Split text into overlapping passages.\n\n        Returns: List of (passage_text, start_idx, end_idx)\n        \"\"\"\n        sentences = re.split(r'(?<=[.!?])\\s+', text)\n        passages = []\n        current_passage = []\n        current_length = 0\n        start_idx = 0\n\n        for sentence in sentences:\n            sentence_length = len(sentence.split())\n\n            if current_length + sentence_length > self.max_passage_length:\n                if current_passage:\n                    passage_text = ' '.join(current_passage)\n                    end_idx = start_idx + len(passage_text)\n                    passages.append((passage_text, start_idx, end_idx))\n\n                    # Keep overlap\n                    overlap_text = current_passage[-self.overlap:]\n                    current_passage = overlap_text + [sentence]\n                    start_idx = end_idx - len(' '.join(overlap_text))\n                    current_length = sum(len(s.split()) for s in current_passage)\n            else:\n                current_passage.append(sentence)\n                current_length += sentence_length\n\n        if current_passage:\n            passage_text = ' '.join(current_passage)\n            passages.append((passage_text, start_idx, start_idx + len(passage_text)))\n\n        return passages\n\n# Usage example\nextractor = PassageExtractor(max_passage_length=100, overlap=20)\ntext = \"This is a long document. \" * 50\npassages = extractor.extract_passages(text)\nprint(f\"Extracted {len(passages)} passages from document\")\nprint(f\"First passage: {passages[0][0][:100]}...\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nExtracted 32 passages from document\nFirst passage: This is a long document. This is a long document. This is a long document. This is a long document. ...\n```\n:::\n:::\n\n\n:::{.callout-tip}\n## Context Window Optimization Best Practices\n\n**Passage extraction:**\n\n- Use sentence embeddings for relevance scoring\n- Keep consecutive sentences for narrative flow\n- Extract different amounts per query type (factual: less, explanation: more)\n\n**Deduplication:**\n\n- Use MinHash or embeddings for semantic similarity\n- Set threshold based on acceptable information loss (0.8-0.9)\n- Keep first occurrence (usually most complete)\n\n**Token counting:**\n\n- Use tokenizer from target LLM (different tokenizers vary)\n- Count precisely, don't estimate (estimation errors compound)\n- Reserve tokens for query, instructions, output (typically 20-30%)\n\n**Hierarchical assembly:**\n\n- Always include document titles/metadata\n- Prioritize key passages over full text\n- Add detail progressively until limit reached\n:::\n\n:::{.callout-warning}\n## Context Window Pitfalls\n\nCommon mistakes that degrade RAG quality:\n\n**Over-truncation**: Cutting documents mid-sentence or mid-paragraph loses context\n- **Solution**: Truncate at sentence/paragraph boundaries\n\n**Lost citations**: After extraction/summarization, can't attribute claims\n- **Solution**: Maintain document IDs throughout processing\n\n**Query not in context**: Forgot to include original query in prompt\n- **Solution**: Always include query, even if redundant\n\n**Exceeding limit**: Token estimation off, actual usage exceeds limit\n- **Solution**: Use actual tokenizer, add 10% safety buffer\n:::\n\n## Multi-Stage Retrieval Systems\n\nSingle-stage retrieval (retrieve top-k, done) sacrifices either recall or latency. **Multi-stage retrieval** separates concerns: early stages optimize for recall (don't miss relevant documents), later stages optimize for precision (rank best documents highest). This enables billion-document search with high accuracy and low latency.\n\n### The Multi-Stage Architecture\n\n**Stage 1: Coarse Retrieval (Recall-focused)**\n- Goal: Don't miss relevant documents\n- Method: Fast vector search (ANN)\n- Scale: Search full corpus (1B+ documents)\n- Output: Top-1000 candidates\n- Latency: 50-100ms\n\n**Stage 2: Reranking (Precision-focused)**\n- Goal: Rank best documents highest\n- Method: Cross-encoder model\n- Scale: Rerank 1000 candidates\n- Output: Top-20 documents\n- Latency: 50-200ms\n\n**Stage 3: Final Selection (Context-focused)**\n- Goal: Maximize context window utilization\n- Method: Passage extraction, deduplication\n- Scale: Process 20 documents\n- Output: Optimized context\n- Latency: 10-50ms\n\n::: {#92be78ff .cell execution_count=3}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show Multi-Stage Retriever\"}\nfrom typing import List\nimport numpy as np\n\n\nclass MultiStageRetriever:\n    \"\"\"Two-stage retrieval: fast first-stage, accurate second-stage.\"\"\"\n    def __init__(self, vector_store, reranker_model=None):\n        self.vector_store = vector_store\n        self.reranker_model = reranker_model\n\n    def retrieve(self, query: str, query_embedding: np.ndarray,\n                 k: int = 5, first_stage_k: int = 20) -> List[Document]:\n        \"\"\"Retrieve documents using two-stage approach.\n\n        Stage 1: Fast vector search retrieves top-N candidates\n        Stage 2: Reranker scores candidates and returns top-K\n        \"\"\"\n        # Stage 1: Fast vector search\n        candidates = self.vector_store.search(query_embedding, k=first_stage_k)\n\n        # Stage 2: Rerank with more expensive model\n        if self.reranker_model:\n            scores = []\n            for doc in candidates:\n                score = np.random.rand()  # Placeholder for reranking\n                scores.append(score)\n\n            # Sort by reranker score\n            ranked_indices = np.argsort(scores)[::-1]\n            candidates = [candidates[i] for i in ranked_indices[:k]]\n\n        return candidates[:k]\n\n# Usage example\nstore = VectorStore(embedding_dim=768)\ndocs = [Document(doc_id=str(i), text=f\"Doc {i}\",\n                 embedding=np.random.rand(768)) for i in range(100)]\nstore.add_documents(docs)\n\nretriever = MultiStageRetriever(vector_store=store)\nresults = retriever.retrieve(\"sample query\", np.random.rand(768), k=5, first_stage_k=20)\nprint(f\"Retrieved {len(results)} documents after two-stage retrieval\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRetrieved 5 documents after two-stage retrieval\n```\n:::\n:::\n\n\n:::{.callout-tip}\n## Multi-Stage Retrieval Best Practices\n\n**Stage separation:**\n\n- Early stages: Fast, high recall (don't miss relevant docs)\n- Later stages: Slow, high precision (rank best docs highest)\n- Each stage should reduce candidates 50-90%\n\n**Stage selection:**\n\n- Always include: Vector retrieval (stage 1) + Reranking (stage 2)\n- Optional: Keyword filter, diversity filter, metadata filter\n- Add stages based on failure analysis (what's missing? what's wrong?)\n\n**Performance optimization:**\n\n- Cache vector search results (query embeddings stable)\n- Batch reranking requests (100 docs × 1ms each = 100ms, batched = 20ms)\n- Run filters in parallel when possible (keyword + metadata)\n- Monitor stage latencies separately (find bottlenecks)\n\n**Quality monitoring:**\n\n- Track recall @ each stage (is stage 1 missing relevant docs?)\n- Track precision @ each stage (is stage 2 improving ranking?)\n- A/B test stage variations (does keyword filter help?)\n:::\n\n## RAG Evaluation Frameworks\n\nRAG systems combine retrieval and generation, requiring evaluation beyond standard IR or NLG metrics. **RAG evaluation frameworks** measure end-to-end quality: retrieval relevance, context utilization, answer accuracy, factual consistency, attribution quality, and user satisfaction.\n\n### The RAG Evaluation Challenge\n\n**Traditional IR metrics** (Recall@k, MRR, NDCG):\n\n- Measure retrieval quality only\n- Don't capture if LLM used retrieved context\n- Don't measure answer accuracy\n\n**Traditional NLG metrics** (BLEU, ROUGE, BERTScore):\n\n- Measure generation quality only\n- Don't capture if answer grounded in context\n- Don't detect hallucinations\n\n**RAG needs both + more**: Did system retrieve relevant docs AND generate accurate answer grounded in those docs?\n\n::: {#2c235338 .cell execution_count=4}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show Hybrid Search\"}\nfrom typing import Dict\nimport numpy as np\n\n\nclass HybridSearch:\n    \"\"\"Combine dense (vector) and sparse (BM25) retrieval.\"\"\"\n    def __init__(self, vector_store, bm25_index=None, alpha: float = 0.5):\n        self.vector_store = vector_store\n        self.bm25_index = bm25_index\n        self.alpha = alpha\n\n    def search(self, query: str, query_embedding: np.ndarray, k: int = 5) -> List[Document]:\n        \"\"\"Hybrid search combining dense and sparse retrieval.\n\n        Score = alpha * dense_score + (1 - alpha) * sparse_score\n        \"\"\"\n        # Dense retrieval\n        dense_results = self.vector_store.search(query_embedding, k=k*2)\n        dense_scores = {doc.doc_id: 1.0 / (i + 1) for i, doc in enumerate(dense_results)}\n\n        # Sparse retrieval (BM25)\n        if self.bm25_index:\n            sparse_scores = {doc.doc_id: np.random.rand() for doc in dense_results}\n        else:\n            sparse_scores = {doc.doc_id: 0.0 for doc in dense_results}\n\n        # Combine scores\n        combined_scores = {}\n        all_doc_ids = set(dense_scores.keys()) | set(sparse_scores.keys())\n\n        for doc_id in all_doc_ids:\n            dense_score = dense_scores.get(doc_id, 0.0)\n            sparse_score = sparse_scores.get(doc_id, 0.0)\n            combined_scores[doc_id] = self.alpha * dense_score + (1 - self.alpha) * sparse_score\n\n        # Sort by combined score\n        sorted_ids = sorted(combined_scores.keys(), key=lambda x: combined_scores[x], reverse=True)\n\n        # Return top-k documents\n        id_to_doc = {doc.doc_id: doc for doc in dense_results}\n        return [id_to_doc[doc_id] for doc_id in sorted_ids[:k] if doc_id in id_to_doc]\n\n# Usage example\nstore = VectorStore(embedding_dim=768)\ndocs = [Document(doc_id=str(i), text=f\"Doc {i}\",\n                 embedding=np.random.rand(768)) for i in range(50)]\nstore.add_documents(docs)\n\nhybrid = HybridSearch(vector_store=store, alpha=0.7)\nresults = hybrid.search(\"sample query\", np.random.rand(768), k=5)\nprint(f\"Hybrid search returned {len(results)} documents\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nHybrid search returned 5 documents\n```\n:::\n:::\n\n\n:::{.callout-tip}\n## RAG Evaluation Best Practices\n\n**Evaluation data:**\n\n- Start with 100-500 query-answer pairs\n- Cover diversity of query types (factual, how-to, comparison, etc.)\n- Include hard cases (contradictory docs, missing info, ambiguous queries)\n- Get human annotations for ground truth (expensive but essential)\n\n**Automated metrics:**\n\n- Retrieval: Recall@10, Recall@100, MRR\n- Generation: Semantic similarity to ground truth (SentenceTransformers)\n- Faithfulness: NLI models (check entailment between context and answer)\n- Attribution: Check if citations support claims\n\n**Human evaluation:**\n\n- Sample 10-20% for human review\n- Ask: Is answer accurate? Is answer complete? Are citations correct?\n- Use majority vote from 3+ annotators\n- Expensive but ground truth for calibrating automated metrics\n\n**Continuous evaluation:**\n\n- Evaluate on every model/prompt change\n- Track metrics over time (detect regressions)\n- A/B test in production (measure user satisfaction)\n:::\n\n## Handling Contradictory Information\n\nReal-world knowledge bases contain contradictions: different sources disagree, information becomes outdated, perspectives conflict. **Contradiction handling** strategies enable RAG systems to navigate disagreements: detecting conflicts, weighing source credibility, presenting multiple perspectives, and updating knowledge as information evolves.\n\n### The Contradiction Challenge\n\n**Types of contradictions:**\n\n1. **Temporal**: Information changes over time\n   - \"Product price is $99\" (2023) vs \"$149\" (2024)\n   - Solution: Prioritize recent information\n\n2. **Source disagreement**: Different sources conflict\n   - Source A: \"API supports OAuth2\" vs Source B: \"API uses API keys\"\n   - Solution: Weigh by source authority/credibility\n\n3. **Perspective differences**: Subjective judgments vary\n   - Review 1: \"Excellent product\" vs Review 2: \"Poor quality\"\n   - Solution: Present multiple perspectives\n\n4. **Partial vs complete**: One source has partial information\n   - Doc 1: \"Supports Python\" vs Doc 2: \"Supports Python, Java, Go\"\n   - Solution: Prefer more complete information\n\n::: {#f704c285 .cell execution_count=5}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show Query Routing\"}\nfrom typing import Dict, List, Optional\nimport numpy as np\n\n\nclass QueryRouter:\n    \"\"\"Route queries to appropriate retrieval strategy.\"\"\"\n    def __init__(self, strategies: Dict[str, any]):\n        self.strategies = strategies\n\n    def route_query(self, query: str, query_embedding: np.ndarray) -> str:\n        \"\"\"Determine which retrieval strategy to use.\n\n        Routes based on query type:\n        - Factual queries -> Dense retrieval\n        - Keyword queries -> Sparse retrieval (BM25)\n        - Complex queries -> Hybrid retrieval\n        \"\"\"\n        query_lower = query.lower()\n\n        if any(word in query_lower for word in ['what', 'when', 'where', 'who']):\n            return 'dense'\n        elif len(query.split()) <= 3:\n            return 'sparse'\n        else:\n            return 'hybrid'\n\n    def retrieve(self, query: str, query_embedding: np.ndarray, k: int = 5) -> List[Document]:\n        \"\"\"Route and retrieve documents.\"\"\"\n        strategy_name = self.route_query(query, query_embedding)\n        strategy = self.strategies.get(strategy_name)\n\n        if strategy:\n            # Handle different strategy interfaces\n            if isinstance(strategy, VectorStore):\n                return strategy.search(query_embedding, k=k)\n            else:\n                return strategy.search(query, query_embedding, k=k)\n        else:\n            first_strategy = list(self.strategies.values())[0]\n            if isinstance(first_strategy, VectorStore):\n                return first_strategy.search(query_embedding, k=k)\n            else:\n                return first_strategy.search(query, query_embedding, k=k)\n\n# Usage example\nstore = VectorStore(embedding_dim=768)\ndocs = [Document(doc_id=str(i), text=f\"Doc {i}\",\n                 embedding=np.random.rand(768)) for i in range(50)]\nstore.add_documents(docs)\n\nstrategies = {\n    'dense': store,\n    'hybrid': HybridSearch(vector_store=store, alpha=0.7)\n}\n\nrouter = QueryRouter(strategies=strategies)\nresults = router.retrieve(\"What is machine learning?\", np.random.rand(768), k=5)\nprint(f\"Query routed and retrieved {len(results)} documents\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nQuery routed and retrieved 5 documents\n```\n:::\n:::\n\n\n:::{.callout-tip}\n## Contradiction Handling Best Practices\n\n**Detection:**\n\n- Use NLI models for semantic contradiction detection\n- Extract claims with high precision (false contradictions confuse users)\n- Focus on factual contradictions (prices, dates, specifications)\n- Ignore stylistic differences (different phrasings of same fact)\n\n**Resolution strategies:**\n\n- **Temporal**: Always prefer recent information (but show date)\n- **Source authority**: Build credibility scores per source type\n- **Confidence**: Use when other signals unavailable\n- **Present multiple**: When confident both are valid (perspectives)\n\n**User experience:**\n\n- Always show sources when contradictions exist\n- Indicate confidence level (\"likely\", \"possibly\", \"conflicting sources\")\n- Provide dates when information might change\n- Allow users to see all perspectives (expandable sections)\n\n**Continuous improvement:**\n\n- Log user selections when presented with contradictions\n- Update source credibility based on user preferences\n- Retrain contradiction detection on corrected examples\n:::\n\n:::{.callout-warning}\n## Contradiction Pitfalls\n\n**Over-resolving**: Automatically picking one answer when both are valid\n- Example: \"Best database for X\" has multiple valid answers\n- Solution: Recognize when question has multiple valid answers\n\n**Temporal confusion**: Using old information because it's higher quality\n- Example: Detailed 2022 guide vs brief 2024 update\n- Solution: Always prioritize recency for rapidly changing topics\n\n**Authority bias**: Always trusting \"authoritative\" source\n- Example: Official docs outdated, community docs current\n- Solution: Consider recency + authority together\n\n**Hidden contradictions**: Not detecting subtle conflicts\n- Example: \"Supports OAuth2\" vs \"Requires API keys\" (implicit contradiction)\n- Solution: Use semantic contradiction detection, not just exact mismatches\n:::\n\n## Key Takeaways\n\n- **RAG combines retrieval and generation for grounded LLM responses**: Retrieving relevant context from vector databases enables accurate answers over billion-document corpora while maintaining attribution and enabling real-time knowledge updates\n\n- **Enterprise RAG requires multi-component architecture**: Query understanding, retrieval, reranking, context assembly, generation, and validation each play critical roles, and each must scale independently\n\n- **Context window optimization maximizes information density**: Passage extraction, deduplication, and hierarchical assembly enable fitting relevant information within LLM token limits while preserving key facts\n\n- **Multi-stage retrieval balances recall and precision**: Early stages (vector search) optimize for recall across billion-doc corpora, later stages (reranking, diversity) optimize for precision with expensive models on small candidate sets\n\n- **RAG evaluation requires measuring beyond retrieval and generation**: End-to-end metrics must capture retrieval relevance, context utilization, answer accuracy, factual consistency, attribution quality, and user satisfaction\n\n- **Contradiction handling enables navigating disagreements in knowledge bases**: Temporal resolution (prefer recent), source authority weighting (prefer credible), and multi-perspective presentation handle conflicts when sources disagree\n\n- **Production RAG demands comprehensive engineering**: Caching, batching, circuit breakers, monitoring, A/B testing, and continuous evaluation separate research prototypes from production systems serving millions of users\n\n## Looking Ahead\n\nThis chapter demonstrated how RAG leverages embeddings for grounded generation at enterprise scale. @sec-semantic-search expands semantic search beyond text: multi-modal search across text, images, audio, and video; code search for software intelligence; scientific literature and patent search with domain-specific understanding; media and content discovery across creative assets; and knowledge graph integration for structured reasoning. These applications demonstrate embeddings' versatility across diverse modalities and domains.\n\n## Further Reading\n\n### RAG Foundations\n- Lewis, Patrick, et al. (2020). \"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.\" NeurIPS.\n- Guu, Kelvin, et al. (2020). \"REALM: Retrieval-Augmented Language Model Pre-Training.\" ICML.\n- Izacard, Gautier, et al. (2021). \"Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering.\" EACL.\n\n### Retrieval Systems\n- Karpukhin, Vladimir, et al. (2020). \"Dense Passage Retrieval for Open-Domain Question Answering.\" EMNLP.\n- Xiong, Lee, et al. (2021). \"Approximate Nearest Neighbor Negative Contrastive Learning for Dense Text Retrieval.\" ICLR.\n- Santhanam, Keshav, et al. (2022). \"ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction.\" NAACL.\n\n### Context Optimization\n- Jiang, Zhengbao, et al. (2023). \"Long-Form Factuality in Large Language Models.\" arxiv.\n- Liu, Nelson F., et al. (2023). \"Lost in the Middle: How Language Models Use Long Contexts.\" arxiv.\n- Press, Ofir, et al. (2022). \"Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation.\" ICLR.\n\n### RAG Evaluation\n- Chen, Daixuan, et al. (2023). \"CRUD-RAG: Benchmarking Retrieval-Augmented Generation for Time-Sensitive Knowledge.\" arxiv.\n- Es, Shahul, et al. (2023). \"RAGAS: Automated Evaluation of Retrieval Augmented Generation.\" arxiv.\n- Liu, Yang, et al. (2023). \"Evaluating the Factuality of Large Language Models.\" ACL.\n\n### Production Systems\n- Anthropic (2023). \"Claude 2 System Card.\"\n- OpenAI (2023). \"GPT-4 Technical Report.\"\n- Thoppilan, Romal, et al. (2022). \"LaMDA: Language Models for Dialog Applications.\" arxiv.\n\n### Contradiction Detection\n- Welleck, Sean, et al. (2019). \"Dialogue Natural Language Inference.\" ACL.\n- Honovich, Or, et al. (2022). \"TRUE: Re-evaluating Factual Consistency Evaluation.\" NAACL.\n- Wang, Cunxiang, et al. (2020). \"CARE: Commonsense-Aware Reasoning for Conversational AI.\" ACL.\n\n### Multi-Stage Retrieval\n- Nogueira, Rodrigo, et al. (2019). \"Passage Re-ranking with BERT.\" arxiv.\n- Gao, Luyu, et al. (2021). \"Rethink Training of BERT Rerankers in Multi-Stage Retrieval Pipeline.\" ECIR.\n- Carbonell, Jaime, and Jade Goldstein (1998). \"The Use of MMR, Diversity-Based Reranking for Reordering Documents and Producing Summaries.\" SIGIR.\n\n",
    "supporting": [
      "ch19_rag_at_scale_files"
    ],
    "filters": [],
    "includes": {}
  }
}