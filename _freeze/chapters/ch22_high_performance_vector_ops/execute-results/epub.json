{
  "hash": "f14cba918953875462a6f2f425cf7660",
  "result": {
    "engine": "jupyter",
    "markdown": "# High-Performance Vector Operations {#sec-high-performance-vector-ops}\n\n:::{.callout-note}\n## Chapter Overview\nTraining embedding models at scale is only half the battle—serving embeddings in production requires extreme optimization of vector operations. This chapter explores the computational techniques that enable sub-millisecond similarity search across billion-vector indices: optimized similarity search algorithms that go beyond naive comparison, approximate nearest neighbor (ANN) methods that trade minimal accuracy for massive speedups, GPU acceleration strategies that exploit parallelism for vector operations, memory-mapped storage approaches that handle datasets exceeding RAM, and parallel query processing architectures that serve thousands of concurrent searches. These optimizations transform embedding systems from research prototypes to production services capable of handling trillion-row scale with single-digit millisecond latency.\n:::\n\nAfter investing weeks in training high-quality embedding models (@sec-scaling-embedding-training) and deploying robust production pipelines (@sec-embedding-pipeline-engineering), the final challenge is **serving embeddings at scale**. A recommendation system might need to search 100 million product embeddings for each user query, processing 10,000 queries per second with p99 latency under 10ms. A fraud detection system might compare incoming transactions against billions of historical embeddings in real-time. These requirements demand optimization at every level: algorithmic improvements, hardware acceleration, memory management, and distributed computation.\n\n## Optimized Similarity Search Algorithms\n\nSimilarity search is the core operation in embedding systems: given a query vector, find the most similar vectors in a large corpus. The naive approach—computing similarity between the query and every vector in the index—is prohibitively expensive at scale. **Optimized algorithms** reduce computation through mathematical insights, data structures, and approximations that maintain high accuracy while dramatically reducing latency.\n\n### The Similarity Search Problem\n\nGiven:\n\n- **Query vector** q ∈ ℝ^d (embedding dimension d)\n- **Corpus** of N vectors {v₁, v₂, ..., vₙ} where each vᵢ ∈ ℝ^d\n- **Similarity metric** (cosine similarity, Euclidean distance, dot product)\n- **k** = number of nearest neighbors to return\n\nFind: The k vectors in the corpus most similar to q\n\n**Naive algorithm complexity**: O(N × d)\n- For each of N vectors, compute d-dimensional similarity\n- Sort results to find top-k\n- At scale: 1B vectors × 512 dims × 4 bytes = 2TB of data to scan\n\n**Challenge**: Reduce from O(N × d) to sub-linear complexity while maintaining high recall\n\n### Exact Search Optimizations\n\nBefore resorting to approximation, several exact search optimizations provide significant speedups:\n\n::: {#cd4862bb .cell execution_count=1}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show NumPy SIMD Operations\"}\nimport numpy as np\nfrom typing import List\n\ndef cosine_similarity_batch(vectors: np.ndarray, query: np.ndarray) -> np.ndarray:\n    \"\"\"Calculate cosine similarity using SIMD-optimized operations.\"\"\"\n    query_norm = query / np.linalg.norm(query)\n    norms = np.linalg.norm(vectors, axis=1, keepdims=True)\n    vectors_norm = vectors / norms\n    return np.dot(vectors_norm, query_norm)\n\ndef euclidean_distance_batch(vectors: np.ndarray, query: np.ndarray) -> np.ndarray:\n    \"\"\"Calculate Euclidean distance using vectorized operations.\"\"\"\n    diff = vectors - query\n    return np.sqrt(np.sum(diff ** 2, axis=1))\n\ndef top_k_indices(similarities: np.ndarray, k: int) -> np.ndarray:\n    \"\"\"Get indices of top k similarities using partial sort.\"\"\"\n    return np.argpartition(similarities, -k)[-k:]\n\n# Usage example\nvectors = np.random.randn(10000, 384)  # 10k vectors of dim 384\nquery = np.random.randn(384)\nsimilarities = cosine_similarity_batch(vectors, query)\ntop_k_idx = top_k_indices(similarities, k=10)\nprint(f\"Top 10 most similar vectors: {top_k_idx}\")\nprint(f\"Their similarity scores: {similarities[top_k_idx]}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTop 10 most similar vectors: [9941  822 9019 2778 7406 2877 5739 2275 5477 8209]\nTheir similarity scores: [0.16590106 0.16827177 0.17126436 0.17351956 0.17388803 0.2206091\n 0.17526233 0.20222725 0.20192127 0.17716573]\n```\n:::\n:::\n\n\n:::{.callout-tip}\n## When to Use Exact vs. Approximate Search\n\n**Use exact search when:**\n\n- Corpus < 10M vectors (exact search fast enough with GPU)\n- Zero approximation error required (regulatory/compliance)\n- Have powerful GPUs (A100: 100M+ vectors/sec)\n- Latency budget > 10ms (allows brute force)\n\n**Use approximate search when:**\n\n- Corpus > 10M vectors (exact search too slow)\n- Can tolerate 95-99% recall (most applications)\n- Latency budget < 10ms (need sub-linear algorithms)\n- Want to scale to billions of vectors\n:::\n\n## Approximate Nearest Neighbor (ANN) at Scale\n\nFor billion-vector indices, exact search becomes infeasible. **Approximate nearest neighbor (ANN)** algorithms trade small amounts of recall for massive speedups—typically achieving 95-99% recall at 10-100× lower latency for million-scale indices, scaling to 100-1000× at billion-scale. Modern ANN methods combine graph-based navigation, quantization, and partitioning to enable sub-millisecond search across trillion-row datasets.\n\n### ANN Algorithm Landscape\n\n**Partitioning methods** (divide space into regions):\n\n- **IVF (Inverted File Index)**: Cluster vectors, search only nearby clusters\n- **LSH (Locality-Sensitive Hashing)**: Hash similar vectors to same buckets\n- Pro: Simple, fast for low-dimensional data\n- Con: Curse of dimensionality, many clusters needed for high recall\n\n**Graph methods** (navigate similarity graph):\n\n- **HNSW (Hierarchical Navigable Small World)**: Multi-layer skip list graph\n- **NSG (Navigating Spreading-out Graph)**: Optimized graph structure\n- Pro: Excellent recall-speed trade-off, robust to dimensionality\n- Con: Higher memory usage, slower index build\n\n**Quantization methods** (compress vectors):\n\n- **Product Quantization (PQ)**: Vector compression via clustering\n- **Scalar Quantization (SQ)**: Reduce precision (FP32 → INT8)\n- Pro: Massive memory reduction (8-32×), enables larger indices\n- Con: Accuracy loss, requires reranking\n\n::: {#81c50068 .cell execution_count=2}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show IVF (Inverted File) Index Implementation\"}\nimport numpy as np\nfrom sklearn.cluster import KMeans\n\nclass IVFIndex:\n    \"\"\"Inverted File Index for fast approximate similarity search.\"\"\"\n\n    def __init__(self, n_clusters: int = 100, n_probes: int = 5):\n        self.n_clusters = n_clusters\n        self.n_probes = n_probes\n        self.cluster_model = None\n        self.inverted_lists = None\n\n    def build(self, vectors: np.ndarray):\n        \"\"\"Build index by clustering vectors.\"\"\"\n        self.cluster_model = KMeans(n_clusters=self.n_clusters, random_state=42)\n        cluster_ids = self.cluster_model.fit_predict(vectors)\n\n        # Build inverted lists\n        self.inverted_lists = [[] for _ in range(self.n_clusters)]\n        for idx, cluster_id in enumerate(cluster_ids):\n            self.inverted_lists[cluster_id].append((idx, vectors[idx]))\n\n    def search(self, query: np.ndarray, k: int = 10) -> list:\n        \"\"\"Search top-k similar vectors by probing nearest clusters.\"\"\"\n        # Find nearest clusters\n        cluster_distances = np.linalg.norm(\n            self.cluster_model.cluster_centers_ - query, axis=1\n        )\n        nearest_clusters = np.argsort(cluster_distances)[:self.n_probes]\n\n        # Search within nearest clusters\n        candidates = []\n        for cluster_id in nearest_clusters:\n            for idx, vec in self.inverted_lists[cluster_id]:\n                sim = np.dot(vec, query) / (np.linalg.norm(vec) * np.linalg.norm(query))\n                candidates.append((idx, sim))\n\n        # Return top-k\n        candidates.sort(key=lambda x: x[1], reverse=True)\n        return candidates[:k]\n\n# Usage example\nvectors = np.random.randn(100000, 128)  # 100k vectors\nindex = IVFIndex(n_clusters=100, n_probes=5)\nindex.build(vectors)\nquery = np.random.randn(128)\nresults = index.search(query, k=10)\nprint(f\"Found {len(results)} results, top similarity: {results[0][1]:.4f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFound 10 results, top similarity: 0.3502\n```\n:::\n:::\n\n\n:::{.callout-tip}\n## Choosing the Right ANN Algorithm\n\n**Use IVF when:**\n\n- Batch processing (can afford slower build)\n- Memory constrained (IVF has lower overhead)\n- Low-dimensional embeddings (< 128 dims)\n- Large clusters acceptable (>1M vectors per cluster)\n\n**Use HNSW when:**\n\n- Online updates (incremental indexing)\n- High-dimensional embeddings (> 128 dims)\n- Need best recall-speed trade-off\n- Have memory for graph structure (~10-20 bytes/vector)\n\n**Use Product Quantization when:**\n\n- Massive scale (> 1B vectors)\n- Memory extremely constrained\n- Can tolerate reranking step\n- Storage cost dominates compute cost\n\n**Production systems often combine:**\n\n- HNSW + Product Quantization (graph structure + compression)\n- IVF + Product Quantization (partitioning + compression)\n- Multi-stage: Coarse filter (IVF) → Fine ranking (exact)\n:::\n\n:::{.callout-warning}\n## Recall-Latency Trade-offs\n\nAll ANN algorithms have tuning parameters that control recall-latency trade-off:\n\n- **IVF**: More probes = higher recall, higher latency\n- **HNSW**: Higher ef_search = higher recall, higher latency\n- **Typical production**: 95-99% recall is acceptable for most applications\n\nAlways measure recall on holdout test set. A 2× speedup at 80% recall may hurt user experience more than the latency improvement helps.\n:::\n\n## GPU Acceleration for Vector Operations\n\nModern GPUs provide 10-100× speedup for vector operations through massive parallelism. A single NVIDIA A100 GPU has 432 TFLOPS of FP16 throughput—equivalent to thousands of CPU cores. Effective GPU acceleration requires understanding memory hierarchies, kernel optimization, and batching strategies.\n\n### GPU Architecture for Vector Operations\n\n::: {#6dc62587 .cell execution_count=3}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show GPU-Accelerated Vector Search\"}\nimport numpy as np\n\nclass GPUVectorSearch:\n    \"\"\"GPU-accelerated similarity search using CuPy.\"\"\"\n\n    def __init__(self, use_fp16: bool = False):\n        try:\n            import cupy as cp\n            self.cp = cp\n            self.use_gpu = True\n            self.dtype = cp.float16 if use_fp16 else cp.float32\n        except ImportError:\n            self.use_gpu = False\n            print(\"CuPy not available, using CPU fallback\")\n\n    def index(self, vectors: np.ndarray):\n        \"\"\"Transfer vectors to GPU memory.\"\"\"\n        if self.use_gpu:\n            self.vectors_gpu = self.cp.asarray(vectors, dtype=self.dtype)\n            self.norms_gpu = self.cp.linalg.norm(self.vectors_gpu, axis=1, keepdims=True)\n        else:\n            self.vectors_cpu = vectors\n\n    def search_batch(self, queries: np.ndarray, k: int = 10) -> list:\n        \"\"\"Search multiple queries in parallel on GPU.\"\"\"\n        if self.use_gpu:\n            queries_gpu = self.cp.asarray(queries, dtype=self.dtype)\n            query_norms = self.cp.linalg.norm(queries_gpu, axis=1, keepdims=True)\n\n            # Normalized cosine similarity on GPU\n            vectors_norm = self.vectors_gpu / self.norms_gpu\n            queries_norm = queries_gpu / query_norms\n            similarities = self.cp.dot(queries_norm, vectors_norm.T)\n\n            # Get top-k indices\n            top_k_indices = self.cp.argsort(similarities, axis=1)[:, -k:]\n            return self.cp.asnumpy(top_k_indices[:, ::-1])\n        else:\n            # CPU fallback\n            results = []\n            for query in queries:\n                sims = np.dot(self.vectors_cpu, query)\n                top_k = np.argsort(sims)[-k:][::-1]\n                results.append(top_k)\n            return np.array(results)\n\n# Usage example\nvectors = np.random.randn(1000000, 256).astype(np.float32)  # 1M vectors\ngpu_search = GPUVectorSearch(use_fp16=True)\ngpu_search.index(vectors)\nqueries = np.random.randn(100, 256).astype(np.float32)  # Batch of 100 queries\nresults = gpu_search.search_batch(queries, k=10)\nprint(f\"Processed {len(queries)} queries, found {results.shape[1]} results each\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCuPy not available, using CPU fallback\nProcessed 100 queries, found 10 results each\n```\n:::\n:::\n\n\n:::{.callout-tip}\n## GPU Optimization Best Practices\n\n**Memory management:**\n\n- Use FP16 when possible (2× capacity, minimal accuracy loss)\n- Pin memory for faster CPU→GPU transfers\n- Keep frequently accessed vectors in GPU memory\n- Use unified memory for > GPU capacity (automatic paging)\n\n**Computation optimization:**\n\n- Batch queries to amortize kernel launch overhead (10-100× speedup)\n- Use Tensor Cores (matrix multiplication) over element-wise ops\n- Minimize CPU-GPU synchronization points\n- Profile with `nvprof` or NSight to find bottlenecks\n\n**Multi-GPU scaling:**\n\n- Shard corpus across GPUs for > 80GB datasets\n- Use NCCL for fast inter-GPU communication\n- Pipeline data transfer and computation\n- Consider model parallelism for very wide embeddings\n:::\n\n## Memory-Mapped Vector Storage\n\nBillion-vector indices exceed RAM capacity (1B × 512 dims × 4 bytes = 2TB). **Memory-mapped files** enable working with datasets larger than memory by loading data on-demand from disk, with the OS managing paging and caching.\n\n::: {#2adc5349 .cell execution_count=4}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show Memory-Mapped Vector Storage\"}\nimport numpy as np\nimport tempfile\nimport os\n\nclass MemoryMappedVectorStore:\n    \"\"\"Efficient vector storage using memory-mapped files for datasets larger than RAM.\"\"\"\n\n    def __init__(self, filepath: str, dim: int, dtype=np.float32):\n        self.filepath = filepath\n        self.dim = dim\n        self.dtype = dtype\n        self.mmap = None\n        self.count = 0\n\n    def create(self, n_vectors: int):\n        \"\"\"Create a new memory-mapped file with pre-allocated capacity.\"\"\"\n        shape = (n_vectors, self.dim)\n        self.mmap = np.memmap(self.filepath, dtype=self.dtype, mode='w+', shape=shape)\n        self.count = 0  # Start with zero vectors written\n\n    def append(self, vectors: np.ndarray):\n        \"\"\"Append vectors to the store.\"\"\"\n        if self.mmap is None:\n            # Pre-allocate space for initial batch (can grow later)\n            self.create(len(vectors))\n        self.mmap[self.count:self.count + len(vectors)] = vectors\n        self.count += len(vectors)\n        self.mmap.flush()\n\n    def load(self):\n        \"\"\"Load existing memory-mapped file.\"\"\"\n        if os.path.exists(self.filepath):\n            self.mmap = np.memmap(self.filepath, dtype=self.dtype, mode='r')\n            self.count = self.mmap.shape[0]\n\n    def get_batch(self, indices: np.ndarray) -> np.ndarray:\n        \"\"\"Retrieve specific vectors by index.\"\"\"\n        return self.mmap[indices]\n\n    def search_top_k(self, query: np.ndarray, k: int = 10) -> tuple:\n        \"\"\"Search for top-k most similar vectors.\"\"\"\n        # Process in chunks to avoid loading entire index into RAM\n        chunk_size = 100000\n        top_indices = []\n        top_similarities = []\n\n        for i in range(0, self.count, chunk_size):\n            end = min(i + chunk_size, self.count)\n            chunk = self.mmap[i:end]\n            sims = np.dot(chunk, query) / (np.linalg.norm(chunk, axis=1) * np.linalg.norm(query))\n            chunk_top_k = np.argsort(sims)[-k:]\n            top_indices.extend(i + chunk_top_k)\n            top_similarities.extend(sims[chunk_top_k])\n\n        # Get global top-k\n        final_top_k = np.argsort(top_similarities)[-k:]\n        return np.array([top_indices[i] for i in final_top_k]), np.array([top_similarities[i] for i in final_top_k])\n\n# Usage example\nwith tempfile.NamedTemporaryFile(delete=False, suffix='.mmap') as f:\n    store = MemoryMappedVectorStore(f.name, dim=256)\n    vectors = np.random.randn(1000000, 256).astype(np.float32)\n    store.create(len(vectors))\n    store.mmap[:] = vectors\n    query = np.random.randn(256).astype(np.float32)\n    indices, sims = store.search_top_k(query, k=10)\n    print(f\"Top 10 indices: {indices}\")\n    print(f\"Top 10 similarities: {sims}\")\n    os.unlink(f.name)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTop 10 indices: []\nTop 10 similarities: []\n```\n:::\n:::\n\n\n:::{.callout-tip}\n## Memory-Mapped Storage Best Practices\n\n**When to use:**\n\n- Dataset > available RAM\n- Can tolerate slightly higher latency (0.1-1ms SSD access vs <0.1ms RAM)\n- Access patterns have locality (similar vectors accessed together)\n- Cost-sensitive (avoid paying for 1TB+ RAM)\n\n**Optimizations:**\n\n- Use SSDs (10× faster than HDDs for random access)\n- Cluster similar vectors together (spatial locality → better caching)\n- Combine with RAM cache for hot vectors (90/10 rule applies)\n- Prefetch next batch during computation\n- Use `madvise` to give OS paging hints\n\n**Avoid for:**\n\n- Real-time serving (< 10ms latency requirements)\n- Truly random access patterns (no cache benefits)\n- Frequently updated indices (mmap flush overhead)\n:::\n\n## Parallel Query Processing\n\nModern embedding systems serve thousands of concurrent queries. **Parallel query processing** distributes load across multiple cores, GPUs, and machines to achieve high throughput while maintaining low latency.\n\n::: {#5968cfcd .cell execution_count=5}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show Parallel Query Processing\"}\nimport numpy as np\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\nfrom queue import Queue\nimport time\n\nclass ParallelQueryProcessor:\n    \"\"\"Process multiple queries in parallel using thread pool.\"\"\"\n\n    def __init__(self, vectors: np.ndarray, n_workers: int = 8, batch_timeout: float = 0.05):\n        self.vectors = vectors\n        self.n_workers = n_workers\n        self.batch_timeout = batch_timeout\n        self.executor = ThreadPoolExecutor(max_workers=n_workers)\n        self.query_queue = Queue()\n\n    def search_single(self, query: np.ndarray, k: int = 10) -> np.ndarray:\n        \"\"\"Search for a single query.\"\"\"\n        similarities = np.dot(self.vectors, query) / (\n            np.linalg.norm(self.vectors, axis=1) * np.linalg.norm(query)\n        )\n        return np.argsort(similarities)[-k:][::-1]\n\n    def search_batch(self, queries: list, k: int = 10) -> list:\n        \"\"\"Process multiple queries in parallel.\"\"\"\n        futures = []\n        for query in queries:\n            future = self.executor.submit(self.search_single, query, k)\n            futures.append(future)\n\n        results = []\n        for future in as_completed(futures):\n            results.append(future.result())\n        return results\n\n    def search_with_batching(self, queries: list, k: int = 10) -> list:\n        \"\"\"Micro-batch queries for better GPU utilization.\"\"\"\n        batch_size = 32\n        all_results = []\n\n        for i in range(0, len(queries), batch_size):\n            batch = queries[i:i + batch_size]\n            # Batch process for efficiency\n            batch_queries = np.array(batch)\n            similarities = np.dot(self.vectors, batch_queries.T)\n            top_k_indices = np.argsort(similarities, axis=0)[-k:, :]\n\n            for col in range(top_k_indices.shape[1]):\n                all_results.append(top_k_indices[:, col][::-1])\n\n        return all_results\n\n    def shutdown(self):\n        \"\"\"Clean up resources.\"\"\"\n        self.executor.shutdown(wait=True)\n\n# Usage example\nvectors = np.random.randn(100000, 256).astype(np.float32)\nprocessor = ParallelQueryProcessor(vectors, n_workers=8)\n\n# Process 100 queries in parallel\nqueries = [np.random.randn(256).astype(np.float32) for _ in range(100)]\nstart = time.time()\nresults = processor.search_batch(queries, k=10)\nelapsed = time.time() - start\nprint(f\"Processed {len(queries)} queries in {elapsed:.3f}s\")\nprint(f\"Throughput: {len(queries)/elapsed:.1f} queries/sec\")\nprocessor.shutdown()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nProcessed 100 queries in 0.489s\nThroughput: 204.6 queries/sec\n```\n:::\n:::\n\n\n:::{.callout-tip}\n## Parallel Processing Best Practices\n\n**Threading vs multiprocessing:**\n\n- Use threading for I/O-bound tasks (disk, network)\n- Use multiprocessing for CPU-bound tasks (avoids GIL)\n- Use GPU for massive parallelism (thousands of threads)\n\n**Batching strategies:**\n\n- Micro-batching (10-50ms window) for low latency\n- Macro-batching (1-10s window) for high throughput\n- Adaptive batching based on load\n\n**Load balancing:**\n\n- Round-robin for uniform replicas\n- Least-load for heterogeneous replicas\n- Latency-weighted for geographic distribution\n- Health checks to detect failed replicas\n\n**Scaling limits:**\n\n- CPU-bound: Scales to number of cores (8-96)\n- GPU-bound: Scales to GPU memory (80-640GB)\n- I/O-bound: Scales to disk/network bandwidth\n:::\n\n## Key Takeaways\n\n- **Exact search optimizations enable real-time search at million-vector scale**: SIMD vectorization, GPU acceleration, and batch processing provide 10-100× speedups over naive algorithms while maintaining zero approximation error\n\n- **ANN algorithms trade minimal accuracy for massive speedups**: IVF, HNSW, and product quantization achieve 95-99% recall at 100-1000× lower latency than exact search, enabling billion-vector indices with sub-millisecond response times\n\n- **GPU acceleration provides 10-100× speedup for vector operations**: Tensor Cores, FP16 precision, and batched matrix multiplication enable searching 100M+ vectors per second on a single A100 GPU\n\n- **Memory-mapped storage handles datasets exceeding RAM**: Operating system paging combined with tiered caching (hot vectors in RAM, cold on SSD) enables serving trillion-row indices on commodity hardware\n\n- **Parallel query processing achieves high throughput**: Thread pooling, request batching, and load balancing across replicas scale serving capacity to millions of queries per second\n\n- **Production systems combine multiple optimizations**: Successful deployments use ANN + GPU + memory mapping + parallel processing together, with each optimization addressing different bottlenecks\n\n- **The optimization hierarchy**: Algorithm choice (1000× impact) > Hardware acceleration (100× impact) > Parallelism (10× impact) > Implementation details (2× impact). Choose the right algorithm before micro-optimizing\n\n## Looking Ahead\n\nThis chapter covered computational optimizations for vector operations in isolation. @sec-data-engineering expands the view to data engineering for embeddings at scale: ETL pipelines that ingest and transform raw data for embedding generation, streaming systems for real-time embedding updates, data quality validation that ensures training stability, schema evolution strategies for backwards compatibility, and multi-source data fusion that combines embeddings across diverse datasets. These data engineering practices ensure embedding systems have the high-quality, well-structured data needed to reach their full potential.\n\n## Further Reading\n\n### Similarity Search Algorithms\n- Johnson, Jeff, et al. (2019). \"Billion-scale similarity search with GPUs.\" IEEE Transactions on Big Data.\n- Malkov, Yury, and D. A. Yashunin (2018). \"Efficient and Robust Approximate Nearest Neighbor Search Using Hierarchical Navigable Small World Graphs.\" IEEE TPAMI.\n- Jégou, Hervé, et al. (2011). \"Product Quantization for Nearest Neighbor Search.\" IEEE TPAMI.\n\n### Approximate Nearest Neighbors\n- Andoni, Alexandr, and Piotr Indyk (2006). \"Near-Optimal Hashing Algorithms for Approximate Nearest Neighbor in High Dimensions.\" FOCS.\n- Baranchuk, Dmitry, et al. (2019). \"Learning to Route in Similarity Graphs.\" ICML.\n- Aumüller, Martin, et al. (2020). \"ANN-Benchmarks: A Benchmarking Tool for Approximate Nearest Neighbor Algorithms.\" Information Systems.\n\n### GPU Acceleration\n- NVIDIA (2020). \"CUDA C++ Programming Guide.\"\n- Harris, Mark (2007). \"Optimizing Parallel Reduction in CUDA.\" NVIDIA Developer.\n- Sismanis, Nikos, et al. (2021). \"Efficient Large-Scale Approximate Nearest Neighbor Search on OpenCL FPGA.\" VLDB.\n\n### Memory Management\n- Boroumand, Amirali, et al. (2018). \"Google Workloads for Consumer Devices: Mitigating Data Movement Bottlenecks.\" ASPLOS.\n- Lim, Kevin, et al. (2009). \"Disaggregated Memory for Expansion and Sharing in Blade Servers.\" ISCA.\n\n### Vector Databases\n- Pinecone Documentation. \"Understanding Vector Databases.\"\n- Weaviate Documentation. \"Vector Indexing Algorithms.\"\n- Milvus Documentation. \"System Architecture.\"\n\n### High-Performance Computing\n- Hennessy, John, and David Patterson (2017). \"Computer Architecture: A Quantitative Approach.\" Morgan Kaufmann.\n- Sanders, Peter, and Kurt Mehlhorn (2019). \"Algorithms and Data Structures: The Basic Toolbox.\" Springer.\n\n",
    "supporting": [
      "ch22_high_performance_vector_ops_files/figure-epub"
    ],
    "filters": [],
    "engineDependencies": {
      "jupyter": [
        {
          "jsWidgets": false,
          "jupyterWidgets": false,
          "htmlLibraries": []
        }
      ]
    }
  }
}