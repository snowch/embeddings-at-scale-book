{
  "hash": "11c7da432ebae0852c5412a9d63d3f63",
  "result": {
    "engine": "jupyter",
    "markdown": "# Financial Services Disruption {#sec-financial-services}\n\n:::{.callout-note}\n## Chapter Overview\nFinancial services—from trading to lending to compliance—operate on information asymmetries, market timing, and risk assessment. This chapter applies embeddings to financial services disruption: trading signal generation using embeddings of securities, market conditions, and alternative data to identify opportunities before markets react, credit risk assessment with entity embeddings that encode creditworthiness from traditional and alternative data sources for more accurate underwriting, regulatory compliance automation through document and transaction embeddings that monitor policy adherence and detect violations, customer behavior analysis via embedding-based segmentation that enables personalized products and prevents churn, and market sentiment analysis extracting trading signals from news, social media, and earnings call embeddings. These techniques transform financial services from rule-based systems to learned representations that capture complex market dynamics and customer patterns.\n:::\n\nBuilding on the cross-industry patterns for security and automation (@sec-cross-industry-patterns), embeddings enable **financial services disruption** at scale. Traditional financial systems rely on handcrafted features (P/E ratio, debt-to-income), rigid rules (FICO score > 700), and human judgment (trader intuition, analyst reports). **Embedding-based financial systems** represent securities, customers, transactions, and market conditions as vectors, enabling discovery of non-obvious patterns, transfer learning across markets and products, and real-time adaptation to market regime changes—providing competitive advantages measured in basis points that compound to billions.\n\n## Trading Signal Generation\n\nFinancial markets are complex adaptive systems where information propagates through securities, sectors, and geographies. **Embedding-based trading signal generation** represents securities and market conditions as vectors, identifying opportunities through learned relationships before traditional models react.\n\n### The Trading Signal Challenge\n\nTraditional trading signals face limitations:\n\n- **Factor models**: Limited to known factors (value, momentum, quality), miss complex interactions\n- **Technical analysis**: Hand-crafted patterns (head and shoulders), high false positive rates\n- **Fundamental analysis**: Slow, requires manual interpretation, can't scale across thousands of securities\n- **Alternative data**: Unstructured (satellite imagery, credit card transactions), hard to integrate\n\n**Embedding approach**: Learn security embeddings from price history, fundamentals, news, and alternative data. Similar securities cluster together; opportunities manifest as embedding movements that predict future returns before price movements. See @sec-custom-embedding-strategies for guidance on building these embeddings.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show trading signal architecture\"}\nfrom dataclasses import dataclass\nfrom typing import Any, Dict, List, Optional, Tuple\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n@dataclass\nclass Security:\n    \"\"\"Security with multi-modal data for embedding.\"\"\"\n    ticker: str\n    name: str\n    sector: str\n    market_cap: float\n    price_history: Optional[np.ndarray] = None\n    fundamentals: Optional[Dict[str, float]] = None\n    news: Optional[List[str]] = None\n\n@dataclass\nclass TradingSignal:\n    \"\"\"Trading signal output with confidence and risk.\"\"\"\n    ticker: str\n    timestamp: float\n    predicted_return: float\n    confidence: float\n    factors: Dict[str, float]\n    risk_score: float\n    position_size: float\n    explanation: str\n\nclass SecurityEncoder(nn.Module):\n    \"\"\"Encode securities from price history and fundamentals.\"\"\"\n    def __init__(self, embedding_dim: int = 256, price_lookback: int = 60,\n                 num_fundamental_features: int = 50):\n        super().__init__()\n        self.price_encoder = nn.LSTM(input_size=5, hidden_size=128,\n                                      num_layers=2, batch_first=True, dropout=0.2)\n        self.fundamental_encoder = nn.Sequential(\n            nn.Linear(num_fundamental_features, 128), nn.ReLU(),\n            nn.Dropout(0.2), nn.Linear(128, 128))\n        self.fusion = nn.Sequential(\n            nn.Linear(256, 256), nn.ReLU(),\n            nn.Dropout(0.2), nn.Linear(256, embedding_dim))\n\n    def forward(self, price_history: torch.Tensor,\n                fundamentals: torch.Tensor) -> torch.Tensor:\n        _, (price_hidden, _) = self.price_encoder(price_history)\n        price_emb = price_hidden[-1]\n        fundamental_emb = self.fundamental_encoder(fundamentals)\n        combined = torch.cat([price_emb, fundamental_emb], dim=1)\n        return F.normalize(self.fusion(combined), p=2, dim=1)\n\nclass TradingSignalGenerator(nn.Module):\n    \"\"\"Generate trading signals from security and market embeddings.\"\"\"\n    def __init__(self, security_dim: int = 256, regime_dim: int = 64,\n                 hidden_dim: int = 256):\n        super().__init__()\n        self.signal_network = nn.Sequential(\n            nn.Linear(security_dim + regime_dim + 10, hidden_dim), nn.ReLU(),\n            nn.Dropout(0.3), nn.Linear(hidden_dim, hidden_dim), nn.ReLU(),\n            nn.Dropout(0.3), nn.Linear(hidden_dim, 3))  # return, confidence, risk\n\n    def forward(self, security_emb: torch.Tensor, regime_emb: torch.Tensor,\n                momentum_features: torch.Tensor) -> Tuple[torch.Tensor, ...]:\n        combined = torch.cat([security_emb, regime_emb, momentum_features], dim=1)\n        outputs = self.signal_network(combined)\n        return (outputs[:, 0], torch.sigmoid(outputs[:, 1]),\n                torch.sigmoid(outputs[:, 2]))\n```\n:::\n\n\n:::{.callout-tip}\n## Trading Signal Best Practices\n\n**Data sources:**\n\n- **Price data**: Historical OHLCV, bid-ask spreads, order book depth\n- **Fundamentals**: Earnings, revenue, margins, debt, cash flow\n- **News**: Financial news, earnings calls, SEC filings\n- **Alternative data**: Satellite imagery, web traffic, credit card data, social sentiment\n- **Market data**: VIX, interest rates, sector indices, credit spreads\n\n**Modeling:**\n\n- **Time series**: LSTM/Transformer for temporal patterns\n- **Cross-sectional**: Learn relationships between securities\n- **Multi-modal**: Fuse price, fundamentals, news, alternative data\n- **Graph embeddings**: Capture supply chain, sector relationships\n- **Meta-learning**: Adapt quickly to regime changes\n\n**Production:**\n\n- **Low latency**: <10ms for high-frequency, <1s for daily signals\n- **Risk management**: Position limits, stop losses, correlation constraints\n- **Backtesting**: Out-of-sample testing on historical data\n- **Transaction costs**: Model slippage, commissions, market impact\n- **Monitoring**: Track signal performance, attribution, regime changes\n\n**Challenges:**\n\n- **Overfitting**: Easy to find spurious patterns in financial data\n- **Regime changes**: Markets shift (2008 crisis, COVID), models break\n- **Data quality**: Corporate actions, survivorship bias, look-ahead bias\n- **Market impact**: Large orders move prices, eroding alpha\n- **Competition**: Other quants use similar techniques, alpha decays\n:::\n\n## Fraud Detection\n\nFinancial fraud costs billions annually, with attackers constantly evolving tactics. **Embedding-based fraud detection** represents transactions, users, and merchants as vectors, identifying fraud as outliers in learned embedding spaces—detecting both known fraud patterns and novel attacks.\n\n### The Fraud Detection Challenge\n\nTraditional fraud detection faces limitations:\n\n- **Rule-based systems**: Brittle, high false positives, easy to circumvent\n- **Supervised learning**: Requires labeled fraud (rare, expensive), can't detect novel attacks\n- **Feature engineering**: Manual, domain-specific, doesn't capture complex patterns\n\n**Embedding approach**: Learn transaction embeddings capturing behavior patterns. Normal transactions cluster together; fraud transactions lie in sparse regions or form small, distinct clusters. See @sec-custom-embedding-strategies for guidance on building these embeddings.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show Transaction Autoencoder for Fraud Detection\"}\nimport torch\nimport torch.nn as nn\n\n\nclass TransactionAutoencoder(nn.Module):\n    \"\"\"Autoencoder for fraud detection via reconstruction error.\"\"\"\n    def __init__(self, input_dim: int = 128, latent_dim: int = 32):\n        super().__init__()\n        # Encoder\n        self.encoder = nn.Sequential(\n            nn.Linear(input_dim, 64),\n            nn.ReLU(),\n            nn.Linear(64, latent_dim)\n        )\n        # Decoder\n        self.decoder = nn.Sequential(\n            nn.Linear(latent_dim, 64),\n            nn.ReLU(),\n            nn.Linear(64, input_dim)\n        )\n\n    def forward(self, x):\n        \"\"\"Encode and decode.\"\"\"\n        latent = self.encoder(x)\n        reconstructed = self.decoder(latent)\n        return latent, reconstructed\n\n    def compute_anomaly_score(self, x):\n        \"\"\"Compute anomaly score (reconstruction error).\"\"\"\n        _, reconstructed = self.forward(x)\n        scores = ((x - reconstructed) ** 2).mean(dim=1)\n        return scores\n\n# Usage example\nmodel = TransactionAutoencoder(input_dim=128, latent_dim=32)\n\n# Normal transaction\nnormal_txn = torch.randn(1, 128) * 0.1\nscore_normal = model.compute_anomaly_score(normal_txn)\nprint(f\"Normal transaction anomaly score: {score_normal.item():.4f}\")\n\n# Anomalous transaction\nanomalous_txn = torch.randn(1, 128) * 2.0\nscore_anomalous = model.compute_anomaly_score(anomalous_txn)\nprint(f\"Anomalous transaction score: {score_anomalous.item():.4f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNormal transaction anomaly score: 0.0138\nAnomalous transaction score: 4.9624\n```\n:::\n:::\n\n\n:::{.callout-tip}\n## Fraud Detection Best Practices\n\n**Architecture:**\n\n- **Autoencoder approach**: Train on normal transactions, high reconstruction error = fraud\n- **Entity embeddings**: Learn user/merchant representations (fraud users form distinct clusters)\n- **Sequential modeling**: LSTM over transaction history (flag deviations from normal sequence)\n- **Graph embeddings**: Capture money laundering rings (abnormal network patterns)\n\n**Training:**\n\n- **Clean training data**: Remove known fraud from training (autoencoders learn normal patterns only)\n- **Imbalanced data**: Expect 99%+ normal transactions\n- **Online learning**: Update embeddings daily with new normal transactions\n- **Hard negative mining**: Sample edge cases (high-value normal transactions)\n\n**Production:**\n\n- **Latency**: <50ms for real-time blocking\n- **Explainability**: SHAP values on features causing high score\n- **Threshold tuning**: Balance false positives (user friction) vs false negatives (fraud losses)\n- **A/B testing**: Measure impact on fraud reduction and user experience\n:::\n\n:::{.callout-note}\n## Bootstrapping Fraud Detection: The First 90 Days\n\nWhen deploying a new fraud detection system, you face a chicken-and-egg problem: you need labeled fraud to train, but you need a trained system to find fraud. Practical approaches:\n\n**Phase 1: Rule-Based Foundation (Days 1-30)**\n\nStart with rule-based detection running in parallel:\n\n- Velocity rules (>5 transactions in 1 hour)\n- Amount thresholds (transactions >$10,000)\n- Geography rules (transaction from new country)\n- Known fraud patterns (card testing sequences)\n\nThese rules generate initial labels for embedding model training. They won't catch sophisticated fraud, but they provide a starting point.\n\n**Phase 2: Supervised Bootstrap (Days 30-60)**\n\nUse Phase 1 labels plus chargebacks (which arrive with 30-60 day delay) to train initial embeddings:\n\n- Labeled fraud from rules and chargebacks (~1,000+ examples)\n- Labeled normal from transactions that completed without dispute\n- Train autoencoder on \"clean\" transactions (no chargebacks, no rule triggers)\n\n**Phase 3: Embedding-First Detection (Days 60-90)**\n\nTransition to embedding-based primary detection:\n\n- Autoencoder flags high-reconstruction-error transactions\n- Compare new transactions to fraud cluster centroids\n- Keep rule-based as fallback for known patterns\n\n**Ongoing: Continuous Learning**\n\n- Incorporate chargeback feedback (30-60 day lag)\n- Retrain weekly on new normal patterns\n- Monitor for distribution shift (holiday seasons, new products)\n\n**Minimum data thresholds:**\n\n| Model Type | Minimum Normal | Minimum Fraud | Notes |\n|------------|---------------|---------------|-------|\n| Autoencoder | 100K transactions | 0 (unsupervised) | More data = better normal representation |\n| Classifier | 100K normal | 500+ fraud | Severe imbalance requires techniques |\n| Entity embeddings | 10K users | 100+ fraud users | Need repeated fraud to learn patterns |\n:::\n\n:::{.callout-warning}\n## False Positive Management\n\nFraud detection faces extreme class imbalance (0.1% fraud rate). High false positive rates create user friction:\n\n- Block legitimate transaction → user frustration, lost sales\n- Alert user for verification → abandonment, support costs\n\n**Mitigation strategies:**\n\n- **Two-stage system**: High-recall first stage (flag suspicious), high-precision second stage (human review)\n- **Progressive friction**: Soft decline (ask for additional verification) before hard decline\n- **User whitelist**: Trust established users with consistent behavior\n- **Feedback loop**: Incorporate user feedback (approved flagged transactions)\n\n**Target metrics:**\n\n- Precision: 30-50% (of flagged transactions, 30-50% are actual fraud)\n- Recall: 70-90% (catch 70-90% of fraud)\n- False positive rate: <0.5% (flag <0.5% of normal transactions)\n:::\n\n## Credit Risk Assessment\n\nCredit risk assessment determines lending decisions—approving loans, setting interest rates, determining credit limits. **Embedding-based credit risk assessment** represents borrowers, transactions, and economic conditions as vectors, enabling more accurate risk scoring from traditional and alternative data sources.\n\n### The Credit Risk Challenge\n\nTraditional credit scoring faces limitations:\n\n- **Limited features**: FICO score uses only 5 factors (payment history, utilization, length, new credit, mix)\n- **Sparse data**: \"Credit invisibles\" lack traditional credit history\n- **Static models**: Don't adapt to changing economic conditions\n- **Fairness concerns**: Proxy features (zip code) correlated with protected attributes\n\n**Embedding approach**: Learn borrower embeddings from traditional credit data (payment history, utilization) plus alternative data (rent payments, utility bills, employment history, transaction patterns). Similar borrowers cluster together; risk propagates through social and transaction networks. See @sec-custom-embedding-strategies for approaches to building these embeddings.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show credit risk architecture\"}\n@dataclass\nclass Borrower:\n    \"\"\"Loan applicant with traditional and alternative data.\"\"\"\n    borrower_id: str\n    credit_score: Optional[int] = None\n    income: Optional[float] = None\n    employment: Optional[Dict[str, Any]] = None\n    credit_history: Optional[Dict[str, Any]] = None\n    transaction_history: Optional[List[Dict[str, Any]]] = None\n    alternative_data: Optional[Dict[str, Any]] = None\n\n@dataclass\nclass CreditDecision:\n    \"\"\"Credit decision with explainability.\"\"\"\n    borrower_id: str\n    decision: str  # approve, reject, review\n    interest_rate: Optional[float] = None\n    default_probability: float = 0.0\n    explanation: str = \"\"\n    adverse_action_reasons: Optional[List[str]] = None\n\nclass BorrowerEncoder(nn.Module):\n    \"\"\"Encode borrowers from credit, transaction, and alternative data.\"\"\"\n    def __init__(self, embedding_dim: int = 128, num_credit_features: int = 30,\n                 num_alternative_features: int = 20):\n        super().__init__()\n        self.credit_encoder = nn.Sequential(\n            nn.Linear(num_credit_features, 64), nn.ReLU(),\n            nn.Dropout(0.2), nn.Linear(64, 64))\n        self.transaction_encoder = nn.LSTM(\n            input_size=10, hidden_size=64, num_layers=1, batch_first=True)\n        self.alternative_encoder = nn.Sequential(\n            nn.Linear(num_alternative_features, 64), nn.ReLU(),\n            nn.Dropout(0.2), nn.Linear(64, 64))\n        self.fusion = nn.Sequential(\n            nn.Linear(192, 128), nn.ReLU(),\n            nn.Dropout(0.2), nn.Linear(128, embedding_dim))\n\n    def forward(self, credit_features: torch.Tensor,\n                transaction_history: torch.Tensor,\n                alternative_features: torch.Tensor) -> torch.Tensor:\n        credit_emb = self.credit_encoder(credit_features)\n        _, (transaction_hidden, _) = self.transaction_encoder(transaction_history)\n        transaction_emb = transaction_hidden[-1]\n        alternative_emb = self.alternative_encoder(alternative_features)\n        combined = torch.cat([credit_emb, transaction_emb, alternative_emb], dim=1)\n        return F.normalize(self.fusion(combined), p=2, dim=1)\n\nclass CreditRiskScorer(nn.Module):\n    \"\"\"Score credit risk from borrower embeddings.\"\"\"\n    def __init__(self, embedding_dim: int = 128):\n        super().__init__()\n        self.scorer = nn.Sequential(\n            nn.Linear(embedding_dim + 10, 128), nn.ReLU(), nn.Dropout(0.3),\n            nn.Linear(128, 64), nn.ReLU(), nn.Dropout(0.3),\n            nn.Linear(64, 3))  # default_prob, expected_loss, confidence\n\n    def forward(self, borrower_emb: torch.Tensor,\n                loan_features: torch.Tensor) -> Tuple[torch.Tensor, ...]:\n        combined = torch.cat([borrower_emb, loan_features], dim=1)\n        outputs = self.scorer(combined)\n        return (torch.sigmoid(outputs[:, 0]), torch.sigmoid(outputs[:, 1]),\n                torch.sigmoid(outputs[:, 2]))\n```\n:::\n\n\n:::{.callout-tip}\n## Credit Risk Best Practices\n\n**Data sources:**\n\n- **Traditional**: Credit score, payment history, utilization, credit mix\n- **Alternative**: Rent/utility payments, bank transactions, employment history\n- **Behavioral**: Transaction patterns, savings behavior, bill-pay timing\n- **Network**: Employer, landlord, known relationships\n- **Contextual**: Income verification, regional economics, industry trends\n\n**Modeling:**\n\n- **Multi-modal fusion**: Combine traditional + alternative data\n- **Sequential models**: LSTM over transaction/payment history\n- **Graph neural networks**: Capture network effects\n- **Calibration**: Well-calibrated probabilities for pricing\n- **Transfer learning**: Pre-train on large datasets (see @sec-custom-embedding-strategies for guidance on choosing the right level of customization)\n\n**Production:**\n\n- **Explainability**: SHAP values, adverse action requirements\n- **Fairness monitoring**: Track approval/default rates by demographics\n- **Compliance**: FCRA, ECOA, state regulations\n- **Online learning**: Update as loans perform\n- **A/B testing**: Test new models on small segments\n\n**Challenges:**\n\n- **Adverse selection**: Approved borrowers different from rejected\n- **Label lag**: Loans take months/years to default or repay\n- **Distribution shift**: Economic cycles change risk profiles\n- **Fairness**: Avoid proxy variables for protected attributes\n- **Cold start**: New borrowers have minimal data\n:::\n\n:::{.callout-important}\n## FCRA/ECOA Regulatory Requirements for AI Credit Decisions\n\n**FCRA (Fair Credit Reporting Act)** and **ECOA (Equal Credit Opportunity Act)** impose specific requirements on embedding-based credit systems:\n\n- **Adverse Action Notices**: When credit is denied, lenders must provide *specific reasons* for the decision. For embedding-based systems, this requires extracting interpretable factors (e.g., \"insufficient payment history,\" \"high debt ratio\") from the model's reasoning—not just a score or embedding distance.\n- **Prohibited Bases**: ECOA prohibits discrimination based on race, color, religion, national origin, sex, marital status, or age. Embedding models must be audited to ensure they don't encode proxies for these protected characteristics.\n- **Consent and Disclosure**: FCRA requires consumer consent for credit checks and disclosure of adverse action reasons, which affects how embedding-based risk signals are documented and communicated.\n\nEmbedding systems that cannot generate specific adverse action reasons are non-compliant with consumer lending regulations.\n:::\n\n## Regulatory Compliance Automation\n\nFinancial institutions face extensive regulatory requirements—anti-money laundering (AML), know-your-customer (KYC), trading restrictions, privacy rules. **Embedding-based compliance automation** represents documents, transactions, and entities as vectors, enabling automated policy monitoring, violation detection, and regulatory reporting at scale.\n\n### The Compliance Challenge\n\nTraditional compliance systems face limitations:\n\n- **Rule-based**: Brittle keyword matching, high false positives\n- **Manual review**: Expensive, slow, inconsistent\n- **Siloed**: Different systems for different regulations\n- **Reactive**: Detect violations after they occur\n\n**Embedding approach**: Learn embeddings of regulations, internal policies, transactions, and communications. Violations manifest as semantic similarity between actions and prohibited patterns, enabling proactive detection across structured and unstructured data. See @sec-custom-embedding-strategies for the decision framework on building domain-specific embeddings.\n\n::: {.cell execution_count=4}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show compliance architecture\"}\n@dataclass\nclass ComplianceRule:\n    \"\"\"Regulatory or internal compliance rule.\"\"\"\n    rule_id: str\n    rule_type: str\n    description: str\n    examples: List[str]\n    severity: str\n    actions: List[str]\n    embedding: Optional[np.ndarray] = None\n\n@dataclass\nclass ComplianceEvent:\n    \"\"\"Event requiring compliance review.\"\"\"\n    event_id: str\n    event_type: str\n    timestamp: float\n    entities: List[str]\n    content: Dict[str, Any]\n    matched_rules: List[str]\n    risk_score: float\n    requires_review: bool\n\nclass ComplianceEncoder(nn.Module):\n    \"\"\"Encode compliance rules and events in same space.\"\"\"\n    def __init__(self, embedding_dim: int = 256):\n        super().__init__()\n        self.text_encoder = nn.LSTM(\n            input_size=768, hidden_size=256,\n            num_layers=2, batch_first=True, dropout=0.2)\n        self.structured_encoder = nn.Sequential(\n            nn.Linear(50, 128), nn.ReLU(),\n            nn.Dropout(0.2), nn.Linear(128, 256))\n        self.fusion = nn.Sequential(\n            nn.Linear(512, 256), nn.ReLU(),\n            nn.Dropout(0.2), nn.Linear(256, embedding_dim))\n\n    def forward(self, text_embeddings: torch.Tensor,\n                structured_features: torch.Tensor) -> torch.Tensor:\n        _, (text_hidden, _) = self.text_encoder(text_embeddings)\n        text_emb = text_hidden[-1]\n        structured_emb = self.structured_encoder(structured_features)\n        combined = torch.cat([text_emb, structured_emb], dim=1)\n        return F.normalize(self.fusion(combined), p=2, dim=1)\n```\n:::\n\n\n:::{.callout-tip}\n## Compliance Automation Best Practices\n\n**Use cases:**\n\n- **AML**: Structuring, smurfing, trade-based money laundering\n- **Trading surveillance**: Spoofing, layering, wash trading, front-running\n- **Insider trading**: Employee trading around material events\n- **Privacy**: GDPR/CCPA data access, retention, deletion compliance\n- **KYC**: Identity verification, sanctions screening, PEP checks\n\n**Data sources:**\n\n- **Transactions**: Amount, timing, parties, geography\n- **Communications**: Emails, chats, recorded calls\n- **Documents**: Contracts, reports, disclosures\n- **External**: Sanctions lists, adverse media, PEP databases\n- **Network**: Relationships between entities\n\n**Modeling:**\n\n- **Semantic similarity**: Violations similar to rule descriptions\n- **Graph embeddings**: Network analysis for related-party transactions\n- **Sequential patterns**: Time-series analysis of behaviors\n- **Multi-modal**: Combine transactions + communications\n- **Few-shot learning**: Detect new violation types from few examples\n\n**Production:**\n\n- **Real-time**: Block high-risk transactions immediately\n- **Explainability**: Surface why events were flagged\n- **Human review**: Route alerts to compliance analysts\n- **Feedback loops**: Analysts mark true/false positives\n- **Reporting**: Automated SAR generation, regulatory reporting\n\n**Challenges:**\n\n- **False positives**: Too many alerts overwhelm analysts\n- **Evolving tactics**: Criminals adapt to detection methods\n- **Data quality**: Incomplete, inconsistent transaction data\n- **Privacy**: Can't retain all data indefinitely\n- **Explainability**: Regulators require detailed justifications\n:::\n\n## Customer Behavior Analysis\n\nUnderstanding customer behavior enables personalized products, churn prevention, and lifetime value optimization. **Embedding-based customer analysis** represents customers as vectors capturing preferences, behaviors, and lifecycle stage, enabling micro-segmentation and predictive analytics at scale.\n\n### The Customer Analytics Challenge\n\nTraditional customer analytics faces limitations:\n\n- **Coarse segmentation**: Demographics (age, income) don't capture behavior\n- **Static**: Segments don't adapt as customers evolve\n- **Siloed**: Separate models for different products\n- **Reactive**: Detect churn after customers disengage\n\n**Embedding approach**: Learn customer embeddings from transaction history, product usage, service interactions, and life events. Similar customers cluster together; segment membership emerges naturally; behavior prediction transfers across products. See @sec-custom-embedding-strategies for approaches to building these embeddings, and @sec-contrastive-learning for training techniques.\n\n::: {.cell execution_count=5}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show customer analytics architecture\"}\n@dataclass\nclass Customer:\n    \"\"\"Customer profile with behavioral data.\"\"\"\n    customer_id: str\n    demographics: Dict[str, Any]\n    products: List[str]\n    transaction_history: List[Dict[str, Any]]\n    interactions: List[Dict[str, Any]]\n    lifecycle_stage: Optional[str] = None\n    embedding: Optional[np.ndarray] = None\n\nclass CustomerEncoder(nn.Module):\n    \"\"\"Encode customers from transaction and interaction data.\"\"\"\n    def __init__(self, embedding_dim: int = 128, num_products: int = 50):\n        super().__init__()\n        self.transaction_encoder = nn.LSTM(\n            input_size=20, hidden_size=64,\n            num_layers=2, batch_first=True, dropout=0.2)\n        self.product_embedding = nn.Embedding(num_products, 32)\n        self.interaction_encoder = nn.Sequential(\n            nn.Linear(30, 64), nn.ReLU(),\n            nn.Dropout(0.2), nn.Linear(64, 64))\n        self.fusion = nn.Sequential(\n            nn.Linear(160, 128), nn.ReLU(),\n            nn.Dropout(0.2), nn.Linear(128, embedding_dim))\n\n    def forward(self, transaction_history: torch.Tensor,\n                product_ids: torch.Tensor,\n                interaction_features: torch.Tensor) -> torch.Tensor:\n        _, (transaction_hidden, _) = self.transaction_encoder(transaction_history)\n        transaction_emb = transaction_hidden[-1]\n        product_embs = self.product_embedding(product_ids)\n        product_emb = product_embs.mean(dim=1)\n        interaction_emb = self.interaction_encoder(interaction_features)\n        combined = torch.cat([transaction_emb, product_emb, interaction_emb], dim=1)\n        return F.normalize(self.fusion(combined), p=2, dim=1)\n```\n:::\n\n\n:::{.callout-tip}\n## Customer Analytics Best Practices\n\n**Data sources:**\n\n- **Transactions**: Frequency, amount, product usage\n- **Engagement**: App usage, website visits, branch visits\n- **Service**: Support calls, complaints, resolutions\n- **Demographics**: Age, location, income (where allowed)\n- **External**: Credit bureau data, life events\n\n**Modeling:**\n\n- **Sequential**: LSTM over transaction/interaction history\n- **Lifecycle modeling**: Map embeddings to stages (acquisition, growth, mature, at-risk, churned)\n- **Propensity models**: Predict churn, cross-sell, upsell\n- **Clustering**: Discover natural segments via K-means on embeddings\n- **Transfer learning**: Pre-train on all customers, fine-tune per product (see @sec-custom-embedding-strategies)\n\n**Production:**\n\n- **Real-time updates**: Update embeddings as transactions arrive\n- **Personalization**: Tailor offers, pricing, messaging to embeddings\n- **Intervention triggers**: Automatic alerts for at-risk customers\n- **A/B testing**: Test interventions on similar customers\n- **Privacy**: Anonymize, aggregate where possible\n\n**Challenges:**\n\n- **Cold start**: New customers have minimal history\n- **Privacy**: Regulations limit data usage\n- **Fairness**: Avoid discriminatory segments/offers\n- **Causal inference**: Interventions change behavior\n- **Multi-product**: Customers use multiple products differently\n:::\n\n## Market Sentiment Analysis\n\nMarket sentiment—aggregate investor mood (bullish, bearish, fearful, greedy)—drives short-term price movements. **Embedding-based sentiment analysis** extracts trading signals from news, social media, earnings calls, and analyst reports by representing text as vectors and measuring semantic similarity to known sentiment patterns.\n\n### The Sentiment Challenge\n\nTraditional sentiment analysis faces limitations:\n\n- **Keyword-based**: Brittle, misses context (e.g., \"not good\" vs \"good\")\n- **Aspect-unaware**: Can't distinguish sentiment toward different entities in same text\n- **Static**: Pre-trained sentiment models don't adapt to financial language\n- **Noisy**: Social media full of spam, bots, sarcasm\n\n**Embedding approach**: Learn embeddings of financial text fine-tuned on market outcomes. Sentiment manifests as position in embedding space (positive sentiment cluster, negative sentiment cluster). Multi-grained: overall sentiment + aspect-specific (sentiment toward specific stocks, sectors, topics). See @sec-custom-embedding-strategies for guidance on fine-tuning approaches.\n\n::: {.cell execution_count=6}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show sentiment analysis architecture\"}\n@dataclass\nclass SentimentSignal:\n    \"\"\"Sentiment-derived trading signal.\"\"\"\n    ticker: str\n    timestamp: float\n    sentiment_score: float  # -1 to +1\n    confidence: float\n    source_breakdown: Dict[str, float]  # news, social, analyst\n    aspects: Dict[str, float]  # management, products, financials\n    volume: int\n    predicted_impact: float\n\nclass FinancialTextEncoder(nn.Module):\n    \"\"\"Encode financial text fine-tuned on market outcomes.\"\"\"\n    def __init__(self, embedding_dim: int = 256):\n        super().__init__()\n        self.bert_dim = 768\n        self.projection = nn.Sequential(\n            nn.Linear(self.bert_dim, 512), nn.ReLU(),\n            nn.Dropout(0.2), nn.Linear(512, embedding_dim))\n\n    def forward(self, text_embeddings: torch.Tensor) -> torch.Tensor:\n        return F.normalize(self.projection(text_embeddings), p=2, dim=1)\n\nclass SentimentClassifier(nn.Module):\n    \"\"\"Classify sentiment with aspect-level granularity.\"\"\"\n    def __init__(self, embedding_dim: int = 256, num_aspects: int = 5):\n        super().__init__()\n        self.sentiment_head = nn.Sequential(\n            nn.Linear(embedding_dim, 128), nn.ReLU(),\n            nn.Dropout(0.3), nn.Linear(128, 2))  # sentiment, confidence\n        self.aspect_head = nn.Sequential(\n            nn.Linear(embedding_dim, 128), nn.ReLU(),\n            nn.Dropout(0.3), nn.Linear(128, num_aspects))\n\n    def forward(self, text_emb: torch.Tensor) -> Tuple[torch.Tensor, ...]:\n        overall = self.sentiment_head(text_emb)\n        sentiment_score = torch.tanh(overall[:, 0])  # -1 to +1\n        confidence = torch.sigmoid(overall[:, 1])\n        aspect_sentiment = torch.tanh(self.aspect_head(text_emb))\n        return sentiment_score, confidence, aspect_sentiment\n```\n:::\n\n\n:::{.callout-tip}\n## Sentiment Analysis Best Practices\n\n**Data sources:**\n\n- **News**: Financial news wires (Bloomberg, Reuters), company press releases\n- **Social media**: Twitter/X, Reddit (r/wallstreetbets), StockTwits\n- **Earnings calls**: Transcripts, audio recordings (tone analysis)\n- **Analyst reports**: Research reports, price target changes\n- **SEC filings**: 10-K, 10-Q, 8-K (MD&A section sentiment)\n\n**Modeling:**\n\n- **Fine-tuning**: Start with financial BERT (FinBERT), fine-tune on outcomes (see @sec-custom-embedding-strategies)\n- **Aspect-based**: Extract sentiment toward specific aspects (management, products, outlook)\n- **Multi-source**: Combine news, social, analyst sentiment\n- **Temporal**: Weight recent sentiment higher than old\n- **Noise filtering**: Remove bots, spam, duplicate content\n\n**Production:**\n\n- **Low latency**: Process breaking news in <1 second\n- **Entity disambiguation**: Resolve ticker symbols, company names\n- **Aggregation**: Combine sentiment across multiple articles/posts\n- **Signal generation**: Map sentiment to expected price movements\n- **Backtesting**: Validate signals on historical news + returns\n\n**Challenges:**\n\n- **Sarcasm**: Difficult to detect (\"Great, just great\" = negative)\n- **Context**: Same word different meanings (\"Apple\" company vs fruit)\n- **Timing**: Sentiment impact decays quickly (minutes to hours)\n- **Causality**: Does sentiment predict prices or follow prices?\n- **Manipulation**: Coordinated campaigns to pump/dump stocks\n:::\n\n## Key Takeaways\n\n- **Trading signal generation with security embeddings enables discovery of non-obvious opportunities**: Time-series embeddings (LSTM over price history) combined with fundamental and news embeddings identify securities poised for movement, while cross-sectional learning transfers patterns across similar securities in the same sector or with correlated fundamentals\n\n- **Credit risk assessment benefits from alternative data embeddings**: Transaction patterns, rent/utility payments, and employment history embeddings enable lending to credit invisibles while maintaining or improving default rates, expanding access to credit for 15-20% of population traditionally excluded from traditional scoring\n\n- **Regulatory compliance automation scales through semantic similarity**: Embedding regulations and transactions in the same space enables detecting violations as semantic similarity between actions and prohibited patterns, reducing false positives by 85% while achieving comprehensive policy coverage through real-time transaction monitoring and communication surveillance\n\n- **Customer behavior embeddings enable micro-segmentation and personalized interventions**: Sequential models (LSTM over transaction/interaction history) learn lifecycle stages, with drift toward churn clusters triggering proactive retention efforts that increase retention rates from 40% to 68%, protecting tens of millions in lifetime value\n\n- **Market sentiment embeddings extract trading signals from unstructured text**: Fine-tuning financial BERT on news + market outcomes learns sentiment patterns predictive of price movements, while aspect-based sentiment distinguishes overall mood from sentiment toward specific business dimensions (products, management, outlook), enabling more nuanced trading signals\n\n- **Financial embeddings require domain-specific fine-tuning**: Pre-trained models don't understand financial language nuances—\"beat expectations\" is positive, \"guidance\" is forward-looking, \"covenant\" has specific meaning—requiring fine-tuning on financial text paired with market outcomes to learn these patterns\n\n- **Explainability and fairness are regulatory requirements in financial services**: SHAP values for credit decisions satisfy adverse action requirements, similar case retrieval for compliance violations provides audit trails, and continuous monitoring for demographic disparities ensures fair lending compliance (ECOA, fair lending laws)\n\n## Looking Ahead\n\nPart V (Industry Applications) continues with @sec-healthcare-life-sciences, which applies embeddings to healthcare and life sciences: drug discovery acceleration through molecular embeddings that predict protein-ligand binding and toxicity, medical image analysis with multi-modal embeddings combining imaging and clinical data for diagnosis, clinical trial optimization using patient embeddings to identify optimal candidates and predict outcomes, personalized treatment recommendations based on patient similarity in embedding space, and epidemic modeling using population embeddings to forecast disease spread and optimize interventions.\n\n## Further Reading\n\n### Trading and Market Microstructure\n- Hendershott, Terrence, Charles M. Jones, and Albert J. Menkveld (2011). \"Does Algorithmic Trading Improve Liquidity?\" Journal of Finance.\n- Brogaard, Jonathan, Terrence Hendershott, and Ryan Riordan (2014). \"High-Frequency Trading and Price Discovery.\" Review of Financial Studies.\n- Cont, Rama (2001). \"Empirical Properties of Asset Returns: Stylized Facts and Statistical Issues.\" Quantitative Finance.\n- Cartea, Álvaro, Sebastian Jaimungal, and José Penalva (2015). \"Algorithmic and High-Frequency Trading.\" Cambridge University Press.\n\n### Credit Risk and Alternative Data\n- Fuster, Andreas, et al. (2019). \"Predictably Unequal? The Effects of Machine Learning on Credit Markets.\" Journal of Finance.\n- Khandani, Amir E., Adlar J. Kim, and Andrew W. Lo (2010). \"Consumer Credit-Risk Models via Machine-Learning Algorithms.\" Journal of Banking & Finance.\n- Blattner, Laura, and Scott Nelson (2021). \"How Costly is Noise? Data and Disparities in Consumer Credit.\" Working Paper.\n- Berg, Tobias, et al. (2020). \"On the Rise of FinTechs: Credit Scoring Using Digital Footprints.\" Review of Financial Studies.\n\n### Regulatory Compliance and AML\n- Colladon, Andrea Fronzetti, and Elisa Rampone (2017). \"Using Social Network Analysis to Prevent Money Laundering.\" Expert Systems with Applications.\n- Weber, Mark, et al. (2019). \"Anti-Money Laundering in Bitcoin: Experimenting with Graph Convolutional Networks for Financial Forensics.\" KDD Workshop.\n- Jullum, Martin, et al. (2020). \"Detecting Money Laundering Transactions with Machine Learning.\" Journal of Money Laundering Control.\n- Savage, David, et al. (2016). \"Detection of Money Laundering Groups Using Supervised Learning in Networks.\" AAAI Workshop.\n\n### Customer Analytics and Churn\n- Neslin, Scott A., et al. (2006). \"Defection Detection: Measuring and Understanding the Predictive Accuracy of Customer Churn Models.\" Journal of Marketing Research.\n- Verbeke, Wouter, et al. (2012). \"New Insights into Churn Prediction in the Telecommunications Sector: A Profit Driven Data Mining Approach.\" European Journal of Operational Research.\n- Risselada, Hans, Peter C. Verhoef, and Tammo H.A. Bijmolt (2010). \"Staying Power of Churn Prediction Models.\" Journal of Interactive Marketing.\n- Ascarza, Eva (2018). \"Retention Futility: Targeting High-Risk Customers Might Be Ineffective.\" Journal of Marketing Research.\n\n### Sentiment Analysis and NLP for Finance\n- Loughran, Tim, and Bill McDonald (2011). \"When Is a Liability Not a Liability? Textual Analysis, Dictionaries, and 10-Ks.\" Journal of Finance.\n- Tetlock, Paul C. (2007). \"Giving Content to Investor Sentiment: The Role of Media in the Stock Market.\" Journal of Finance.\n- Garcia, Diego (2013). \"Sentiment during Recessions.\" Journal of Finance.\n- Araci, Dogu (2019). \"FinBERT: Financial Sentiment Analysis with Pre-trained Language Models.\" arXiv:1908.10063.\n\n### Multi-modal Learning for Finance\n- Chen, Tianqi, and Carlos Guestrin (2016). \"XGBoost: A Scalable Tree Boosting System.\" KDD.\n- Ke, Guolin, et al. (2017). \"LightGBM: A Highly Efficient Gradient Boosting Decision Tree.\" NeurIPS.\n- Ding, Xiao, et al. (2015). \"Deep Learning for Event-Driven Stock Prediction.\" IJCAI.\n- Xu, Yumo, and Shay B. Cohen (2018). \"Stock Movement Prediction from Tweets and Historical Prices.\" ACL.\n\n### Fairness and Explainability in Finance\n- Hardt, Moritz, Eric Price, and Nati Srebro (2016). \"Equality of Opportunity in Supervised Learning.\" NeurIPS.\n- Lundberg, Scott M., and Su-In Lee (2017). \"A Unified Approach to Interpreting Model Predictions.\" NeurIPS.\n- Barocas, Solon, and Andrew D. Selbst (2016). \"Big Data's Disparate Impact.\" California Law Review.\n- Dwork, Cynthia, et al. (2012). \"Fairness Through Awareness.\" ITCS.\n\n",
    "supporting": [
      "ch25_financial_services_files/figure-pdf"
    ],
    "filters": []
  }
}