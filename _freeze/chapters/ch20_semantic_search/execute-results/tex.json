{
  "hash": "bb332fb7ad3a50d86beec375b6457baa",
  "result": {
    "engine": "jupyter",
    "markdown": "# Semantic Search Beyond Text {#sec-semantic-search}\n\n:::{.callout-note}\n## Chapter Overview\nSemantic search transcends traditional keyword matching, enabling organizations to find meaning across modalities: images, code, scientific papers, media assets, and interconnected knowledge. This chapter explores multi-modal semantic search architectures that unify text, vision, and audio embeddings for cross-modal retrieval, code search systems that understand program semantics beyond syntax for software intelligence, scientific literature and patent search at research scale with citation networks and entity resolution, media and content discovery engines that match visual style and creative intent, and enterprise knowledge graphs that connect entities through learned embeddings. These capabilities transform search from keyword matching to semantic understanding, unlocking insights hidden in unstructured data across modalities.\n:::\n\nAfter mastering RAG for text (@sec-rag-at-scale), the next frontier is **semantic search beyond text**. Traditional search operates on keywords: match query terms to document terms, rank by term frequency. This works for text but fails for images (no keywords), code (syntax vs semantics), scientific literature (citation networks matter), media (style and composition), and knowledge graphs (relationships matter more than attributes). **Embedding-based semantic search** solves these challenges by representing all modalities—text, images, code, papers, media, entities—in a unified vector space where similarity reflects semantic meaning, not surface features.\n\n## Multi-Modal Semantic Search\n\nMulti-modal search finds content across different modalities: search images with text queries (\"sunset over mountains\"), search text with image queries (upload photo, find similar articles), search videos with audio queries (hum a melody, find the song). **Multi-modal embeddings** map different modalities into a shared vector space where cross-modal similarity is meaningful.\n\n### The Multi-Modal Challenge\n\nEach modality has unique characteristics:\n\n- **Text**: Sequential, compositional, high-dimensional vocabulary\n- **Images**: Spatial, hierarchical features (pixels → edges → objects)\n- **Audio**: Temporal, frequency-based, variable length\n- **Video**: Spatial-temporal, combines images + audio + text (captions)\n\n**Challenge**: Map these heterogeneous modalities into a unified space where \"cat\" (text) is near cat images (vision) and \"meow\" sounds (audio).\n\n::: {.cell execution_count=1}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show Multi-Modal Encoder\"}\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass MultiModalEncoder:\n    \"\"\"Encode text, images, and audio into shared embedding space.\"\"\"\n    def __init__(self, text_encoder, image_encoder, audio_encoder, embedding_dim=512):\n        self.text_encoder = text_encoder\n        self.image_encoder = image_encoder\n        self.audio_encoder = audio_encoder\n        self.embedding_dim = embedding_dim\n\n    def encode_text(self, text):\n        \"\"\"Encode text to shared embedding space.\"\"\"\n        text_features = self.text_encoder(text)\n        return F.normalize(text_features, p=2, dim=-1)\n\n    def encode_image(self, image):\n        \"\"\"Encode image to shared embedding space.\"\"\"\n        image_features = self.image_encoder(image)\n        return F.normalize(image_features, p=2, dim=-1)\n\n    def encode_audio(self, audio):\n        \"\"\"Encode audio to shared embedding space.\"\"\"\n        audio_features = self.audio_encoder(audio)\n        return F.normalize(audio_features, p=2, dim=-1)\n\n    def cross_modal_search(self, query, query_modality, candidates, candidate_modality):\n        \"\"\"Search across modalities (e.g., text query finds images).\"\"\"\n        # Encode query\n        if query_modality == 'text':\n            query_emb = self.encode_text(query)\n        elif query_modality == 'image':\n            query_emb = self.encode_image(query)\n        else:\n            query_emb = self.encode_audio(query)\n\n        # Encode candidates\n        if candidate_modality == 'text':\n            candidate_embs = torch.stack([self.encode_text(c) for c in candidates])\n        elif candidate_modality == 'image':\n            candidate_embs = torch.stack([self.encode_image(c) for c in candidates])\n        else:\n            candidate_embs = torch.stack([self.encode_audio(c) for c in candidates])\n\n        # Compute similarities\n        similarities = torch.matmul(candidate_embs, query_emb.T)\n        return similarities\n\n# Usage example (in production, provide actual encoder models)\nencoder = MultiModalEncoder(\n    text_encoder=nn.Linear(768, 512),  # Placeholder for text encoder\n    image_encoder=nn.Linear(2048, 512),  # Placeholder for image encoder\n    audio_encoder=nn.Linear(128, 512)  # Placeholder for audio encoder\n)\nprint(f\"Multi-modal encoder with {encoder.embedding_dim}-dim shared space\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMulti-modal encoder with 512-dim shared space\n```\n:::\n:::\n\n\n:::{.callout-tip}\n## Multi-Modal Search Best Practices\n\n**Architecture:**\n\n- **Separate encoders**: Train modality-specific encoders (don't share weights)\n- **Shared embedding space**: Project to common space (512-1024 dim)\n- **Contrastive training**: Align modalities via paired data (image-caption pairs) (see @sec-contrastive-learning)\n- **Late fusion**: Combine scores at query time (more flexible than early fusion)\n\n**Training data:**\n\n- **Paired examples**: Need (text, image) or (audio, video) pairs\n- **Web-scale data**: LAION-5B (5 billion image-text pairs)\n- **Data quality**: Filter low-quality pairs (CLIP score, aesthetic score)\n- **Augmentation**: Augment images/audio, not text (preserve semantics)\n\n**Performance:**\n\n- **Pre-encode documents**: Encode offline, store embeddings\n- **Per-modality indices**: Separate HNSW index per modality\n- **GPU inference**: Batch encode queries on GPU\n- **Caching**: Cache popular queries (50% of queries are repeats)\n:::\n\n:::{.callout-warning}\n## Cross-Modal Alignment Challenges\n\nMulti-modal embeddings require **aligned training data**:\n\n- Image-text pairs must be semantically related\n- Noisy pairs degrade alignment (web data often mismatched)\n- Rare concepts harder to align (less training data)\n\n**Mitigation strategies:**\n\n- Filter pairs by CLIP score (cosine similarity threshold)\n- Use curated datasets for critical domains\n- Hard negative mining (find hard mismatches to learn from) (see @sec-contrastive-learning)\n- Domain-specific fine-tuning (medical, legal, etc.)\n:::\n\n## Code Search and Software Intelligence\n\nCode search finds functions, classes, and patterns in massive codebases—but traditional search fails because code semantics differ from syntax. **Semantic code search** uses embeddings to find code by intent (\"sort a list\"), not keywords, enabling software intelligence for code completion, bug detection, and API discovery.\n\n### The Code Search Challenge\n\nCode has unique properties:\n\n- **Syntax vs semantics**: `list.sort()` and `sorted(list)` are syntactically different but semantically similar\n- **Multiple representations**: Code, comments, docstrings, test cases all describe intent\n- **Compositional**: Functions compose; understanding requires context\n- **Polyglot**: Multiple languages (Python, Java, C++, JavaScript)\n\n**Challenge**: Find code that **does X** (semantic intent), not code that **contains X** (keyword match).\n\n::: {.cell execution_count=2}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show Code Search System\"}\nimport ast\nfrom typing import List, Tuple\n\n\nclass CodeSearchEngine:\n    \"\"\"Semantic code search using embeddings.\"\"\"\n    def __init__(self, code_encoder, embedding_dim=768):\n        self.code_encoder = code_encoder\n        self.embedding_dim = embedding_dim\n        self.code_snippets = []\n        self.embeddings = []\n\n    def add_code(self, code: str, metadata: dict = None):\n        \"\"\"Index code snippet.\"\"\"\n        # Parse AST to extract functions/classes\n        try:\n            tree = ast.parse(code)\n            for node in ast.walk(tree):\n                if isinstance(node, (ast.FunctionDef, ast.ClassDef)):\n                    snippet = ast.get_source_segment(code, node)\n                    self.code_snippets.append({'code': snippet, 'metadata': metadata})\n        except:\n            # If parsing fails, index as-is\n            self.code_snippets.append({'code': code, 'metadata': metadata})\n\n    def search(self, natural_language_query: str, k: int = 5) -> List[dict]:\n        \"\"\"Search code using natural language query.\"\"\"\n        # Encode query\n        query_emb = self.encode_query(natural_language_query)\n\n        # Encode all code snippets\n        code_embs = [self.encode_code(s['code']) for s in self.code_snippets]\n\n        # Compute similarities\n        similarities = [self.cosine_similarity(query_emb, code_emb)\n                        for code_emb in code_embs]\n\n        # Return top-k\n        top_indices = sorted(range(len(similarities)),\n                             key=lambda i: similarities[i], reverse=True)[:k]\n        return [self.code_snippets[i] for i in top_indices]\n\n    def encode_query(self, query: str):\n        \"\"\"Encode natural language query.\"\"\"\n        return [0.0] * self.embedding_dim  # Placeholder\n\n    def encode_code(self, code: str):\n        \"\"\"Encode code snippet.\"\"\"\n        return [0.0] * self.embedding_dim  # Placeholder\n\n    def cosine_similarity(self, a, b):\n        \"\"\"Compute cosine similarity.\"\"\"\n        import numpy as np\n        return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n\n# Usage example\nengine = CodeSearchEngine(code_encoder=None)\nengine.add_code(\"def sort_list(items): return sorted(items)\")\nresults = engine.search(\"sort a list\", k=3)\nprint(f\"Found {len(results)} code snippets\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFound 1 code snippets\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n/var/folders/j6/195rqgcs37z88kmyck3fqg_h0000gn/T/ipykernel_4950/1050269118.py:54: RuntimeWarning: invalid value encountered in scalar divide\n  return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n```\n:::\n:::\n\n\n:::{.callout-tip}\n## Code Search Best Practices\n\n**Training:**\n\n- **Pre-training**: Use CodeBERT or GraphCodeBERT (pre-trained on GitHub)\n- **Fine-tuning**: Fine-tune on domain-specific code (internal codebase) (see @sec-custom-embedding-strategies for guidance on when to fine-tune vs. train from scratch)\n- **Data augmentation**: Rename variables, reformat code (preserve semantics)\n- **Hard negatives**: Mine hard negatives (similar code, different semantics) (see @sec-contrastive-learning)\n\n**Indexing:**\n\n- **Function-level**: Index individual functions, not entire files\n- **Deduplication**: Remove duplicate functions (common in forks)\n- **Metadata**: Include docstrings, comments, test cases\n- **Incremental**: Update index as new code is added (CI/CD integration)\n\n**Search quality:**\n\n- **Reranking**: Use cross-encoder to rerank top-100 results\n- **Diversity**: Ensure diverse results (not all bubble sort variants)\n- **Filtering**: Filter by language, library, recency\n- **Personalization**: Rank by user's coding style and preferences\n:::\n\n## Scientific Literature and Patent Search\n\nScientific research produces millions of papers annually—PubMed has 35M+ articles, arXiv adds 200K/year, and patent offices hold 100M+ patents. **Semantic literature search** finds relevant research by understanding concepts, methods, and relationships, enabling discovery across citation networks and entity resolution for authors, institutions, and compounds.\n\n### The Scientific Search Challenge\n\nScientific literature has unique characteristics:\n\n- **Domain terminology**: Specialized vocabulary (medical, chemistry, physics)\n- **Citation networks**: Papers cite related work (graph structure matters)\n- **Multi-entity**: Authors, institutions, chemicals, genes (entity linking)\n- **Temporal evolution**: Concepts evolve over time\n- **Multimodal**: Text + figures + tables + equations\n\n**Challenge**: Find **relevant research** (concept match), not **keyword match** (term frequency).\n\n::: {.cell execution_count=3}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show Scientific Literature Search\"}\nfrom typing import List, Dict, Optional\nimport numpy as np\n\n\nclass ScientificPaperSearch:\n    \"\"\"Search scientific papers using embeddings and citation networks.\"\"\"\n    def __init__(self, paper_encoder, embedding_dim=768):\n        self.paper_encoder = paper_encoder\n        self.embedding_dim = embedding_dim\n        self.papers = []\n        self.citation_graph = {}  # paper_id -> [cited_paper_ids]\n\n    def add_paper(self, paper_id: str, title: str, abstract: str,\n                  citations: List[str] = None):\n        \"\"\"Index scientific paper.\"\"\"\n        # Encode paper\n        text = f\"{title} {abstract}\"\n        embedding = self.encode_paper(text)\n\n        self.papers.append({\n            'paper_id': paper_id,\n            'title': title,\n            'abstract': abstract,\n            'embedding': embedding\n        })\n\n        # Update citation graph\n        if citations:\n            self.citation_graph[paper_id] = citations\n\n    def search(self, query: str, k: int = 10, use_citations: bool = True) -> List[dict]:\n        \"\"\"Search papers using semantic similarity and citation network.\"\"\"\n        # Encode query\n        query_emb = self.encode_paper(query)\n\n        # Compute semantic similarities\n        scores = []\n        for paper in self.papers:\n            semantic_score = self.cosine_similarity(query_emb, paper['embedding'])\n\n            # Boost score using citation network\n            if use_citations and paper['paper_id'] in self.citation_graph:\n                citation_boost = len(self.citation_graph[paper['paper_id']]) * 0.01\n                final_score = semantic_score + citation_boost\n            else:\n                final_score = semantic_score\n\n            scores.append(final_score)\n\n        # Return top-k\n        top_indices = sorted(range(len(scores)),\n                             key=lambda i: scores[i], reverse=True)[:k]\n        return [self.papers[i] for i in top_indices]\n\n    def encode_paper(self, text: str):\n        \"\"\"Encode paper text to embedding.\"\"\"\n        return np.random.rand(self.embedding_dim)  # Placeholder\n\n    def cosine_similarity(self, a, b):\n        \"\"\"Compute cosine similarity.\"\"\"\n        return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n\n# Usage example\nsearch_engine = ScientificPaperSearch(paper_encoder=None)\nsearch_engine.add_paper(\"paper1\", \"Deep Learning\", \"Neural networks...\", citations=[\"paper2\"])\nresults = search_engine.search(\"machine learning\", k=5)\nprint(f\"Found {len(results)} relevant papers\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFound 1 relevant papers\n```\n:::\n:::\n\n\n:::{.callout-tip}\n## Scientific Search Best Practices\n\n**Domain-specific embeddings:**\n\n- **Pre-training**: Use SPECTER (citation-based), SciBERT (scientific text)\n- **Fine-tuning**: Fine-tune on domain-specific corpora (biomedical, physics)\n- **Multi-field**: Encode title + abstract + full text (weight by importance)\n- **Citation context**: Include sentences that cite the paper\n\n**Citation graph:**\n\n- **Co-citation**: Papers cited together are related\n- **Bibliographic coupling**: Papers citing the same work are related\n- **PageRank**: Rank by citation graph centrality\n- **Temporal weighting**: Recent citations matter more\n\n**Entity linking:**\n\n- **Named entity recognition**: Extract entities (chemicals, genes, diseases)\n- **Entity disambiguation**: Link to knowledge base (PubChem, UniProt)\n- **Relation extraction**: Extract relationships between entities\n- **Entity embeddings**: Embed entities in same space as papers\n:::\n\n## Media and Content Discovery\n\nMedia assets—images, videos, audio—represent trillions of files across organizations. **Semantic media search** finds content by visual style, composition, audio characteristics, and creative intent, enabling discovery beyond metadata tagging and filename matching.\n\n### The Media Discovery Challenge\n\nMedia has unique properties:\n\n- **Visual style**: Color palette, composition, lighting\n- **Creative intent**: Mood, emotion, message\n- **Temporal dynamics**: Video and audio evolve over time\n- **Quality variation**: Resolution, noise, compression artifacts\n- **Massive scale**: Petabytes of media files\n\n**Challenge**: Find **visually similar** or **stylistically related** media, not **keyword matches** on filenames.\n\n::: {.cell execution_count=4}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show Media Discovery System\"}\nfrom typing import List, Tuple\nimport numpy as np\nfrom dataclasses import dataclass\n\n\n@dataclass\nclass MediaAsset:\n    \"\"\"Media asset with content and style embeddings.\"\"\"\n    asset_id: str\n    file_path: str\n    content_embedding: np.ndarray\n    style_embedding: np.ndarray\n    metadata: dict = None\n\n\nclass MediaDiscoveryEngine:\n    \"\"\"Search media by visual similarity and style.\"\"\"\n    def __init__(self, content_encoder, style_encoder):\n        self.content_encoder = content_encoder\n        self.style_encoder = style_encoder\n        self.assets = []\n\n    def add_asset(self, asset_id: str, file_path: str, image):\n        \"\"\"Index media asset.\"\"\"\n        # Extract content and style embeddings\n        content_emb = self.encode_content(image)\n        style_emb = self.encode_style(image)\n\n        asset = MediaAsset(\n            asset_id=asset_id,\n            file_path=file_path,\n            content_embedding=content_emb,\n            style_embedding=style_emb\n        )\n        self.assets.append(asset)\n\n    def search_by_content(self, query_image, k: int = 10) -> List[MediaAsset]:\n        \"\"\"Find visually similar content.\"\"\"\n        query_emb = self.encode_content(query_image)\n\n        similarities = [np.dot(query_emb, asset.content_embedding)\n                        for asset in self.assets]\n        top_indices = np.argsort(similarities)[-k:][::-1]\n        return [self.assets[i] for i in top_indices]\n\n    def search_by_style(self, query_image, k: int = 10) -> List[MediaAsset]:\n        \"\"\"Find assets with similar visual style.\"\"\"\n        query_emb = self.encode_style(query_image)\n\n        similarities = [np.dot(query_emb, asset.style_embedding)\n                        for asset in self.assets]\n        top_indices = np.argsort(similarities)[-k:][::-1]\n        return [self.assets[i] for i in top_indices]\n\n    def encode_content(self, image):\n        \"\"\"Encode semantic content.\"\"\"\n        return np.random.rand(512)  # Placeholder\n\n    def encode_style(self, image):\n        \"\"\"Encode visual style.\"\"\"\n        return np.random.rand(256)  # Placeholder\n\n# Usage example\nengine = MediaDiscoveryEngine(content_encoder=None, style_encoder=None)\nprint(\"Media discovery engine initialized with content and style encoders\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMedia discovery engine initialized with content and style encoders\n```\n:::\n:::\n\n\n:::{.callout-tip}\n## Media Search Best Practices\n\n**Visual features:**\n\n- **Content embeddings**: Use CLIP, ResNet, or ViT for semantic content\n- **Style embeddings**: Use Gram matrices or style-specific encoders\n- **Multi-scale**: Extract features at multiple resolutions\n- **Color histograms**: Supplement embeddings with color features\n\n**Duplicate detection:**\n\n- **Perceptual hashing**: pHash, dHash for near-duplicate detection\n- **Hamming distance**: Fast comparison (XOR + popcount)\n- **Clustering**: Group near-duplicates for review\n- **Threshold tuning**: Balance false positives vs false negatives\n\n**Performance:**\n\n- **Pre-compute embeddings**: Encode assets offline during ingestion\n- **GPU batching**: Batch encode 100-1000 images per GPU\n- **Caching**: Cache embeddings in vector database\n- **Progressive loading**: Show low-res previews while searching\n:::\n\n## Enterprise Knowledge Graphs\n\nEnterprise knowledge graphs connect entities—customers, products, employees, documents—through relationships. **Embedding-based knowledge graphs** use learned embeddings to represent entities and relations, enabling link prediction, entity resolution, and graph-aware search that understands how entities relate.\n\n### The Knowledge Graph Challenge\n\nTraditional knowledge graphs use discrete representations (triples: subject-predicate-object). **Embedding-based graphs** represent entities and relations as vectors, enabling:\n\n- **Link prediction**: Predict missing relationships\n- **Entity resolution**: Merge duplicate entities\n- **Multi-hop reasoning**: Answer complex queries across relationships\n- **Similarity search**: Find similar entities by embeddings\n\n**Challenge**: Learn embeddings that preserve graph structure and semantics.\n\n::: {.cell execution_count=5}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show Knowledge Graph Embeddings\"}\nimport torch\nimport torch.nn as nn\nimport numpy as np\nfrom typing import List, Tuple, Dict\n\n\nclass KnowledgeGraphEmbedding:\n    \"\"\"TransE-based knowledge graph embedding.\"\"\"\n    def __init__(self, num_entities: int, num_relations: int, embedding_dim: int = 128):\n        self.embedding_dim = embedding_dim\n\n        # Entity and relation embeddings\n        self.entity_embeddings = nn.Embedding(num_entities, embedding_dim)\n        self.relation_embeddings = nn.Embedding(num_relations, embedding_dim)\n\n        # Initialize\n        nn.init.xavier_uniform_(self.entity_embeddings.weight)\n        nn.init.xavier_uniform_(self.relation_embeddings.weight)\n\n    def score_triple(self, head: torch.Tensor, relation: torch.Tensor,\n                     tail: torch.Tensor) -> torch.Tensor:\n        \"\"\"Score a triple (head, relation, tail) using TransE.\"\"\"\n        head_emb = self.entity_embeddings(head)\n        rel_emb = self.relation_embeddings(relation)\n        tail_emb = self.entity_embeddings(tail)\n\n        # TransE: h + r ≈ t\n        score = torch.norm(head_emb + rel_emb - tail_emb, p=2, dim=-1)\n        return -score  # Negate so higher is better\n\n    def predict_tail(self, head: int, relation: int, k: int = 10) -> List[Tuple[int, float]]:\n        \"\"\"Predict most likely tail entities for (head, relation, ?).\"\"\"\n        head_tensor = torch.tensor([head])\n        rel_tensor = torch.tensor([relation])\n\n        # Score all possible tails\n        all_tails = torch.arange(self.entity_embeddings.num_embeddings)\n        scores = []\n\n        for tail in all_tails:\n            tail_tensor = torch.tensor([tail])\n            score = self.score_triple(head_tensor, rel_tensor, tail_tensor)\n            scores.append((tail.item(), score.item()))\n\n        # Return top-k\n        scores.sort(key=lambda x: x[1], reverse=True)\n        return scores[:k]\n\n# Usage example\nkg = KnowledgeGraphEmbedding(num_entities=1000, num_relations=50, embedding_dim=128)\npredictions = kg.predict_tail(head=0, relation=5, k=5)\nprint(f\"Top 5 predicted tail entities: {[p[0] for p in predictions]}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTop 5 predicted tail entities: [674, 229, 777, 0, 146]\n```\n:::\n:::\n\n\n:::{.callout-tip}\n## Knowledge Graph Embedding Best Practices\n\n**Model selection:**\n\n- **TransE**: Simple, works well for 1-to-1 relations\n- **DistMult**: Better for symmetric relations\n- **ComplEx**: Handles asymmetric and inverse relations\n- **RotatE**: State-of-the-art for complex relations\n\n**Training:**\n\n- **Negative sampling**: Sample false triples for contrastive learning\n- **Hard negatives**: Mine hard negatives (plausible but false)\n- **Regularization**: L2 regularization on embeddings\n- **Batch training**: Use large batches (1000-10000 triples)\n\n**Applications:**\n\n- **Link prediction**: Predict missing relationships\n- **Entity resolution**: Merge duplicate entities by embedding similarity\n- **Graph completion**: Fill in missing edges\n- **Multi-hop reasoning**: Answer complex queries (e.g., \"customers who bought products similar to X\")\n:::\n\n:::{.callout-warning}\n## Graph Embedding Challenges\n\n**Data quality:**\n\n- Incomplete graphs (missing edges) degrade embeddings\n- Noisy relations (incorrect edges) poison training\n- Entity disambiguation (same name, different entities)\n\n**Scalability:**\n\n- Billion-entity graphs require distributed training\n- Full graph materialization doesn't fit in memory\n- Subgraph sampling required for large graphs\n\n**Interpretability:**\n\n- Embeddings are black boxes (hard to debug)\n- Relation semantics may not align with vector operations\n- Need attribution methods to explain predictions\n:::\n\n## Key Takeaways\n\n- **Multi-modal search unifies text, images, audio, and video in shared embedding spaces**: Cross-modal retrieval (query text, retrieve images) requires contrastive training on paired data and separate per-modality encoders that project to a common vector space\n\n- **Code search transcends syntax to find code by semantic intent**: Semantic code embeddings trained on code-docstring pairs enable natural language queries like \"sort a list\" to find relevant implementations across languages and coding styles\n\n- **Scientific literature search leverages citation networks and domain embeddings**: SPECTER and SciBERT embeddings combined with citation graph analysis (co-citation, bibliographic coupling) enable discovery of related research beyond keyword matching\n\n- **Media discovery finds visual similarity and creative style**: Separate embeddings for content (semantic meaning) and style (color, composition, texture) enable both \"find similar images\" and \"find images with similar aesthetic\" use cases\n\n- **Knowledge graph embeddings enable link prediction and entity resolution**: TransE and related models represent entities and relations as vectors, enabling prediction of missing relationships, merging of duplicate entities, and graph-aware similarity search\n\n- **Semantic search beyond text requires domain-specific encoders**: General-purpose embeddings (CLIP, BERT) provide baseline capabilities, but production systems need fine-tuning on domain-specific data (code repositories, scientific papers, media assets)—see @sec-custom-embedding-strategies for a decision framework on choosing the right level of customization\n\n- **Search quality depends on training data quality**: Multi-modal alignment requires clean paired data, code search needs accurate code-docstring pairs, and knowledge graphs need high-quality relationship annotations\n\n## Looking Ahead\n\nPart IV (Advanced Applications) continues with @sec-recommendation-systems, which revolutionizes recommendation systems with embeddings: embedding-based collaborative filtering that scales to billions of users and items, cold start solutions using content embeddings and meta-learning, real-time personalization with streaming embeddings, diversity and fairness constraints that prevent filter bubbles, and cross-domain recommendation transfer that leverages embeddings across product categories and platforms.\n\n## Further Reading\n\n### Multi-Modal Learning\n- Radford, Alec, et al. (2021). \"Learning Transferable Visual Models From Natural Language Supervision (CLIP).\" ICML.\n- Jia, Chao, et al. (2021). \"Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision (ALIGN).\" ICML.\n- Girdhar, Rohit, et al. (2023). \"ImageBind: One Embedding Space To Bind Them All.\" CVPR.\n- Baltrusaitis, Tadas, et al. (2019). \"Multimodal Machine Learning: A Survey and Taxonomy.\" IEEE TPAMI.\n\n### Code Search and Software Intelligence\n- Feng, Zhangyin, et al. (2020). \"CodeBERT: A Pre-Trained Model for Programming and Natural Languages.\" EMNLP.\n- Guo, Daya, et al. (2021). \"GraphCodeBERT: Pre-training Code Representations with Data Flow.\" ICLR.\n- Husain, Hamel, et al. (2019). \"CodeSearchNet Challenge: Evaluating the State of Semantic Code Search.\" arXiv.\n- Chen, Mark, et al. (2021). \"Evaluating Large Language Models Trained on Code (Codex).\" arXiv.\n\n### Scientific Literature Search\n- Cohan, Arman, et al. (2020). \"SPECTER: Document-level Representation Learning using Citation-informed Transformers.\" ACL.\n- Beltagy, Iz, et al. (2019). \"SciBERT: A Pretrained Language Model for Scientific Text.\" EMNLP.\n- Lo, Kyle, et al. (2020). \"S2ORC: The Semantic Scholar Open Research Corpus.\" ACL.\n- Priem, Jason, et al. (2022). \"OpenAlex: A Fully-Open Index of Scholarly Works, Authors, Venues, and Concepts.\" arXiv.\n\n### Media and Content Discovery\n- Gatys, Leon A., et al. (2016). \"Image Style Transfer Using Convolutional Neural Networks.\" CVPR.\n- Johnson, Justin, et al. (2016). \"Perceptual Losses for Real-Time Style Transfer and Super-Resolution.\" ECCV.\n- Simonyan, Karen, and Andrew Zisserman (2014). \"Very Deep Convolutional Networks for Large-Scale Image Recognition.\" ICLR.\n- Dosovitskiy, Alexey, et al. (2021). \"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (ViT).\" ICLR.\n\n### Knowledge Graph Embeddings\n- Bordes, Antoine, et al. (2013). \"Translating Embeddings for Modeling Multi-relational Data (TransE).\" NeurIPS.\n- Yang, Bishan, et al. (2015). \"Embedding Entities and Relations for Learning and Inference in Knowledge Bases (DistMult).\" ICLR.\n- Trouillon, Théo, et al. (2016). \"Complex Embeddings for Simple Link Prediction (ComplEx).\" ICML.\n- Sun, Zhiqing, et al. (2019). \"RotatE: Knowledge Graph Embedding by Relational Rotation in Complex Space.\" ICLR.\n- Wang, Quan, et al. (2017). \"Knowledge Graph Embedding: A Survey of Approaches and Applications.\" IEEE TKDE.\n\n",
    "supporting": [
      "ch20_semantic_search_files/figure-pdf"
    ],
    "filters": []
  }
}