{
  "hash": "841cc4f7fca095bdeec25bd9e0f33bb1",
  "result": {
    "engine": "jupyter",
    "markdown": "# Image, Audio, and Video Embeddings {#sec-image-video-embeddings}\n\n::: callout-note\n## Chapter Overview\n\nThis chapter covers embeddings for visual and audio data—images, audio, and video. We explore how these modalities are represented as vectors, when to use each type, and the architectures that create them. An optional advanced section explains how the underlying models learn visual features.\n:::\n\n## Image Embeddings {#sec-image-embedding-types}\n\nImage embeddings convert visual content into vectors that capture visual semantics—shapes, colors, textures, objects, and spatial relationships. Unlike pixel-by-pixel comparison, embeddings understand that two photos of the same cat are similar even if taken from different angles or lighting conditions.\n\nThe example below uses ResNet50 [@he2016resnet], a CNN pre-trained on ImageNet's 1.4 million images. ResNet learns hierarchical visual features—early layers detect edges and textures, middle layers recognize shapes and parts, and deep layers understand objects and scenes. We remove the final classification layer to extract the 2048-dimensional feature vector as our embedding.\n\nDigital images are stored as 3D arrays with shape (height, width, 3)—the third dimension holds Red, Green, and Blue color intensities for each pixel:\n\n::: {#cell-fig-image-rgb-cube .cell execution_count=1}\n\n::: {.cell-output .cell-output-display}\n![An image is a 3D array: height × width × 3 color channels (Red, Green, Blue). Each pixel has three values (0-255).](ch05_image_video_embeddings_files/figure-epub/fig-image-rgb-cube-output-1.png){#fig-image-rgb-cube}\n:::\n:::\n\n\n::: {#aa2be897 .cell execution_count=2}\n``` {.python .cell-code code-fold=\"false\"}\n\"\"\"\nImage Embeddings: Visual Content as Vectors\n\"\"\"\n\nimport torch\nimport numpy as np\nfrom PIL import Image\nfrom torchvision import models, transforms\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Suppress download messages\nimport logging\nlogging.getLogger('torch').setLevel(logging.ERROR)\n\nfrom torchvision.models import ResNet50_Weights\n\n# Load pretrained ResNet50 as feature extractor\nweights = ResNet50_Weights.IMAGENET1K_V1\nmodel = models.resnet50(weights=None)\nmodel.load_state_dict(weights.get_state_dict(progress=False))\nmodel.eval()\n\n# Remove classification head to get embeddings\nfeature_extractor = torch.nn.Sequential(*list(model.children())[:-1])\n\n# Standard ImageNet preprocessing\npreprocess = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\ndef get_image_embedding(image):\n    \"\"\"Extract embedding from PIL Image.\"\"\"\n    tensor = preprocess(image).unsqueeze(0)\n    with torch.no_grad():\n        embedding = feature_extractor(tensor)\n    return embedding.squeeze().numpy()\n\n# Create synthetic test images with different color patterns\n# np.random.randint([low_r, low_g, low_b], [high_r, high_g, high_b], shape)\n# generates random RGB values within the specified range for each pixel\nnp.random.seed(42)\nimages = {\n    'red_pattern': Image.fromarray(\n        np.random.randint([180, 0, 0], [255, 80, 80], (224, 224, 3), dtype=np.uint8)\n    ),\n    'blue_pattern': Image.fromarray(\n        np.random.randint([0, 0, 180], [80, 80, 255], (224, 224, 3), dtype=np.uint8)\n    ),\n    'orange_pattern': Image.fromarray(\n        np.random.randint([200, 100, 0], [255, 150, 50], (224, 224, 3), dtype=np.uint8)\n    ),\n}\n\n# Get embeddings\nembeddings = {name: get_image_embedding(img) for name, img in images.items()}\n\nprint(\"Image embedding similarities:\\n\")\nprint(\"Red and orange (similar warm colors) should be more similar than red and blue:\")\nred_orange = cosine_similarity([embeddings['red_pattern']], [embeddings['orange_pattern']])[0][0]\nred_blue = cosine_similarity([embeddings['red_pattern']], [embeddings['blue_pattern']])[0][0]\nprint(f\"  red ↔ orange: {red_orange:.3f}\")\nprint(f\"  red ↔ blue:   {red_blue:.3f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nImage embedding similarities:\n\nRed and orange (similar warm colors) should be more similar than red and blue:\n  red ↔ orange: 0.898\n  red ↔ blue:   0.717\n```\n:::\n:::\n\n\n::: {#cell-fig-rgb-channels .cell execution_count=3}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show RGB channel visualization code\"}\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\n\nfig = plt.figure(figsize=(12, 4))\ngs = gridspec.GridSpec(3, 7, width_ratios=[3, 0.5, 3, 0.5, 3, 0.5, 3], hspace=0.1, wspace=0.05)\n\nfor row, (name, img) in enumerate(images.items()):\n    img_array = np.array(img)\n    label = name.replace(\"_pattern\", \"\").title()\n\n    ax = fig.add_subplot(gs[row, 0])\n    ax.imshow(img)\n    if row == 0:\n        ax.set_title(label, fontsize=10)\n    ax.axis('off')\n\n    ax = fig.add_subplot(gs[row, 1])\n    ax.text(0.5, 0.5, '=', fontsize=16, ha='center', va='center', fontweight='bold')\n    ax.axis('off')\n\n    ax = fig.add_subplot(gs[row, 2])\n    ax.imshow(img_array[:,:,0], cmap='Reds', vmin=0, vmax=255)\n    if row == 0:\n        ax.set_title('R', fontsize=10)\n    ax.axis('off')\n\n    ax = fig.add_subplot(gs[row, 3])\n    ax.text(0.5, 0.5, '+', fontsize=16, ha='center', va='center', fontweight='bold')\n    ax.axis('off')\n\n    ax = fig.add_subplot(gs[row, 4])\n    ax.imshow(img_array[:,:,1], cmap='Greens', vmin=0, vmax=255)\n    if row == 0:\n        ax.set_title('G', fontsize=10)\n    ax.axis('off')\n\n    ax = fig.add_subplot(gs[row, 5])\n    ax.text(0.5, 0.5, '+', fontsize=16, ha='center', va='center', fontweight='bold')\n    ax.axis('off')\n\n    ax = fig.add_subplot(gs[row, 6])\n    ax.imshow(img_array[:,:,2], cmap='Blues', vmin=0, vmax=255)\n    if row == 0:\n        ax.set_title('B', fontsize=10)\n    ax.axis('off')\n\nplt.suptitle('RGB: Three 2D channels combine to create color images', fontsize=11, fontweight='bold', y=0.98)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Each image is a 3D array (224 × 224 × 3). The combined color equals the sum of Red, Green, and Blue channel intensities.](ch05_image_video_embeddings_files/figure-epub/fig-rgb-channels-output-1.png){#fig-rgb-channels}\n:::\n:::\n\n\nWhen comparing image embeddings, we use cosine similarity just like with text. Images with similar visual features—colors, textures, shapes, or objects—will have embeddings that point in similar directions, yielding high similarity scores. The red and orange patterns share warm color features, so their embeddings are closer together than red and blue. In practice, this means a photo of a red dress will be more similar to an orange dress than a blue one, even though all three are \"dresses.\"\n\nHow does the model \"understand\" colors? As @fig-rgb-channels shows, each image is stored as three 2D arrays—one for Red, Green, and Blue intensity. The red pattern has high values in the R channel and low values in G and B. Orange combines high R with medium G (red + green = orange).\n\n::: {.callout-note}\n## RGB Input ≠ Embedding Output\nDon't confuse the 3-channel RGB input with the embedding output. ResNet50 takes the 3 RGB channels as input but produces a **2048-dimensional** embedding vector that captures far more than color: edges, textures, shapes, and high-level visual concepts learned from millions of images.\n:::\n\nThe model receives the three RGB channels as input, and early CNN layers learn filters that activate for specific patterns—some respond to warm tones, others to edges or textures. As layers get deeper, the network combines these low-level features into increasingly abstract representations. By training on millions of labeled images, the model learns that red and orange often appear together (sunsets, autumn leaves, fire) more frequently than red and blue, encoding this statistical relationship across all 2048 dimensions.\n\nBeyond colors, early CNN layers also learn edge detectors—filters that respond to boundaries between light and dark regions. For a hands-on introduction to how a single neuron learns to detect edges, see [How Neural Networks Learn to See](https://snowch.github.io/nn_edge_detector_blog.html).\n\n**When to use image embeddings:**\n\n- Visual recommendation systems—suggest visually similar items (see @sec-recommendation-systems)\n- Content moderation—detect variations of prohibited images (see @sec-content-moderation)\n- Forensic video search—find specific people or objects in footage (see @sec-video-surveillance) *(reverse image lookup)*\n- Face recognition—identify or verify individuals from photos (see @sec-video-surveillance)\n- Duplicate and near-duplicate detection—identify copied or modified images (see @sec-entity-resolution) *(reverse image lookup)*\n- Medical imaging—find similar X-rays, scans, or pathology slides (see @sec-healthcare-life-sciences)\n- Visual product search—find products similar to a photo (see @sec-retail-ecommerce) *(reverse image lookup)*\n- Quality control—detect defects by comparing to reference images (see @sec-manufacturing-industry40)\n\nThere are dozens of other applications including art style matching, stock photo search, image classification, trademark and logo detection, scene recognition, wildlife identification, satellite imagery analysis, document scanning, and autonomous vehicle perception.\n\n**Popular architectures:**\n\n| Architecture | Type | Strengths | Use Cases |\n|-------------|------|-----------|-----------|\n| [ResNet](https://pytorch.org/vision/main/models/resnet.html) | CNN | Fast, proven | General visual search |\n| [EfficientNet](https://pytorch.org/vision/main/models/efficientnet.html) | CNN | Efficient, accurate | Mobile/edge deployment |\n| [ViT](https://pytorch.org/vision/main/models/vision_transformer.html) | Transformer | Best accuracy | High-quality requirements |\n| [CLIP](https://github.com/openai/CLIP) | Multi-modal | Text-image alignment | Zero-shot classification |\n\n: Image embedding architectures {.striped}\n\n## Audio Embeddings {#sec-audio-embedding-types}\n\nAudio embeddings convert sound into vectors that capture acoustic properties—pitch, timbre, rhythm, and spectral characteristics. Unlike raw waveform comparison, embeddings understand that two recordings of the same spoken word are similar even with different speakers, background noise, or recording equipment.\n\nThe example below uses MFCC (Mel-frequency cepstral coefficients), a classic signal processing technique—not a neural network, but a fixed mathematical transformation that extracts acoustic features. MFCCs transform audio into a representation that mimics human hearing—emphasizing frequencies we're sensitive to while compressing less important details. While modern systems use learned embeddings from models like Wav2Vec2, MFCCs remain useful as a baseline and for understanding what acoustic features matter.\n\nWe extract 20 values per time frame, called *coefficients*—each describes a different aspect of the sound's frequency content (overall loudness, balance between low and high frequencies, etc.). Since audio clips vary in length (a 3-second clip has more frames than a 1-second clip), we aggregate across time using mean and standard deviation to create a fixed-size embedding that works regardless of duration:\n\n::: {#cell-fig-mfcc-aggregation .cell execution_count=4}\n\n::: {.cell-output .cell-output-display}\n![MFCC aggregation: 20 coefficients per frame become a 40-dimensional embedding via mean and standard deviation.](ch05_image_video_embeddings_files/figure-epub/fig-mfcc-aggregation-output-1.png){#fig-mfcc-aggregation}\n:::\n:::\n\n\n::: {#7eee91b6 .cell execution_count=5}\n``` {.python .cell-code code-fold=\"false\"}\n\"\"\"\nAudio Embeddings: Sound as Vectors\n\"\"\"\n\nimport numpy as np\nimport librosa\nfrom sklearn.metrics.pairwise import cosine_similarity\n\ndef audio_to_embedding(audio, sr, n_mfcc=20):\n    \"\"\"Convert audio waveform to a fixed-size embedding using MFCCs.\"\"\"\n    # Extract MFCCs: 20 coefficients per time frame\n    mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=n_mfcc)\n    # Aggregate over time: mean captures average timbre, std captures variation\n    return np.concatenate([mfccs.mean(axis=1), mfccs.std(axis=1)])\n\n# Load librosa's built-in trumpet sample\naudio, sr = librosa.load(librosa.ex('trumpet'))\ntrumpet_embedding = audio_to_embedding(audio, sr)\n\n# Create variations to demonstrate similarity\ntrumpet_slow = librosa.effects.time_stretch(audio, rate=0.8)\ntrumpet_pitch_up = librosa.effects.pitch_shift(audio, sr=sr, n_steps=12)  # One octave up\n\n# Generate embeddings for each variation\nembeddings = {\n    'trumpet_original': trumpet_embedding,\n    'trumpet_slower': audio_to_embedding(trumpet_slow, sr),\n    'trumpet_higher_pitch': audio_to_embedding(trumpet_pitch_up, sr),\n}\n\nprint(f\"Embedding dimension: {len(trumpet_embedding)} (20 MFCCs × 2 stats)\\n\")\n\n# Compare similarities\nprint(\"Audio embedding similarities:\")\nsim_slow = cosine_similarity(\n    [embeddings['trumpet_original']], [embeddings['trumpet_slower']]\n)[0][0]\nprint(f\"  Original ↔ Slower tempo:   {sim_slow:.3f}\")\n\nsim_pitch = cosine_similarity(\n    [embeddings['trumpet_original']], [embeddings['trumpet_higher_pitch']]\n)[0][0]\nprint(f\"  Original ↔ Higher pitch:   {sim_pitch:.3f}\")\n\nsim_variations = cosine_similarity(\n    [embeddings['trumpet_slower']], [embeddings['trumpet_higher_pitch']]\n)[0][0]\nprint(f\"  Slower ↔ Higher pitch:     {sim_variations:.3f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nEmbedding dimension: 40 (20 MFCCs × 2 stats)\n\nAudio embedding similarities:\n  Original ↔ Slower tempo:   1.000\n  Original ↔ Higher pitch:   0.978\n  Slower ↔ Higher pitch:     0.980\n```\n:::\n:::\n\n\nWhen comparing audio embeddings, cosine similarity measures how acoustically similar two sounds are. Notice that all similarities are high (>0.97)—this is by design. MFCCs capture timbre (the trumpet's characteristic \"brassy\" quality) rather than absolute pitch or tempo. The tempo change preserves timbre almost perfectly (1.000), while shifting pitch by an octave causes only a small drop (~0.98) because the overall spectral shape remains trumpet-like. This robustness is useful for applications like speaker identification, where you want to match voices regardless of speaking speed or emotional pitch variations.\n\nHow do MFCCs capture sound? Audio is first split into short overlapping frames (typically 25ms). For each frame, we compute the frequency spectrum, then apply a mel filterbank that groups frequencies into bands matching human perception—more resolution at low frequencies where we hear pitch differences, less at high frequencies. The cepstral coefficients compress this further, capturing the overall \"shape\" of the spectrum. The result: a compact representation of timbre that's robust to volume changes.\n\n**When to use audio embeddings:** Content moderation, audio fingerprinting, speaker identification, music recommendation, podcast and video search, sound event detection, voice cloning detection, acoustic quality control, wildlife monitoring, and medical diagnostics from sounds (coughs, heartbeats).\n\nThis book covers music recommendation with audio embeddings in @sec-recommendation-systems. If you'd like to see other audio applications covered in future editions, reach out to the author.\n\n**Popular architectures:**\n\n| Architecture | Type | Strengths | Use Cases |\n|-------------|------|-----------|-----------|\n| [Wav2Vec2](https://huggingface.co/facebook/wav2vec2-base) | Self-supervised | Rich speech features | Speech recognition, speaker ID |\n| [Whisper](https://github.com/openai/whisper) | Multi-task | Transcription + embeddings | Speech search, subtitles |\n| [CLAP](https://github.com/LAION-AI/CLAP) | Multi-modal | Audio-text alignment | Zero-shot audio classification |\n| [VGGish](https://github.com/tensorflow/models/tree/master/research/audioset/vggish) | CNN | General audio events | Sound classification |\n\n: Audio embedding architectures {.striped}\n\nFor audio model architecture details, see the linked documentation above or explore the [Hugging Face Audio Course](https://huggingface.co/learn/audio-course) for a comprehensive introduction.\n\n## Video Embeddings {#sec-video-embedding-types}\n\nVideo embeddings convert video clips into vectors that capture both visual content and temporal dynamics—actions, motion patterns, scene transitions, and narrative flow. Unlike image embeddings that capture a single moment, video embeddings understand that \"a person sitting down\" and \"a person standing up\" are different actions even though individual frames might look similar.\n\nThe challenge with video is combining spatial information (what's in each frame) with temporal information (how things change over time). The simplest approach extracts image embeddings from sampled frames and aggregates them. More sophisticated models process multiple frames together to capture motion directly.\n\nJust as images are 3D arrays (height × width × 3 channels), videos add a fourth dimension: time. A video is a sequence of frames, each of which is an image:\n\n::: {#cell-fig-video-4d-structure .cell execution_count=6}\n\n::: {.cell-output .cell-output-display}\n![A video is a 4D array: frames × height × width × 3 RGB channels. Each frame is a complete image.](ch05_image_video_embeddings_files/figure-epub/fig-video-4d-structure-output-1.png){#fig-video-4d-structure}\n:::\n:::\n\n\nThe example below demonstrates the frame sampling approach: we extract image embeddings from frames sampled throughout a video, then average them to create a single video embedding. Since videos vary in length, aggregation (like we saw with audio) produces a fixed-size embedding regardless of duration.\n\n::: {#cell-fig-video-aggregation .cell execution_count=7}\n\n::: {.cell-output .cell-output-display}\n![Video embedding via frame sampling: extract image embeddings from sampled frames, then aggregate into a single video embedding.](ch05_image_video_embeddings_files/figure-epub/fig-video-aggregation-output-1.png){#fig-video-aggregation}\n:::\n:::\n\n\n::: {#2a31d980 .cell execution_count=8}\n``` {.python .cell-code code-fold=\"false\"}\n\"\"\"\nVideo Embeddings: From Clips to Vectors\n\"\"\"\n\nimport torch\nimport numpy as np\nfrom torchvision import models, transforms\nfrom torchvision.models import ResNet50_Weights\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Suppress download messages\nimport logging\nlogging.getLogger('torch').setLevel(logging.ERROR)\n\n# Load pretrained ResNet50 as frame feature extractor\nweights = ResNet50_Weights.IMAGENET1K_V1\nmodel = models.resnet50(weights=None)\nmodel.load_state_dict(weights.get_state_dict(progress=False))\nmodel.eval()\nfeature_extractor = torch.nn.Sequential(*list(model.children())[:-1])\n\n# Normalize using ImageNet statistics: the mean and std of RGB values across\n# 1.2M training images. This ensures our input has the same distribution the\n# model was trained on. These specific values are standard for ImageNet models.\npreprocess = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\ndef frames_to_video_embedding(frames):\n    \"\"\"Convert a list of frames (as numpy arrays) to a video embedding.\"\"\"\n    frame_embeddings = []\n    for frame in frames:\n        # Convert numpy array to tensor and preprocess\n        if frame.dtype == np.uint8:\n            frame = frame.astype(np.float32) / 255.0\n        tensor = preprocess(frame).unsqueeze(0)\n        with torch.no_grad():\n            emb = feature_extractor(tensor).squeeze().numpy()\n        frame_embeddings.append(emb)\n    # Aggregate: mean across all frames\n    return np.mean(frame_embeddings, axis=0)\n\n# Simulate video frames as colored images (224x224 RGB)\n# \"Running video\": frames transition from green (grass) to blue (sky) - outdoor motion\nrunning_frames = [\n    np.full((224, 224, 3), [0.2, 0.6 + i*0.05, 0.2], dtype=np.float32)  # Green to lighter\n    for i in range(5)\n]\n\n# \"Cooking video\": frames stay warm orange/red tones - kitchen scene\ncooking_frames = [\n    np.full((224, 224, 3), [0.8, 0.4 + i*0.02, 0.1], dtype=np.float32)  # Warm tones\n    for i in range(5)\n]\n\n# \"Jogging video\": similar to running - outdoor greens and blues\njogging_frames = [\n    np.full((224, 224, 3), [0.25, 0.55 + i*0.05, 0.25], dtype=np.float32)  # Similar greens\n    for i in range(5)\n]\n\n# Generate video embeddings\nembeddings = {\n    'running': frames_to_video_embedding(running_frames),\n    'cooking': frames_to_video_embedding(cooking_frames),\n    'jogging': frames_to_video_embedding(jogging_frames),\n}\n\nprint(f\"Video embedding dimension: {len(embeddings['running'])}\\n\")\nprint(\"Video embedding similarities:\")\n\nrun_jog = cosine_similarity([embeddings['running']], [embeddings['jogging']])[0][0]\nrun_cook = cosine_similarity([embeddings['running']], [embeddings['cooking']])[0][0]\njog_cook = cosine_similarity([embeddings['jogging']], [embeddings['cooking']])[0][0]\n\nprint(f\"  Running ↔ Jogging: {run_jog:.3f}  (similar outdoor scenes)\")\nprint(f\"  Running ↔ Cooking: {run_cook:.3f}  (different scenes)\")\nprint(f\"  Jogging ↔ Cooking: {jog_cook:.3f}  (different scenes)\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nVideo embedding dimension: 2048\n\nVideo embedding similarities:\n  Running ↔ Jogging: 0.995  (similar outdoor scenes)\n  Running ↔ Cooking: 0.807  (different scenes)\n  Jogging ↔ Cooking: 0.808  (different scenes)\n```\n:::\n:::\n\n\n::: {#cell-fig-video-frames .cell execution_count=9}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show video frame visualization code\"}\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\n\nfig = plt.figure(figsize=(12, 4))\ngs = gridspec.GridSpec(3, 7, width_ratios=[3, 0.3, 3, 0.3, 3, 0.3, 3], hspace=0.15, wspace=0.05)\n\nvideos = [\n    ('Running', running_frames),\n    ('Cooking', cooking_frames),\n    ('Jogging', jogging_frames),\n]\n\nfor row, (name, frames) in enumerate(videos):\n    # Show 4 sample frames from the video\n    frame_indices = [0, 1, 3, 4]  # Sample frames\n\n    for col, fidx in enumerate(frame_indices):\n        ax = fig.add_subplot(gs[row, col * 2])\n        ax.imshow(frames[fidx])\n        if row == 0:\n            ax.set_title(f'Frame {fidx + 1}', fontsize=9)\n        ax.set_xticks([])\n        ax.set_yticks([])\n        for spine in ax.spines.values():\n            spine.set_visible(False)\n\n        # Add video name label on left side of first frame\n        if col == 0:\n            ax.text(-0.15, 0.5, name, fontsize=10, fontweight='bold',\n                    transform=ax.transAxes, ha='right', va='center')\n\n        # Add arrow between frames (except last)\n        if col < 3:\n            ax_arrow = fig.add_subplot(gs[row, col * 2 + 1])\n            ax_arrow.text(0.5, 0.5, '→', fontsize=14, ha='center', va='center', color='gray')\n            ax_arrow.axis('off')\n\nplt.suptitle('Video frames: similar scenes have similar color patterns across time', fontsize=11, fontweight='bold', y=0.98)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Sample frames from each video. Running and jogging share similar green outdoor tones, while cooking has warm orange/red indoor tones.](ch05_image_video_embeddings_files/figure-epub/fig-video-frames-output-1.png){#fig-video-frames}\n:::\n:::\n\n\nWhen comparing video embeddings, similar activities and scenes cluster together. Running and jogging videos both contain outdoor scenes with similar color palettes (greens, blues), so their embeddings are close. Cooking videos have warm indoor tones (oranges, reds) that differ significantly from outdoor activities.\n\nThe frame sampling approach shown above is simple but misses motion information—it can't distinguish \"sitting down\" from \"standing up\" since both might have similar individual frames. More advanced architectures like SlowFast and Video Swin (see table below) process multiple frames together using 3D convolutions or temporal attention, capturing how pixels change over time to understand motion and actions.\n\n**When to use video embeddings:** Action recognition and search, video recommendation, content moderation, surveillance and anomaly detection, video summarization, sports analytics, and gesture recognition.\n\nThis book covers video surveillance applications in @sec-video-surveillance. If you'd like to see other video applications covered in future editions, reach out to the author.\n\n**Popular architectures:**\n\n| Architecture | Type | Strengths | Use Cases |\n|-------------|------|-----------|-----------|\n| Frame sampling + CNN | Aggregated image embeddings | Simple, fast (no motion) | Scene classification |\n| [SlowFast](https://github.com/facebookresearch/SlowFast) | Two-pathway 3D CNN | Captures fast and slow motion | Action recognition |\n| [X3D](https://github.com/facebookresearch/SlowFast) | Efficient 3D CNN | Mobile-friendly | Real-time applications |\n| [Video Swin](https://github.com/SwinTransformer/Video-Swin-Transformer) | Video transformer | State-of-the-art accuracy | High-quality requirements |\n\n: Video embedding architectures {.striped}\n\nFor video model architecture details, see the linked documentation above.\n\n## Advanced: How Visual Models Learn\n\n::: {.callout-note}\n## Optional Section\nThis section explains how image embedding models learn visual features. Understanding these fundamentals helps you choose the right model and fine-tune for your domain. Skip this if you just need to use embeddings.\n:::\n\n### Convolutional Neural Networks (CNNs)\n\nCNNs learn hierarchical visual features through layers of convolution operations:\n\n- **Early layers** detect edges and textures—horizontal lines, vertical lines, gradients\n- **Middle layers** combine edges into shapes—corners, curves, patterns\n- **Deep layers** recognize objects—faces, cars, animals\n\nEach convolutional filter acts as a pattern detector, sliding across the image and activating when it finds its learned pattern. Through training on millions of labeled images, these filters learn increasingly abstract representations.\n\n### Vision Transformers (ViT)\n\nVision Transformers apply the same attention mechanism from text to images by:\n\n1. **Splitting the image into patches** (e.g., 16×16 pixels each)\n2. **Treating each patch as a \"token\"** (like words in text)\n3. **Applying transformer layers** to learn relationships between patches\n\nThis allows ViT to capture long-range dependencies—understanding that distant parts of an image are related—which CNNs struggle with due to their local receptive fields.\n\n### Transfer Learning\n\nMost applications don't train image models from scratch. Instead:\n\n1. Start with a model pre-trained on ImageNet (1.4M labeled images)\n2. Remove the classification head to get feature vectors\n3. Optionally fine-tune on your domain-specific data\n\nThis works because low-level features (edges, textures) transfer across domains—a model that learned to detect edges on ImageNet can detect edges in medical images or satellite photos.\n\n## Key Takeaways\n\n- **Image embeddings** capture visual semantics—similar objects, colors, and textures cluster together regardless of exact pixel values\n- **Audio embeddings** capture acoustic properties—timbre, rhythm, and spectral features that identify sounds across variations\n- **Video embeddings** combine spatial and temporal information—understanding both what's in each frame and how things change over time\n- **CNNs** learn hierarchical features through convolution layers; **Vision Transformers** use attention to capture long-range relationships\n- **Transfer learning** is standard practice—start with pre-trained models and adapt to your domain\n\n## Looking Ahead\n\nNow that you understand single-modality embeddings for text, images, audio, and video, @sec-multimodal-embeddings explores how to combine these modalities into unified multi-modal representations.\n\n## Further Reading\n\n- He, K., et al. (2016). \"Deep Residual Learning for Image Recognition.\" *CVPR*\n- Dosovitskiy, A., et al. (2020). \"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.\" *ICLR*\n- Gemmeke, J., et al. (2017). \"Audio Set: An ontology and human-labeled dataset for audio events.\" *ICASSP*\n- Feichtenhofer, C., et al. (2019). \"SlowFast Networks for Video Recognition.\" *ICCV*\n\n",
    "supporting": [
      "ch05_image_video_embeddings_files/figure-epub"
    ],
    "filters": [],
    "engineDependencies": {
      "jupyter": [
        {
          "jsWidgets": false,
          "jupyterWidgets": false,
          "htmlLibraries": []
        }
      ]
    }
  }
}