{
  "hash": "db1a85dfb73b7740668b7ac8d9ba7dd3",
  "result": {
    "engine": "jupyter",
    "markdown": "# Image, Audio, and Video Embeddings {#sec-image-video-embeddings}\n\n::: callout-note\n## Chapter Overview\n\nThis chapter covers embeddings for visual and audio data—images, audio, and video. We explore how these modalities are represented as vectors, when to use each type, and the architectures that create them. An optional advanced section explains how the underlying models learn visual features.\n:::\n\n## Image Embeddings {#sec-image-embedding-types}\n\nImage embeddings convert visual content into vectors that capture visual semantics—shapes, colors, textures, objects, and spatial relationships. Unlike pixel-by-pixel comparison, embeddings understand that two photos of the same cat are similar even if taken from different angles or lighting conditions.\n\n### How Images Are Represented\n\nDigital images are stored as 3D arrays with shape (height, width, 3)—the third dimension holds Red, Green, and Blue color intensities for each pixel:\n\n::: {#cell-fig-image-rgb-cube .cell execution_count=1}\n\n::: {.cell-output .cell-output-display}\n![An image is a 3D array: height × width × 3 color channels (Red, Green, Blue). Each pixel has three values (0-255).](ch05_image_video_embeddings_files/figure-html/fig-image-rgb-cube-output-1.png){#fig-image-rgb-cube width=456 height=469}\n:::\n:::\n\n\n### Creating Image Embeddings\n\nThe example below uses ResNet50 [@he2016resnet], a CNN pre-trained on ImageNet's 1.4 million images. ResNet learns hierarchical visual features—early layers detect edges and textures, middle layers recognize shapes and parts, and deep layers understand objects and scenes. We remove the final classification layer to extract the 2048-dimensional feature vector as our embedding.\n\n::: {#fa6478e2 .cell execution_count=2}\n``` {.python .cell-code code-fold=\"false\"}\n\"\"\"\nImage Embeddings: Visual Content as Vectors\n\"\"\"\n\nimport torch\nimport numpy as np\nfrom PIL import Image\nfrom torchvision import models, transforms\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nimport logging\nlogging.getLogger('torch').setLevel(logging.ERROR)\n\nfrom torchvision.models import ResNet50_Weights\n\n# Load pretrained ResNet50 as feature extractor\nweights = ResNet50_Weights.IMAGENET1K_V1\nmodel = models.resnet50(weights=None)\nmodel.load_state_dict(weights.get_state_dict(progress=False))\nmodel.eval()\n\n# Remove classification head to get embeddings\nfeature_extractor = torch.nn.Sequential(*list(model.children())[:-1])\n\n# Standard ImageNet preprocessing\npreprocess = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\ndef get_image_embedding(image):\n    \"\"\"Extract embedding from PIL Image.\"\"\"\n    tensor = preprocess(image).unsqueeze(0)\n    with torch.no_grad():\n        embedding = feature_extractor(tensor)\n    return embedding.squeeze().numpy()\n\n# Create synthetic test images with different color patterns\nnp.random.seed(42)\nimages = {\n    'red_pattern': Image.fromarray(\n        np.random.randint([180, 0, 0], [255, 80, 80], (224, 224, 3), dtype=np.uint8)\n    ),\n    'blue_pattern': Image.fromarray(\n        np.random.randint([0, 0, 180], [80, 80, 255], (224, 224, 3), dtype=np.uint8)\n    ),\n    'orange_pattern': Image.fromarray(\n        np.random.randint([200, 100, 0], [255, 150, 50], (224, 224, 3), dtype=np.uint8)\n    ),\n}\n\n# Get embeddings\nembeddings = {name: get_image_embedding(img) for name, img in images.items()}\n\nprint(\"Image embedding similarities:\\n\")\nprint(\"Red and orange (similar warm colors) should be more similar than red and blue:\")\nred_orange = cosine_similarity([embeddings['red_pattern']], [embeddings['orange_pattern']])[0][0]\nred_blue = cosine_similarity([embeddings['red_pattern']], [embeddings['blue_pattern']])[0][0]\nprint(f\"  red ↔ orange: {red_orange:.3f}\")\nprint(f\"  red ↔ blue:   {red_blue:.3f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nImage embedding similarities:\n\nRed and orange (similar warm colors) should be more similar than red and blue:\n  red ↔ orange: 0.898\n  red ↔ blue:   0.717\n```\n:::\n:::\n\n\n::: {.callout-note}\n## RGB Input vs Embedding Output\nDon't confuse the 3-channel RGB input with the embedding output. ResNet50 takes the 3 RGB channels as input but produces a **2048-dimensional** embedding vector that captures far more than color: edges, textures, shapes, and high-level visual concepts learned from millions of images.\n:::\n\n### When to Use Image Embeddings\n\n- **Visual recommendation systems**—suggest visually similar items (see @sec-recommendation-systems)\n- **Content moderation**—detect variations of prohibited images\n- **Reverse image search**—find similar products, people, or objects\n- **Medical imaging**—find similar X-rays, scans, or pathology slides (see @sec-healthcare-life-sciences)\n- **Quality control**—detect defects by comparing to reference images (see @sec-manufacturing-industry40)\n- **Face recognition**—identify or verify individuals (see @sec-video-surveillance)\n\n### Popular Image Architectures\n\n| Architecture | Type | Strengths | Use Cases |\n|-------------|------|-----------|-----------|\n| [ResNet](https://pytorch.org/vision/main/models/resnet.html) | CNN | Fast, proven | General visual search |\n| [EfficientNet](https://pytorch.org/vision/main/models/efficientnet.html) | CNN | Efficient, accurate | Mobile/edge deployment |\n| [ViT](https://pytorch.org/vision/main/models/vision_transformer.html) | Transformer | Best accuracy | High-quality requirements |\n| [CLIP](https://github.com/openai/CLIP) | Multi-modal | Text-image alignment | Zero-shot classification |\n\n: Image embedding architectures {.striped}\n\n## Audio Embeddings {#sec-audio-embedding-types}\n\nAudio embeddings convert sound into vectors that capture acoustic properties—pitch, timbre, rhythm, and spectral characteristics. Unlike raw waveform comparison, embeddings understand that two recordings of the same spoken word are similar even with different speakers, background noise, or recording equipment.\n\n### How Audio Is Represented\n\nAudio is processed by first splitting into short overlapping frames (typically 25ms), then extracting features from each frame. MFCC (Mel-frequency cepstral coefficients) is a classic technique that transforms audio into a representation mimicking human hearing.\n\n::: {#ec4cec47 .cell execution_count=3}\n``` {.python .cell-code code-fold=\"false\"}\n\"\"\"\nAudio Embeddings: Sound as Vectors\n\"\"\"\n\nimport numpy as np\nimport librosa\nfrom sklearn.metrics.pairwise import cosine_similarity\n\ndef audio_to_embedding(audio, sr, n_mfcc=20):\n    \"\"\"Convert audio waveform to a fixed-size embedding using MFCCs.\"\"\"\n    # Extract MFCCs: 20 coefficients per time frame\n    mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=n_mfcc)\n    # Aggregate over time: mean captures average timbre, std captures variation\n    return np.concatenate([mfccs.mean(axis=1), mfccs.std(axis=1)])\n\n# Load librosa's built-in trumpet sample\naudio, sr = librosa.load(librosa.ex('trumpet'))\ntrumpet_embedding = audio_to_embedding(audio, sr)\n\n# Create variations to demonstrate similarity\ntrumpet_slow = librosa.effects.time_stretch(audio, rate=0.8)\ntrumpet_pitch_up = librosa.effects.pitch_shift(audio, sr=sr, n_steps=12)\n\nembeddings = {\n    'trumpet_original': trumpet_embedding,\n    'trumpet_slower': audio_to_embedding(trumpet_slow, sr),\n    'trumpet_higher_pitch': audio_to_embedding(trumpet_pitch_up, sr),\n}\n\nprint(f\"Embedding dimension: {len(trumpet_embedding)} (20 MFCCs × 2 stats)\\n\")\n\nprint(\"Audio embedding similarities:\")\nsim_slow = cosine_similarity([embeddings['trumpet_original']], [embeddings['trumpet_slower']])[0][0]\nprint(f\"  Original ↔ Slower tempo:   {sim_slow:.3f}\")\n\nsim_pitch = cosine_similarity([embeddings['trumpet_original']], [embeddings['trumpet_higher_pitch']])[0][0]\nprint(f\"  Original ↔ Higher pitch:   {sim_pitch:.3f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nEmbedding dimension: 40 (20 MFCCs × 2 stats)\n\nAudio embedding similarities:\n  Original ↔ Slower tempo:   1.000\n  Original ↔ Higher pitch:   0.978\n```\n:::\n:::\n\n\n### When to Use Audio Embeddings\n\n- **Speaker identification**—match voices regardless of what's said\n- **Music recommendation**—find similar songs by acoustic features\n- **Audio fingerprinting**—identify songs from short clips\n- **Sound event detection**—classify environmental sounds\n- **Voice cloning detection**—identify synthetic speech\n\n### Popular Audio Architectures\n\n| Architecture | Type | Strengths | Use Cases |\n|-------------|------|-----------|-----------|\n| [Wav2Vec2](https://huggingface.co/facebook/wav2vec2-base) | Self-supervised | Rich speech features | Speech recognition |\n| [Whisper](https://github.com/openai/whisper) | Multi-task | Transcription + embeddings | Speech search |\n| [CLAP](https://github.com/LAION-AI/CLAP) | Multi-modal | Audio-text alignment | Zero-shot classification |\n\n: Audio embedding architectures {.striped}\n\n## Video Embeddings {#sec-video-embedding-types}\n\nVideo embeddings convert video clips into vectors that capture both visual content and temporal dynamics—actions, motion patterns, scene transitions, and narrative flow. Unlike image embeddings that capture a single moment, video embeddings understand that \"sitting down\" and \"standing up\" are different actions even though individual frames might look similar.\n\n### How Video Is Represented\n\nVideos add a fourth dimension to images: time. A video is a sequence of frames, each of which is an image. The simplest approach extracts image embeddings from sampled frames and aggregates them:\n\n::: {#531425ea .cell execution_count=4}\n``` {.python .cell-code code-fold=\"false\"}\n\"\"\"\nVideo Embeddings: From Clips to Vectors\n\"\"\"\n\nimport torch\nimport numpy as np\nfrom torchvision import models, transforms\nfrom torchvision.models import ResNet50_Weights\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nimport logging\nlogging.getLogger('torch').setLevel(logging.ERROR)\n\n# Load pretrained ResNet50 as frame feature extractor\nweights = ResNet50_Weights.IMAGENET1K_V1\nmodel = models.resnet50(weights=None)\nmodel.load_state_dict(weights.get_state_dict(progress=False))\nmodel.eval()\nfeature_extractor = torch.nn.Sequential(*list(model.children())[:-1])\n\npreprocess = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\ndef frames_to_video_embedding(frames):\n    \"\"\"Convert a list of frames to a video embedding.\"\"\"\n    frame_embeddings = []\n    for frame in frames:\n        if frame.dtype == np.uint8:\n            frame = frame.astype(np.float32) / 255.0\n        tensor = preprocess(frame).unsqueeze(0)\n        with torch.no_grad():\n            emb = feature_extractor(tensor).squeeze().numpy()\n        frame_embeddings.append(emb)\n    return np.mean(frame_embeddings, axis=0)\n\n# Simulate video frames\nrunning_frames = [np.full((224, 224, 3), [0.2, 0.6 + i*0.05, 0.2], dtype=np.float32) for i in range(5)]\ncooking_frames = [np.full((224, 224, 3), [0.8, 0.4 + i*0.02, 0.1], dtype=np.float32) for i in range(5)]\njogging_frames = [np.full((224, 224, 3), [0.25, 0.55 + i*0.05, 0.25], dtype=np.float32) for i in range(5)]\n\nembeddings = {\n    'running': frames_to_video_embedding(running_frames),\n    'cooking': frames_to_video_embedding(cooking_frames),\n    'jogging': frames_to_video_embedding(jogging_frames),\n}\n\nprint(f\"Video embedding dimension: {len(embeddings['running'])}\\n\")\nprint(\"Video embedding similarities:\")\n\nrun_jog = cosine_similarity([embeddings['running']], [embeddings['jogging']])[0][0]\nrun_cook = cosine_similarity([embeddings['running']], [embeddings['cooking']])[0][0]\nprint(f\"  Running ↔ Jogging: {run_jog:.3f}  (similar outdoor scenes)\")\nprint(f\"  Running ↔ Cooking: {run_cook:.3f}  (different scenes)\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nVideo embedding dimension: 2048\n\nVideo embedding similarities:\n  Running ↔ Jogging: 0.995  (similar outdoor scenes)\n  Running ↔ Cooking: 0.807  (different scenes)\n```\n:::\n:::\n\n\n### When to Use Video Embeddings\n\n- **Action recognition**—classify activities in videos\n- **Video recommendation**—find similar content\n- **Surveillance**—detect anomalies and track activities (see @sec-video-surveillance)\n- **Sports analytics**—analyze plays and techniques\n- **Content moderation**—detect prohibited video content\n\n### Popular Video Architectures\n\n| Architecture | Type | Strengths | Use Cases |\n|-------------|------|-----------|-----------|\n| Frame sampling + CNN | Simple | Fast, no motion | Scene classification |\n| [SlowFast](https://github.com/facebookresearch/SlowFast) | 3D CNN | Captures motion | Action recognition |\n| [Video Swin](https://github.com/SwinTransformer/Video-Swin-Transformer) | Transformer | Best accuracy | High-quality needs |\n\n: Video embedding architectures {.striped}\n\n## Advanced: How Visual Models Learn {.unnumbered}\n\n::: {.callout-note}\n## Optional Section\nThis section explains how image embedding models learn visual features. Understanding these fundamentals helps you choose the right model and fine-tune for your domain. Skip this if you just need to use embeddings.\n:::\n\n### Convolutional Neural Networks (CNNs)\n\nCNNs learn hierarchical visual features through layers of convolution operations:\n\n- **Early layers** detect edges and textures—horizontal lines, vertical lines, gradients\n- **Middle layers** combine edges into shapes—corners, curves, patterns\n- **Deep layers** recognize objects—faces, cars, animals\n\nEach convolutional filter acts as a pattern detector, sliding across the image and activating when it finds its learned pattern. Through training on millions of labeled images, these filters learn increasingly abstract representations.\n\n### Vision Transformers (ViT)\n\nVision Transformers apply the same attention mechanism from text to images by:\n\n1. **Splitting the image into patches** (e.g., 16×16 pixels each)\n2. **Treating each patch as a \"token\"** (like words in text)\n3. **Applying transformer layers** to learn relationships between patches\n\nThis allows ViT to capture long-range dependencies—understanding that distant parts of an image are related—which CNNs struggle with due to their local receptive fields.\n\n### Transfer Learning\n\nMost applications don't train image models from scratch. Instead:\n\n1. Start with a model pre-trained on ImageNet (1.4M labeled images)\n2. Remove the classification head to get feature vectors\n3. Optionally fine-tune on your domain-specific data\n\nThis works because low-level features (edges, textures) transfer across domains—a model that learned to detect edges on ImageNet can detect edges in medical images or satellite photos.\n\n## Key Takeaways\n\n- **Image embeddings** capture visual semantics—similar objects, colors, and textures cluster together regardless of exact pixel values\n\n- **Audio embeddings** capture acoustic properties—timbre, rhythm, and spectral features that identify sounds across variations\n\n- **Video embeddings** combine spatial and temporal information—understanding both what's in each frame and how things change over time\n\n- **CNNs** learn hierarchical features through convolution layers; **Vision Transformers** use attention to capture long-range relationships\n\n- **Transfer learning** is standard practice—start with pre-trained models and adapt to your domain\n\n## Looking Ahead\n\nNow that you understand single-modality embeddings for text, images, audio, and video, @sec-multimodal-embeddings explores how to combine these modalities into unified multi-modal representations.\n\n## Further Reading\n\n- He, K., et al. (2016). \"Deep Residual Learning for Image Recognition.\" *CVPR*\n- Dosovitskiy, A., et al. (2020). \"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.\" *ICLR*\n- Gemmeke, J., et al. (2017). \"Audio Set: An ontology and human-labeled dataset for audio events.\" *ICASSP*\n- Feichtenhofer, C., et al. (2019). \"SlowFast Networks for Video Recognition.\" *ICCV*\n\n",
    "supporting": [
      "ch05_image_video_embeddings_files"
    ],
    "filters": [],
    "includes": {}
  }
}