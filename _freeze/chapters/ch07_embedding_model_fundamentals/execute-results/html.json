{
  "hash": "c7ba8c58fb8d61db0f29eab16ed8b251",
  "result": {
    "engine": "jupyter",
    "markdown": "# How Embedding Models Learn {#sec-embedding-model-fundamentals}\n\n::: callout-note\n## Chapter Overview\n\nThis chapter explains how the embedding models you use actually learn to create meaningful vector representations. We start with Word2Vec—historically important and conceptually simple—then progress to transformer-based models like BERT that dominate today. Understanding these fundamentals helps you choose the right model for your use case and diagnose issues when embeddings don't behave as expected.\n:::\n\n## The Learning Problem\n\nBefore diving into specific architectures, let's understand what embedding models are actually learning. The core insight is deceptively simple: **words (or images, or any objects) that appear in similar contexts should have similar embeddings**.\n\nConsider these sentences:\n- \"The **cat** sat on the mat\"\n- \"The **dog** sat on the rug\"\n- \"A **kitten** played on the floor\"\n\nA model that sees millions of such sentences learns that \"cat,\" \"dog,\" and \"kitten\" appear in similar contexts (near words like \"sat,\" \"played,\" \"mat,\" \"rug\"). Without any explicit labeling, the model discovers that these are all animals, and positions them close together in embedding space.\n\nThis is the foundation of all embedding models: **learning from context**.\n\n## Word2Vec: The Breakthrough {#sec-word2vec}\n\nWord2Vec, introduced by Mikolov et al. in 2013 [@mikolov2013efficient], revolutionized natural language processing by showing that simple neural networks could learn rich semantic representations from raw text. Despite its simplicity, Word2Vec remains influential and helps build intuition for more complex models.\n\n### The Skip-Gram Architecture\n\nWord2Vec's skip-gram model learns embeddings by predicting context words given a target word. Here's the intuition:\n\n```\nTraining sentence: \"The cat sat on the mat\"\n\nIf target word = \"sat\", predict context words within a window:\n  - \"The\" (2 words before)\n  - \"cat\" (1 word before)\n  - \"on\"  (1 word after)\n  - \"the\" (2 words after)\n```\n\nThe model learns embeddings such that words appearing in similar contexts get similar vectors.\n\n::: {#5854da93 .cell execution_count=1}\n``` {.python .cell-code code-fold=\"false\"}\n\"\"\"\nWord2Vec Skip-Gram: Simplified Implementation\n\nThis demonstrates the core concept of Word2Vec's skip-gram model.\nIn practice, you'd use optimized libraries like Gensim, but this\nshows what's happening under the hood.\n\"\"\"\n\nimport numpy as np\n\n# Vocabulary and hyperparameters\nvocab = [\"the\", \"cat\", \"sat\", \"on\", \"mat\", \"dog\", \"rug\", \"a\", \"kitten\", \"played\"]\nvocab_size = len(vocab)\nembedding_dim = 4  # Small for illustration; real models use 100-300\nword_to_idx = {w: i for i, w in enumerate(vocab)}\n\n# Initialize two embedding matrices:\n# - W_target: embeddings for target words\n# - W_context: embeddings for context words\nnp.random.seed(42)\nW_target = np.random.randn(vocab_size, embedding_dim) * 0.1\nW_context = np.random.randn(vocab_size, embedding_dim) * 0.1\n\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n\n\ndef train_skipgram_pair(target_word, context_word, negative_words, lr=0.1):\n    \"\"\"\n    Train on one (target, context) pair with negative sampling.\n\n    The model learns to:\n    1. Maximize similarity between target and true context\n    2. Minimize similarity between target and random (negative) words\n    \"\"\"\n    global W_target, W_context\n\n    t_idx = word_to_idx[target_word]\n    c_idx = word_to_idx[context_word]\n\n    # Get embeddings\n    target_emb = W_target[t_idx]\n    context_emb = W_context[c_idx]\n\n    # Positive example: target and context should be similar\n    score = np.dot(target_emb, context_emb)\n    pred = sigmoid(score)\n\n    # Gradient for positive pair (label = 1)\n    grad = (pred - 1) * context_emb\n    W_target[t_idx] -= lr * grad\n    W_context[c_idx] -= lr * (pred - 1) * target_emb\n\n    # Negative examples: target and random words should be dissimilar\n    for neg_word in negative_words:\n        n_idx = word_to_idx[neg_word]\n        neg_emb = W_context[n_idx]\n\n        score = np.dot(target_emb, neg_emb)\n        pred = sigmoid(score)\n\n        # Gradient for negative pair (label = 0)\n        W_target[t_idx] -= lr * pred * neg_emb\n        W_context[n_idx] -= lr * pred * target_emb\n\n\n# Training corpus (simplified)\ncorpus = [\n    [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"],\n    [\"the\", \"dog\", \"sat\", \"on\", \"the\", \"rug\"],\n    [\"a\", \"kitten\", \"played\", \"on\", \"the\", \"mat\"],\n]\n\n# Train for a few epochs\nwindow_size = 2\nfor epoch in range(100):\n    for sentence in corpus:\n        for i, target in enumerate(sentence):\n            # Get context words within window\n            start = max(0, i - window_size)\n            end = min(len(sentence), i + window_size + 1)\n            context_words = [sentence[j] for j in range(start, end) if j != i]\n\n            # Get negative samples (words not in context)\n            negatives = [w for w in vocab if w not in context_words and w != target][:3]\n\n            for context in context_words:\n                train_skipgram_pair(target, context, negatives)\n\n# After training, similar words should have similar embeddings\ndef cosine_similarity(w1, w2):\n    v1, v2 = W_target[word_to_idx[w1]], W_target[word_to_idx[w2]]\n    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n\nprint(\"Learned similarities (after training on tiny corpus):\")\nprint(f\"  cat ↔ dog:    {cosine_similarity('cat', 'dog'):.3f}\")\nprint(f\"  cat ↔ kitten: {cosine_similarity('cat', 'kitten'):.3f}\")\nprint(f\"  sat ↔ played: {cosine_similarity('sat', 'played'):.3f}\")\nprint(f\"  cat ↔ mat:    {cosine_similarity('cat', 'mat'):.3f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLearned similarities (after training on tiny corpus):\n  cat ↔ dog:    0.999\n  cat ↔ kitten: -0.120\n  sat ↔ played: 0.783\n  cat ↔ mat:    0.273\n```\n:::\n:::\n\n\n### Why Word2Vec Works\n\nThe magic of Word2Vec comes from a simple observation: if we force the model to predict context from target words (or vice versa), the learned embeddings must encode semantic meaning. Words that can substitute for each other in sentences end up with similar embeddings.\n\n**Key innovations:**\n\n1. **Negative sampling**: Instead of computing softmax over the entire vocabulary (expensive), sample a few \"negative\" words and train the model to distinguish real context from random words.\n\n2. **Subsampling frequent words**: Common words like \"the\" and \"a\" provide less information, so they're downsampled during training.\n\n3. **Vector arithmetic**: A surprising emergent property—relationships are encoded as vector offsets:\n   ```\n   king - man + woman ≈ queen\n   Paris - France + Italy ≈ Rome\n   ```\n\n### Limitations of Word2Vec\n\nWhile groundbreaking, Word2Vec has significant limitations:\n\n- **Static embeddings**: Each word has one embedding, regardless of context. \"Bank\" (financial) and \"bank\" (river) share the same vector.\n- **Out-of-vocabulary words**: Words not seen during training have no embedding.\n- **Limited context window**: Only considers nearby words, missing long-range dependencies.\n\nThese limitations motivated the development of contextual embeddings.\n\n## From Words to Sentences: Extending Word2Vec\n\nBefore transformers, several approaches extended word embeddings to longer text:\n\n### GloVe (Global Vectors)\n\nGloVe [@pennington2014glove] improves on Word2Vec by explicitly factorizing the word co-occurrence matrix. While Word2Vec processes text sequentially, GloVe computes global statistics first, then learns embeddings that preserve co-occurrence patterns.\n\n```python\n# GloVe is typically used via pre-trained vectors\nfrom gensim.models import KeyedVectors\n\n# Load pre-trained GloVe (converted to Word2Vec format)\n# glove = KeyedVectors.load_word2vec_format('glove.6B.100d.txt', no_header=True)\n# similar = glove.most_similar('king')\n```\n\n### Doc2Vec\n\nDoc2Vec extends Word2Vec to documents by adding a \"paragraph vector\" that's trained alongside word vectors. Each document gets a unique embedding that captures its overall meaning.\n\n### Limitations of Pre-Transformer Approaches\n\nAll these approaches share a fundamental limitation: **the embedding is computed independently of the specific context**. The word \"bank\" gets the same embedding whether it appears in \"river bank\" or \"bank account.\"\n\n## Transformers and BERT: Contextual Embeddings {#sec-transformers-bert}\n\nThe transformer architecture [@vaswani2017attention], introduced in 2017, and BERT [@devlin2018bert] (2018) fundamentally changed how we create embeddings. The key innovation: **attention mechanisms** that allow each word to attend to all other words in the input.\n\n### The Attention Mechanism\n\nAttention answers the question: \"When processing word X, how much should I focus on each other word in the sentence?\"\n\n::: {#5f3b10bd .cell execution_count=2}\n``` {.python .cell-code code-fold=\"false\"}\n\"\"\"\nSimplified Self-Attention Mechanism\n\nThis demonstrates the core concept of transformer attention.\nReal implementations include multiple heads, layer normalization,\nand many optimizations—but this captures the essence.\n\"\"\"\n\nimport numpy as np\n\ndef softmax(x, axis=-1):\n    exp_x = np.exp(x - np.max(x, axis=axis, keepdims=True))\n    return exp_x / np.sum(exp_x, axis=axis, keepdims=True)\n\n\ndef self_attention(embeddings, W_query, W_key, W_value):\n    \"\"\"\n    Self-attention: each position attends to all positions.\n\n    Args:\n        embeddings: (seq_len, embed_dim) - input embeddings\n        W_query, W_key, W_value: projection matrices\n\n    Returns:\n        Contextualized embeddings where each position incorporates\n        information from all other positions.\n    \"\"\"\n    # Project embeddings to queries, keys, and values\n    queries = embeddings @ W_query  # What am I looking for?\n    keys = embeddings @ W_key       # What do I contain?\n    values = embeddings @ W_value   # What information do I provide?\n\n    # Compute attention scores: how much should each position\n    # attend to each other position?\n    d_k = keys.shape[-1]\n    scores = (queries @ keys.T) / np.sqrt(d_k)  # Scale for stability\n\n    # Softmax to get attention weights (each row sums to 1)\n    attention_weights = softmax(scores, axis=-1)\n\n    # Weighted sum of values\n    output = attention_weights @ values\n\n    return output, attention_weights\n\n\n# Example: process \"The bank by the river\"\nsentence = [\"The\", \"bank\", \"by\", \"the\", \"river\"]\nseq_len = len(sentence)\nembed_dim = 4\nhidden_dim = 4\n\n# Random initial embeddings (in practice, these are learned)\nnp.random.seed(42)\nembeddings = np.random.randn(seq_len, embed_dim)\n\n# Random projection matrices (in practice, these are learned)\nW_q = np.random.randn(embed_dim, hidden_dim) * 0.1\nW_k = np.random.randn(embed_dim, hidden_dim) * 0.1\nW_v = np.random.randn(embed_dim, hidden_dim) * 0.1\n\n# Apply self-attention\noutput, attn_weights = self_attention(embeddings, W_q, W_k, W_v)\n\nprint(\"Attention weights (which words attend to which):\")\nprint(\"Rows = query word, Columns = key word\")\nprint(f\"Words: {sentence}\\n\")\nfor i, word in enumerate(sentence):\n    weights = [f\"{w:.2f}\" for w in attn_weights[i]]\n    print(f\"  {word:6s} attends to: {weights}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAttention weights (which words attend to which):\nRows = query word, Columns = key word\nWords: ['The', 'bank', 'by', 'the', 'river']\n\n  The    attends to: ['0.21', '0.20', '0.20', '0.20', '0.19']\n  bank   attends to: ['0.21', '0.21', '0.20', '0.20', '0.19']\n  by     attends to: ['0.20', '0.20', '0.20', '0.20', '0.20']\n  the    attends to: ['0.19', '0.19', '0.21', '0.21', '0.21']\n  river  attends to: ['0.19', '0.20', '0.20', '0.20', '0.21']\n```\n:::\n:::\n\n\n### How BERT Creates Contextual Embeddings\n\nBERT stacks multiple transformer layers, each applying self-attention. The key insight: **the same word gets different embeddings depending on its context**.\n\n```\nInput:  \"I went to the bank to deposit money\"\n        \"I sat on the bank of the river\"\n\nWord2Vec: \"bank\" → same embedding in both sentences\n\nBERT:    \"bank\" (sentence 1) → embedding influenced by \"deposit\", \"money\"\n         \"bank\" (sentence 2) → embedding influenced by \"river\", \"sat\"\n```\n\n### BERT's Training Objectives\n\nBERT learns through two self-supervised tasks:\n\n**1. Masked Language Modeling (MLM)**\n\nRandomly mask 15% of tokens and train the model to predict them:\n\n```\nInput:  \"The cat [MASK] on the mat\"\nTarget: \"sat\"\n```\n\nThis forces the model to understand context deeply—it must use surrounding words to infer the masked word.\n\n**2. Next Sentence Prediction (NSP)**\n\nGiven two sentences, predict whether the second follows the first in the original text. This helps the model understand document-level coherence.\n\n::: {#bfe93af1 .cell execution_count=3}\n``` {.python .cell-code code-fold=\"false\"}\n\"\"\"\nDemonstrating BERT's Contextual Embeddings\n\nUsing a real BERT model to show how the same word gets different\nembeddings in different contexts.\n\"\"\"\n\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport numpy as np\n\n# Load a sentence transformer (built on BERT architecture)\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\n\n# Same word, different contexts\nsentences = [\n    \"I deposited money at the bank\",      # bank = financial institution\n    \"The bank approved my loan\",          # bank = financial institution\n    \"We had a picnic on the river bank\",  # bank = riverside\n    \"Fish swim near the bank\",            # bank = riverside\n]\n\nembeddings = model.encode(sentences)\n\nprint(\"Contextual similarity (same word, different meanings):\\n\")\nlabels = [\"financial-1\", \"financial-2\", \"river-1\", \"river-2\"]\nfor i in range(len(sentences)):\n    for j in range(i + 1, len(sentences)):\n        sim = cosine_similarity([embeddings[i]], [embeddings[j]])[0][0]\n        print(f\"  {labels[i]:12s} ↔ {labels[j]:12s}: {sim:.3f}\")\n\nprint(\"\\nNotice: financial contexts cluster together, river contexts cluster together\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nContextual similarity (same word, different meanings):\n\n  financial-1  ↔ financial-2 : 0.486\n  financial-1  ↔ river-1     : 0.416\n  financial-1  ↔ river-2     : 0.286\n  financial-2  ↔ river-1     : 0.288\n  financial-2  ↔ river-2     : 0.248\n  river-1      ↔ river-2     : 0.382\n\nNotice: financial contexts cluster together, river contexts cluster together\n```\n:::\n:::\n\n\n### Why Transformers Dominate\n\nTransformers offer several advantages over previous architectures:\n\n1. **Parallelization**: Unlike RNNs, all positions can be processed simultaneously\n2. **Long-range dependencies**: Attention can directly connect distant words\n3. **Transfer learning**: Pre-trained models work well across many tasks\n4. **Scalability**: Performance improves predictably with more data and compute\n\n## Sentence Transformers: From BERT to Embeddings {#sec-sentence-transformers}\n\nWhile BERT produces contextual word embeddings, we often need a single embedding for an entire sentence or document. Sentence Transformers [@reimers2019sentence] fine-tune BERT-like models specifically for this purpose.\n\n### The Training Process\n\nSentence Transformers use **contrastive learning** (covered in depth in @sec-contrastive-learning):\n\n1. Take pairs of similar sentences (e.g., paraphrases, question-answer pairs)\n2. Train the model to produce similar embeddings for related sentences\n3. Push apart embeddings of unrelated sentences\n\n::: {#e506d581 .cell execution_count=4}\n``` {.python .cell-code code-fold=\"false\"}\n\"\"\"\nSentence Transformers in Action\n\nShowing how sentence-level embeddings capture semantic similarity\neven when surface forms differ significantly.\n\"\"\"\n\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\n\n# Semantically similar sentences with different words\nsentence_pairs = [\n    (\"The cat is sleeping on the couch\", \"A feline rests on the sofa\"),\n    (\"How do I reset my password?\", \"I forgot my login credentials\"),\n    (\"The stock market crashed today\", \"Financial markets saw major losses\"),\n]\n\nprint(\"Semantic similarity (different words, same meaning):\\n\")\nfor s1, s2 in sentence_pairs:\n    emb1, emb2 = model.encode([s1, s2])\n    sim = cosine_similarity([emb1], [emb2])[0][0]\n    print(f\"  \\\"{s1[:40]}...\\\"\")\n    print(f\"  \\\"{s2[:40]}...\\\"\")\n    print(f\"  Similarity: {sim:.3f}\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSemantic similarity (different words, same meaning):\n\n  \"The cat is sleeping on the couch...\"\n  \"A feline rests on the sofa...\"\n  Similarity: 0.501\n\n  \"How do I reset my password?...\"\n  \"I forgot my login credentials...\"\n  Similarity: 0.678\n\n  \"The stock market crashed today...\"\n  \"Financial markets saw major losses...\"\n  Similarity: 0.631\n\n```\n:::\n:::\n\n\n## Image Embeddings: CNNs and Vision Transformers {#sec-image-embedding-models}\n\nImage embedding models follow similar principles but with architectures suited to visual data.\n\n### Convolutional Neural Networks (CNNs)\n\nCNNs like ResNet learn hierarchical visual features:\n- Early layers detect edges and textures\n- Middle layers recognize shapes and patterns\n- Later layers identify objects and scenes\n\nThe embedding comes from the layer just before the final classification head—a dense vector capturing visual semantics.\n\n### Vision Transformers (ViT)\n\nVision Transformers apply the same attention mechanism to images by:\n1. Splitting the image into patches (e.g., 16x16 pixels)\n2. Treating each patch as a \"token\" (like words in text)\n3. Applying transformer layers to learn relationships between patches\n\n### Multi-Modal Models: CLIP\n\nCLIP [@radford2021learning] learns joint embeddings for text and images by training on 400 million image-caption pairs. It learns to:\n- Place images near their text descriptions\n- Place text descriptions near matching images\n\nThis enables powerful zero-shot capabilities—CLIP can match images to text descriptions it's never seen during training.\n\n## Choosing the Right Model\n\nDifferent use cases call for different embedding models:\n\n| Use Case | Recommended Model Type | Why |\n|----------|----------------------|-----|\n| **Keyword search** | BM25 (not embeddings) | Exact matching still wins for precise queries |\n| **Semantic search** | Sentence Transformers | Captures meaning beyond keywords |\n| **Multilingual** | Multilingual models (mBERT, LaBSE) | Aligned cross-lingual embeddings |\n| **Domain-specific** | Fine-tuned models | Generic models miss domain vocabulary |\n| **Image similarity** | ResNet, ViT, CLIP | Visual feature extraction |\n| **Image + text** | CLIP, BLIP | Unified multi-modal space |\n\n: Model selection guide {.striped}\n\n## Key Takeaways\n\n- **Word2Vec** learns embeddings by predicting context words, capturing semantic relationships through co-occurrence patterns\n\n- **Transformers** use attention mechanisms to create contextual embeddings where the same word gets different representations based on surrounding context\n\n- **BERT** learns through masked language modeling—predicting masked words forces deep contextual understanding\n\n- **Sentence Transformers** adapt BERT for producing fixed-size embeddings for entire sentences, trained with contrastive learning\n\n- **Image models** (CNNs, ViT) learn visual features hierarchically; CLIP extends this to joint text-image embeddings\n\n- **Model choice** depends on your use case: domain, modality, language requirements, and whether you need contextual understanding\n\n## Looking Ahead\n\nNow that you understand how embedding models work internally, @sec-custom-embedding-strategies covers when and how to customize these models for your specific domain and use case.\n\n## Further Reading\n\n- Mikolov, T., et al. (2013). \"Efficient Estimation of Word Representations in Vector Space.\" *arXiv:1301.3781* — The original Word2Vec paper\n- Pennington, J., et al. (2014). \"GloVe: Global Vectors for Word Representation.\" *EMNLP* — GloVe's approach to word embeddings\n- Vaswani, A., et al. (2017). \"Attention Is All You Need.\" *NeurIPS* — The transformer architecture\n- Devlin, J., et al. (2018). \"BERT: Pre-training of Deep Bidirectional Transformers.\" *arXiv:1810.04805* — BERT's training methodology\n- Reimers, N. & Gurevych, I. (2019). \"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks.\" *arXiv:1908.10084* — Adapting BERT for sentence embeddings\n- Radford, A., et al. (2021). \"Learning Transferable Visual Models From Natural Language Supervision.\" *arXiv:2103.00020* — CLIP's multi-modal approach\n\n",
    "supporting": [
      "ch07_embedding_model_fundamentals_files"
    ],
    "filters": [],
    "includes": {}
  }
}