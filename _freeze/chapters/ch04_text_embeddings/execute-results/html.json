{
  "hash": "e10247c308378838bee21fae6b395b19",
  "result": {
    "engine": "jupyter",
    "markdown": "# Text Embeddings {#sec-text-embeddings}\n\n::: callout-note\n## Chapter Overview\n\nThis chapter covers text embeddings—the most mature and widely used embedding type. We explore what text embeddings are, when to use them, and the practical applications they enable. An optional advanced section explains how the underlying models learn to create these representations.\n:::\n\n## What Are Text Embeddings?\n\nText embeddings convert words, sentences, or documents into dense numerical vectors that capture semantic meaning. Unlike simple approaches like bag-of-words or TF-IDF, embeddings understand that \"happy\" and \"joyful\" are related, even though they share no letters.\n\nThe key insight: **text that appears in similar contexts should have similar embeddings**. This emerges from training on massive text corpora where the model learns to predict words from their surrounding context.\n\n## Word Embeddings\n\nThe foundation of modern NLP, word embeddings map individual words to vectors:\n\n::: {#5e7eb45b .cell execution_count=1}\n``` {.python .cell-code code-fold=\"false\"}\n\"\"\"\nWord Embeddings: From Words to Vectors\n\nWord embeddings capture semantic relationships between individual words.\nWords with similar meanings cluster together in the embedding space.\n\"\"\"\n\nimport os\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport numpy as np\n\n# Use a sentence model to embed individual words\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\n\n# Embed words across different categories\nwords = {\n    'animals': ['cat', 'dog', 'elephant', 'whale'],\n    'vehicles': ['car', 'truck', 'airplane', 'boat'],\n    'colors': ['red', 'blue', 'green', 'yellow'],\n}\n\nall_words = [w for group in words.values() for w in group]\nembeddings = model.encode(all_words)\n\n# Show that words cluster by category\nprint(\"Word similarities (same category = higher similarity):\\n\")\nprint(\"Within categories:\")\nprint(f\"  cat ↔ dog:      {cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]:.3f}\")\nprint(f\"  car ↔ truck:    {cosine_similarity([embeddings[4]], [embeddings[5]])[0][0]:.3f}\")\nprint(f\"  red ↔ blue:     {cosine_similarity([embeddings[8]], [embeddings[9]])[0][0]:.3f}\")\n\nprint(\"\\nAcross categories:\")\nprint(f\"  cat ↔ car:      {cosine_similarity([embeddings[0]], [embeddings[4]])[0][0]:.3f}\")\nprint(f\"  dog ↔ red:      {cosine_similarity([embeddings[1]], [embeddings[8]])[0][0]:.3f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nWord similarities (same category = higher similarity):\n\nWithin categories:\n  cat ↔ dog:      0.661\n  car ↔ truck:    0.689\n  red ↔ blue:     0.729\n\nAcross categories:\n  cat ↔ car:      0.463\n  dog ↔ red:      0.377\n```\n:::\n:::\n\n\n**Key characteristics:**\n\n- One vector per word (static, context-independent in classic models)\n- Typically 100-300 dimensions\n- Captures synonyms, analogies, and semantic relationships\n\n## Sentence and Document Embeddings\n\nModern applications need to embed entire sentences or documents:\n\n::: {#af86089a .cell execution_count=2}\n``` {.python .cell-code code-fold=\"false\"}\n\"\"\"\nSentence Embeddings: Capturing Complete Thoughts\n\nSentence embeddings represent the meaning of entire sentences,\nenabling semantic search and similarity comparison.\n\"\"\"\n\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\n\n# Sentences with similar meaning but different words\nsentences = [\n    \"The quick brown fox jumps over the lazy dog\",\n    \"A fast auburn fox leaps above a sleepy canine\",\n    \"Machine learning models require lots of training data\",\n    \"AI systems need substantial amounts of examples to learn\",\n]\n\nembeddings = model.encode(sentences)\n\nprint(\"Sentence similarities:\\n\")\nprint(\"Similar meaning (paraphrases):\")\nprint(f\"  Sentence 1 ↔ 2: {cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]:.3f}\")\nprint(f\"  Sentence 3 ↔ 4: {cosine_similarity([embeddings[2]], [embeddings[3]])[0][0]:.3f}\")\n\nprint(\"\\nDifferent topics:\")\nprint(f\"  Sentence 1 ↔ 3: {cosine_similarity([embeddings[0]], [embeddings[2]])[0][0]:.3f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSentence similarities:\n\nSimilar meaning (paraphrases):\n  Sentence 1 ↔ 2: 0.733\n  Sentence 3 ↔ 4: 0.525\n\nDifferent topics:\n  Sentence 1 ↔ 3: -0.024\n```\n:::\n:::\n\n\n## When to Use Text Embeddings\n\nText embeddings are the right choice for:\n\n- **Text classification, clustering, and sentiment analysis** (see @sec-text-classification-clustering)\n- **Question answering and RAG systems** (see @sec-rag-at-scale)\n- **Chatbots and conversational AI**—intent matching, response selection (see @sec-conversational-ai)\n- **Summarization**—finding representative sentences (see @sec-embedding-summarization)\n- **Semantic search**—finding documents by meaning (see @sec-semantic-search)\n- **Recommendation systems**—content-based filtering (see @sec-recommendation-systems)\n- **Customer support**—ticket routing, finding similar issues (see @sec-cross-industry-patterns)\n- **Content moderation**—detecting similar problematic content (see @sec-content-moderation)\n- **Duplicate and near-duplicate detection** (see @sec-entity-resolution)\n- **Entity resolution**—matching names and descriptions (see @sec-entity-resolution)\n- **Machine translation**—cross-lingual embeddings (see @sec-defense-intelligence)\n\n## Popular Text Embedding Models\n\n| Model | Dimensions | Speed | Quality | Best For |\n|-------|-----------|-------|---------|----------|\n| [all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) | 384 | Fast | Good | General purpose |\n| [all-mpnet-base-v2](https://huggingface.co/sentence-transformers/all-mpnet-base-v2) | 768 | Medium | Better | Higher quality needs |\n| [text-embedding-3-small](https://platform.openai.com/docs/guides/embeddings) | 1536 | API | Excellent | Production systems |\n| [text-embedding-3-large](https://platform.openai.com/docs/guides/embeddings) | 3072 | API | Best | Maximum quality |\n\n: Popular text embedding models {.striped}\n\n## Classification, Clustering, and Sentiment Analysis {#sec-text-classification-clustering}\n\nOnce you have text embeddings, three foundational tasks become straightforward: **classification** (assigning labels), **clustering** (discovering groups), and **sentiment analysis** (a special case of classification). All three leverage the same principle—similar texts have similar embeddings.\n\n**Classification with embeddings:**\n\nTrain a simple classifier on top of frozen embeddings, or use nearest-neighbor approaches. The k-NN (k-nearest neighbors) method shown below works as follows: during training, embed each text and store the embedding alongside its label. To predict a new text's label, embed it, find the k training embeddings most similar to it (using cosine similarity), and return the most common label among those neighbors.\n\n::: {#e822ed4b .cell execution_count=3}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show Text Classifier\"}\nimport numpy as np\nfrom collections import Counter\n\n\nclass EmbeddingClassifier:\n    \"\"\"Simple k-NN classifier using embeddings.\"\"\"\n\n    def __init__(self, encoder, k: int = 5):\n        self.encoder = encoder\n        self.k = k\n        self.embeddings = []\n        self.labels = []\n\n    def fit(self, texts: list, labels: list):\n        \"\"\"Embed texts and store with their labels.\"\"\"\n        self.embeddings = [self.encoder.encode(text) for text in texts]\n        self.labels = labels\n\n    def predict(self, text: str) -> str:\n        \"\"\"Predict label using k-NN.\"\"\"\n        query_emb = self.encoder.encode(text)\n\n        # Cosine similarity: (A · B) / (||A|| × ||B||)\n        distances = []\n        for i, emb in enumerate(self.embeddings):\n            dist = np.dot(query_emb, emb) / (np.linalg.norm(query_emb) * np.linalg.norm(emb))\n            distances.append((dist, self.labels[i]))\n\n        # Get k nearest neighbors\n        distances.sort(reverse=True)\n        k_nearest = [label for _, label in distances[: self.k]]\n\n        # Return most common label\n        return Counter(k_nearest).most_common(1)[0][0]\n\n\n# Example: Sentiment classification\nfrom sentence_transformers import SentenceTransformer\nencoder = SentenceTransformer('all-MiniLM-L6-v2')\n\nclassifier = EmbeddingClassifier(encoder, k=3)\nclassifier.fit(\n    texts=[\"Great product!\", \"Loved it\", \"Terrible\", \"Waste of money\", \"Amazing quality\"],\n    labels=[\"positive\", \"positive\", \"negative\", \"negative\", \"positive\"],\n)\nprint(f\"Prediction for 'This is wonderful!': {classifier.predict('This is wonderful!')}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPrediction for 'This is wonderful!': positive\n```\n:::\n:::\n\n\n:::{.callout-tip}\n## Classification Best Practices\n\n- **Few-shot is often enough**: With good embeddings, 10-50 examples per class often suffices (see @sec-siamese-networks for few-shot techniques)\n- **k-NN for simplicity**: No training required, just store examples\n- **Logistic regression for speed**: Train a simple linear classifier on embeddings\n- **Fine-tune for best quality**: When you have thousands of examples, fine-tune the embedding model itself (see @sec-custom-embedding-strategies)\n:::\n\n**Clustering with embeddings:**\n\nClustering discovers natural groups in your data without predefined labels. Since similar texts have similar embeddings, texts on the same topic will cluster together in embedding space.\n\nK-means is a popular clustering algorithm. You specify k (the number of clusters), and the algorithm finds k groups by positioning a *centroid* (center point) for each cluster. Each text belongs to the cluster whose centroid is closest to its embedding.\n\nThe algorithm works as follows: first, embed all texts. Then pick k random embeddings as initial centroids—the algorithm needs starting points before it can begin refining. Next, iterate until convergence: (1) assign each embedding to its nearest centroid (measured by Euclidean distance), and (2) update each centroid to be the mean of its assigned embeddings. The algorithm converges when assignments stop changing.\n\n::: {#cell-fig-kmeans-clustering .cell execution_count=4}\n\n::: {.cell-output .cell-output-display}\n![K-means clustering in 2D embedding space. Points are colored by cluster assignment, with centroids marked as X.](ch04_text_embeddings_files/figure-html/fig-kmeans-clustering-output-1.png){#fig-kmeans-clustering width=566 height=470}\n:::\n:::\n\n\n::: {#5698c8fe .cell execution_count=5}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show Text Clustering\"}\nimport numpy as np\nfrom typing import List, Dict\n\n\nclass EmbeddingClusterer:\n    \"\"\"K-means clustering on text embeddings.\"\"\"\n\n    def __init__(self, encoder, n_clusters: int = 3):\n        self.encoder = encoder\n        self.n_clusters = n_clusters\n        self.centroids = None\n\n    def fit(self, texts: List[str], max_iters: int = 100):\n        \"\"\"Cluster texts and return assignments.\"\"\"\n        embeddings = np.array([self.encoder.encode(text) for text in texts])\n\n        # Initialize centroids by picking k random embeddings as starting points\n        indices = np.random.choice(len(embeddings), self.n_clusters, replace=False)\n        self.centroids = embeddings[indices].copy()\n\n        for _ in range(max_iters):\n            # Assign points to nearest centroid\n            assignments = []\n            for emb in embeddings:\n                distances = [np.linalg.norm(emb - c) for c in self.centroids]\n                assignments.append(np.argmin(distances))\n\n            # Update centroids\n            new_centroids = []\n            for i in range(self.n_clusters):\n                cluster_points = embeddings[np.array(assignments) == i]\n                if len(cluster_points) > 0:\n                    new_centroids.append(cluster_points.mean(axis=0))\n                else:\n                    new_centroids.append(self.centroids[i])\n\n            self.centroids = np.array(new_centroids)\n\n        return assignments\n\n    def get_cluster_examples(self, texts: List[str], assignments: List[int]) -> Dict[int, List[str]]:\n        \"\"\"Group texts by cluster.\"\"\"\n        clusters = {i: [] for i in range(self.n_clusters)}\n        for text, cluster_id in zip(texts, assignments):\n            clusters[cluster_id].append(text)\n        return clusters\n\n\n# Example: Topic discovery\ntexts = [\n    # Cooking\n    \"Chop the onions and garlic finely\",\n    \"Simmer the sauce for twenty minutes\",\n    \"Season with salt and pepper to taste\",\n    \"Preheat the oven to 350 degrees\",\n    # Space\n    \"The telescope discovered a new exoplanet\",\n    \"Astronauts completed their spacewalk today\",\n    \"The Mars rover collected soil samples\",\n    \"A new comet is visible this month\",\n    # Weather\n    \"Heavy rain expected throughout the weekend\",\n    \"Temperatures will drop below freezing tonight\",\n    \"A warm front is moving in from the south\",\n    \"Clear skies and sunshine forecast for Monday\",\n]\n\nnp.random.seed(42)  # For reproducible results\nclusterer = EmbeddingClusterer(encoder, n_clusters=3)\nassignments = clusterer.fit(texts)\nclusters = clusterer.get_cluster_examples(texts, assignments)\nfor cluster_id, examples in clusters.items():\n    print(f\"\\nCluster {cluster_id}:\")\n    for text in examples:\n        print(f\"  - {text}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCluster 0:\n  - The Mars rover collected soil samples\n  - A new comet is visible this month\n  - A warm front is moving in from the south\n\nCluster 1:\n  - Preheat the oven to 350 degrees\n  - Astronauts completed their spacewalk today\n  - Heavy rain expected throughout the weekend\n  - Temperatures will drop below freezing tonight\n  - Clear skies and sunshine forecast for Monday\n\nCluster 2:\n  - Chop the onions and garlic finely\n  - Simmer the sauce for twenty minutes\n  - Season with salt and pepper to taste\n  - The telescope discovered a new exoplanet\n```\n:::\n:::\n\n\nNotice that some items may appear in unexpected clusters. Embeddings capture semantic similarity that doesn't always match our intuitive topic categories—\"Preheat the oven to 350 degrees\" mentions temperature, which may pull it toward weather texts, while \"A warm front is moving in\" uses directional language similar to space descriptions. This is a feature, not a bug: embeddings capture meaning patterns that humans might overlook.\n\n:::{.callout-tip}\n## Clustering Best Practices\n\n- **Choose k carefully**: Use elbow method or silhouette scores to find optimal cluster count\n- **[HDBSCAN](https://hdbscan.readthedocs.io/) for unknown k**: Unlike k-means, HDBSCAN doesn't require specifying cluster count upfront—it discovers clusters based on density and labels sparse points as outliers rather than forcing them into clusters\n- **Reduce dimensions first**: For visualization, use UMAP or t-SNE on embeddings\n- **Label clusters post-hoc**: Examine cluster members to assign meaningful names\n:::\n\n**Sentiment analysis:**\n\nSentiment analysis determines whether text expresses positive, negative, or neutral opinions. While you could treat this as classification (train on labeled examples), an elegant alternative uses *anchor texts*—words or phrases with known sentiment.\n\nThe approach works as follows: embed a set of clearly positive words (\"excellent\", \"amazing\", \"love it\") and compute their centroid. Do the same for negative words. To analyze new text, embed it and measure which centroid it's closer to. The difference in distances gives both a label and a confidence score—text much closer to the positive centroid is strongly positive.\n\n::: {#cell-fig-sentiment-anchors .cell execution_count=6}\n\n::: {.cell-output .cell-output-display}\n![Sentiment analysis using anchor texts. Positive and negative anchor words form centroids. New text is classified by which centroid it's closer to.](ch04_text_embeddings_files/figure-html/fig-sentiment-anchors-output-1.png){#fig-sentiment-anchors width=758 height=470}\n:::\n:::\n\n\n::: {#970b219e .cell execution_count=7}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show Sentiment Analyzer\"}\nimport numpy as np\nfrom typing import Tuple\n\n\nclass SentimentAnalyzer:\n    \"\"\"Embedding-based sentiment analysis using anchor texts.\"\"\"\n\n    def __init__(self, encoder):\n        self.encoder = encoder\n        # Anchor texts define the sentiment space\n        positive_anchors = [\"excellent\", \"amazing\", \"wonderful\", \"fantastic\", \"love it\"]\n        negative_anchors = [\"terrible\", \"awful\", \"horrible\", \"hate it\", \"worst ever\"]\n\n        # Compute anchor centroids: average the embeddings of all anchor words\n        # to find the \"center\" of positive/negative regions in embedding space.\n        # Using multiple anchors makes the centroid more robust than any single word.\n        self.positive_centroid = np.mean(\n            [encoder.encode(t) for t in positive_anchors], axis=0\n        )\n        self.negative_centroid = np.mean(\n            [encoder.encode(t) for t in negative_anchors], axis=0\n        )\n\n    def analyze(self, text: str) -> Tuple[str, float]:\n        \"\"\"\n        Return sentiment label and confidence score.\n        Score ranges from -1 (negative) to +1 (positive).\n        \"\"\"\n        emb = self.encoder.encode(text)\n\n        # Cosine similarity: (A · B) / (||A|| × ||B||)\n        pos_sim = np.dot(emb, self.positive_centroid) / (\n            np.linalg.norm(emb) * np.linalg.norm(self.positive_centroid)\n        )\n        neg_sim = np.dot(emb, self.negative_centroid) / (\n            np.linalg.norm(emb) * np.linalg.norm(self.negative_centroid)\n        )\n\n        # Score: positive if closer to positive centroid\n        score = pos_sim - neg_sim\n        label = \"positive\" if score > 0 else \"negative\"\n        confidence = abs(score)\n\n        return label, confidence\n\n\n# Example usage\nnp.random.seed(42)\nanalyzer = SentimentAnalyzer(encoder)\nfor text in [\"This product exceeded expectations!\", \"Complete waste of money\"]:\n    label, conf = analyzer.analyze(text)\n    print(f\"'{text[:30]}...' -> {label} ({conf:.2f})\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n'This product exceeded expectat...' -> positive (0.02)\n'Complete waste of money...' -> negative (0.07)\n```\n:::\n:::\n\n\nThe number in parentheses is the confidence score—the difference between cosine similarities to the positive and negative centroids. These values appear low because general-purpose embeddings capture broad semantics, not just sentiment. The key insight is that the *relative* scores still correctly distinguish positive from negative text, even when absolute differences are small. For production sentiment analysis, you'd typically fine-tune embeddings on sentiment-labeled data (see @sec-custom-embedding-strategies).\n\n:::{.callout-tip}\n## Sentiment Analysis Best Practices\n\n- **Domain matters**: Financial sentiment differs from product reviews—use domain-specific anchors (see @sec-financial-services for financial sentiment)\n- **Beyond binary**: Instead of just positive/negative centroids, create centroids for multiple emotions (joy, anger, sadness, fear, surprise). Measure distance to each and return the closest emotion, or return a distribution across all emotions for nuanced analysis.\n- **Aspect-based**: Reviews often mix sentiment across topics (\"great battery, terrible screen\"). First extract aspects (product features, service elements), then run sentiment analysis on each aspect separately to understand what users love and hate.\n:::\n\n## Advanced: How Text Embedding Models Learn\n\n::: {.callout-note}\n## Optional Section\nThis section explains how text embedding models actually learn. Understanding these fundamentals helps you choose the right model and diagnose issues. Skip this if you just need to use embeddings.\n:::\n\n### Word2Vec: The Breakthrough {#sec-word2vec}\n\nWord2Vec [@mikolov2013efficient] revolutionized NLP by showing that simple neural networks could learn rich semantic representations from raw text. The key insight: **words appearing in similar contexts should have similar embeddings**.\n\nWord2Vec uses a technique called *skip-gram*: given a target word, predict the words that typically appear nearby. For example, given \"cat\", predict that \"furry\", \"pet\", and \"meow\" often appear in the same sentences. By training on millions of such predictions, the model learns that \"cat\" and \"dog\" should have similar embeddings (both appear near \"pet\", \"feed\", \"vet\") while \"cat\" and \"algebra\" should be far apart.\n\n::: {#cell-fig-skipgram-concept .cell execution_count=8}\n\n::: {.cell-output .cell-output-display}\n![Skip-gram: given a target word, predict context words within a window. Here, 'sat' predicts 'the', 'cat', 'on', 'the'.](ch04_text_embeddings_files/figure-html/fig-skipgram-concept-output-1.png){#fig-skipgram-concept width=950 height=230}\n:::\n:::\n\n\nThe implementation below shows the core training loop: for each target-context pair from real text, push their embeddings closer together; for random \"negative\" pairs, push them apart. After enough iterations, similar words cluster in embedding space.\n\n::: {#cell-fig-skipgram-training .cell execution_count=9}\n\n::: {.cell-output .cell-output-display}\n![Training pushes positive pairs (target + context) together and negative pairs (target + random) apart.](ch04_text_embeddings_files/figure-html/fig-skipgram-training-output-1.png){#fig-skipgram-training width=950 height=277}\n:::\n:::\n\n\n::: {#37adbed8 .cell execution_count=10}\n``` {.python .cell-code code-fold=\"false\"}\n\"\"\"\nWord2Vec Skip-Gram: Simplified Implementation\n\"\"\"\n\nimport numpy as np\n\n# Training data: sentences we learn word relationships from\ncorpus = [[\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"], [\"the\", \"dog\", \"sat\", \"on\", \"the\", \"rug\"]]\n\n# Vocabulary: unique words extracted from corpus\nvocab = list(set(word for sentence in corpus for word in sentence))\nvocab_size = len(vocab)\nembedding_dim = 4\nword_to_idx = {w: i for i, w in enumerate(vocab)}\n\nnp.random.seed(42)\nW_target = np.random.randn(vocab_size, embedding_dim) * 0.1\nW_context = np.random.randn(vocab_size, embedding_dim) * 0.1\n\n\ndef sigmoid(x):\n    \"\"\"Squash any value to range (0, 1). Clip prevents overflow.\"\"\"\n    return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n\n\ndef train_skipgram_pair(target_word, context_word, negative_words, lr=0.1):\n    \"\"\"Train on one (target, context) pair with negative sampling.\n\n    Example: in \"the cat sat\", target=\"cat\", context=\"the\" or \"sat\",\n    negatives=[\"dog\", \"rug\"] (random words not in context).\n    \"\"\"\n    global W_target, W_context\n\n    t_idx = word_to_idx[target_word]\n    c_idx = word_to_idx[context_word]\n\n    target_emb = W_target[t_idx]\n    context_emb = W_context[c_idx]\n\n    # Positive example: target and context should be similar\n    score = np.dot(target_emb, context_emb)\n    pred = sigmoid(score)\n    W_target[t_idx] -= lr * (pred - 1) * context_emb\n    W_context[c_idx] -= lr * (pred - 1) * target_emb\n\n    # Negative examples: push apart words that don't appear together.\n    # Without this, model would collapse all embeddings to the same point.\n    for neg_word in negative_words:\n        n_idx = word_to_idx[neg_word]\n        neg_emb = W_context[n_idx]\n        score = np.dot(target_emb, neg_emb)\n        pred = sigmoid(score)\n        W_target[t_idx] -= lr * pred * neg_emb\n        W_context[n_idx] -= lr * pred * target_emb\n\n\n# Train for 50 passes through the corpus\nfor epoch in range(50):\n    for sentence in corpus:\n        for i, target in enumerate(sentence):\n            # Context = words within 2 positions of target\n            start = max(0, i - 2)\n            end = min(len(sentence), i + 3)\n            context_words = [sentence[j] for j in range(start, end) if j != i]\n\n            # Negatives = random words not appearing near target\n            negatives = [w for w in vocab if w not in context_words and w != target][:2]\n\n            for context in context_words:\n                train_skipgram_pair(target, context, negatives)\n\n\n# cosine_similarity = (A · B) / (||A|| × ||B||)\ndef cosine_similarity(w1, w2):\n    v1, v2 = W_target[word_to_idx[w1]], W_target[word_to_idx[w2]]\n    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n\n\nprint(\"Learned similarities:\")\nprint(f\"  cat ↔ dog: {cosine_similarity('cat', 'dog'):.3f}\")\nprint(f\"  cat ↔ mat: {cosine_similarity('cat', 'mat'):.3f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLearned similarities:\n  cat ↔ dog: 0.949\n  cat ↔ mat: 0.972\n```\n:::\n:::\n\n\nWhy is cat↔mat higher than cat↔dog? In our tiny corpus, \"cat\" and \"mat\" appear in the same sentence, sharing more context words. Word2Vec learns from co-occurrence patterns in the training data—with millions of sentences, \"cat\" and \"dog\" would cluster together as animals, but our two-sentence corpus doesn't capture that relationship.\n\n### Transformers and BERT {#sec-transformers-bert}\n\nThe transformer architecture [@vaswani2017attention] and BERT [@devlin2018bert] introduced **contextual embeddings**—the same word gets different representations based on context.\n\nThe key innovation is the **attention mechanism**: when processing a word, the model can attend to all other words in the sentence.\n\n::: {#2a730b59 .cell execution_count=11}\n``` {.python .cell-code code-fold=\"false\"}\n\"\"\"\nDemonstrating Contextual Embeddings\n\nThe same word gets different embeddings in different contexts.\n\"\"\"\n\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\n\nsentences = [\n    \"I deposited money at the bank\",      # bank = financial institution\n    \"The bank approved my loan\",          # bank = financial institution\n    \"We had a picnic on the river bank\",  # bank = riverside\n    \"Fish swim near the bank\",            # bank = riverside\n]\n\nembeddings = model.encode(sentences)\n\nprint(\"Contextual similarity:\\n\")\nlabels = [\"financial-1\", \"financial-2\", \"river-1\", \"river-2\"]\nfor i in range(len(sentences)):\n    for j in range(i + 1, len(sentences)):\n        sim = cosine_similarity([embeddings[i]], [embeddings[j]])[0][0]\n        print(f\"  {labels[i]:12s} ↔ {labels[j]:12s}: {sim:.3f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nContextual similarity:\n\n  financial-1  ↔ financial-2 : 0.486\n  financial-1  ↔ river-1     : 0.416\n  financial-1  ↔ river-2     : 0.286\n  financial-2  ↔ river-1     : 0.288\n  financial-2  ↔ river-2     : 0.248\n  river-1      ↔ river-2     : 0.382\n```\n:::\n:::\n\n\n**Why transformers dominate:**\n\n1. **Parallelization**: All positions processed simultaneously (unlike RNNs)\n2. **Long-range dependencies**: Attention connects distant words directly\n3. **Transfer learning**: Pre-trained models work across many tasks\n4. **Scalability**: Performance improves with more data and compute\n\n### Sentence Transformers {#sec-sentence-transformers}\n\nSentence Transformers [@reimers2019sentence] fine-tune BERT-like models specifically for producing sentence embeddings. They use **contrastive learning** (see @sec-contrastive-learning): train the model to produce similar embeddings for related sentences, and push apart unrelated ones.\n\n## Key Takeaways\n\n- **Text embeddings** convert words, sentences, or documents into vectors capturing semantic meaning\n- **Similar text → similar vectors**: This enables semantic search, clustering, and classification without explicit rules\n- **Popular models** range from fast (MiniLM) to high-quality (OpenAI text-embedding-3) depending on your needs\n- **Word2Vec** learns from word co-occurrence patterns—words in similar contexts get similar embeddings\n- **Transformers (BERT)** create contextual embeddings where the same word gets different vectors based on surrounding context\n- **Sentence Transformers** adapt these for producing single embeddings for entire sentences\n\n## Looking Ahead\n\nNow that you understand text embeddings, @sec-image-video-embeddings explores how similar principles apply to visual data—images and video.\n\n## Further Reading\n\n- Mikolov, T., et al. (2013). \"Efficient Estimation of Word Representations in Vector Space.\" *arXiv:1301.3781*\n- Vaswani, A., et al. (2017). \"Attention Is All You Need.\" *NeurIPS*\n- Devlin, J., et al. (2018). \"BERT: Pre-training of Deep Bidirectional Transformers.\" *arXiv:1810.04805*\n- Reimers, N. & Gurevych, I. (2019). \"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks.\" *arXiv:1908.10084*\n\n",
    "supporting": [
      "ch04_text_embeddings_files"
    ],
    "filters": [],
    "includes": {}
  }
}