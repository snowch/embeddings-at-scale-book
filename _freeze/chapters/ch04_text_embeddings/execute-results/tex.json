{
  "hash": "11b71d472182e54d3fb9bee0dfbf0f0b",
  "result": {
    "engine": "jupyter",
    "markdown": "# Text Embeddings {#sec-text-embeddings}\n\n::: callout-note\n## Chapter Overview\n\nThis chapter covers text embeddings—the most mature and widely used embedding type. We explore what text embeddings are, when to use them, and the practical applications they enable. An optional advanced section explains how the underlying models learn to create these representations.\n:::\n\n## What Are Text Embeddings?\n\nText embeddings convert words, sentences, or documents into dense numerical vectors that capture semantic meaning. Unlike simple approaches like bag-of-words or TF-IDF, embeddings understand that \"happy\" and \"joyful\" are related, even though they share no letters.\n\nThe key insight: **text that appears in similar contexts should have similar embeddings**. This emerges from training on massive text corpora where the model learns to predict words from their surrounding context.\n\n## Word Embeddings\n\nThe foundation of modern NLP, word embeddings map individual words to vectors:\n\n::: {.cell execution_count=1}\n``` {.python .cell-code code-fold=\"false\"}\n\"\"\"\nWord Embeddings: From Words to Vectors\n\nWord embeddings capture semantic relationships between individual words.\nWords with similar meanings cluster together in the embedding space.\n\"\"\"\n\nimport os\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport numpy as np\n\n# Use a sentence model to embed individual words\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\n\n# Embed words across different categories\nwords = {\n    'animals': ['cat', 'dog', 'elephant', 'whale'],\n    'vehicles': ['car', 'truck', 'airplane', 'boat'],\n    'colors': ['red', 'blue', 'green', 'yellow'],\n}\n\nall_words = [w for group in words.values() for w in group]\nembeddings = model.encode(all_words)\n\n# Show that words cluster by category\nprint(\"Word similarities (same category = higher similarity):\\n\")\nprint(\"Within categories:\")\nprint(f\"  cat ↔ dog:      {cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]:.3f}\")\nprint(f\"  car ↔ truck:    {cosine_similarity([embeddings[4]], [embeddings[5]])[0][0]:.3f}\")\nprint(f\"  red ↔ blue:     {cosine_similarity([embeddings[8]], [embeddings[9]])[0][0]:.3f}\")\n\nprint(\"\\nAcross categories:\")\nprint(f\"  cat ↔ car:      {cosine_similarity([embeddings[0]], [embeddings[4]])[0][0]:.3f}\")\nprint(f\"  dog ↔ red:      {cosine_similarity([embeddings[1]], [embeddings[8]])[0][0]:.3f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nWord similarities (same category = higher similarity):\n\nWithin categories:\n  cat ↔ dog:      0.661\n  car ↔ truck:    0.689\n  red ↔ blue:     0.729\n\nAcross categories:\n  cat ↔ car:      0.463\n  dog ↔ red:      0.377\n```\n:::\n:::\n\n\n**Key characteristics:**\n\n- One vector per word (static, context-independent in classic models)\n- Typically 100-300 dimensions\n- Captures synonyms, analogies, and semantic relationships\n\n## Sentence and Document Embeddings\n\nModern applications need to embed entire sentences or documents:\n\n::: {.cell execution_count=2}\n``` {.python .cell-code code-fold=\"false\"}\n\"\"\"\nSentence Embeddings: Capturing Complete Thoughts\n\nSentence embeddings represent the meaning of entire sentences,\nenabling semantic search and similarity comparison.\n\"\"\"\n\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\n\n# Sentences with similar meaning but different words\nsentences = [\n    \"The quick brown fox jumps over the lazy dog\",\n    \"A fast auburn fox leaps above a sleepy canine\",\n    \"Machine learning models require lots of training data\",\n    \"AI systems need substantial amounts of examples to learn\",\n]\n\nembeddings = model.encode(sentences)\n\nprint(\"Sentence similarities:\\n\")\nprint(\"Similar meaning (paraphrases):\")\nprint(f\"  Sentence 1 ↔ 2: {cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]:.3f}\")\nprint(f\"  Sentence 3 ↔ 4: {cosine_similarity([embeddings[2]], [embeddings[3]])[0][0]:.3f}\")\n\nprint(\"\\nDifferent topics:\")\nprint(f\"  Sentence 1 ↔ 3: {cosine_similarity([embeddings[0]], [embeddings[2]])[0][0]:.3f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSentence similarities:\n\nSimilar meaning (paraphrases):\n  Sentence 1 ↔ 2: 0.733\n  Sentence 3 ↔ 4: 0.525\n\nDifferent topics:\n  Sentence 1 ↔ 3: -0.024\n```\n:::\n:::\n\n\n## When to Use Text Embeddings\n\nText embeddings are the right choice for:\n\n- **Semantic search**—finding documents by meaning, not just keywords (see @sec-semantic-search)\n- **RAG systems**—retrieval-augmented generation for LLMs (see @sec-rag-at-scale)\n- **Text classification and clustering**—grouping similar documents\n- **Sentiment analysis**—understanding opinion and emotion\n- **Recommendation systems**—content-based filtering (see @sec-recommendation-systems)\n- **Duplicate detection**—finding near-duplicate content (see @sec-entity-resolution)\n- **Customer support**—ticket routing and similar issue finding (see @sec-cross-industry-patterns)\n\n## Popular Text Embedding Models\n\n| Model | Dimensions | Speed | Quality | Best For |\n|-------|-----------|-------|---------|----------|\n| [all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) | 384 | Fast | Good | General purpose |\n| [all-mpnet-base-v2](https://huggingface.co/sentence-transformers/all-mpnet-base-v2) | 768 | Medium | Better | Higher quality needs |\n| [text-embedding-3-small](https://platform.openai.com/docs/guides/embeddings) | 1536 | API | Excellent | Production systems |\n| [text-embedding-3-large](https://platform.openai.com/docs/guides/embeddings) | 3072 | API | Best | Maximum quality |\n\n: Popular text embedding models {.striped}\n\n## Classification, Clustering, and Sentiment Analysis {#sec-text-classification-clustering}\n\nOnce you have text embeddings, three foundational tasks become straightforward: **classification** (assigning labels), **clustering** (discovering groups), and **sentiment analysis** (a special case of classification). All three leverage the same principle—similar texts have similar embeddings.\n\n### Classification with Embeddings\n\nTrain a simple classifier on top of frozen embeddings, or use nearest-neighbor approaches:\n\n::: {.cell execution_count=3}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show Text Classifier\"}\nimport numpy as np\nfrom collections import Counter\n\n\nclass EmbeddingClassifier:\n    \"\"\"Simple k-NN classifier using embeddings.\"\"\"\n\n    def __init__(self, encoder, k: int = 5):\n        self.encoder = encoder\n        self.k = k\n        self.embeddings = []\n        self.labels = []\n\n    def fit(self, texts: list, labels: list):\n        \"\"\"Embed texts and store with their labels.\"\"\"\n        self.embeddings = [self.encoder.encode(text) for text in texts]\n        self.labels = labels\n\n    def predict(self, text: str) -> str:\n        \"\"\"Predict label using k-NN.\"\"\"\n        query_emb = self.encoder.encode(text)\n\n        # Cosine similarity: (A · B) / (||A|| × ||B||)\n        distances = []\n        for i, emb in enumerate(self.embeddings):\n            dist = np.dot(query_emb, emb) / (np.linalg.norm(query_emb) * np.linalg.norm(emb))\n            distances.append((dist, self.labels[i]))\n\n        # Get k nearest neighbors\n        distances.sort(reverse=True)\n        k_nearest = [label for _, label in distances[: self.k]]\n\n        # Return most common label\n        return Counter(k_nearest).most_common(1)[0][0]\n\n\n# Example: Sentiment classification\nfrom sentence_transformers import SentenceTransformer\nencoder = SentenceTransformer('all-MiniLM-L6-v2')\n\nclassifier = EmbeddingClassifier(encoder, k=3)\nclassifier.fit(\n    texts=[\"Great product!\", \"Loved it\", \"Terrible\", \"Waste of money\", \"Amazing quality\"],\n    labels=[\"positive\", \"positive\", \"negative\", \"negative\", \"positive\"],\n)\nprint(f\"Prediction: {classifier.predict('This is wonderful!')}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPrediction: positive\n```\n:::\n:::\n\n\n### Clustering with Embeddings\n\nClustering discovers natural groups in your data without predefined labels:\n\n::: {.cell execution_count=4}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show Text Clustering\"}\nimport numpy as np\nfrom typing import List, Dict\n\n\nclass EmbeddingClusterer:\n    \"\"\"K-means clustering on text embeddings.\"\"\"\n\n    def __init__(self, encoder, n_clusters: int = 3):\n        self.encoder = encoder\n        self.n_clusters = n_clusters\n        self.centroids = None\n\n    def fit(self, texts: List[str], max_iters: int = 100):\n        \"\"\"Cluster texts and return assignments.\"\"\"\n        embeddings = np.array([self.encoder.encode(text) for text in texts])\n\n        # Initialize centroids by picking k random embeddings as starting points\n        indices = np.random.choice(len(embeddings), self.n_clusters, replace=False)\n        self.centroids = embeddings[indices].copy()\n\n        for _ in range(max_iters):\n            # Assign points to nearest centroid\n            assignments = []\n            for emb in embeddings:\n                distances = [np.linalg.norm(emb - c) for c in self.centroids]\n                assignments.append(np.argmin(distances))\n\n            # Update centroids\n            new_centroids = []\n            for i in range(self.n_clusters):\n                cluster_points = embeddings[np.array(assignments) == i]\n                if len(cluster_points) > 0:\n                    new_centroids.append(cluster_points.mean(axis=0))\n                else:\n                    new_centroids.append(self.centroids[i])\n\n            self.centroids = np.array(new_centroids)\n\n        return assignments\n\n    def get_cluster_examples(self, texts: List[str], assignments: List[int]) -> Dict[int, List[str]]:\n        \"\"\"Group texts by cluster.\"\"\"\n        clusters = {i: [] for i in range(self.n_clusters)}\n        for text, cluster_id in zip(texts, assignments):\n            clusters[cluster_id].append(text)\n        return clusters\n\n\n# Example: Topic discovery\ntexts = [\n    \"Chop the onions and garlic finely\",\n    \"Simmer the sauce for twenty minutes\",\n    \"The telescope discovered a new exoplanet\",\n    \"Astronauts completed their spacewalk today\",\n    \"Heavy rain expected throughout the weekend\",\n    \"Temperatures will drop below freezing tonight\",\n]\n\nnp.random.seed(42)\nclusterer = EmbeddingClusterer(encoder, n_clusters=3)\nassignments = clusterer.fit(texts)\nclusters = clusterer.get_cluster_examples(texts, assignments)\nfor cluster_id, examples in clusters.items():\n    print(f\"\\nCluster {cluster_id}:\")\n    for text in examples:\n        print(f\"  - {text}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCluster 0:\n  - Chop the onions and garlic finely\n  - The telescope discovered a new exoplanet\n\nCluster 1:\n  - Simmer the sauce for twenty minutes\n  - Astronauts completed their spacewalk today\n\nCluster 2:\n  - Heavy rain expected throughout the weekend\n  - Temperatures will drop below freezing tonight\n```\n:::\n:::\n\n\n### Sentiment Analysis\n\nUse anchor texts to define sentiment regions in embedding space:\n\n::: {.cell execution_count=5}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show Sentiment Analyzer\"}\nimport numpy as np\nfrom typing import Tuple\n\n\nclass SentimentAnalyzer:\n    \"\"\"Embedding-based sentiment analysis using anchor texts.\"\"\"\n\n    def __init__(self, encoder):\n        self.encoder = encoder\n        positive_anchors = [\"excellent\", \"amazing\", \"wonderful\", \"fantastic\", \"love it\"]\n        negative_anchors = [\"terrible\", \"awful\", \"horrible\", \"hate it\", \"worst ever\"]\n\n        self.positive_centroid = np.mean(\n            [encoder.encode(t) for t in positive_anchors], axis=0\n        )\n        self.negative_centroid = np.mean(\n            [encoder.encode(t) for t in negative_anchors], axis=0\n        )\n\n    def analyze(self, text: str) -> Tuple[str, float]:\n        \"\"\"Return sentiment label and confidence score.\"\"\"\n        emb = self.encoder.encode(text)\n\n        pos_sim = np.dot(emb, self.positive_centroid) / (\n            np.linalg.norm(emb) * np.linalg.norm(self.positive_centroid)\n        )\n        neg_sim = np.dot(emb, self.negative_centroid) / (\n            np.linalg.norm(emb) * np.linalg.norm(self.negative_centroid)\n        )\n\n        score = pos_sim - neg_sim\n        label = \"positive\" if score > 0 else \"negative\"\n        return label, abs(score)\n\n\nanalyzer = SentimentAnalyzer(encoder)\nfor text in [\"This product exceeded expectations!\", \"Complete waste of money\"]:\n    label, conf = analyzer.analyze(text)\n    print(f\"'{text[:30]}...' -> {label} ({conf:.2f})\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n'This product exceeded expectat...' -> positive (0.02)\n'Complete waste of money...' -> negative (0.07)\n```\n:::\n:::\n\n\n:::{.callout-tip}\n## Best Practices\n\n**For classification:**\n\n- Few-shot is often enough: 10-50 examples per class with good embeddings\n- k-NN for simplicity, logistic regression for speed\n- Fine-tune for best quality when you have thousands of examples\n\n**For clustering:**\n\n- Use elbow method or silhouette scores to find optimal k\n- Consider HDBSCAN when you don't know the number of clusters\n- Reduce dimensions with UMAP/t-SNE for visualization\n\n**For sentiment:**\n\n- Use domain-specific anchors (financial sentiment differs from product reviews)\n- Consider aspect-based sentiment for detailed analysis\n:::\n\n## Advanced: How Text Embedding Models Learn {.unnumbered}\n\n::: {.callout-note}\n## Optional Section\nThis section explains how text embedding models actually learn. Understanding these fundamentals helps you choose the right model and diagnose issues. Skip this if you just need to use embeddings.\n:::\n\n### Word2Vec: The Breakthrough {#sec-word2vec}\n\nWord2Vec [@mikolov2013efficient] revolutionized NLP by showing that simple neural networks could learn rich semantic representations from raw text. The key insight: **words appearing in similar contexts should have similar embeddings**.\n\nThe skip-gram model learns by predicting context words given a target word:\n\n::: {.cell execution_count=6}\n``` {.python .cell-code code-fold=\"false\"}\n\"\"\"\nWord2Vec Skip-Gram: Simplified Implementation\n\"\"\"\n\nimport numpy as np\n\nvocab = [\"the\", \"cat\", \"sat\", \"on\", \"mat\", \"dog\", \"rug\"]\nvocab_size = len(vocab)\nembedding_dim = 4\nword_to_idx = {w: i for i, w in enumerate(vocab)}\n\nnp.random.seed(42)\nW_target = np.random.randn(vocab_size, embedding_dim) * 0.1\nW_context = np.random.randn(vocab_size, embedding_dim) * 0.1\n\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n\n\ndef train_skipgram_pair(target_word, context_word, negative_words, lr=0.1):\n    \"\"\"Train on one (target, context) pair with negative sampling.\"\"\"\n    global W_target, W_context\n\n    t_idx = word_to_idx[target_word]\n    c_idx = word_to_idx[context_word]\n\n    target_emb = W_target[t_idx]\n    context_emb = W_context[c_idx]\n\n    # Positive example: target and context should be similar\n    score = np.dot(target_emb, context_emb)\n    pred = sigmoid(score)\n    W_target[t_idx] -= lr * (pred - 1) * context_emb\n    W_context[c_idx] -= lr * (pred - 1) * target_emb\n\n    # Negative examples: target and random words should be dissimilar\n    for neg_word in negative_words:\n        n_idx = word_to_idx[neg_word]\n        neg_emb = W_context[n_idx]\n        score = np.dot(target_emb, neg_emb)\n        pred = sigmoid(score)\n        W_target[t_idx] -= lr * pred * neg_emb\n        W_context[n_idx] -= lr * pred * target_emb\n\n\n# Training corpus\ncorpus = [[\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"], [\"the\", \"dog\", \"sat\", \"on\", \"the\", \"rug\"]]\n\nfor epoch in range(50):\n    for sentence in corpus:\n        for i, target in enumerate(sentence):\n            context_words = [sentence[j] for j in range(max(0, i-2), min(len(sentence), i+3)) if j != i]\n            negatives = [w for w in vocab if w not in context_words and w != target][:2]\n            for context in context_words:\n                train_skipgram_pair(target, context, negatives)\n\n\ndef cosine_similarity(w1, w2):\n    v1, v2 = W_target[word_to_idx[w1]], W_target[word_to_idx[w2]]\n    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n\n\nprint(\"Learned similarities:\")\nprint(f\"  cat ↔ dog: {cosine_similarity('cat', 'dog'):.3f}\")\nprint(f\"  cat ↔ mat: {cosine_similarity('cat', 'mat'):.3f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLearned similarities:\n  cat ↔ dog: 0.991\n  cat ↔ mat: 0.408\n```\n:::\n:::\n\n\n### Transformers and BERT {#sec-transformers-bert}\n\nThe transformer architecture [@vaswani2017attention] and BERT [@devlin2018bert] introduced **contextual embeddings**—the same word gets different representations based on context.\n\nThe key innovation is the **attention mechanism**: when processing a word, the model can attend to all other words in the sentence.\n\n::: {.cell execution_count=7}\n``` {.python .cell-code code-fold=\"false\"}\n\"\"\"\nDemonstrating Contextual Embeddings\n\nThe same word gets different embeddings in different contexts.\n\"\"\"\n\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\n\nsentences = [\n    \"I deposited money at the bank\",      # bank = financial institution\n    \"The bank approved my loan\",          # bank = financial institution\n    \"We had a picnic on the river bank\",  # bank = riverside\n    \"Fish swim near the bank\",            # bank = riverside\n]\n\nembeddings = model.encode(sentences)\n\nprint(\"Contextual similarity:\\n\")\nlabels = [\"financial-1\", \"financial-2\", \"river-1\", \"river-2\"]\nfor i in range(len(sentences)):\n    for j in range(i + 1, len(sentences)):\n        sim = cosine_similarity([embeddings[i]], [embeddings[j]])[0][0]\n        print(f\"  {labels[i]:12s} ↔ {labels[j]:12s}: {sim:.3f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nContextual similarity:\n\n  financial-1  ↔ financial-2 : 0.486\n  financial-1  ↔ river-1     : 0.416\n  financial-1  ↔ river-2     : 0.286\n  financial-2  ↔ river-1     : 0.288\n  financial-2  ↔ river-2     : 0.248\n  river-1      ↔ river-2     : 0.382\n```\n:::\n:::\n\n\n**Why transformers dominate:**\n\n1. **Parallelization**: All positions processed simultaneously (unlike RNNs)\n2. **Long-range dependencies**: Attention connects distant words directly\n3. **Transfer learning**: Pre-trained models work across many tasks\n4. **Scalability**: Performance improves with more data and compute\n\n### Sentence Transformers {#sec-sentence-transformers}\n\nSentence Transformers [@reimers2019sentence] fine-tune BERT-like models specifically for producing sentence embeddings. They use **contrastive learning** (see @sec-contrastive-learning): train the model to produce similar embeddings for related sentences, and push apart unrelated ones.\n\n## Key Takeaways\n\n- **Text embeddings** convert words, sentences, or documents into vectors capturing semantic meaning\n\n- **Similar text → similar vectors**: This enables semantic search, clustering, and classification without explicit rules\n\n- **Popular models** range from fast (MiniLM) to high-quality (OpenAI text-embedding-3) depending on your needs\n\n- **Word2Vec** learns from word co-occurrence patterns—words in similar contexts get similar embeddings\n\n- **Transformers (BERT)** create contextual embeddings where the same word gets different vectors based on surrounding context\n\n- **Sentence Transformers** adapt these for producing single embeddings for entire sentences\n\n## Looking Ahead\n\nNow that you understand text embeddings, @sec-image-video-embeddings explores how similar principles apply to visual data—images and video.\n\n## Further Reading\n\n- Mikolov, T., et al. (2013). \"Efficient Estimation of Word Representations in Vector Space.\" *arXiv:1301.3781*\n- Vaswani, A., et al. (2017). \"Attention Is All You Need.\" *NeurIPS*\n- Devlin, J., et al. (2018). \"BERT: Pre-training of Deep Bidirectional Transformers.\" *arXiv:1810.04805*\n- Reimers, N. & Gurevych, I. (2019). \"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks.\" *arXiv:1908.10084*\n\n",
    "supporting": [
      "ch04_text_embeddings_files/figure-pdf"
    ],
    "filters": []
  }
}