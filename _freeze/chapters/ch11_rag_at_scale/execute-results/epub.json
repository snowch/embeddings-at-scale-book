{
  "hash": "67bc11dd598d1e6e78adc8f1d3f9addc",
  "result": {
    "engine": "jupyter",
    "markdown": "# Retrieval-Augmented Generation (RAG) at Scale {#sec-rag-at-scale}\n\n:::{.callout-note}\n## Chapter Overview\nRetrieval-Augmented Generation combines the power of embedding-based retrieval with large language model generation, enabling LLMs to answer questions grounded in enterprise knowledge rather than relying solely on parametric memory. This chapter explores production RAG systems at scale: enterprise architecture patterns that handle billion-document corpora, context window optimization strategies that maximize information density while respecting token limits, multi-stage retrieval pipelines that balance recall and precision across filtering and reranking stages, evaluation frameworks that measure end-to-end quality beyond simple metrics, and techniques for handling contradictory information when sources disagree. These patterns enable RAG systems that serve millions of users with accurate, attributable, up-to-date responses.\n:::\n\nWith robust data engineering in place (@sec-data-engineering), the foundation exists to build advanced applications that leverage embeddings at scale. **Retrieval-Augmented Generation (RAG)** has emerged as the dominant pattern for grounding large language models in enterprise knowledge. Rather than fine-tuning models on proprietary data (expensive, slow to update, risk of hallucination), RAG retrieves relevant context from vector databases and includes it in the LLM prompt. This approach enables accurate answers over billion-document corpora, maintains attribution to sources, updates knowledge in real-time, and scales to trillion-row datasets—all critical requirements for enterprise deployment.\n\n## Enterprise RAG Architecture Patterns\n\nProduction RAG systems serve thousands of concurrent users querying billion-document knowledge bases with sub-second latency and high accuracy. **Enterprise RAG architectures** decompose this challenge into specialized components: query understanding, retrieval, reranking, context assembly, generation, and response validation. Each component must scale independently while maintaining end-to-end quality.\n\n### The RAG Pipeline\n\nA complete RAG system comprises six stages:\n\n1. **Query Understanding**: Parse user intent, extract entities, expand with synonyms\n2. **Retrieval**: Vector search for top-k relevant documents (k=100-1000)\n3. **Reranking**: Reorder results by relevance using cross-encoder (reduce to k=5-20)\n4. **Context Assembly**: Fit selected documents into context window\n5. **Generation**: LLM generates response given query + context\n6. **Validation**: Verify response accuracy, check for hallucinations\n\n::: {#ca57be5a .cell execution_count=1}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show Vector Store Setup\"}\nfrom dataclasses import dataclass\nfrom typing import List, Optional\n\nimport faiss\nimport numpy as np\n\n\n@dataclass\nclass Document:\n    \"\"\"Document with embedding.\"\"\"\n    doc_id: str\n    text: str\n    embedding: Optional[np.ndarray] = None\n    metadata: dict = None\n\n    def __post_init__(self):\n        if self.metadata is None:\n            self.metadata = {}\n\n\nclass VectorStore:\n    \"\"\"FAISS-based vector store for document retrieval.\"\"\"\n    def __init__(self, embedding_dim: int = 768):\n        self.embedding_dim = embedding_dim\n        self.index = faiss.IndexFlatIP(embedding_dim)\n        self.documents: List[Document] = []\n\n    def add_documents(self, documents: List[Document]):\n        \"\"\"Add documents to the vector store.\"\"\"\n        embeddings = np.array([doc.embedding for doc in documents]).astype('float32')\n        faiss.normalize_L2(embeddings)\n        self.index.add(embeddings)\n        self.documents.extend(documents)\n\n    def search(self, query_embedding: np.ndarray, k: int = 5) -> List[Document]:\n        \"\"\"Search for top-k most similar documents.\"\"\"\n        query_embedding = query_embedding.astype('float32').reshape(1, -1)\n        faiss.normalize_L2(query_embedding)\n        distances, indices = self.index.search(query_embedding, k)\n        return [self.documents[i] for i in indices[0]]\n\n# Usage example\nstore = VectorStore(embedding_dim=768)\ndocs = [\n    Document(doc_id=\"1\", text=\"Machine learning basics\", embedding=np.random.rand(768)),\n    Document(doc_id=\"2\", text=\"Deep learning with PyTorch\", embedding=np.random.rand(768))\n]\nstore.add_documents(docs)\nresults = store.search(np.random.rand(768), k=2)\nprint(f\"Found {len(results)} documents\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFound 2 documents\n```\n:::\n:::\n\n\n:::{.callout-tip}\n## Enterprise RAG Best Practices\n\n**Architecture:**\n\n- Decouple components (retrieval, reranking, generation)\n- Use async/parallel processing where possible\n- Implement circuit breakers for each component\n- Cache frequent queries and intermediate results\n\n**Query Processing:**\n\n- Always classify intent (different strategies per type)\n- Extract and normalize entities\n- Use query expansion for better recall\n- Parse metadata filters from natural language\n\n**Retrieval:**\n\n- Start with high k (100-1000) for recall\n- Use multiple retrieval strategies (vector + keyword)\n- Apply metadata filters early (before reranking)\n- Log retrieval metrics for continuous improvement\n\n**Reranking:**\n\n- Essential for production accuracy (10-20% improvement)\n- Use cross-encoder models (more accurate than bi-encoders)\n- Batch reranking requests for efficiency\n- Consider two-stage reranking (coarse then fine)\n:::\n\n## Context Window Optimization\n\nLLMs have fixed context windows (4K-128K tokens), but enterprise knowledge bases contain millions of documents. **Context window optimization** maximizes information density: selecting the most relevant passages, removing redundancy, compressing verbose content, and structuring information for LLM comprehension.\n\n### The Context Window Challenge\n\n**Problem**: Retrieved documents often exceed context limits\n- 10 documents × 1000 tokens each = 10K tokens\n- Typical LLM limit: 4K-8K tokens\n- Need to reduce 10K → 4K while preserving key information\n\n**Naive approach**: Truncate each document\n- **Problem**: May cut off critical information, often removes conclusions\n\n**Better approach**: Extract relevant passages, deduplicate, compress\n\n::: {#85174733 .cell execution_count=2}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show Passage Extractor\"}\nfrom typing import List, Tuple\nimport re\n\n\nclass PassageExtractor:\n    \"\"\"Extract relevant passages from long documents.\"\"\"\n    def __init__(self, max_passage_length: int = 512, overlap: int = 50):\n        self.max_passage_length = max_passage_length\n        self.overlap = overlap\n\n    def extract_passages(self, text: str) -> List[Tuple[str, int, int]]:\n        \"\"\"Split text into overlapping passages.\n\n        Returns: List of (passage_text, start_idx, end_idx)\n        \"\"\"\n        sentences = re.split(r'(?<=[.!?])\\s+', text)\n        passages = []\n        current_passage = []\n        current_length = 0\n        start_idx = 0\n\n        for sentence in sentences:\n            sentence_length = len(sentence.split())\n\n            if current_length + sentence_length > self.max_passage_length:\n                if current_passage:\n                    passage_text = ' '.join(current_passage)\n                    end_idx = start_idx + len(passage_text)\n                    passages.append((passage_text, start_idx, end_idx))\n\n                    # Keep overlap\n                    overlap_text = current_passage[-self.overlap:]\n                    current_passage = overlap_text + [sentence]\n                    start_idx = end_idx - len(' '.join(overlap_text))\n                    current_length = sum(len(s.split()) for s in current_passage)\n            else:\n                current_passage.append(sentence)\n                current_length += sentence_length\n\n        if current_passage:\n            passage_text = ' '.join(current_passage)\n            passages.append((passage_text, start_idx, start_idx + len(passage_text)))\n\n        return passages\n\n# Usage example\nextractor = PassageExtractor(max_passage_length=100, overlap=20)\ntext = \"This is a long document. \" * 50\npassages = extractor.extract_passages(text)\nprint(f\"Extracted {len(passages)} passages from document\")\nprint(f\"First passage: {passages[0][0][:100]}...\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nExtracted 32 passages from document\nFirst passage: This is a long document. This is a long document. This is a long document. This is a long document. ...\n```\n:::\n:::\n\n\n:::{.callout-tip}\n## Context Window Optimization Best Practices\n\n**Passage extraction:**\n\n- Use sentence embeddings for relevance scoring\n- Keep consecutive sentences for narrative flow\n- Extract different amounts per query type (factual: less, explanation: more)\n\n**Deduplication:**\n\n- Use MinHash or embeddings for semantic similarity\n- Set threshold based on acceptable information loss (0.8-0.9)\n- Keep first occurrence (usually most complete)\n\n**Token counting:**\n\n- Use tokenizer from target LLM (different tokenizers vary)\n- Count precisely, don't estimate (estimation errors compound)\n- Reserve tokens for query, instructions, output (typically 20-30%)\n\n**Hierarchical assembly:**\n\n- Always include document titles/metadata\n- Prioritize key passages over full text\n- Add detail progressively until limit reached\n:::\n\n:::{.callout-warning}\n## Context Window Pitfalls\n\nCommon mistakes that degrade RAG quality:\n\n**Over-truncation**: Cutting documents mid-sentence or mid-paragraph loses context\n- **Solution**: Truncate at sentence/paragraph boundaries\n\n**Lost citations**: After extraction/summarization, can't attribute claims\n- **Solution**: Maintain document IDs throughout processing\n\n**Query not in context**: Forgot to include original query in prompt\n- **Solution**: Always include query, even if redundant\n\n**Exceeding limit**: Token estimation off, actual usage exceeds limit\n- **Solution**: Use actual tokenizer, add 10% safety buffer\n:::\n\n## Multi-Stage Retrieval Systems\n\nSingle-stage retrieval (retrieve top-k, done) sacrifices either recall or latency. **Multi-stage retrieval** separates concerns: early stages optimize for recall (don't miss relevant documents), later stages optimize for precision (rank best documents highest). This enables billion-document search with high accuracy and low latency.\n\n### The Multi-Stage Architecture\n\n**Stage 1: Coarse Retrieval (Recall-focused)**\n- Goal: Don't miss relevant documents\n- Method: Fast vector search (ANN)\n- Scale: Search full corpus (1B+ documents)\n- Output: Top-1000 candidates\n- Latency: 50-100ms\n\n**Stage 2: Reranking (Precision-focused)**\n- Goal: Rank best documents highest\n- Method: Cross-encoder model\n- Scale: Rerank 1000 candidates\n- Output: Top-20 documents\n- Latency: 50-200ms\n\n**Stage 3: Final Selection (Context-focused)**\n- Goal: Maximize context window utilization\n- Method: Passage extraction, deduplication\n- Scale: Process 20 documents\n- Output: Optimized context\n- Latency: 10-50ms\n\n::: {#c6325a56 .cell execution_count=3}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show Multi-Stage Retriever\"}\nfrom typing import List\nimport numpy as np\n\n\nclass MultiStageRetriever:\n    \"\"\"Two-stage retrieval: fast first-stage, accurate second-stage.\"\"\"\n    def __init__(self, vector_store, reranker_model=None):\n        self.vector_store = vector_store\n        self.reranker_model = reranker_model\n\n    def retrieve(self, query: str, query_embedding: np.ndarray,\n                 k: int = 5, first_stage_k: int = 20) -> List[Document]:\n        \"\"\"Retrieve documents using two-stage approach.\n\n        Stage 1: Fast vector search retrieves top-N candidates\n        Stage 2: Reranker scores candidates and returns top-K\n        \"\"\"\n        # Stage 1: Fast vector search\n        candidates = self.vector_store.search(query_embedding, k=first_stage_k)\n\n        # Stage 2: Rerank with more expensive model\n        if self.reranker_model:\n            scores = []\n            for doc in candidates:\n                score = np.random.rand()  # Placeholder for reranking\n                scores.append(score)\n\n            # Sort by reranker score\n            ranked_indices = np.argsort(scores)[::-1]\n            candidates = [candidates[i] for i in ranked_indices[:k]]\n\n        return candidates[:k]\n\n# Usage example\nstore = VectorStore(embedding_dim=768)\ndocs = [Document(doc_id=str(i), text=f\"Doc {i}\",\n                 embedding=np.random.rand(768)) for i in range(100)]\nstore.add_documents(docs)\n\nretriever = MultiStageRetriever(vector_store=store)\nresults = retriever.retrieve(\"sample query\", np.random.rand(768), k=5, first_stage_k=20)\nprint(f\"Retrieved {len(results)} documents after two-stage retrieval\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRetrieved 5 documents after two-stage retrieval\n```\n:::\n:::\n\n\n:::{.callout-tip}\n## Multi-Stage Retrieval Best Practices\n\n**Stage separation:**\n\n- Early stages: Fast, high recall (don't miss relevant docs)\n- Later stages: Slow, high precision (rank best docs highest)\n- Each stage should reduce candidates 50-90%\n\n**Stage selection:**\n\n- Always include: Vector retrieval (stage 1) + Reranking (stage 2)\n- Optional: Keyword filter, diversity filter, metadata filter\n- Add stages based on failure analysis (what's missing? what's wrong?)\n\n**Performance optimization:**\n\n- Cache vector search results (query embeddings stable)\n- Batch reranking requests (100 docs × 1ms each = 100ms, batched = 20ms)\n- Run filters in parallel when possible (keyword + metadata)\n- Monitor stage latencies separately (find bottlenecks)\n\n**Quality monitoring:**\n\n- Track recall @ each stage (is stage 1 missing relevant docs?)\n- Track precision @ each stage (is stage 2 improving ranking?)\n- A/B test stage variations (does keyword filter help?)\n:::\n\n## RAG Evaluation Frameworks\n\nRAG systems combine retrieval and generation, requiring evaluation beyond standard IR or NLG metrics. **RAG evaluation frameworks** measure end-to-end quality: retrieval relevance, context utilization, answer accuracy, factual consistency, attribution quality, and user satisfaction.\n\n### The RAG Evaluation Challenge\n\n**Traditional IR metrics** (Recall@k, MRR, NDCG):\n\n- Measure retrieval quality only\n- Don't capture if LLM used retrieved context\n- Don't measure answer accuracy\n\n**Traditional NLG metrics** (BLEU, ROUGE, BERTScore):\n\n- Measure generation quality only\n- Don't capture if answer grounded in context\n- Don't detect hallucinations\n\n**RAG needs both + more**: Did system retrieve relevant docs AND generate accurate answer grounded in those docs?\n\n::: {#ec1accbe .cell execution_count=4}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show Hybrid Search\"}\nfrom typing import Dict\nimport numpy as np\n\n\nclass HybridSearch:\n    \"\"\"Combine dense (vector) and sparse (BM25) retrieval.\"\"\"\n    def __init__(self, vector_store, bm25_index=None, alpha: float = 0.5):\n        self.vector_store = vector_store\n        self.bm25_index = bm25_index\n        self.alpha = alpha\n\n    def search(self, query: str, query_embedding: np.ndarray, k: int = 5) -> List[Document]:\n        \"\"\"Hybrid search combining dense and sparse retrieval.\n\n        Score = alpha * dense_score + (1 - alpha) * sparse_score\n        \"\"\"\n        # Dense retrieval\n        dense_results = self.vector_store.search(query_embedding, k=k*2)\n        dense_scores = {doc.doc_id: 1.0 / (i + 1) for i, doc in enumerate(dense_results)}\n\n        # Sparse retrieval (BM25)\n        if self.bm25_index:\n            sparse_scores = {doc.doc_id: np.random.rand() for doc in dense_results}\n        else:\n            sparse_scores = {doc.doc_id: 0.0 for doc in dense_results}\n\n        # Combine scores\n        combined_scores = {}\n        all_doc_ids = set(dense_scores.keys()) | set(sparse_scores.keys())\n\n        for doc_id in all_doc_ids:\n            dense_score = dense_scores.get(doc_id, 0.0)\n            sparse_score = sparse_scores.get(doc_id, 0.0)\n            combined_scores[doc_id] = self.alpha * dense_score + (1 - self.alpha) * sparse_score\n\n        # Sort by combined score\n        sorted_ids = sorted(combined_scores.keys(), key=lambda x: combined_scores[x], reverse=True)\n\n        # Return top-k documents\n        id_to_doc = {doc.doc_id: doc for doc in dense_results}\n        return [id_to_doc[doc_id] for doc_id in sorted_ids[:k] if doc_id in id_to_doc]\n\n# Usage example\nstore = VectorStore(embedding_dim=768)\ndocs = [Document(doc_id=str(i), text=f\"Doc {i}\",\n                 embedding=np.random.rand(768)) for i in range(50)]\nstore.add_documents(docs)\n\nhybrid = HybridSearch(vector_store=store, alpha=0.7)\nresults = hybrid.search(\"sample query\", np.random.rand(768), k=5)\nprint(f\"Hybrid search returned {len(results)} documents\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nHybrid search returned 5 documents\n```\n:::\n:::\n\n\n:::{.callout-tip}\n## RAG Evaluation Best Practices\n\n**Evaluation data:**\n\n- Start with 100-500 query-answer pairs\n- Cover diversity of query types (factual, how-to, comparison, etc.)\n- Include hard cases (contradictory docs, missing info, ambiguous queries)\n- Get human annotations for ground truth (expensive but essential)\n\n**Automated metrics:**\n\n- Retrieval: Recall@10, Recall@100, MRR\n- Generation: Semantic similarity to ground truth (SentenceTransformers)\n- Faithfulness: NLI models (check entailment between context and answer)\n- Attribution: Check if citations support claims\n\n**Human evaluation:**\n\n- Sample 10-20% for human review\n- Ask: Is answer accurate? Is answer complete? Are citations correct?\n- Use majority vote from 3+ annotators\n- Expensive but ground truth for calibrating automated metrics\n\n**Continuous evaluation:**\n\n- Evaluate on every model/prompt change\n- Track metrics over time (detect regressions)\n- A/B test in production (measure user satisfaction)\n:::\n\n## Handling Contradictory Information\n\nReal-world knowledge bases contain contradictions: different sources disagree, information becomes outdated, perspectives conflict. **Contradiction handling** strategies enable RAG systems to navigate disagreements: detecting conflicts, weighing source credibility, presenting multiple perspectives, and updating knowledge as information evolves.\n\n### The Contradiction Challenge\n\n**Types of contradictions:**\n\n1. **Temporal**: Information changes over time\n   - \"Product price is $99\" (2023) vs \"$149\" (2024)\n   - Solution: Prioritize recent information\n\n2. **Source disagreement**: Different sources conflict\n   - Source A: \"API supports OAuth2\" vs Source B: \"API uses API keys\"\n   - Solution: Weigh by source authority/credibility\n\n3. **Perspective differences**: Subjective judgments vary\n   - Review 1: \"Excellent product\" vs Review 2: \"Poor quality\"\n   - Solution: Present multiple perspectives\n\n4. **Partial vs complete**: One source has partial information\n   - Doc 1: \"Supports Python\" vs Doc 2: \"Supports Python, Java, Go\"\n   - Solution: Prefer more complete information\n\n::: {#867ee3d3 .cell execution_count=5}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show Query Routing\"}\nfrom typing import Dict, List, Optional\nimport numpy as np\n\n\nclass QueryRouter:\n    \"\"\"Route queries to appropriate retrieval strategy.\"\"\"\n    def __init__(self, strategies: Dict[str, any]):\n        self.strategies = strategies\n\n    def route_query(self, query: str, query_embedding: np.ndarray) -> str:\n        \"\"\"Determine which retrieval strategy to use.\n\n        Routes based on query type:\n        - Factual queries -> Dense retrieval\n        - Keyword queries -> Sparse retrieval (BM25)\n        - Complex queries -> Hybrid retrieval\n        \"\"\"\n        query_lower = query.lower()\n\n        if any(word in query_lower for word in ['what', 'when', 'where', 'who']):\n            return 'dense'\n        elif len(query.split()) <= 3:\n            return 'sparse'\n        else:\n            return 'hybrid'\n\n    def retrieve(self, query: str, query_embedding: np.ndarray, k: int = 5) -> List[Document]:\n        \"\"\"Route and retrieve documents.\"\"\"\n        strategy_name = self.route_query(query, query_embedding)\n        strategy = self.strategies.get(strategy_name)\n\n        if strategy:\n            # Handle different strategy interfaces\n            if isinstance(strategy, VectorStore):\n                return strategy.search(query_embedding, k=k)\n            else:\n                return strategy.search(query, query_embedding, k=k)\n        else:\n            first_strategy = list(self.strategies.values())[0]\n            if isinstance(first_strategy, VectorStore):\n                return first_strategy.search(query_embedding, k=k)\n            else:\n                return first_strategy.search(query, query_embedding, k=k)\n\n# Usage example\nstore = VectorStore(embedding_dim=768)\ndocs = [Document(doc_id=str(i), text=f\"Doc {i}\",\n                 embedding=np.random.rand(768)) for i in range(50)]\nstore.add_documents(docs)\n\nstrategies = {\n    'dense': store,\n    'hybrid': HybridSearch(vector_store=store, alpha=0.7)\n}\n\nrouter = QueryRouter(strategies=strategies)\nresults = router.retrieve(\"What is machine learning?\", np.random.rand(768), k=5)\nprint(f\"Query routed and retrieved {len(results)} documents\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nQuery routed and retrieved 5 documents\n```\n:::\n:::\n\n\n:::{.callout-tip}\n## Contradiction Handling Best Practices\n\n**Detection:**\n\n- Use NLI models for semantic contradiction detection\n- Extract claims with high precision (false contradictions confuse users)\n- Focus on factual contradictions (prices, dates, specifications)\n- Ignore stylistic differences (different phrasings of same fact)\n\n**Resolution strategies:**\n\n- **Temporal**: Always prefer recent information (but show date)\n- **Source authority**: Build credibility scores per source type\n- **Confidence**: Use when other signals unavailable\n- **Present multiple**: When confident both are valid (perspectives)\n\n**User experience:**\n\n- Always show sources when contradictions exist\n- Indicate confidence level (\"likely\", \"possibly\", \"conflicting sources\")\n- Provide dates when information might change\n- Allow users to see all perspectives (expandable sections)\n\n**Continuous improvement:**\n\n- Log user selections when presented with contradictions\n- Update source credibility based on user preferences\n- Retrain contradiction detection on corrected examples\n:::\n\n:::{.callout-warning}\n## Contradiction Pitfalls\n\n**Over-resolving**: Automatically picking one answer when both are valid\n- Example: \"Best database for X\" has multiple valid answers\n- Solution: Recognize when question has multiple valid answers\n\n**Temporal confusion**: Using old information because it's higher quality\n- Example: Detailed 2022 guide vs brief 2024 update\n- Solution: Always prioritize recency for rapidly changing topics\n\n**Authority bias**: Always trusting \"authoritative\" source\n- Example: Official docs outdated, community docs current\n- Solution: Consider recency + authority together\n\n**Hidden contradictions**: Not detecting subtle conflicts\n- Example: \"Supports OAuth2\" vs \"Requires API keys\" (implicit contradiction)\n- Solution: Use semantic contradiction detection, not just exact mismatches\n:::\n\n## Conversational AI and Chatbots {#sec-conversational-ai}\n\nRAG powers modern conversational AI systems—customer service bots, internal assistants, and domain-specific copilots. **Embedding-based chatbots** move beyond scripted responses to semantic understanding: matching user intent to relevant knowledge, maintaining conversation context, and generating grounded responses.\n\n### Intent Classification with Embeddings\n\nTraditional chatbots use keyword matching or rule-based intent classification. Embedding-based systems understand semantic intent:\n\n::: {#191f72b1 .cell execution_count=6}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show Intent Classifier\"}\nimport numpy as np\nfrom dataclasses import dataclass\nfrom typing import List, Tuple\n\n\n@dataclass\nclass Intent:\n    \"\"\"Chatbot intent with example utterances.\"\"\"\n    name: str\n    description: str\n    examples: List[str]\n    embedding: np.ndarray = None  # Centroid of example embeddings\n\n\nclass IntentClassifier:\n    \"\"\"Embedding-based intent classification for chatbots.\"\"\"\n\n    def __init__(self, intents: List[Intent], encoder):\n        self.intents = intents\n        self.encoder = encoder\n        self._compute_intent_embeddings()\n\n    def _compute_intent_embeddings(self):\n        \"\"\"Compute centroid embedding for each intent from examples.\"\"\"\n        for intent in self.intents:\n            if intent.examples:\n                example_embeddings = [self.encoder.encode(ex) for ex in intent.examples]\n                intent.embedding = np.mean(example_embeddings, axis=0)\n\n    def classify(self, user_message: str, threshold: float = 0.5) -> Tuple[str, float]:\n        \"\"\"Classify user message into intent with confidence score.\"\"\"\n        message_embedding = self.encoder.encode(user_message)\n\n        best_intent = None\n        best_score = -1\n\n        for intent in self.intents:\n            if intent.embedding is not None:\n                # Cosine similarity\n                score = np.dot(message_embedding, intent.embedding) / (\n                    np.linalg.norm(message_embedding) * np.linalg.norm(intent.embedding)\n                )\n                if score > best_score:\n                    best_score = score\n                    best_intent = intent.name\n\n        if best_score < threshold:\n            return \"unknown\", best_score\n\n        return best_intent, best_score\n\n    def get_similar_examples(self, user_message: str, k: int = 3) -> List[Tuple[str, str, float]]:\n        \"\"\"Find most similar training examples for few-shot prompting.\"\"\"\n        message_embedding = self.encoder.encode(user_message)\n\n        all_examples = []\n        for intent in self.intents:\n            for example in intent.examples:\n                example_embedding = self.encoder.encode(example)\n                score = np.dot(message_embedding, example_embedding) / (\n                    np.linalg.norm(message_embedding) * np.linalg.norm(example_embedding)\n                )\n                all_examples.append((intent.name, example, score))\n\n        all_examples.sort(key=lambda x: x[2], reverse=True)\n        return all_examples[:k]\n\n\n# Example usage with mock encoder\nclass MockEncoder:\n    def encode(self, text):\n        # In production, use sentence-transformers or similar\n        np.random.seed(hash(text) % 2**32)\n        return np.random.randn(384)\n\nencoder = MockEncoder()\nintents = [\n    Intent(\"order_status\", \"Check order status\", [\"Where is my order?\", \"Track my package\", \"Order status\"]),\n    Intent(\"return_request\", \"Request a return\", [\"I want to return this\", \"How do I return?\", \"Return policy\"]),\n    Intent(\"product_info\", \"Product information\", [\"Tell me about this product\", \"Product specs\", \"Features\"]),\n]\n\nclassifier = IntentClassifier(intents, encoder)\nintent, confidence = classifier.classify(\"When will my package arrive?\")\nprint(f\"Intent: {intent}, Confidence: {confidence:.3f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nIntent: unknown, Confidence: 0.055\n```\n:::\n:::\n\n\n### Conversation Context Management\n\nChatbots must maintain context across conversation turns. Embeddings enable **semantic context windows** that retrieve relevant conversation history:\n\n::: {#6e6a0bc1 .cell execution_count=7}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show Conversation Manager\"}\nfrom dataclasses import dataclass, field\nfrom typing import List, Optional\nimport numpy as np\n\n\n@dataclass\nclass ConversationTurn:\n    \"\"\"Single turn in conversation.\"\"\"\n    role: str  # \"user\" or \"assistant\"\n    content: str\n    embedding: Optional[np.ndarray] = None\n    timestamp: float = 0.0\n\n\n@dataclass\nclass ConversationContext:\n    \"\"\"Manages conversation history with semantic retrieval.\"\"\"\n    turns: List[ConversationTurn] = field(default_factory=list)\n    max_turns: int = 50\n\n    def add_turn(self, role: str, content: str, encoder, timestamp: float = 0.0):\n        \"\"\"Add a turn to conversation history.\"\"\"\n        embedding = encoder.encode(content)\n        turn = ConversationTurn(role=role, content=content, embedding=embedding, timestamp=timestamp)\n        self.turns.append(turn)\n\n        # Trim old turns if needed\n        if len(self.turns) > self.max_turns:\n            self.turns = self.turns[-self.max_turns:]\n\n    def get_relevant_context(self, current_query: str, encoder, k: int = 5) -> List[ConversationTurn]:\n        \"\"\"Retrieve most relevant previous turns for current query.\"\"\"\n        if not self.turns:\n            return []\n\n        query_embedding = encoder.encode(current_query)\n\n        # Score each turn by relevance\n        scored_turns = []\n        for i, turn in enumerate(self.turns[:-1]):  # Exclude current turn\n            if turn.embedding is not None:\n                similarity = np.dot(query_embedding, turn.embedding) / (\n                    np.linalg.norm(query_embedding) * np.linalg.norm(turn.embedding)\n                )\n                # Boost recent turns slightly\n                recency_boost = 0.1 * (i / len(self.turns))\n                scored_turns.append((turn, similarity + recency_boost))\n\n        # Sort by score and return top-k\n        scored_turns.sort(key=lambda x: x[1], reverse=True)\n        return [turn for turn, score in scored_turns[:k]]\n\n    def build_context_prompt(self, current_query: str, encoder, max_tokens: int = 2000) -> str:\n        \"\"\"Build context string for LLM prompt.\"\"\"\n        relevant = self.get_relevant_context(current_query, encoder)\n\n        context_parts = []\n        token_estimate = 0\n\n        for turn in relevant:\n            turn_text = f\"{turn.role}: {turn.content}\"\n            turn_tokens = len(turn_text.split()) * 1.3  # Rough token estimate\n\n            if token_estimate + turn_tokens > max_tokens:\n                break\n\n            context_parts.append(turn_text)\n            token_estimate += turn_tokens\n\n        return \"\\n\".join(context_parts)\n\n\n# Example usage\ncontext = ConversationContext()\nencoder = MockEncoder()\n\ncontext.add_turn(\"user\", \"I ordered a laptop last week\", encoder)\ncontext.add_turn(\"assistant\", \"I can help you track your laptop order. What's your order number?\", encoder)\ncontext.add_turn(\"user\", \"It's ORDER-12345\", encoder)\ncontext.add_turn(\"assistant\", \"Order ORDER-12345 shipped yesterday and should arrive Friday.\", encoder)\ncontext.add_turn(\"user\", \"What about the warranty?\", encoder)\n\n# Retrieve relevant context for warranty question\nrelevant = context.get_relevant_context(\"What about the warranty?\", encoder, k=3)\nprint(f\"Retrieved {len(relevant)} relevant turns for warranty question\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRetrieved 3 relevant turns for warranty question\n```\n:::\n:::\n\n\n### Response Selection vs Generation\n\nChatbots can either **select** from pre-written responses or **generate** new ones. Embeddings enable hybrid approaches:\n\n::: {#d2e02238 .cell execution_count=8}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show Hybrid Response System\"}\nfrom dataclasses import dataclass\nfrom typing import List, Optional, Tuple\nimport numpy as np\n\n\n@dataclass\nclass CannedResponse:\n    \"\"\"Pre-written response for common queries.\"\"\"\n    id: str\n    intent: str\n    response: str\n    embedding: Optional[np.ndarray] = None\n\n\nclass HybridResponseSystem:\n    \"\"\"Combines response selection with RAG-based generation.\"\"\"\n\n    def __init__(self, canned_responses: List[CannedResponse], encoder,\n                 selection_threshold: float = 0.85):\n        self.responses = canned_responses\n        self.encoder = encoder\n        self.selection_threshold = selection_threshold\n        self._compute_response_embeddings()\n\n    def _compute_response_embeddings(self):\n        \"\"\"Pre-compute embeddings for canned responses.\"\"\"\n        for response in self.responses:\n            response.embedding = self.encoder.encode(response.response)\n\n    def get_response(self, user_query: str, intent: str) -> Tuple[str, str]:\n        \"\"\"\n        Get response for user query.\n        Returns (response_text, method) where method is 'selected' or 'generated'.\n        \"\"\"\n        query_embedding = self.encoder.encode(user_query)\n\n        # Find best matching canned response for this intent\n        best_response = None\n        best_score = -1\n\n        for response in self.responses:\n            if response.intent == intent and response.embedding is not None:\n                score = np.dot(query_embedding, response.embedding) / (\n                    np.linalg.norm(query_embedding) * np.linalg.norm(response.embedding)\n                )\n                if score > best_score:\n                    best_score = score\n                    best_response = response\n\n        # If high confidence match, use canned response\n        if best_score >= self.selection_threshold and best_response:\n            return best_response.response, \"selected\"\n\n        # Otherwise, would trigger RAG generation (placeholder)\n        return f\"[Generated response for: {user_query}]\", \"generated\"\n\n\n# Example usage\nresponses = [\n    CannedResponse(\"r1\", \"order_status\", \"You can track your order at example.com/track\"),\n    CannedResponse(\"r2\", \"return_request\", \"Returns are accepted within 30 days. Visit example.com/returns\"),\n    CannedResponse(\"r3\", \"product_info\", \"Our products come with a 1-year warranty.\"),\n]\n\nsystem = HybridResponseSystem(responses, MockEncoder())\nresponse, method = system.get_response(\"How do I track my package?\", \"order_status\")\nprint(f\"Response ({method}): {response}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nResponse (generated): [Generated response for: How do I track my package?]\n```\n:::\n:::\n\n\n:::{.callout-tip}\n## Conversational AI Best Practices\n\n**Intent Classification:**\n\n- **Few-shot examples**: 5-10 examples per intent is often sufficient with good embeddings\n- **Hierarchical intents**: Parent → child classification for complex domains\n- **Fallback handling**: Route low-confidence queries to human agents or clarification\n- **Active learning**: Log low-confidence queries for labeling and model improvement\n\n**Context Management:**\n\n- **Semantic retrieval**: Don't just use last N turns—retrieve semantically relevant history\n- **Entity tracking**: Maintain extracted entities (order numbers, product names) across turns\n- **Session boundaries**: Clear context appropriately between sessions\n- **Privacy**: Exclude sensitive information from context retrieval\n\n**Response Strategy:**\n\n- **Canned for compliance**: Use pre-written responses for legal, safety, policy questions\n- **Generated for flexibility**: Use RAG for complex, context-dependent queries\n- **Hybrid routing**: Classify query type to select response strategy\n- **Guardrails**: Always validate generated responses before sending\n:::\n\n## Embedding-Based Summarization {#sec-embedding-summarization}\n\nSummarization with embeddings identifies **representative content**—selecting sentences or passages that best capture document meaning. Unlike generative summarization, embedding-based approaches are extractive, selecting existing text rather than generating new text.\n\n### Representative Sentence Selection\n\nThe core idea: sentences with embeddings closest to the document centroid are most representative:\n\n::: {#27cf3979 .cell execution_count=9}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show Extractive Summarizer\"}\nfrom dataclasses import dataclass\nfrom typing import List\nimport numpy as np\n\n\n@dataclass\nclass Sentence:\n    \"\"\"Sentence with embedding.\"\"\"\n    text: str\n    embedding: np.ndarray\n    position: int  # Position in original document\n\n\nclass ExtractiveSummarizer:\n    \"\"\"Embedding-based extractive summarization.\"\"\"\n\n    def __init__(self, encoder):\n        self.encoder = encoder\n\n    def summarize(self, document: str, num_sentences: int = 3,\n                  diversity_weight: float = 0.3) -> List[str]:\n        \"\"\"\n        Extract representative sentences from document.\n\n        Args:\n            document: Input text\n            num_sentences: Number of sentences to extract\n            diversity_weight: Balance between relevance (0) and diversity (1)\n        \"\"\"\n        # Split into sentences (simplified)\n        raw_sentences = [s.strip() for s in document.replace('!', '.').replace('?', '.').split('.') if s.strip()]\n\n        if len(raw_sentences) <= num_sentences:\n            return raw_sentences\n\n        # Compute embeddings\n        sentences = []\n        for i, text in enumerate(raw_sentences):\n            embedding = self.encoder.encode(text)\n            sentences.append(Sentence(text=text, embedding=embedding, position=i))\n\n        # Compute document centroid\n        all_embeddings = np.array([s.embedding for s in sentences])\n        centroid = np.mean(all_embeddings, axis=0)\n\n        # Select sentences using MMR (Maximal Marginal Relevance)\n        selected = []\n        remaining = sentences.copy()\n\n        for _ in range(num_sentences):\n            best_sentence = None\n            best_score = -float('inf')\n\n            for sentence in remaining:\n                # Relevance: similarity to centroid\n                relevance = np.dot(sentence.embedding, centroid) / (\n                    np.linalg.norm(sentence.embedding) * np.linalg.norm(centroid)\n                )\n\n                # Diversity: dissimilarity to already selected sentences\n                if selected:\n                    max_sim_to_selected = max(\n                        np.dot(sentence.embedding, s.embedding) / (\n                            np.linalg.norm(sentence.embedding) * np.linalg.norm(s.embedding)\n                        )\n                        for s in selected\n                    )\n                    diversity = 1 - max_sim_to_selected\n                else:\n                    diversity = 1\n\n                # MMR score\n                score = (1 - diversity_weight) * relevance + diversity_weight * diversity\n\n                if score > best_score:\n                    best_score = score\n                    best_sentence = sentence\n\n            if best_sentence:\n                selected.append(best_sentence)\n                remaining.remove(best_sentence)\n\n        # Return in original document order\n        selected.sort(key=lambda s: s.position)\n        return [s.text for s in selected]\n\n    def summarize_multi_document(self, documents: List[str], num_sentences: int = 5) -> List[str]:\n        \"\"\"Summarize multiple documents by finding representative sentences across all.\"\"\"\n        all_sentences = []\n\n        for doc_idx, document in enumerate(documents):\n            raw_sentences = [s.strip() for s in document.replace('!', '.').replace('?', '.').split('.') if s.strip()]\n            for i, text in enumerate(raw_sentences):\n                embedding = self.encoder.encode(text)\n                all_sentences.append(Sentence(text=text, embedding=embedding, position=i + doc_idx * 1000))\n\n        if len(all_sentences) <= num_sentences:\n            return [s.text for s in all_sentences]\n\n        # Compute global centroid\n        all_embeddings = np.array([s.embedding for s in all_sentences])\n        centroid = np.mean(all_embeddings, axis=0)\n\n        # Score by distance to centroid\n        scores = []\n        for sentence in all_sentences:\n            score = np.dot(sentence.embedding, centroid) / (\n                np.linalg.norm(sentence.embedding) * np.linalg.norm(centroid)\n            )\n            scores.append((sentence, score))\n\n        scores.sort(key=lambda x: x[1], reverse=True)\n        return [s.text for s, _ in scores[:num_sentences]]\n\n\n# Example usage\nsummarizer = ExtractiveSummarizer(MockEncoder())\ndocument = \"\"\"\nMachine learning has transformed how we process data.\nDeep learning models can recognize patterns in images and text.\nNeural networks require large amounts of training data.\nTransfer learning allows models to leverage pre-trained knowledge.\nEmbeddings represent data as dense vectors for similarity computation.\n\"\"\"\nsummary = summarizer.summarize(document, num_sentences=2)\nprint(f\"Summary ({len(summary)} sentences):\")\nfor s in summary:\n    print(f\"  - {s}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSummary (2 sentences):\n  - Deep learning models can recognize patterns in images and text\n  - Embeddings represent data as dense vectors for similarity computation\n```\n:::\n:::\n\n\n### Cluster-Based Summarization\n\nFor longer documents, cluster sentences first, then select representatives from each cluster:\n\n::: {#3b98a67a .cell execution_count=10}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show Cluster-Based Summarizer\"}\nfrom typing import List, Dict\nimport numpy as np\n\n\nclass ClusterSummarizer:\n    \"\"\"Cluster-based summarization for long documents.\"\"\"\n\n    def __init__(self, encoder):\n        self.encoder = encoder\n\n    def summarize(self, document: str, num_clusters: int = 3) -> List[str]:\n        \"\"\"\n        Summarize by clustering sentences and selecting cluster representatives.\n        \"\"\"\n        # Split and embed sentences\n        raw_sentences = [s.strip() for s in document.replace('!', '.').replace('?', '.').split('.') if s.strip()]\n\n        if len(raw_sentences) <= num_clusters:\n            return raw_sentences\n\n        embeddings = np.array([self.encoder.encode(s) for s in raw_sentences])\n\n        # Simple k-means clustering\n        centroids = self._kmeans(embeddings, num_clusters)\n\n        # Assign sentences to clusters\n        clusters: Dict[int, List[tuple]] = {i: [] for i in range(num_clusters)}\n        for i, (sentence, embedding) in enumerate(zip(raw_sentences, embeddings)):\n            distances = [np.linalg.norm(embedding - c) for c in centroids]\n            cluster_id = np.argmin(distances)\n            clusters[cluster_id].append((sentence, embedding, i))\n\n        # Select representative from each cluster (closest to centroid)\n        representatives = []\n        for cluster_id, members in clusters.items():\n            if not members:\n                continue\n\n            centroid = centroids[cluster_id]\n            best_sentence = min(\n                members,\n                key=lambda x: np.linalg.norm(x[1] - centroid)\n            )\n            representatives.append((best_sentence[0], best_sentence[2]))  # text, position\n\n        # Return in document order\n        representatives.sort(key=lambda x: x[1])\n        return [text for text, _ in representatives]\n\n    def _kmeans(self, embeddings: np.ndarray, k: int, max_iters: int = 100) -> np.ndarray:\n        \"\"\"Simple k-means clustering.\"\"\"\n        # Initialize centroids randomly\n        indices = np.random.choice(len(embeddings), k, replace=False)\n        centroids = embeddings[indices].copy()\n\n        for _ in range(max_iters):\n            # Assign points to nearest centroid\n            assignments = []\n            for emb in embeddings:\n                distances = [np.linalg.norm(emb - c) for c in centroids]\n                assignments.append(np.argmin(distances))\n\n            # Update centroids\n            new_centroids = []\n            for i in range(k):\n                cluster_points = embeddings[np.array(assignments) == i]\n                if len(cluster_points) > 0:\n                    new_centroids.append(cluster_points.mean(axis=0))\n                else:\n                    new_centroids.append(centroids[i])\n\n            new_centroids = np.array(new_centroids)\n\n            # Check convergence\n            if np.allclose(centroids, new_centroids):\n                break\n\n            centroids = new_centroids\n\n        return centroids\n\n\n# Example\ncluster_summarizer = ClusterSummarizer(MockEncoder())\nlong_doc = \"\"\"\nThe economy grew by 3% this quarter. Employment rates improved significantly.\nNew technology startups raised record funding. AI companies led the investment surge.\nClimate change policies face opposition. Environmental groups demand stronger action.\nSports teams prepare for the championship. Fans eagerly await the final matches.\n\"\"\"\nsummary = cluster_summarizer.summarize(long_doc, num_clusters=3)\nprint(f\"Cluster-based summary:\")\nfor s in summary:\n    print(f\"  - {s}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCluster-based summary:\n  - The economy grew by 3% this quarter\n  - Climate change policies face opposition\n  - Environmental groups demand stronger action\n```\n:::\n:::\n\n\n:::{.callout-tip}\n## Summarization Best Practices\n\n**Extraction Strategy:**\n\n- **MMR for diversity**: Avoid selecting redundant sentences\n- **Position bias**: First/last sentences often contain key information\n- **Length normalization**: Don't over-favor short or long sentences\n- **Cluster-based**: For long documents, cluster then select representatives\n\n**Quality Considerations:**\n\n- **Coherence**: Selected sentences should flow logically\n- **Coverage**: Summary should cover main topics, not just one aspect\n- **Redundancy**: Remove near-duplicate information\n- **Context preservation**: Include enough context for sentences to be understandable\n\n**Scale Considerations:**\n\n- **Pre-compute embeddings**: For document collections, embed once and reuse\n- **Hierarchical summarization**: Summarize sections, then summarize summaries\n- **Incremental updates**: For streaming documents, maintain running summaries\n- **Caching**: Cache summaries for frequently accessed documents\n:::\n\n## Key Takeaways\n\n- **RAG combines retrieval and generation for grounded LLM responses**: Retrieving relevant context from vector databases enables accurate answers over billion-document corpora while maintaining attribution and enabling real-time knowledge updates\n\n- **Enterprise RAG requires multi-component architecture**: Query understanding, retrieval, reranking, context assembly, generation, and validation each play critical roles, and each must scale independently\n\n- **Context window optimization maximizes information density**: Passage extraction, deduplication, and hierarchical assembly enable fitting relevant information within LLM token limits while preserving key facts\n\n- **Multi-stage retrieval balances recall and precision**: Early stages (vector search) optimize for recall across billion-doc corpora, later stages (reranking, diversity) optimize for precision with expensive models on small candidate sets\n\n- **RAG evaluation requires measuring beyond retrieval and generation**: End-to-end metrics must capture retrieval relevance, context utilization, answer accuracy, factual consistency, attribution quality, and user satisfaction\n\n- **Contradiction handling enables navigating disagreements in knowledge bases**: Temporal resolution (prefer recent), source authority weighting (prefer credible), and multi-perspective presentation handle conflicts when sources disagree\n\n- **Production RAG demands comprehensive engineering**: Caching, batching, circuit breakers, monitoring, A/B testing, and continuous evaluation separate research prototypes from production systems serving millions of users\n\n- **Conversational AI leverages embeddings for semantic intent matching**: Embedding-based chatbots classify user intent from examples, retrieve semantically relevant conversation history, and combine canned responses with generated content for appropriate flexibility and compliance\n\n- **Embedding-based summarization extracts representative content**: Centroid-based selection and MMR diversity ensure summaries capture key information without redundancy, while cluster-based approaches handle long documents by selecting representatives from each topic cluster\n\n## Looking Ahead\n\nThis chapter demonstrated how RAG leverages embeddings for grounded generation at enterprise scale. @sec-semantic-search expands semantic search beyond text: multi-modal search across text, images, audio, and video; code search for software intelligence; scientific literature and patent search with domain-specific understanding; media and content discovery across creative assets; and knowledge graph integration for structured reasoning. These applications demonstrate embeddings' versatility across diverse modalities and domains.\n\n## Further Reading\n\n### RAG Foundations\n- Lewis, Patrick, et al. (2020). \"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.\" NeurIPS.\n- Guu, Kelvin, et al. (2020). \"REALM: Retrieval-Augmented Language Model Pre-Training.\" ICML.\n- Izacard, Gautier, et al. (2021). \"Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering.\" EACL.\n\n### Retrieval Systems\n- Karpukhin, Vladimir, et al. (2020). \"Dense Passage Retrieval for Open-Domain Question Answering.\" EMNLP.\n- Xiong, Lee, et al. (2021). \"Approximate Nearest Neighbor Negative Contrastive Learning for Dense Text Retrieval.\" ICLR.\n- Santhanam, Keshav, et al. (2022). \"ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction.\" NAACL.\n\n### Context Optimization\n- Jiang, Zhengbao, et al. (2023). \"Long-Form Factuality in Large Language Models.\" arxiv.\n- Liu, Nelson F., et al. (2023). \"Lost in the Middle: How Language Models Use Long Contexts.\" arxiv.\n- Press, Ofir, et al. (2022). \"Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation.\" ICLR.\n\n### RAG Evaluation\n- Chen, Daixuan, et al. (2023). \"CRUD-RAG: Benchmarking Retrieval-Augmented Generation for Time-Sensitive Knowledge.\" arxiv.\n- Es, Shahul, et al. (2023). \"RAGAS: Automated Evaluation of Retrieval Augmented Generation.\" arxiv.\n- Liu, Yang, et al. (2023). \"Evaluating the Factuality of Large Language Models.\" ACL.\n\n### Production Systems\n- Anthropic (2023). \"Claude 2 System Card.\"\n- OpenAI (2023). \"GPT-4 Technical Report.\"\n- Thoppilan, Romal, et al. (2022). \"LaMDA: Language Models for Dialog Applications.\" arxiv.\n\n### Contradiction Detection\n- Welleck, Sean, et al. (2019). \"Dialogue Natural Language Inference.\" ACL.\n- Honovich, Or, et al. (2022). \"TRUE: Re-evaluating Factual Consistency Evaluation.\" NAACL.\n- Wang, Cunxiang, et al. (2020). \"CARE: Commonsense-Aware Reasoning for Conversational AI.\" ACL.\n\n### Multi-Stage Retrieval\n- Nogueira, Rodrigo, et al. (2019). \"Passage Re-ranking with BERT.\" arxiv.\n- Gao, Luyu, et al. (2021). \"Rethink Training of BERT Rerankers in Multi-Stage Retrieval Pipeline.\" ECIR.\n- Carbonell, Jaime, and Jade Goldstein (1998). \"The Use of MMR, Diversity-Based Reranking for Reordering Documents and Producing Summaries.\" SIGIR.\n\n",
    "supporting": [
      "ch11_rag_at_scale_files/figure-epub"
    ],
    "filters": [],
    "engineDependencies": {
      "jupyter": [
        {
          "jsWidgets": false,
          "jupyterWidgets": false,
          "htmlLibraries": []
        }
      ]
    }
  }
}