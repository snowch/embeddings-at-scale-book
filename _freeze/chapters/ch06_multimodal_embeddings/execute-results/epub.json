{
  "hash": "eb0f9173a4d171418f30e205b75aaf37",
  "result": {
    "engine": "jupyter",
    "markdown": "# Multi-Modal Embeddings {#sec-multimodal-embeddings}\n\n::: callout-note\n## Chapter Overview\n\nThis chapter covers multi-modal embeddings—representations that map different data types (text, images, audio) into a shared vector space where they can be directly compared. We explore how these unified spaces enable powerful cross-modal capabilities like searching images with text queries, zero-shot classification, and multi-sensor fusion.\n:::\n\n## What Are Multi-Modal Embeddings?\n\nMulti-modal embeddings map different data types into a **shared vector space** where they can be directly compared. This enables powerful cross-modal capabilities: searching images with text queries, finding text descriptions for images, or categorizing images without any training examples.\n\nThe key insight is training two (or more) encoders so that matching pairs produce similar vectors. CLIP, trained on 400 million image-text pairs from the internet, learns that \"a photo of a cat\" and an actual cat photo should have nearby embeddings.\n\n::: {.callout-note}\n## Zero-Shot Categorization via Embeddings\n**Zero-shot categorization** means assigning categories the model was never explicitly trained on—and it works through embedding similarity, not a traditional classifier. Instead of training a \"sunset vs ocean vs forest\" classifier, you describe categories in text (\"a photo of a sunset\"), embed both the text and image, and find the closest match. The model generalizes from its pre-training on millions of image-text pairs to recognize new concepts.\n:::\n\n## How Multi-Modal Spaces Work\n\n::: {#cell-fig-multimodal-concept .cell execution_count=1}\n\n::: {.cell-output .cell-output-display}\n![Multi-modal embeddings: separate encoders map text and images into a shared space where matching concepts are close together.](ch06_multimodal_embeddings_files/figure-epub/fig-multimodal-concept-output-1.png){#fig-multimodal-concept}\n:::\n:::\n\n\n## Zero-Shot Image Classification with CLIP\n\n::: {#d8016de9 .cell execution_count=2}\n``` {.python .cell-code code-fold=\"false\"}\n\"\"\"\nMulti-Modal Embeddings with CLIP: Zero-Shot Image Categorization\n\nCLIP embeds text and images into the same 512-dimensional space.\nWe can categorize images by comparing them to text descriptions—\nno training on the target categories required.\n\"\"\"\n\nimport torch\nimport numpy as np\nfrom PIL import Image\nfrom transformers import CLIPProcessor, CLIPModel\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport warnings\nimport logging\n\nlogging.getLogger(\"transformers\").setLevel(logging.ERROR)\nwarnings.filterwarnings(\"ignore\")\n\n# Load CLIP model\nmodel = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\nprocessor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\", use_fast=True)\n\n# Create test images with distinct characteristics\nnp.random.seed(42)\nimages = {\n    'sunset': Image.fromarray(\n        np.random.randint([200, 100, 50], [255, 150, 100], (224, 224, 3), dtype=np.uint8)\n    ),\n    'ocean': Image.fromarray(\n        np.random.randint([30, 80, 150], [80, 150, 220], (224, 224, 3), dtype=np.uint8)\n    ),\n    'forest': Image.fromarray(\n        np.random.randint([20, 80, 20], [80, 150, 80], (224, 224, 3), dtype=np.uint8)\n    ),\n}\n\n# Text descriptions to match against\ntext_labels = [\n    \"a photo of a sunset with warm orange colors\",\n    \"a photo of the ocean with blue water\",\n    \"a photo of a green forest\",\n]\n\n# Encode both modalities\nimage_inputs = processor(images=list(images.values()), return_tensors=\"pt\", padding=True)\ntext_inputs = processor(text=text_labels, return_tensors=\"pt\", padding=True)\n\nwith torch.no_grad():\n    image_embeds = model.get_image_features(**image_inputs).numpy()\n    text_embeds = model.get_text_features(**text_inputs).numpy()\n\n# Compare via cosine similarity\nsimilarities = cosine_similarity(image_embeds, text_embeds)\n\nprint(\"Zero-shot categorization: matching images to text descriptions\\n\")\nprint(f\"Embedding dimension: {image_embeds.shape[1]}\\n\")\n\nfor i, img_name in enumerate(images.keys()):\n    best_match = text_labels[similarities[i].argmax()]\n    print(f\"{img_name:8} → {best_match}\")\n    print(f\"          scores: {[f'{s:.2f}' for s in similarities[i]]}\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nZero-shot categorization: matching images to text descriptions\n\nEmbedding dimension: 512\n\nsunset   → a photo of a sunset with warm orange colors\n          scores: ['0.25', '0.20', '0.21']\n\nocean    → a photo of the ocean with blue water\n          scores: ['0.18', '0.27', '0.22']\n\nforest   → a photo of a green forest\n          scores: ['0.17', '0.22', '0.28']\n\n```\n:::\n:::\n\n\n## When to Use Multi-Modal Embeddings\n\n- **Cross-modal search**—find images using text queries, or find text descriptions for images\n- **Zero-shot classification**—categorize images without training on specific categories\n- **Visual question answering**—answer questions about image content\n- **Product search**—search catalogs using both text and uploaded images (see @sec-semantic-search)\n- **Content moderation**—match policy descriptions to visual content\n- **Multi-sensor fusion**—combine camera, audio, and other sensor data\n\n## Popular Multi-Modal Architectures\n\n| Architecture | Type | Strengths | Use Cases |\n|-------------|------|-----------|-----------|\n| [CLIP](https://github.com/openai/CLIP) | Text-image | Fast, versatile | Search, classification |\n| [BLIP-2](https://github.com/salesforce/LAVIS) | Text-image | Captioning + retrieval | Image understanding |\n| [ImageBind](https://github.com/facebookresearch/ImageBind) | 6 modalities | Audio, depth, thermal | Multi-sensor fusion |\n| [LLaVA](https://github.com/haotian-liu/LLaVA) | Vision-language | Conversational | Visual Q&A |\n\n: Multi-modal embedding architectures {.striped}\n\n## Advanced: Multi-Modal Fusion Strategies {.unnumbered}\n\n::: {.callout-note}\n## Optional Section\nThis section covers advanced techniques for combining modalities. Skip if you just need basic multi-modal capabilities.\n:::\n\nWhen working with multiple modalities, you can combine embeddings in several ways:\n\n### Early Fusion\n\nCombine embeddings before indexing:\n\n```python\ndef early_fusion(text_emb, image_emb, weights=(0.5, 0.5)):\n    \"\"\"Combine modalities into a single embedding.\"\"\"\n    fused = weights[0] * text_emb + weights[1] * image_emb\n    return fused / np.linalg.norm(fused)  # Normalize\n```\n\n**Best for**: Static entities where all modalities are always available (products with descriptions and images).\n\n### Late Fusion\n\nCombine similarity scores after separate retrieval:\n\n```python\ndef late_fusion(query_embs, candidate_embs, weights):\n    \"\"\"Combine similarity scores from multiple modalities.\"\"\"\n    total_score = 0\n    for modality, weight in weights.items():\n        if modality in query_embs:\n            score = cosine_similarity(query_embs[modality], candidate_embs[modality])\n            total_score += weight * score\n    return total_score\n```\n\n**Best for**: Queries where available modalities vary (user might search with text only, or text + image).\n\n### Attention Fusion\n\nLearn which modalities to emphasize for each query:\n\n```python\ndef attention_fusion(modality_embeddings):\n    \"\"\"Dynamically weight modalities using attention.\"\"\"\n    stacked = torch.stack(modality_embeddings)\n    attention_weights = torch.softmax(\n        torch.matmul(stacked, stacked.transpose(0, 1)), dim=-1\n    )\n    attended = torch.matmul(attention_weights, stacked)\n    return attended.mean(dim=0)\n```\n\n**Best for**: Complex queries where modality importance varies by context.\n\n## Key Takeaways\n\n- **Multi-modal embeddings** create a shared space where different data types (text, images, audio) can be directly compared\n\n- **Zero-shot classification** works by comparing embeddings to text descriptions—no training on specific categories required\n\n- **CLIP** is the most popular approach, trained on 400M image-text pairs to align visual and textual concepts\n\n- **Fusion strategies** determine how to combine modalities: early (before indexing), late (after retrieval), or attention-based (learned weighting)\n\n## Looking Ahead\n\nNow that you understand multi-modal embeddings, @sec-graph-embeddings explores graph embeddings—representations that capture network structure and relationships.\n\n## Further Reading\n\n- Radford, A., et al. (2021). \"Learning Transferable Visual Models From Natural Language Supervision.\" *arXiv:2103.00020*\n- Li, J., et al. (2023). \"BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models.\" *ICML*\n- Girdhar, R., et al. (2023). \"ImageBind: One Embedding Space To Bind Them All.\" *CVPR*\n\n",
    "supporting": [
      "ch06_multimodal_embeddings_files/figure-epub"
    ],
    "filters": [],
    "engineDependencies": {
      "jupyter": [
        {
          "jsWidgets": false,
          "jupyterWidgets": false,
          "htmlLibraries": []
        }
      ]
    }
  }
}