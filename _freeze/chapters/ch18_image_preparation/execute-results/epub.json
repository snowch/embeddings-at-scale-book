{
  "hash": "238316eb20da60bf8336671dbaff3e9b",
  "result": {
    "engine": "jupyter",
    "markdown": "# Image Preparation for Embeddings {#sec-image-preparation}\n\n:::{.callout-note}\n## Chapter Overview\nImage embedding systems face different challenges than text: preprocessing requirements, internal patch-based processing, handling large images, and extracting regions of interest. This chapter covers how modern vision models create embeddings, practical preprocessing strategies, approaches for large-scale imagery (satellite, medical), and techniques for multi-object scenes. You'll learn to prepare images for optimal embedding quality across diverse visual domains.\n:::\n\nThe previous chapter explored how text documents are chunked into semantic units for embedding. Images present a parallel but distinct challenge: while text chunking is primarily a user decision, image \"chunking\" often happens inside the model itself. However, image preparation decisions—preprocessing, cropping, tiling, and region extraction—significantly impact embedding quality. Understanding these choices is essential for building effective visual search and multi-modal systems.\n\n## How Image Embedding Models Work\n\nBefore diving into preparation strategies, let's understand what happens inside modern image embedding models.\n\n### From Pixels to Vectors\n\nImage embedding models transform raw pixels into dense vector representations:\n\n```\nInput: RGB Image (224 × 224 × 3 = 150,528 values)\n                    ↓\n        Image Embedding Model\n                    ↓\nOutput: Embedding Vector (768 or 1024 dimensions)\n\nCompression ratio: ~150x to ~200x\n```\n\nUnlike text where chunking is explicit, image models handle spatial \"chunking\" internally through their architecture.\n\n### CNN-Based Embeddings (ResNet, EfficientNet)\n\nConvolutional Neural Networks process images through hierarchical feature extraction:\n\n::: {#97947b5a .cell execution_count=1}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show CNN Embeddings\"}\nimport torch\nimport torchvision.models as models\nimport torchvision.transforms as transforms\nimport numpy as np\nfrom typing import List\n\nclass CNNEmbedder:\n    \"\"\"CNN-based image embedding with batched inference.\"\"\"\n    def __init__(self, model_name: str = \"resnet50\", device: str = None, batch_size: int = 32):\n        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.batch_size = batch_size\n\n        # Load model\n        if model_name == \"resnet50\":\n            model = models.resnet50(weights=\"IMAGENET1K_V1\")\n            self.embedding_dim = 2048\n        elif model_name == \"resnet18\":\n            model = models.resnet18(weights=\"IMAGENET1K_V1\")\n            self.embedding_dim = 512\n        else:\n            raise ValueError(f\"Unknown model: {model_name}\")\n\n        # Remove classifier\n        modules = list(model.children())[:-1]\n        self.model = torch.nn.Sequential(*modules).to(self.device)\n        self.model.eval()\n\n        self.preprocess = transforms.Compose([\n            transforms.Resize(256),\n            transforms.CenterCrop(224),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n        ])\n\n    def encode(self, images: List) -> np.ndarray:\n        \"\"\"Encode images to embeddings.\"\"\"\n        from PIL import Image\n        all_embeddings = []\n\n        for i in range(0, len(images), self.batch_size):\n            batch_images = images[i:i + self.batch_size]\n            tensors = [self.preprocess(img if isinstance(img, Image.Image) else Image.fromarray(img))\n                      for img in batch_images]\n            batch_tensor = torch.stack(tensors).to(self.device)\n\n            with torch.no_grad():\n                features = self.model(batch_tensor)\n                embeddings = features.squeeze(-1).squeeze(-1)\n                all_embeddings.append(embeddings.cpu().numpy())\n\n        return np.vstack(all_embeddings)\n\n# Usage example\nprint(\"CNNEmbedder ready for ResNet50 embeddings (2048-dim)\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCNNEmbedder ready for ResNet50 embeddings (2048-dim)\n```\n:::\n:::\n\n\n**How CNNs create embeddings:**\n\n1. **Convolutional layers**: Detect local features (edges, textures, shapes)\n2. **Pooling layers**: Reduce spatial dimensions while preserving important features\n3. **Deeper layers**: Combine local features into semantic concepts\n4. **Global pooling**: Collapse spatial dimensions into a single vector\n\n```\n224×224×3 → [Conv] → 112×112×64 → [Conv] → 56×56×128 → ... → 7×7×2048 → [Pool] → 2048-dim vector\n   Input      Early features        Mid features           Late features    Embedding\n            (edges, colors)     (textures, parts)      (objects, scenes)\n```\n\n### Transformer-Based Embeddings (ViT, CLIP)\n\nVision Transformers take a fundamentally different approach—they explicitly split images into patches:\n\n::: {#30517602 .cell execution_count=2}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show ViT Embeddings\"}\nimport torch\nimport torchvision.models as models\nimport torchvision.transforms as transforms\nimport numpy as np\nfrom typing import List\n\nclass ViTEmbedder:\n    \"\"\"Vision Transformer embedder with patch-based processing.\"\"\"\n    def __init__(self, model_name: str = \"vit_b_16\", device: str = None, batch_size: int = 32):\n        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.batch_size = batch_size\n        self.embedding_dims = {\"vit_b_16\": 768, \"vit_b_32\": 768, \"vit_l_16\": 1024}\n        self.embedding_dim = self.embedding_dims[model_name]\n\n        # Load model\n        model_fn = {\"vit_b_16\": models.vit_b_16, \"vit_b_32\": models.vit_b_32, \"vit_l_16\": models.vit_l_16}\n        self.model = model_fn[model_name](weights=\"IMAGENET1K_V1\")\n        self.model.heads = torch.nn.Identity()\n        self.model = self.model.to(self.device)\n        self.model.eval()\n\n        self.preprocess = transforms.Compose([\n            transforms.Resize(256),\n            transforms.CenterCrop(224),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n        ])\n\n    def encode(self, images: List, normalize: bool = True) -> np.ndarray:\n        \"\"\"Encode images to embeddings.\"\"\"\n        from PIL import Image\n        all_embeddings = []\n\n        for i in range(0, len(images), self.batch_size):\n            batch_images = images[i:i + self.batch_size]\n            tensors = [self.preprocess(img if isinstance(img, Image.Image) else Image.fromarray(img))\n                      for img in batch_images]\n            batch_tensor = torch.stack(tensors).to(self.device)\n\n            with torch.no_grad():\n                embeddings = self.model(batch_tensor)\n                if normalize:\n                    embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)\n                all_embeddings.append(embeddings.cpu().numpy())\n\n        return np.vstack(all_embeddings)\n\n# Usage example\nprint(\"ViTEmbedder ready: 224x224 images split into 196 patches (14x14 grid)\")\nprint(\"CLS token provides 768-dim embedding (ViT-Base; variants differ: ViT-Large=1024, ViT-Huge=1280)\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nViTEmbedder ready: 224x224 images split into 196 patches (14x14 grid)\nCLS token provides 768-dim embedding (ViT-Base; variants differ: ViT-Large=1024, ViT-Huge=1280)\n```\n:::\n:::\n\n\n**How ViT creates embeddings:**\n\n1. **Patch extraction**: Split image into fixed-size patches (typically 16×16 or 14×14 pixels)\n2. **Linear projection**: Each patch becomes a token embedding\n3. **Position encoding**: Add spatial position information\n4. **Transformer layers**: Self-attention lets patches interact\n5. **CLS token**: Special token aggregates information into final embedding\n\n```\n224×224 image → 196 patches (14×14 grid of 16×16 patches)\n                    ↓\nEach patch → 768-dim token (linear projection)\n                    ↓\n[CLS] + 196 patch tokens + position embeddings\n                    ↓\nTransformer layers (self-attention)\n                    ↓\n[CLS] token output → embedding (768-dim for ViT-Base, 1024 for ViT-Large)\n```\n\n### The Key Insight: Internal vs External Chunking\n\n| Aspect | Text Embeddings | Image Embeddings |\n|--------|-----------------|------------------|\n| **User chunking** | Required (documents → chunks) | Optional (whole images often work) |\n| **Model chunking** | Tokenization (subwords) | Patches (ViT) or receptive fields (CNN) |\n| **Semantic units** | Sentences, paragraphs | Objects, regions, scenes |\n| **Boundary decisions** | Made during preprocessing | Made by model architecture |\n\n: Text vs image chunking comparison {.striped}\n\nFor images, the model handles spatial decomposition. Your preparation decisions focus on: input quality, scale, cropping, and whether to embed whole images or extracted regions.\n\n## Preprocessing for Optimal Embeddings\n\nImage preprocessing significantly impacts embedding quality. Each model expects specific input formats.\n\n### Standard Preprocessing Pipeline\n\n::: {#6041e723 .cell execution_count=3}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show Preprocessing Pipeline\"}\nfrom dataclasses import dataclass\nfrom typing import Tuple\nimport numpy as np\n\n@dataclass\nclass PreprocessConfig:\n    \"\"\"Configuration for image preprocessing.\"\"\"\n    target_size: Tuple[int, int] = (224, 224)\n    resize_method: str = \"resize\"  # 'resize', 'crop', 'pad'\n    normalize: bool = True\n    mean: Tuple[float, ...] = (0.485, 0.456, 0.406)\n    std: Tuple[float, ...] = (0.229, 0.224, 0.225)\n\nclass ImagePreprocessor:\n    \"\"\"Standard preprocessing pipeline for image embeddings.\"\"\"\n    def __init__(self, config: PreprocessConfig = None):\n        self.config = config or PreprocessConfig()\n\n    def preprocess(self, image) -> np.ndarray:\n        \"\"\"Preprocess a single image.\"\"\"\n        from PIL import Image\n\n        if isinstance(image, np.ndarray):\n            image = Image.fromarray(image)\n\n        # Resize\n        if self.config.resize_method == \"resize\":\n            image = image.resize(self.config.target_size)\n        elif self.config.resize_method == \"crop\":\n            image = self._center_crop(image)\n        elif self.config.resize_method == \"pad\":\n            image = self._resize_with_pad(image)\n\n        # Convert to numpy and scale to [0, 1]\n        img_array = np.array(image, dtype=np.float32)\n        if img_array.max() > 1.0:\n            img_array = img_array / 255.0\n\n        # Normalize\n        if self.config.normalize:\n            mean = np.array(self.config.mean)\n            std = np.array(self.config.std)\n            img_array = (img_array - mean) / std\n\n        return img_array\n\n    def _center_crop(self, image) -> \"Image\":\n        \"\"\"Resize then center crop to target size.\"\"\"\n        w, h = image.size\n        target_w, target_h = self.config.target_size\n        scale = max(target_w / w, target_h / h)\n        new_w, new_h = int(w * scale), int(h * scale)\n        image = image.resize((new_w, new_h))\n        left = (new_w - target_w) // 2\n        top = (new_h - target_h) // 2\n        return image.crop((left, top, left + target_w, top + target_h))\n\n    def _resize_with_pad(self, image) -> \"Image\":\n        \"\"\"Resize preserving aspect ratio with padding.\"\"\"\n        from PIL import Image as PILImage\n        w, h = image.size\n        target_w, target_h = self.config.target_size\n        scale = min(target_w / w, target_h / h)\n        new_w, new_h = int(w * scale), int(h * scale)\n        image = image.resize((new_w, new_h))\n        padded = PILImage.new(\"RGB\", self.config.target_size, (128, 128, 128))\n        left = (target_w - new_w) // 2\n        top = (target_h - new_h) // 2\n        padded.paste(image, (left, top))\n        return padded\n\n# Usage example\npreprocessor = ImagePreprocessor()\nprint(\"ImagePreprocessor ready with ImageNet normalization\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nImagePreprocessor ready with ImageNet normalization\n```\n:::\n:::\n\n\n### Resolution and Aspect Ratio\n\nMost models expect fixed input sizes (224×224, 384×384, etc.). How you achieve this matters:\n\n::: {#6fa262ca .cell execution_count=4}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show Resolution Handling\"}\nfrom enum import Enum\nimport numpy as np\n\nclass ResizeStrategy(Enum):\n    \"\"\"Available resize strategies.\"\"\"\n    STRETCH = \"stretch\"\n    CENTER_CROP = \"center_crop\"\n    PAD = \"pad\"\n    MULTI_CROP = \"multi_crop\"\n\ndef resize_for_embedding(image, target_size=(224, 224), strategy=ResizeStrategy.CENTER_CROP):\n    \"\"\"Resize image using specified strategy.\"\"\"\n    from PIL import Image\n\n    if isinstance(image, np.ndarray):\n        image = Image.fromarray(image)\n\n    if strategy == ResizeStrategy.STRETCH:\n        return np.array(image.resize(target_size))\n    elif strategy == ResizeStrategy.CENTER_CROP:\n        # Resize so smaller dimension matches, then crop\n        w, h = image.size\n        scale = max(target_size[0] / w, target_size[1] / h)\n        new_size = (int(w * scale), int(h * scale))\n        image = image.resize(new_size)\n        left = (image.size[0] - target_size[0]) // 2\n        top = (image.size[1] - target_size[1]) // 2\n        return np.array(image.crop((left, top, left + target_size[0], top + target_size[1])))\n    elif strategy == ResizeStrategy.PAD:\n        # Resize with padding\n        w, h = image.size\n        scale = min(target_size[0] / w, target_size[1] / h)\n        new_w, new_h = int(w * scale), int(h * scale)\n        image = image.resize((new_w, new_h))\n        from PIL import Image as PILImage\n        padded = PILImage.new(\"RGB\", target_size, (0, 0, 0))\n        padded.paste(image, ((target_size[0] - new_w) // 2, (target_size[1] - new_h) // 2))\n        return np.array(padded)\n\n# Usage example\nprint(\"Resize strategies: STRETCH, CENTER_CROP, PAD, MULTI_CROP\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nResize strategies: STRETCH, CENTER_CROP, PAD, MULTI_CROP\n```\n:::\n:::\n\n\n| Strategy | Pros | Cons | Best For |\n|----------|------|------|----------|\n| **Center crop** | Preserves resolution, fast | Loses edge content | Centered subjects |\n| **Resize** | Keeps all content | Distorts aspect ratio | Square-ish images |\n| **Pad** | Preserves aspect ratio | Adds uninformative pixels | Varied aspect ratios |\n| **Multi-crop** | Comprehensive coverage | Multiple embeddings per image | High-value images |\n\n: Resize strategy comparison {.striped}\n\n### Color and Normalization\n\n::: {#4f0618a7 .cell execution_count=5}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show Color Normalization\"}\nimport numpy as np\n\n# Standard normalization values\nIMAGENET_MEAN = (0.485, 0.456, 0.406)\nIMAGENET_STD = (0.229, 0.224, 0.225)\nCLIP_MEAN = (0.48145466, 0.4578275, 0.40821073)\nCLIP_STD = (0.26862954, 0.26130258, 0.27577711)\n\ndef normalize_image(image: np.ndarray, mean=IMAGENET_MEAN, std=IMAGENET_STD) -> np.ndarray:\n    \"\"\"Apply channel-wise normalization.\"\"\"\n    image = image.astype(np.float32)\n    if image.max() > 1.0:\n        image = image / 255.0\n    mean = np.array(mean, dtype=np.float32)\n    std = np.array(std, dtype=np.float32)\n    return (image - mean) / std\n\nclass ColorNormalizer:\n    \"\"\"Comprehensive color normalization for consistent embeddings.\"\"\"\n    def __init__(self, model_type: str = \"imagenet\"):\n        if model_type == \"imagenet\":\n            self.mean = IMAGENET_MEAN\n            self.std = IMAGENET_STD\n        elif model_type == \"clip\":\n            self.mean = CLIP_MEAN\n            self.std = CLIP_STD\n        else:\n            self.mean = (0.5, 0.5, 0.5)\n            self.std = (0.5, 0.5, 0.5)\n\n    def normalize(self, image: np.ndarray) -> np.ndarray:\n        \"\"\"Apply model-specific normalization.\"\"\"\n        return normalize_image(image, self.mean, self.std)\n\n# Usage example\nnormalizer = ColorNormalizer(model_type=\"imagenet\")\nprint(f\"Normalizer ready with mean={IMAGENET_MEAN}, std={IMAGENET_STD}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNormalizer ready with mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)\n```\n:::\n:::\n\n\n### Quality Assessment\n\nNot all images are worth embedding. Filter low-quality inputs:\n\n::: {#3668d1db .cell execution_count=6}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show Quality Assessment\"}\nimport numpy as np\nfrom dataclasses import dataclass\nfrom typing import List\n\n@dataclass\nclass QualityResult:\n    \"\"\"Result of image quality assessment.\"\"\"\n    passed: bool\n    blur_score: float\n    brightness: float\n    contrast: float\n    issues: List[str]\n\ndef assess_image_quality(image, min_resolution: int = 100, blur_threshold: float = 100.0) -> QualityResult:\n    \"\"\"Assess image quality for embedding suitability.\"\"\"\n    import cv2\n    from PIL import Image\n\n    if isinstance(image, Image.Image):\n        image_array = np.array(image)\n    else:\n        image_array = image\n\n    issues = []\n\n    # Resolution check\n    h, w = image_array.shape[:2]\n    if min(h, w) < min_resolution:\n        issues.append(f\"Resolution too low: {min(h, w)}px < {min_resolution}px\")\n\n    # Convert to grayscale for analysis\n    gray = cv2.cvtColor(image_array, cv2.COLOR_RGB2GRAY) if len(image_array.shape) == 3 else image_array\n\n    # Brightness and contrast\n    brightness = np.mean(gray) / 255.0\n    contrast = np.std(gray) / 255.0\n\n    # Blur detection using Laplacian variance\n    blur_score = cv2.Laplacian(gray, cv2.CV_64F).var()\n    if blur_score < blur_threshold:\n        issues.append(f\"Image too blurry: {blur_score:.1f}\")\n\n    return QualityResult(passed=len(issues)==0, blur_score=blur_score,\n                        brightness=brightness, contrast=contrast, issues=issues)\n\n# Usage example\nprint(\"Quality checks: resolution, blur, brightness, contrast\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nQuality checks: resolution, blur, brightness, contrast\n```\n:::\n:::\n\n\n## Handling Large Images\n\nStandard embedding models expect ~224×224 inputs. Large images (satellite imagery, medical scans, gigapixel pathology) require special handling.\n\n### Tiling Strategies\n\nSplit large images into overlapping tiles, embed each, then aggregate:\n\n::: {#54dbdf71 .cell execution_count=7}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show Tiling Strategy\"}\nimport numpy as np\nfrom dataclasses import dataclass\nfrom typing import List, Tuple\n\n@dataclass\nclass Tile:\n    \"\"\"A tile extracted from a larger image.\"\"\"\n    image: np.ndarray\n    x: int\n    y: int\n    row: int\n    col: int\n\ndef tile_image(image, tile_size: Tuple[int, int] = (224, 224), overlap: float = 0.1) -> List[Tile]:\n    \"\"\"Split a large image into overlapping tiles.\"\"\"\n    from PIL import Image\n    if isinstance(image, Image.Image):\n        image = np.array(image)\n\n    h, w = image.shape[:2]\n    tile_w, tile_h = tile_size\n    stride_x = int(tile_w * (1 - overlap))\n    stride_y = int(tile_h * (1 - overlap))\n\n    tiles = []\n    row, y = 0, 0\n    while y < h:\n        col, x = 0, 0\n        while x < w:\n            x_end, y_end = min(x + tile_w, w), min(y + tile_h, h)\n            tile_img = image[y:y_end, x:x_end]\n            if tile_img.shape[0] >= tile_h * 0.5 and tile_img.shape[1] >= tile_w * 0.5:\n                if tile_img.shape[0] < tile_h or tile_img.shape[1] < tile_w:\n                    padded = np.zeros((tile_h, tile_w, image.shape[2]), dtype=image.dtype)\n                    padded[:tile_img.shape[0], :tile_img.shape[1]] = tile_img\n                    tile_img = padded\n                tiles.append(Tile(image=tile_img, x=x, y=y, row=row, col=col))\n            x += stride_x\n            col += 1\n        y += stride_y\n        row += 1\n    return tiles\n\n# Usage example\nprint(\"Tiling: split large images, embed tiles, aggregate (mean/max/weighted)\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTiling: split large images, embed tiles, aggregate (mean/max/weighted)\n```\n:::\n:::\n\n\n### Multi-Resolution Pyramids\n\nCreate embeddings at multiple scales for scale-invariant retrieval:\n\n::: {#ce6cda76 .cell execution_count=8}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show Multi-Resolution Embedding\"}\nimport numpy as np\n\nclass MultiResolutionEmbedder:\n    \"\"\"Create embeddings at multiple scales.\"\"\"\n    def __init__(self, encoder, scales=None, target_size=224):\n        self.encoder = encoder\n        self.scales = scales or [0.5, 1.0, 2.0]\n        self.target_size = target_size\n\n    def embed(self, image):\n        \"\"\"Create embeddings at multiple resolutions.\"\"\"\n        from PIL import Image as PILImage\n        embeddings = {}\n        for scale in self.scales:\n            new_w, new_h = int(image.width * scale), int(image.height * scale)\n            if new_w < self.target_size or new_h < self.target_size:\n                continue\n            scaled = image.resize((new_w, new_h), PILImage.LANCZOS)\n            left, top = (new_w - self.target_size) // 2, (new_h - self.target_size) // 2\n            cropped = scaled.crop((left, top, left + self.target_size, top + self.target_size))\n            embeddings[f\"scale_{scale:.1f}\"] = self.encoder.encode(cropped)\n        return embeddings\n\n# Usage example\nprint(\"Multi-resolution: embed at scales [0.5, 1.0, 2.0], aggregate for scale invariance\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMulti-resolution: embed at scales [0.5, 1.0, 2.0], aggregate for scale invariance\n```\n:::\n:::\n\n\n### Domain-Specific Large Image Handling\n\n#### Satellite and Aerial Imagery\n\n::: {#e3e62673 .cell execution_count=9}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show Satellite Imagery Processing\"}\nimport numpy as np\nfrom dataclasses import dataclass\n\n@dataclass\nclass SatelliteEmbedding:\n    \"\"\"Embedding for a satellite image tile.\"\"\"\n    embedding: np.ndarray\n    tile_id: str\n    bounds: tuple\n\nclass SatelliteImageProcessor:\n    \"\"\"Process satellite imagery for embedding.\"\"\"\n    def __init__(self, encoder, tile_size=256, overlap=0.1):\n        self.encoder = encoder\n        self.tile_size = tile_size\n        self.overlap = overlap\n\n    def process_large_image(self, image: np.ndarray, bounds=None):\n        \"\"\"Process large satellite image into embedded tiles.\"\"\"\n        height, width = image.shape[:2]\n        step = int(self.tile_size * (1 - self.overlap))\n        embeddings = []\n        tile_idx = 0\n        for y in range(0, height - self.tile_size + 1, step):\n            for x in range(0, width - self.tile_size + 1, step):\n                tile = image[y:y + self.tile_size, x:x + self.tile_size]\n                if self._is_valid_tile(tile):\n                    embedding = self.encoder.encode(tile)\n                    embeddings.append(SatelliteEmbedding(embedding=embedding, tile_id=f\"tile_{tile_idx}\",\n                                                         bounds=(x, y, x + self.tile_size, y + self.tile_size)))\n                    tile_idx += 1\n        return embeddings\n\n    def _is_valid_tile(self, tile):\n        \"\"\"Check if tile has enough valid data.\"\"\"\n        valid_pixels = np.all(tile > 0, axis=2) & np.all(tile < 255, axis=2) if len(tile.shape) == 3 else (tile > 0) & (tile < 255)\n        return np.mean(valid_pixels) >= 0.7\n\n# Usage example\nprint(\"Satellite processing: tile large images, filter nodata, georeference\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSatellite processing: tile large images, filter nodata, georeference\n```\n:::\n:::\n\n\n#### Medical Imaging (Pathology Slides)\n\n::: {#944f71e5 .cell execution_count=10}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show Pathology Slide Processing\"}\nimport numpy as np\nfrom dataclasses import dataclass\n\n@dataclass\nclass PathologyEmbedding:\n    \"\"\"Embedding for a pathology patch.\"\"\"\n    embedding: np.ndarray\n    patch_id: str\n    location: tuple\n\nclass PathologySlideProcessor:\n    \"\"\"Process whole slide images (WSI) for embedding.\"\"\"\n    def __init__(self, encoder, patch_size=256, tissue_threshold=0.5):\n        self.encoder = encoder\n        self.patch_size = patch_size\n        self.tissue_threshold = tissue_threshold\n\n    def extract_patches(self, slide_image, tissue_mask=None):\n        \"\"\"Extract tissue patches from slide image.\"\"\"\n        from PIL import Image\n        width, height = slide_image.size if isinstance(slide_image, Image.Image) else (slide_image.shape[1], slide_image.shape[0])\n        step = self.patch_size\n        if tissue_mask is None:\n            tissue_mask = self._detect_tissue(slide_image)\n        patches = []\n        patch_idx = 0\n        for y in range(0, height - self.patch_size + 1, step):\n            for x in range(0, width - self.patch_size + 1, step):\n                mask_patch = tissue_mask[y:y + self.patch_size, x:x + self.patch_size]\n                tissue_ratio = np.mean(mask_patch)\n                if tissue_ratio >= self.tissue_threshold:\n                    patch_image = slide_image.crop((x, y, x + self.patch_size, y + self.patch_size)) if hasattr(slide_image, 'crop') else slide_image[y:y+self.patch_size, x:x+self.patch_size]\n                    embedding = self.encoder.encode(patch_image)\n                    patches.append(PathologyEmbedding(embedding=embedding, patch_id=f\"patch_{patch_idx}\", location=(x, y)))\n                    patch_idx += 1\n        return patches\n\n    def _detect_tissue(self, image):\n        \"\"\"Detect tissue regions using thresholding.\"\"\"\n        img_array = np.array(image)\n        gray = np.mean(img_array, axis=2) if len(img_array.shape) == 3 else img_array\n        return ((gray < 220) & (gray > 20)).astype(np.uint8)\n\n# Usage example\nprint(\"Pathology: extract tissue patches, filter background, aggregate to slide-level\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPathology: extract tissue patches, filter background, aggregate to slide-level\n```\n:::\n:::\n\n\n#### Document Images\n\n::: {#b6ba5266 .cell execution_count=11}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show Document Image Processing\"}\nimport numpy as np\nfrom dataclasses import dataclass\nfrom enum import Enum\n\nclass DocumentRegionType(Enum):\n    TEXT = \"text\"\n    FIGURE = \"figure\"\n    TABLE = \"table\"\n\n@dataclass\nclass DocumentEmbedding:\n    embedding: np.ndarray\n    region_type: DocumentRegionType\n    page_number: int\n\nclass DocumentImageProcessor:\n    \"\"\"Process document images for embedding.\"\"\"\n    def __init__(self, encoder, target_size=224):\n        self.encoder = encoder\n        self.target_size = target_size\n\n    def process_page(self, image, page_number=0):\n        \"\"\"Process a document page.\"\"\"\n        from PIL import Image, ImageOps\n        # Preprocess: grayscale, enhance contrast, denoise\n        gray = image.convert(\"L\") if hasattr(image, 'convert') else image\n        enhanced = ImageOps.autocontrast(gray, cutoff=2)\n        processed = enhanced.convert(\"RGB\")\n        # Resize and embed\n        page_resized = processed.resize((self.target_size, self.target_size))\n        embedding = self.encoder.encode(page_resized)\n        return [DocumentEmbedding(embedding=embedding, region_type=DocumentRegionType.TEXT, page_number=page_number)]\n\n# Usage example\nprint(\"Document processing: preprocess, detect regions (text/figures/tables), embed separately\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDocument processing: preprocess, detect regions (text/figures/tables), embed separately\n```\n:::\n:::\n\n\n## Region-of-Interest Extraction\n\nSometimes you want embeddings for specific regions rather than whole images.\n\n### Object Detection + Cropping\n\nDetect objects first, then embed each separately:\n\n::: {#a7a068c3 .cell execution_count=12}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show Object Detection + Embedding\"}\nimport numpy as np\nfrom dataclasses import dataclass\n\n@dataclass\nclass DetectedObject:\n    bbox: tuple\n    class_name: str\n    confidence: float\n    embedding: np.ndarray = None\n\ndef detect_and_embed(image, detector, encoder):\n    \"\"\"Detect objects, crop them, and embed each.\"\"\"\n    # Detect objects (using YOLO, Faster R-CNN, etc.)\n    detections = detector.detect(image)\n    # Crop and embed each object\n    for det in detections:\n        x1, y1, x2, y2 = det.bbox\n        cropped = image[y1:y2, x1:x2]\n        det.embedding = encoder.encode(cropped)\n    return detections\n\n# Usage example\nprint(\"Object detection: YOLO/Faster R-CNN -> crop objects -> embed each\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nObject detection: YOLO/Faster R-CNN -> crop objects -> embed each\n```\n:::\n:::\n\n\n### Segmentation-Based Regions\n\nUse semantic or instance segmentation for precise region extraction:\n\n::: {#d6092b99 .cell execution_count=13}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show Segmentation-Based Embedding\"}\nimport numpy as np\n\ndef embed_segmented_regions(image, segmentation_mask, encoder):\n    \"\"\"Extract embeddings from segmented regions.\"\"\"\n    embeddings = []\n    for segment_id in np.unique(segmentation_mask)[1:]:  # Skip background (0)\n        mask = (segmentation_mask == segment_id)\n        # Get bounding box\n        rows, cols = np.where(mask)\n        if len(rows) == 0:\n            continue\n        y1, y2, x1, x2 = rows.min(), rows.max(), cols.min(), cols.max()\n        # Extract and mask region\n        region = image[y1:y2+1, x1:x2+1]\n        region_mask = mask[y1:y2+1, x1:x2+1]\n        region[~region_mask] = 255  # White background\n        # Embed\n        embedding = encoder.encode(region)\n        embeddings.append((segment_id, embedding))\n    return embeddings\n\n# Usage example\nprint(\"Segmentation: semantic/instance seg -> extract regions -> embed with masked background\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSegmentation: semantic/instance seg -> extract regions -> embed with masked background\n```\n:::\n:::\n\n\n### Attention-Guided Regions\n\nUse model attention to identify important regions:\n\n::: {#07395183 .cell execution_count=14}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show Attention-Guided Regions\"}\nimport numpy as np\n\ndef extract_attention_regions(image, model, top_k=5):\n    \"\"\"Use model attention to identify important regions.\"\"\"\n    # Get attention maps from model (e.g., ViT attention, GradCAM)\n    attention_map = model.get_attention(image)\n    # Find top-k regions with highest attention\n    flat_idx = np.argsort(attention_map.flatten())[-top_k:]\n    regions = []\n    for idx in flat_idx:\n        y, x = np.unravel_index(idx, attention_map.shape)\n        # Extract region around attention peak\n        region = image[max(0,y-56):y+56, max(0,x-56):x+56]\n        regions.append(region)\n    return regions\n\n# Usage example\nprint(\"Attention-guided: use ViT attention/GradCAM -> extract salient regions -> embed\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAttention-guided: use ViT attention/GradCAM -> extract salient regions -> embed\n```\n:::\n:::\n\n\n## Multi-Object Scene Handling\n\nScenes with multiple objects present a choice: one embedding for the whole scene, or separate embeddings per object?\n\n### Scene-Level vs Object-Level Embeddings\n\n::: {#cd5b3884 .cell execution_count=15}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show Scene vs Object Embeddings\"}\nimport numpy as np\n\ndef scene_level_embedding(image, encoder):\n    \"\"\"Single embedding for entire scene.\"\"\"\n    return encoder.encode(image)\n\ndef object_level_embeddings(image, detector, encoder):\n    \"\"\"Separate embeddings for each object.\"\"\"\n    objects = detector.detect(image)\n    embeddings = []\n    for obj in objects:\n        x1, y1, x2, y2 = obj.bbox\n        cropped = image[y1:y2, x1:x2]\n        embeddings.append((obj.class_name, encoder.encode(cropped)))\n    return embeddings\n\n# Usage example\nprint(\"Scene-level: 1 embedding. Object-level: N embeddings. Hybrid: both\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nScene-level: 1 embedding. Object-level: N embeddings. Hybrid: both\n```\n:::\n:::\n\n\n### Hybrid Approaches\n\n::: {#e8c47142 .cell execution_count=16}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show Hybrid Embedding Approach\"}\nimport numpy as np\n\ndef hybrid_embedding(image, detector, encoder):\n    \"\"\"Combine scene-level and object-level embeddings.\"\"\"\n    # Scene embedding\n    scene_emb = encoder.encode(image)\n    # Object embeddings\n    objects = detector.detect(image)\n    object_embs = []\n    for obj in objects:\n        x1, y1, x2, y2 = obj.bbox\n        cropped = image[y1:y2, x1:x2]\n        object_embs.append(encoder.encode(cropped))\n    return {\"scene\": scene_emb, \"objects\": object_embs, \"count\": len(object_embs)}\n\n# Usage example\nprint(\"Hybrid: store both scene and object embeddings for comprehensive search\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nHybrid: store both scene and object embeddings for comprehensive search\n```\n:::\n:::\n\n\n| Approach | Storage | Query Types Supported | Best For |\n|----------|---------|----------------------|----------|\n| Scene-only | 1× | \"Show me kitchen scenes\" | Scene retrieval |\n| Objects-only | N× | \"Find red chairs\" | Object retrieval |\n| Hybrid | (N+1)× | Both scene and object queries | Comprehensive search |\n\n: Multi-object embedding strategies {.striped}\n\n## Augmentation for Training Embeddings\n\nWhen training or fine-tuning embedding models, augmentation creates diverse views of the same image—essential for contrastive learning.\n\n### Standard Augmentation Pipeline\n\n::: {#b8b856cf .cell execution_count=17}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show Augmentation Pipeline\"}\nimport torchvision.transforms as T\n\ndef create_training_augmentation():\n    \"\"\"Standard augmentation for training embedding models.\"\"\"\n    return T.Compose([\n        T.RandomResizedCrop(224, scale=(0.8, 1.0)),\n        T.RandomHorizontalFlip(),\n        T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n        T.RandomGrayscale(p=0.1),\n        T.ToTensor(),\n        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    ])\n\ndef create_test_augmentation():\n    \"\"\"Minimal augmentation for testing.\"\"\"\n    return T.Compose([\n        T.Resize(256),\n        T.CenterCrop(224),\n        T.ToTensor(),\n        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    ])\n\n# Usage example\nprint(\"Augmentation: crop, flip, color jitter for training; crop for test\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAugmentation: crop, flip, color jitter for training; crop for test\n```\n:::\n:::\n\n\n### Augmentation for Contrastive Learning\n\n::: {#045f9190 .cell execution_count=18}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show Contrastive Augmentation\"}\nimport torchvision.transforms as T\n\ndef create_contrastive_augmentation():\n    \"\"\"Strong augmentation for contrastive learning (SimCLR-style).\"\"\"\n    return T.Compose([\n        T.RandomResizedCrop(224, scale=(0.2, 1.0)),\n        T.RandomHorizontalFlip(p=0.5),\n        T.RandomApply([T.ColorJitter(0.8, 0.8, 0.8, 0.2)], p=0.8),\n        T.RandomGrayscale(p=0.2),\n        T.GaussianBlur(kernel_size=23, sigma=(0.1, 2.0)),\n        T.ToTensor(),\n        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    ])\n\n# Usage example\nprint(\"Contrastive: strong augmentations (SimCLR) to create positive pairs\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nContrastive: strong augmentations (SimCLR) to create positive pairs\n```\n:::\n:::\n\n\n### Domain-Specific Augmentation\n\n::: {#e2662ad9 .cell execution_count=19}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show Domain-Specific Augmentation\"}\nimport torchvision.transforms as T\n\ndef medical_augmentation():\n    \"\"\"Augmentation for medical images.\"\"\"\n    return T.Compose([\n        T.RandomRotation(15),\n        T.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n        T.ColorJitter(brightness=0.1, contrast=0.1),\n        T.ToTensor(),\n    ])\n\ndef satellite_augmentation():\n    \"\"\"Augmentation for satellite imagery.\"\"\"\n    return T.Compose([\n        T.RandomRotation(90),  # Any rotation valid\n        T.RandomHorizontalFlip(),\n        T.RandomVerticalFlip(),\n        T.ToTensor(),\n    ])\n\n# Usage example\nprint(\"Domain-specific: tailored augmentations for medical, satellite, etc.\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDomain-specific: tailored augmentations for medical, satellite, etc.\n```\n:::\n:::\n\n\n## Video Frame Extraction\n\nVideos require selecting which frames to embed:\n\n::: {#f9f007a2 .cell execution_count=20}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show Video Frame Embedding\"}\nimport numpy as np\n\ndef embed_video_frames(video_path, encoder, sample_rate=30):\n    \"\"\"Extract and embed video frames.\"\"\"\n    import cv2\n    cap = cv2.VideoCapture(video_path)\n    frame_embeddings = []\n    frame_idx = 0\n    while cap.isOpened():\n        ret, frame = cap.read()\n        if not ret:\n            break\n        if frame_idx % sample_rate == 0:\n            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n            embedding = encoder.encode(frame_rgb)\n            frame_embeddings.append((frame_idx, embedding))\n        frame_idx += 1\n    cap.release()\n    return frame_embeddings\n\n# Usage example\nprint(\"Video: sample frames at intervals -> embed each -> temporal indexing\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nVideo: sample frames at intervals -> embed each -> temporal indexing\n```\n:::\n:::\n\n\n## Production Image Pipeline\n\nPutting it all together into a production system:\n\n::: {#73171821 .cell execution_count=21}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show Production Pipeline\"}\nfrom dataclasses import dataclass\nimport numpy as np\n\n@dataclass\nclass ImageEmbeddingResult:\n    embedding: np.ndarray\n    image_id: str\n    metadata: dict\n\nclass ProductionImagePipeline:\n    \"\"\"Production-ready image embedding pipeline.\"\"\"\n    def __init__(self, encoder, preprocessor, quality_filter=None):\n        self.encoder = encoder\n        self.preprocessor = preprocessor\n        self.quality_filter = quality_filter\n\n    def process(self, image, image_id: str):\n        \"\"\"Process single image through full pipeline.\"\"\"\n        # Quality check\n        if self.quality_filter and not self.quality_filter.is_valid(image):\n            return None\n        # Preprocess\n        processed = self.preprocessor.preprocess(image)\n        # Embed\n        embedding = self.encoder.encode(processed)\n        # Return with metadata\n        return ImageEmbeddingResult(embedding=embedding, image_id=image_id,\n                                   metadata={\"size\": image.shape[:2]})\n\n# Usage example\nprint(\"Production: quality filter -> preprocess -> embed -> store with metadata\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nProduction: quality filter -> preprocess -> embed -> store with metadata\n```\n:::\n:::\n\n\n## Quality and Consistency\n\n### Embedding Consistency Checks\n\n::: {#9349d315 .cell execution_count=22}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show Consistency Checks\"}\nimport numpy as np\n\ndef check_embedding_consistency(embeddings, threshold=0.95):\n    \"\"\"Check for duplicate or near-duplicate embeddings.\"\"\"\n    from sklearn.metrics.pairwise import cosine_similarity\n    similarities = cosine_similarity(embeddings)\n    np.fill_diagonal(similarities, 0)\n    duplicates = np.where(similarities > threshold)\n    return list(zip(duplicates[0], duplicates[1]))\n\ndef validate_embedding_distribution(embeddings):\n    \"\"\"Check if embeddings have reasonable distribution.\"\"\"\n    norms = np.linalg.norm(embeddings, axis=1)\n    mean_sim = np.mean(cosine_similarity(embeddings))\n    return {\n        \"mean_norm\": float(np.mean(norms)),\n        \"std_norm\": float(np.std(norms)),\n        \"mean_similarity\": float(mean_sim),\n    }\n\n# Usage example\nprint(\"Consistency: check for duplicates, validate distribution, monitor quality\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nConsistency: check for duplicates, validate distribution, monitor quality\n```\n:::\n:::\n\n\n### Batch Processing Best Practices\n\n::: {#7695d582 .cell execution_count=23}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show Batch Processing\"}\nimport numpy as np\nfrom typing import List\n\nclass BatchImageProcessor:\n    \"\"\"Efficient batch processing for large image datasets.\"\"\"\n    def __init__(self, encoder, batch_size=32, num_workers=4):\n        self.encoder = encoder\n        self.batch_size = batch_size\n        self.num_workers = num_workers\n\n    def process_batch(self, images: List):\n        \"\"\"Process images in batches for efficiency.\"\"\"\n        embeddings = []\n        for i in range(0, len(images), self.batch_size):\n            batch = images[i:i + self.batch_size]\n            batch_embeddings = self.encoder.encode(batch)\n            embeddings.append(batch_embeddings)\n        return np.vstack(embeddings) if embeddings else np.array([])\n\n    def process_directory(self, image_dir: str):\n        \"\"\"Process all images in a directory.\"\"\"\n        from PIL import Image\n        import os\n        images = []\n        image_paths = []\n        for filename in os.listdir(image_dir):\n            if filename.endswith(('.jpg', '.png', '.jpeg')):\n                path = os.path.join(image_dir, filename)\n                images.append(Image.open(path))\n                image_paths.append(path)\n                if len(images) >= self.batch_size:\n                    embeddings = self.process_batch(images)\n                    yield list(zip(image_paths, embeddings))\n                    images, image_paths = [], []\n        if images:\n            embeddings = self.process_batch(images)\n            yield list(zip(image_paths, embeddings))\n\n# Usage example\nprint(\"Batch processing: process images in batches, use DataLoader for efficiency\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nBatch processing: process images in batches, use DataLoader for efficiency\n```\n:::\n:::\n\n\n## Comparing Text and Image Preparation\n\n| Aspect | Text Chunking | Image Preparation |\n|--------|---------------|-------------------|\n| **Primary decision** | Chunk boundaries and size | Preprocessing and cropping strategy |\n| **Model handles** | Tokenization | Patch extraction (ViT) or convolution |\n| **Multi-part content** | Split into chunks | Tile large images |\n| **Object-level** | Extract sentences/paragraphs | Detect and crop objects |\n| **Quality filtering** | Language detection, deduplication | Blur detection, resolution checks |\n| **Metadata** | Source, section, page | EXIF, geolocation, timestamp |\n| **Augmentation use** | Rarely for retrieval | Essential for training |\n\n: Text vs image preparation comparison {.striped}\n\n## Key Takeaways\n\n- **Image embedding models handle spatial \"chunking\" internally**: Unlike text where you explicitly chunk documents, CNNs use hierarchical convolutions and ViTs use patch extraction—your preparation focuses on input quality and scale\n\n- **Preprocessing choices significantly impact embedding quality**: Resize strategy (crop vs pad vs stretch), normalization, and color handling should match model expectations and content characteristics\n\n- **Large images require tiling with overlap**: Satellite imagery, medical scans, and gigapixel images should be split into overlapping tiles, embedded separately, with optional aggregation strategies\n\n- **Multi-object scenes offer embedding design choices**: Whole-scene embeddings support scene queries, object-level embeddings support object queries, hybrid approaches support both at increased storage cost\n\n- **Quality filtering prevents garbage embeddings**: Blur detection, resolution checks, and content filtering should precede embedding to avoid polluting your vector database\n\n- **Augmentation is essential for training, optional for inference**: When training embedding models, augmentation creates diverse views for contrastive learning; for inference, consider multi-crop only for high-value retrieval scenarios\n\n## Looking Ahead\n\nWith text and image preparation covered, you're ready to build complete retrieval systems. @sec-rag-at-scale explores RAG at scale—combining these preparation techniques with efficient retrieval pipelines, context assembly, and LLM integration for production question-answering systems.\n\n## Further Reading\n\n- Dosovitskiy, A., et al. (2020). \"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.\" *arXiv:2010.11929* (ViT)\n- Radford, A., et al. (2021). \"Learning Transferable Visual Models From Natural Language Supervision.\" *arXiv:2103.00020* (CLIP)\n- He, K., et al. (2016). \"Deep Residual Learning for Image Recognition.\" *CVPR* (ResNet)\n- Chen, T., et al. (2020). \"A Simple Framework for Contrastive Learning of Visual Representations.\" *ICML* (SimCLR)\n- Caron, M., et al. (2021). \"Emerging Properties in Self-Supervised Vision Transformers.\" *ICCV* (DINO)\n- Campanella, G., et al. (2019). \"Clinical-grade computational pathology using weakly supervised deep learning on whole slide images.\" *Nature Medicine*\n\n",
    "supporting": [
      "ch18_image_preparation_files/figure-epub"
    ],
    "filters": [],
    "engineDependencies": {
      "jupyter": [
        {
          "jsWidgets": false,
          "jupyterWidgets": false,
          "htmlLibraries": []
        }
      ]
    }
  }
}