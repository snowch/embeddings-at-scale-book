{
  "hash": "50fdb058cf8811125f903986b7c44f1b",
  "result": {
    "engine": "jupyter",
    "markdown": "# Data Engineering for Embeddings {#sec-data-engineering}\n\n:::{.callout-note}\n## Chapter Overview\nHigh-quality embeddings demand high-quality data engineering. This chapter explores the data infrastructure that enables trillion-row embedding systems: ETL pipelines that transform raw data into training-ready formats while preserving semantic relationships, streaming architectures that update embeddings in near-real-time as data evolves, data quality frameworks that detect and remediate issues before they corrupt embeddings, schema evolution strategies that maintain backwards compatibility across model versions, and multi-source data fusion techniques that combine embeddings from heterogeneous datasets. These data engineering practices ensure embedding models have the clean, consistent, well-structured data needed to achieve their potential in production.\n:::\n\nAfter optimizing vector operations for sub-millisecond search (@sec-high-performance-vector-ops), the remaining production challenge is **data engineering**. Embeddings are only as good as the data they're trained on. A model trained on corrupted data produces corrupted embeddings. A pipeline that can't handle schema changes breaks during routine database migrations. A system that can't fuse data from multiple sources misses critical context. This chapter addresses the data engineering practices that separate prototype embedding systems from production-ready platforms serving billions of users.\n\n## ETL Pipelines for Embedding Generation\n\nEmbedding generation requires transforming raw data—database records, documents, images, logs—into vector representations while preserving semantic meaning. **ETL (Extract, Transform, Load) pipelines** orchestrate this transformation at scale, handling data extraction from diverse sources, feature engineering that captures relevant signals, quality validation that ensures training stability, and efficient loading into training systems.\n\n### The Embedding ETL Challenge\n\nTraditional ETL optimizes for data warehousing: schema normalization, aggregation, and SQL-friendly formats. **Embedding ETL** has unique requirements:\n\n- **Semantic preservation**: Transformations must preserve meaning (normalization can destroy signal)\n- **Feature engineering**: Extract features that capture relationships (not just facts)\n- **Scale**: Process billions of records efficiently (trillion-row datasets)\n- **Freshness**: Keep training data current (embedding drift occurs within weeks)\n- **Multimodal**: Handle text, images, structured data, time series simultaneously\n\n::: {.cell execution_count=1}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show ETL Pipeline for Embedding Generation\"}\nfrom dataclasses import dataclass\nfrom typing import List, Dict, Any\nimport pandas as pd\n\n@dataclass\nclass EmbeddingRecord:\n    \"\"\"Record prepared for embedding generation.\"\"\"\n    id: str\n    text: str\n    metadata: Dict[str, Any]\n\nclass EmbeddingETLPipeline:\n    \"\"\"ETL pipeline for preparing data for embedding generation.\"\"\"\n\n    def extract(self, source: str) -> pd.DataFrame:\n        \"\"\"Extract data from source (database, file, API).\"\"\"\n        # Example: Load from CSV\n        return pd.read_csv(source)\n\n    def transform(self, df: pd.DataFrame) -> List[EmbeddingRecord]:\n        \"\"\"Transform raw data into embedding-ready format.\"\"\"\n        records = []\n        for _, row in df.iterrows():\n            # Clean text\n            text = str(row.get('content', '')).strip()\n\n            # Skip empty or invalid records\n            if not text or len(text) < 10:\n                continue\n\n            # Extract metadata\n            metadata = {\n                'category': row.get('category'),\n                'timestamp': row.get('created_at'),\n                'source': 'etl_pipeline'\n            }\n\n            records.append(EmbeddingRecord(\n                id=str(row['id']),\n                text=text,\n                metadata=metadata\n            ))\n\n        return records\n\n    def load(self, records: List[EmbeddingRecord], output_path: str):\n        \"\"\"Load prepared records for embedding.\"\"\"\n        df = pd.DataFrame([\n            {'id': r.id, 'text': r.text, **r.metadata}\n            for r in records\n        ])\n        df.to_parquet(output_path, index=False)\n        return len(records)\n\n# Usage example\npipeline = EmbeddingETLPipeline()\n# Create sample data\nsample_df = pd.DataFrame({\n    'id': [1, 2, 3],\n    'content': ['Machine learning article', 'AI research paper', 'Data science tutorial'],\n    'category': ['ML', 'AI', 'DS'],\n    'created_at': ['2024-01-01', '2024-01-02', '2024-01-03']\n})\nrecords = pipeline.transform(sample_df)\nprint(f\"Processed {len(records)} records ready for embedding\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nProcessed 3 records ready for embedding\n```\n:::\n:::\n\n\n:::{.callout-tip}\n## ETL Best Practices for Embeddings\n\n**Data quality:**\n\n- Validate at every stage (extract, transform, load)\n- Implement deduplication (exact and near-duplicate)\n- Handle missing values explicitly (don't drop silently)\n- Monitor data drift (distribution shifts over time)\n\n**Performance:**\n\n- Partition data for parallel processing (100-1000 partitions)\n- Use columnar formats (Parquet) for analytics\n- Implement checkpointing for fault tolerance\n- Optimize for I/O (sequential reads, batching)\n\n**Maintainability:**\n\n- Keep transformations simple and composable\n- Document feature engineering logic\n- Version control pipeline code\n- Test with representative samples before production runs\n:::\n\n## Streaming Embedding Updates\n\nBatch ETL processes data hourly or daily, but many applications need **real-time embeddings**. A news recommender must embed articles seconds after publication. A fraud detector must embed transactions milliseconds after they occur. **Streaming architectures** enable continuous embedding updates with end-to-end latency measured in seconds, not hours.\n\n### Streaming vs. Batch: The Trade-off\n\n**Batch processing** (hourly/daily):\n\n- **Advantages**: Simple, efficient, easy to debug, supports complex aggregations\n- **Disadvantages**: Stale embeddings (hours old), high latency for new items\n- **Use when**: Daily updates sufficient, complex transformations required\n\n**Stream processing** (seconds):\n\n- **Advantages**: Fresh embeddings (seconds old), low latency for new items, event-driven\n- **Disadvantages**: Complex architecture, harder to debug, limited aggregation window\n- **Use when**: Real-time updates critical, simple transformations, event-driven workflows\n\n::: {.cell execution_count=2}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show Streaming Embedding Processor\"}\nfrom queue import Queue\nfrom typing import List\nimport time\nimport threading\n\nclass StreamingEmbeddingProcessor:\n    \"\"\"Process embeddings in real-time from streaming data.\"\"\"\n\n    def __init__(self, batch_size: int = 32, batch_timeout_ms: int = 100):\n        self.batch_size = batch_size\n        self.batch_timeout_ms = batch_timeout_ms\n        self.queue = Queue()\n        self.running = False\n        self.processor_thread = None\n\n    def start(self):\n        \"\"\"Start the streaming processor.\"\"\"\n        self.running = True\n        self.processor_thread = threading.Thread(target=self._process_loop)\n        self.processor_thread.start()\n\n    def stop(self):\n        \"\"\"Stop the streaming processor.\"\"\"\n        self.running = False\n        if self.processor_thread:\n            self.processor_thread.join()\n\n    def submit(self, item: dict):\n        \"\"\"Submit an item for processing.\"\"\"\n        self.queue.put(item)\n\n    def _process_loop(self):\n        \"\"\"Main processing loop with micro-batching.\"\"\"\n        while self.running:\n            batch = self._collect_batch()\n            if batch:\n                self._process_batch(batch)\n\n    def _collect_batch(self) -> List[dict]:\n        \"\"\"Collect items into micro-batch.\"\"\"\n        batch = []\n        deadline = time.time() + (self.batch_timeout_ms / 1000.0)\n\n        while len(batch) < self.batch_size and time.time() < deadline:\n            try:\n                item = self.queue.get(timeout=0.01)\n                batch.append(item)\n            except:\n                break\n\n        return batch\n\n    def _process_batch(self, batch: List[dict]):\n        \"\"\"Process a batch of items.\"\"\"\n        # Simulate embedding generation\n        texts = [item['text'] for item in batch]\n        print(f\"Processing batch of {len(batch)} items\")\n        # Here you would call your embedding model\n        # embeddings = model.encode(texts)\n\n# Usage example\nprocessor = StreamingEmbeddingProcessor(batch_size=10, batch_timeout_ms=100)\nprocessor.start()\n\n# Simulate streaming data\nfor i in range(25):\n    processor.submit({'id': i, 'text': f'Document {i}'})\n    time.sleep(0.01)  # Simulate streaming arrival\n\ntime.sleep(0.2)  # Wait for processing\nprocessor.stop()\nprint(\"Streaming processor completed\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nProcessing batch of 10 items\nProcessing batch of 3 items\nProcessing batch of 4 items\nProcessing batch of 1 items\nProcessing batch of 1 items\nProcessing batch of 3 items\nProcessing batch of 2 items\nProcessing batch of 1 items\nStreaming processor completed\n```\n:::\n:::\n\n\n:::{.callout-tip}\n## Streaming Architecture Best Practices\n\n**Micro-batching:**\n\n- Batch window: 100-1000ms (balance latency vs throughput)\n- Batch size: 10-100 items (optimize for GPU)\n- Adaptive batching: Adjust based on load\n\n**Fault tolerance:**\n\n- Checkpointing: Save progress every N events\n- Exactly-once semantics: Idempotent operations\n- Dead letter queue: Handle failed events separately\n- Retry logic: Exponential backoff for transient failures\n\n**Monitoring:**\n\n- End-to-end latency (p50, p95, p99)\n- Throughput (events/second)\n- Error rate (failures / total events)\n- Queue depth (backpressure indicator)\n:::\n\n:::{.callout-warning}\n## Streaming Complexity\n\nStreaming pipelines are significantly more complex than batch:\n\n- **Debugging**: Harder to reproduce issues (event order matters)\n- **Testing**: Need to simulate real-time event streams\n- **Operations**: 24/7 monitoring required\n- **Cost**: Higher infrastructure costs (always running)\n\nStart with batch, migrate to streaming only when business value justifies complexity.\n:::\n\n## Data Quality for Embedding Training\n\nPoor data quality causes poor embeddings. **Data quality frameworks** detect and remediate issues before they corrupt training: duplicate detection prevents training on repeated examples, outlier detection identifies corrupted or adversarial data, consistency validation ensures relationships hold across updates, and drift detection alerts when distributions shift unexpectedly.\n\n### The Data Quality Challenge for Embeddings\n\nTraditional data quality focuses on completeness and correctness. **Embedding quality** has additional requirements:\n\n- **Semantic consistency**: Similar items must have similar features\n- **Label quality**: Incorrect labels poison contrastive learning\n- **Distribution stability**: Embedding space shifts when data distribution changes\n- **Relationship preservation**: Entity relationships must remain consistent\n\n::: {.cell execution_count=3}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show Data Quality Framework\"}\nfrom dataclasses import dataclass\nfrom typing import List, Optional\nimport numpy as np\nimport hashlib\n\n@dataclass\nclass QualityReport:\n    \"\"\"Report of data quality issues.\"\"\"\n    total_records: int\n    duplicates: int\n    outliers: int\n    missing_values: int\n    passed: bool\n\nclass DataQualityFramework:\n    \"\"\"Framework for validating embedding training data quality.\"\"\"\n\n    def __init__(self, outlier_threshold: float = 3.0):\n        self.outlier_threshold = outlier_threshold\n        self.seen_hashes = set()\n\n    def validate(self, records: List[dict]) -> QualityReport:\n        \"\"\"Run comprehensive quality checks.\"\"\"\n        duplicates = self._check_duplicates(records)\n        outliers = self._check_outliers(records)\n        missing = self._check_missing_values(records)\n\n        passed = (duplicates == 0 and outliers == 0 and missing < len(records) * 0.05)\n\n        return QualityReport(\n            total_records=len(records),\n            duplicates=duplicates,\n            outliers=outliers,\n            missing_values=missing,\n            passed=passed\n        )\n\n    def _check_duplicates(self, records: List[dict]) -> int:\n        \"\"\"Detect exact and near-duplicate records.\"\"\"\n        duplicates = 0\n        for record in records:\n            text = record.get('text', '')\n            # Create hash for duplicate detection\n            text_hash = hashlib.md5(text.encode()).hexdigest()\n            if text_hash in self.seen_hashes:\n                duplicates += 1\n            else:\n                self.seen_hashes.add(text_hash)\n        return duplicates\n\n    def _check_outliers(self, records: List[dict]) -> int:\n        \"\"\"Detect statistical outliers in text length.\"\"\"\n        lengths = [len(r.get('text', '')) for r in records]\n        if not lengths:\n            return 0\n\n        mean_length = np.mean(lengths)\n        std_length = np.std(lengths)\n\n        outliers = 0\n        for length in lengths:\n            z_score = abs((length - mean_length) / std_length) if std_length > 0 else 0\n            if z_score > self.outlier_threshold:\n                outliers += 1\n\n        return outliers\n\n    def _check_missing_values(self, records: List[dict]) -> int:\n        \"\"\"Count records with missing required fields.\"\"\"\n        missing = 0\n        for record in records:\n            if not record.get('text') or not record.get('id'):\n                missing += 1\n        return missing\n\n# Usage example\nquality_framework = DataQualityFramework(outlier_threshold=3.0)\nsample_records = [\n    {'id': '1', 'text': 'Normal text'},\n    {'id': '2', 'text': 'Another normal text'},\n    {'id': '3', 'text': 'Normal text'},  # Duplicate\n    {'id': '4', 'text': 'x' * 10000},  # Outlier (very long)\n    {'id': '5', 'text': ''},  # Missing\n]\nreport = quality_framework.validate(sample_records)\nprint(f\"Quality Report: {report.total_records} records\")\nprint(f\"  Duplicates: {report.duplicates}\")\nprint(f\"  Outliers: {report.outliers}\")\nprint(f\"  Missing: {report.missing_values}\")\nprint(f\"  Passed: {report.passed}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nQuality Report: 5 records\n  Duplicates: 1\n  Outliers: 0\n  Missing: 1\n  Passed: False\n```\n:::\n:::\n\n\n:::{.callout-tip}\n## Data Quality Best Practices\n\n**Prevention:**\n\n- Validate at ingestion (catch issues early)\n- Implement schema contracts (enforce structure)\n- Use type systems (prevent type errors)\n- Automate quality checks (continuous validation)\n\n**Detection:**\n\n- Statistical profiling (baseline distributions)\n- Anomaly detection (outliers, drift)\n- Relationship validation (foreign keys, consistency)\n- Duplicate detection (exact and near-duplicate)\n\n**Remediation:**\n\n- Automated fixes (fill missing values, clip outliers)\n- Human review queue (ambiguous cases)\n- Dead letter queue (unfixable records)\n- Feedback loops (fix upstream sources)\n\n**Monitoring:**\n\n- Quality dashboards (real-time metrics)\n- Alerts on degradation (threshold breaches)\n- Trend analysis (quality over time)\n- Root cause analysis (trace issues to source)\n:::\n\n## Schema Evolution and Backwards Compatibility\n\nProduction embedding systems evolve: new features are added, old features deprecated, data types change. **Schema evolution** enables safe changes while maintaining backwards compatibility with existing embeddings, models, and downstream consumers.\n\n### The Schema Evolution Challenge\n\nEmbedding systems have complex dependencies:\n\n- **Trained models**: Expect specific feature schema\n- **Vector indices**: Store embeddings from specific model versions\n- **Downstream consumers**: Query embeddings with specific schemas\n- **Historical data**: May use old schemas\n\n**Change one component**, and the entire system can break.\n\n::: {.cell execution_count=4}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show Schema Evolution Manager\"}\nfrom typing import Dict, Any, Optional\nfrom enum import Enum\n\nclass SchemaVersion(Enum):\n    \"\"\"Schema version enumeration.\"\"\"\n    V1 = \"1.0\"\n    V2 = \"2.0\"\n\nclass SchemaEvolutionManager:\n    \"\"\"Manage schema evolution with backwards compatibility.\"\"\"\n\n    def __init__(self):\n        self.current_version = SchemaVersion.V2\n\n    def migrate(self, record: Dict[str, Any], from_version: SchemaVersion) -> Dict[str, Any]:\n        \"\"\"Migrate record from old schema to current schema.\"\"\"\n        if from_version == self.current_version:\n            return record  # No migration needed\n\n        # Apply migration chain\n        if from_version == SchemaVersion.V1:\n            record = self._migrate_v1_to_v2(record)\n\n        return record\n\n    def _migrate_v1_to_v2(self, record: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Migrate from v1 to v2 schema.\"\"\"\n        migrated = record.copy()\n\n        # V2 added 'category' field with default value\n        if 'category' not in migrated:\n            migrated['category'] = 'uncategorized'\n\n        # V2 renamed 'title' to 'name'\n        if 'title' in migrated:\n            migrated['name'] = migrated.pop('title')\n\n        # V2 added required 'embedding_model' field\n        if 'embedding_model' not in migrated:\n            migrated['embedding_model'] = 'default-v1'\n\n        # Tag with schema version\n        migrated['__schema_version__'] = SchemaVersion.V2.value\n\n        return migrated\n\n    def validate_schema(self, record: Dict[str, Any], version: SchemaVersion) -> bool:\n        \"\"\"Validate record against schema version.\"\"\"\n        if version == SchemaVersion.V1:\n            required = ['id', 'text', 'title']\n        elif version == SchemaVersion.V2:\n            required = ['id', 'text', 'name', 'category', 'embedding_model']\n        else:\n            return False\n\n        return all(field in record for field in required)\n\n    def get_schema_version(self, record: Dict[str, Any]) -> Optional[SchemaVersion]:\n        \"\"\"Detect schema version from record.\"\"\"\n        version_str = record.get('__schema_version__')\n        if version_str:\n            try:\n                return SchemaVersion(version_str)\n            except ValueError:\n                pass\n\n        # Fallback: infer from fields\n        if 'name' in record and 'category' in record:\n            return SchemaVersion.V2\n        return SchemaVersion.V1\n\n# Usage example\nmanager = SchemaEvolutionManager()\n\n# Old v1 record\nv1_record = {\n    'id': '123',\n    'text': 'Machine learning content',\n    'title': 'ML Article'\n}\n\n# Detect version\nversion = manager.get_schema_version(v1_record)\nprint(f\"Detected schema version: {version.value}\")\n\n# Migrate to current version\nv2_record = manager.migrate(v1_record, version)\nprint(f\"Migrated record: {v2_record}\")\nprint(f\"Valid v2 schema: {manager.validate_schema(v2_record, SchemaVersion.V2)}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDetected schema version: 1.0\nMigrated record: {'id': '123', 'text': 'Machine learning content', 'category': 'uncategorized', 'name': 'ML Article', 'embedding_model': 'default-v1', '__schema_version__': '2.0'}\nValid v2 schema: True\n```\n:::\n:::\n\n\n:::{.callout-tip}\n## Schema Evolution Best Practices\n\n**Safe evolution strategies:**\n\n- **Additive changes only**: Add fields, don't remove (backwards compatible)\n- **Deprecation before removal**: Mark fields deprecated for 1-2 versions\n- **Default values**: Provide defaults for new required fields\n- **Version tagging**: Tag data with schema version explicitly\n\n**Migration strategies:**\n\n- **Online migration**: Transform data on-read (lazy)\n- **Offline migration**: Reprocess entire dataset (eager)\n- **Hybrid**: Migrate hot data online, cold data offline\n\n**Compatibility levels:**\n\n- **Forward compatible**: New consumers can read old data\n- **Backward compatible**: Old consumers can read new data\n- **Full compatibility**: Both directions work\n:::\n\n:::{.callout-warning}\n## Breaking Changes\n\nSome changes cannot be made backwards-compatible:\n\n- Removing required fields\n- Changing field types incompatibly\n- Removing entire entities\n\nFor breaking changes:\n1. **Version bump**: Increment major version (v1 → v2)\n2. **Parallel operation**: Run both versions simultaneously\n3. **Gradual migration**: Migrate consumers incrementally\n4. **Deprecation timeline**: Announce timeline (3-6 months)\n5. **Sunset old version**: Remove after migration complete\n:::\n\n## Multi-Source Data Fusion\n\nProduction embedding systems integrate data from multiple sources: user profiles from CRM, product data from inventory, behavioral logs from analytics, external data from partners. **Multi-source data fusion** combines these heterogeneous datasets into unified embeddings while handling schema mismatches, different update frequencies, and varying data quality.\n\n### The Data Fusion Challenge\n\nEach data source has unique characteristics:\n\n- **Schema**: Different field names, types, structures\n- **Frequency**: Some update real-time, others daily/weekly\n- **Quality**: Varying completeness, correctness, timeliness\n- **Scale**: Some have millions of records, others billions\n- **Access**: APIs, databases, files, streams\n\n**Challenge**: Combine these sources into training data that preserves relationships across sources.\n\n::: {.cell execution_count=5}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show Multi-Source Data Fusion\"}\nfrom typing import Dict, List, Any, Optional\nfrom datetime import datetime\n\nclass MultiSourceDataFusion:\n    \"\"\"Fuse data from multiple sources into unified records.\"\"\"\n\n    def __init__(self):\n        self.source_priorities = {\n            'primary_db': 1,\n            'external_api': 2,\n            'user_input': 3\n        }\n\n    def fuse(self, entity_id: str, sources: Dict[str, Dict[str, Any]]) -> Dict[str, Any]:\n        \"\"\"Fuse data from multiple sources for a single entity.\"\"\"\n        fused = {'entity_id': entity_id}\n\n        # Collect all fields from all sources\n        all_fields = set()\n        for source_data in sources.values():\n            all_fields.update(source_data.keys())\n\n        # Resolve conflicts for each field\n        for field in all_fields:\n            value = self._resolve_field(field, sources)\n            if value is not None:\n                fused[field] = value\n\n        return fused\n\n    def _resolve_field(self, field: str, sources: Dict[str, Dict[str, Any]]) -> Any:\n        \"\"\"Resolve conflicts for a single field across sources.\"\"\"\n        candidates = []\n\n        for source_name, source_data in sources.items():\n            if field in source_data:\n                value = source_data[field]\n                priority = self.source_priorities.get(source_name, 999)\n                timestamp = source_data.get('_timestamp', datetime.min)\n                candidates.append((value, priority, timestamp, source_name))\n\n        if not candidates:\n            return None\n\n        # Resolution strategy: Priority-based with recency as tiebreaker\n        candidates.sort(key=lambda x: (x[1], -x[2].timestamp() if isinstance(x[2], datetime) else 0))\n\n        return candidates[0][0]\n\n    def batch_fuse(self, entity_sources: Dict[str, Dict[str, Dict[str, Any]]]) -> List[Dict[str, Any]]:\n        \"\"\"Fuse data for multiple entities in batch.\"\"\"\n        return [\n            self.fuse(entity_id, sources)\n            for entity_id, sources in entity_sources.items()\n        ]\n\n# Usage example\nfusion = MultiSourceDataFusion()\n\n# Data from multiple sources for same entity\nentity_data = {\n    'primary_db': {\n        'name': 'John Doe',\n        'email': 'john@example.com',\n        '_timestamp': datetime(2024, 1, 1)\n    },\n    'external_api': {\n        'name': 'John D.',\n        'phone': '+1-555-0100',\n        '_timestamp': datetime(2024, 1, 5)\n    },\n    'user_input': {\n        'email': 'john.doe@example.com',  # More recent\n        'preferences': {'theme': 'dark'},\n        '_timestamp': datetime(2024, 1, 10)\n    }\n}\n\nfused_record = fusion.fuse('user_123', entity_data)\nprint(f\"Fused record: {fused_record}\")\nprint(f\"  Name from: primary_db (highest priority)\")\nprint(f\"  Email from: user_input (most recent)\")\nprint(f\"  Phone from: external_api (only source)\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFused record: {'entity_id': 'user_123', 'phone': '+1-555-0100', 'email': 'john@example.com', 'name': 'John Doe', '_timestamp': datetime.datetime(2024, 1, 1, 0, 0), 'preferences': {'theme': 'dark'}}\n  Name from: primary_db (highest priority)\n  Email from: user_input (most recent)\n  Phone from: external_api (only source)\n```\n:::\n:::\n\n\n:::{.callout-tip}\n## Multi-Source Fusion Best Practices\n\n**Schema management:**\n\n- **Canonical schema**: Define single target schema\n- **Schema registry**: Centralize source schema definitions\n- **Schema evolution**: Version schemas and migrate incrementally\n- **Type safety**: Validate types during alignment\n\n**Conflict resolution:**\n\n- **Priority-based**: Assign priority to sources (authority)\n- **Recency-based**: Prefer most recently updated value\n- **Quality-based**: Weight by source quality score\n- **Context-aware**: Consider semantic meaning\n\n**Performance:**\n\n- **Incremental fusion**: Only fuse changed entities\n- **Partitioning**: Partition by entity_id for parallel fusion\n- **Caching**: Cache fused results (invalidate on update)\n- **Lazy loading**: Fuse on-demand for rarely accessed entities\n:::\n\n## Key Takeaways\n\n- **ETL pipelines must preserve semantic relationships**: Unlike traditional ETL that optimizes for SQL analytics, embedding ETL requires feature engineering that captures similarity and meaning, not just facts\n\n- **Streaming enables real-time embeddings with sub-second latency**: Micro-batching architectures (100-1000ms windows) balance throughput and latency, enabling fresh embeddings for dynamic content like news and social media\n\n- **Data quality directly determines embedding quality**: Comprehensive validation (schema, values, semantics, duplicates, drift) prevents training on corrupted data that would poison embeddings for months\n\n- **Schema evolution requires careful coordination across components**: Backwards-compatible changes (add fields, provide defaults) enable safe evolution while breaking changes (remove fields, change types) require parallel operation and gradual migration\n\n- **Multi-source fusion combines heterogeneous datasets into unified embeddings**: Schema alignment, entity resolution, conflict resolution, and temporal alignment enable leveraging data from multiple systems with different schemas and update frequencies\n\n- **Data engineering is the foundation of embedding systems**: High-quality embeddings require high-quality data engineering; invest in pipelines, quality frameworks, and fusion strategies before scaling models\n\n- **The data engineering hierarchy**: Quality > Schema design > Performance. Impact ratios vary by domain, but garbage-in-garbage-out applies universally—focus on correctness before optimizing throughput\n\n## Looking Ahead\n\nPart III (Production Engineering) concludes with robust data engineering practices that ensure embedding systems have the clean, consistent, high-quality data needed to achieve their potential. Part IV (Advanced Applications) begins with @sec-text-chunking, which explores how to prepare text documents for embedding systems, covering chunking strategies, document-type specific approaches, and metadata preservation for optimal retrieval quality.\n\n## Further Reading\n\n### Data Engineering\n- Kleppmann, Martin (2017). \"Designing Data-Intensive Applications.\" O'Reilly Media.\n- Reis, Cathy, and Rupal Mahajan (2019). \"Data Engineering with Apache Spark, Delta Lake, and Lakehouse.\" O'Reilly Media.\n- Kalidindi, Santhosh (2021). \"Data Engineering with Python.\" Packt Publishing.\n\n### ETL and Pipelines\n- Kimball, Ralph, and Margy Ross (2013). \"The Data Warehouse Toolkit.\" Wiley.\n- Apache Airflow Documentation. \"Best Practices.\"\n- dbt Documentation. \"Best Practices for Data Transformation.\"\n\n### Streaming Systems\n- Kleppmann, Martin (2016). \"Making Sense of Stream Processing.\" O'Reilly Media.\n- Narkhede, Neha, et al. (2017). \"Kafka: The Definitive Guide.\" O'Reilly Media.\n- Apache Flink Documentation. \"Streaming Concepts.\"\n\n### Data Quality\n- Redman, Thomas (2016). \"Getting in Front on Data Quality.\" Harvard Business Review.\n- Batini, Carlo, and Monica Scannapieco (2016). \"Data and Information Quality.\" Springer.\n- Talend Data Quality Documentation. \"Data Quality Best Practices.\"\n\n### Schema Evolution\n- Kleppmann, Martin (2015). \"Schema Evolution in Avro, Protocol Buffers and Thrift.\" Blog post.\n- Confluent Documentation. \"Schema Evolution and Compatibility.\"\n- Fowler, Martin (2016). \"Evolutionary Database Design.\" martinfowler.com.\n\n### Data Integration\n- Doan, AnHai, et al. (2012). \"Principles of Data Integration.\" Morgan Kaufmann.\n- Haas, Laura, et al. (2005). \"Clio Grows Up: From Research Prototype to Industrial Tool.\" SIGMOD.\n- Madhavan, Jayant, et al. (2001). \"Generic Schema Matching with Cupid.\" VLDB.\n\n",
    "supporting": [
      "ch16_data_engineering_files/figure-pdf"
    ],
    "filters": []
  }
}