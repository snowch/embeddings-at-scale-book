{
  "hash": "1e5c0450beb68d543ea964e8a5cee2f7",
  "result": {
    "engine": "jupyter",
    "markdown": "# Scaling Embedding Training {#sec-scaling-embedding-training}\n\n:::{.callout-note}\n## Chapter Overview\nTraining embedding models on trillion-row datasets requires computational infrastructure that goes far beyond single-GPU training. This chapter explores the architectures and techniques that enable embedding training at unprecedented scale: distributed training across hundreds of GPUs, gradient accumulation and mixed precision for memory efficiency, advanced memory optimization techniques, multi-GPU and multi-node coordination strategies, and cost optimization approaches that make large-scale training economically viable. These techniques transform embedding training from a multi-day single-machine task to a multi-hour distributed operation, enabling rapid iteration and larger, more powerful models.\n:::\n\nEmbedding model training faces unique scaling challenges. Unlike image classification models that process fixed-size inputs, embedding models often work with variable-length sequences, sparse features, and massive vocabularies. Contrastive learning requires large batch sizes (4K-32K samples) for effective negative sampling. Self-supervised pre-training demands processing billions of documents. These requirements push standard training infrastructure to its limits, requiring specialized techniques for efficient distributed training.\n\n## Distributed Training Architectures\n\nDistributed training parallelizes model training across multiple devices, reducing training time from weeks to hours. However, embedding training has unique requirements that distinguish it from standard distributed training: **large batch sizes for contrastive learning**, **sparse feature handling**, **vocabulary parallelism for large embedding tables**, and **efficient negative sampling across devices**. This section explores architectures that address these challenges.\n\n### Parallelism Strategies for Embedding Training\n\nModern distributed training employs multiple parallelism strategies simultaneously:\n\n::: {.cell execution_count=1}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show Distributed Embedding Table\"}\nimport torch\nimport torch.distributed as dist\nimport torch.nn as nn\n\n\nclass DistributedEmbeddingTable(nn.Module):\n    \"\"\"Model-parallel embedding table for large vocabularies split across GPUs.\"\"\"\n\n    def __init__(self, total_vocab_size, embedding_dim, world_size, rank):\n        super().__init__()\n        self.total_vocab_size = total_vocab_size\n        self.embedding_dim = embedding_dim\n        self.world_size = world_size\n        self.rank = rank\n\n        # Each GPU holds a slice of vocabulary\n        self.vocab_per_gpu = total_vocab_size // world_size\n        self.vocab_start = rank * self.vocab_per_gpu\n        self.vocab_end = (rank + 1) * self.vocab_per_gpu\n\n        # Local embedding table (subset of vocabulary)\n        self.embeddings = nn.Embedding(self.vocab_per_gpu, embedding_dim)\n        print(f\"Rank {rank}: Vocabulary [{self.vocab_start}, {self.vocab_end})\")\n\n    def forward(self, input_ids):\n        \"\"\"Lookup embeddings across distributed vocabulary.\"\"\"\n        batch_size, seq_len = input_ids.shape\n        output = torch.zeros(batch_size, seq_len, self.embedding_dim, device=input_ids.device)\n\n        # Mask for tokens this GPU is responsible for\n        local_mask = (input_ids >= self.vocab_start) & (input_ids < self.vocab_end)\n\n        if local_mask.any():\n            local_ids = input_ids[local_mask] - self.vocab_start\n            local_embeddings = self.embeddings(local_ids)\n            output[local_mask] = local_embeddings\n\n        # All-reduce: Sum embeddings from all GPUs\n        dist.all_reduce(output, op=dist.ReduceOp.SUM)\n        return output\n\n\n# Usage example (conceptual - requires distributed setup)\n# model = DistributedEmbeddingTable(total_vocab_size=100000, embedding_dim=512, world_size=8, rank=0)\nprint(\"Distributed embedding table for model-parallel training\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDistributed embedding table for model-parallel training\n```\n:::\n:::\n\n\nFor multi-GPU training with PyTorch's distributed module, you typically launch with `torchrun`:\n\n```bash\n# Single node, 8 GPUs\ntorchrun --nproc_per_node=8 train.py\n\n# Multi-node (4 nodes, 8 GPUs each)\ntorchrun --nproc_per_node=8 --nnodes=4 --node_rank=0 \\\n         --master_addr=node0 --master_port=1234 train.py\n```\n\n::: {.cell execution_count=2}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show Distributed Training Example\"}\nimport torch\nimport torch.nn as nn\n\n\nclass DistributedContrastiveEmbedding(nn.Module):\n    \"\"\"Embedding model for distributed contrastive training.\"\"\"\n    def __init__(self, vocab_size, embedding_dim):\n        super().__init__()\n        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n        self.projection = nn.Linear(embedding_dim, embedding_dim)\n\n    def forward(self, ids):\n        return self.projection(self.embeddings(ids))\n\n\nclass DistributedTrainer:\n    \"\"\"Trainer for distributed embedding model.\"\"\"\n    def __init__(self, model, local_rank, world_size):\n        self.model = model\n        self.device = f\"cuda:{local_rank}\" if torch.cuda.is_available() else \"cpu\"\n        self.model.to(self.device)\n        self.world_size = world_size\n        self.local_rank = local_rank\n\n    def train_step(self, batch, optimizer):\n        anchor = self.model(batch['anchor_ids'])\n        positive = self.model(batch['positive_ids'])\n        loss = nn.functional.mse_loss(anchor, positive)  # Simplified\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        return loss.item()\n\n    def save_checkpoint(self, path, epoch, optimizer):\n        torch.save({'epoch': epoch, 'model': self.model.state_dict()}, path)\n\n    def cleanup(self):\n        pass  # In practice: dist.destroy_process_group()\n\n\n# Initialize distributed trainer\nmodel = DistributedContrastiveEmbedding(vocab_size=100000, embedding_dim=512)\ntrainer = DistributedTrainer(model=model, local_rank=0, world_size=1)\n\n# Optimizer\noptimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n\n# Training step demo\nbatch = {\n    'anchor_ids': torch.randint(0, 100000, (256,), device=trainer.device),\n    'positive_ids': torch.randint(0, 100000, (256,), device=trainer.device)\n}\nloss = trainer.train_step(batch, optimizer)\nprint(f\"Training step loss: {loss:.4f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTraining step loss: 0.6668\n```\n:::\n:::\n\n\n:::{.callout-tip}\n## Choosing the Right Parallelism Strategy\n\n**Use Data Parallelism when:**\n\n- Model fits on single GPU\n- Batch size is primary bottleneck\n- Most layers are data-parallel friendly (convolutions, transformers)\n\n**Add Model Parallelism when:**\n\n- Embedding tables > GPU memory (100M+ vocabulary)\n- Single layer > GPU memory (very wide transformer layers)\n\n**Add Pipeline Parallelism when:**\n\n- Model depth > memory capacity (100+ transformer layers)\n- High arithmetic intensity (can hide communication latency)\n\n**For embedding training:**\n\n- Start with Data Parallelism for encoder\n- Add Model Parallelism for large embedding tables\n- Consider Pipeline Parallelism for deep architectures (BERT-Large, GPT-3 scale)\n:::\n\n:::{.callout-warning}\n## Communication Bottlenecks\n\nDistributed training speedup is limited by communication:\n\n- **All-reduce** (gradient sync): O(parameters × world_size)\n- **All-gather** (activations): O(batch_size × hidden_dim × world_size)\n- **Point-to-point** (pipeline): O(hidden_dim × micro_batch_size)\n\nOptimizations:\n\n- **Gradient compression**: Reduce precision (FP32 → FP16 gradients)\n- **Overlap communication and computation**: Backward pass while communicating gradients\n- **Hierarchical reduction**: Node-local reduction, then cross-node\n- **Faster interconnect**: InfiniBand (200 Gbps) vs Ethernet (10-100 Gbps)\n:::\n\n## Gradient Accumulation and Mixed Precision\n\nMemory is the primary constraint in deep learning training. A single NVIDIA A100 GPU has 80GB memory, yet training large embedding models with contrastive learning (32K batch size × 512 dims × 4 bytes ≈ 64GB just for embeddings) quickly exceeds capacity. **Gradient accumulation** enables large effective batch sizes by splitting batches into smaller micro-batches, while **mixed precision** reduces memory footprint and accelerates computation by using FP16 for most operations while maintaining FP32 for numerical stability.\n\n### Gradient Accumulation for Large Batch Training\n\nContrastive learning benefits from large batch sizes—more negatives improve representation quality. But memory limits batch size. Gradient accumulation solves this:\n\n::: {.cell execution_count=3}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show Gradient Accumulation Trainer\"}\nimport torch\nimport torch.nn as nn\n\n\nclass GradientAccumulationTrainer:\n    \"\"\"Enable large effective batch sizes through gradient accumulation.\"\"\"\n\n    def __init__(self, model, accumulation_steps=4):\n        self.model = model\n        self.accumulation_steps = accumulation_steps\n\n    def train_step(self, dataloader, optimizer, device=\"cuda\"):\n        \"\"\"Training step with gradient accumulation.\"\"\"\n        self.model.train()\n        optimizer.zero_grad()\n        total_loss = 0.0\n\n        for i, batch in enumerate(dataloader):\n            if i >= self.accumulation_steps:\n                break\n\n            anchor_ids = batch[\"anchor_ids\"].to(device)\n            positive_ids = batch[\"positive_ids\"].to(device)\n\n            loss = self.model(anchor_ids, positive_ids)\n            loss = loss / self.accumulation_steps  # Scale loss\n            loss.backward()  # Accumulate gradients\n            total_loss += loss.item()\n\n        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n        optimizer.step()\n        optimizer.zero_grad()\n\n        return total_loss\n\n\n# Usage example\n# model = EmbeddingModel()\n# trainer = GradientAccumulationTrainer(model, accumulation_steps=32)\nprint(\"Gradient accumulation enables 32K+ effective batch sizes\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nGradient accumulation enables 32K+ effective batch sizes\n```\n:::\n:::\n\n\n### Mixed Precision Training\n\nModern GPUs (Volta, Turing, Ampere architectures) have specialized Tensor Cores that accelerate FP16 matrix multiplications by 2-8×. **Mixed precision** uses FP16 for computation while maintaining FP32 for numerical stability:\n\n::: {.cell execution_count=4}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show Mixed Precision Trainer\"}\nimport torch\nimport torch.nn as nn\nfrom torch.cuda.amp import GradScaler, autocast\n\n\nclass MixedPrecisionTrainer:\n    \"\"\"Automatic mixed precision (AMP) training for 1.5-2x speedup (workload-dependent).\"\"\"\n\n    def __init__(self, model, device=\"cuda\"):\n        self.model = model.to(device)\n        self.device = device\n        self.scaler = GradScaler()\n\n    def train_step(self, batch, optimizer):\n        \"\"\"Training step with automatic mixed precision.\"\"\"\n        self.model.train()\n        anchor_ids = batch[\"anchor_ids\"].to(self.device)\n        positive_ids = batch[\"positive_ids\"].to(self.device)\n\n        optimizer.zero_grad()\n\n        # Forward pass in FP16\n        with autocast():\n            loss = self.model(anchor_ids, positive_ids)\n\n        # Backward with gradient scaling\n        self.scaler.scale(loss).backward()\n        self.scaler.unscale_(optimizer)\n        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n        self.scaler.step(optimizer)\n        self.scaler.update()\n\n        return loss.item()\n\n\n# Usage example\n# trainer = MixedPrecisionTrainer(model)\nprint(\"Mixed precision training: 1.5-2x speedup typical on modern GPUs\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMixed precision training: 1.5-2x speedup typical on modern GPUs\n```\n:::\n:::\n\n\n:::{.callout-tip}\n## When to Use Gradient Accumulation vs Larger Hardware\n\n**Use gradient accumulation when:**\n\n- Memory-constrained (batch won't fit on GPU)\n- Want to experiment with very large batches (64K+)\n- Training on cloud instances with limited GPU memory\n\n**Upgrade hardware when:**\n\n- Wall-clock time is critical (accumulation is slower)\n- Training very frequently (hardware cost amortizes)\n- Need to scale beyond single node (distributed > accumulation)\n\n**Use mixed precision almost always:**\n\n- Modern GPUs (V100, A100) have Tensor Cores\n- 1.5-2× speedup with minimal code changes\n- Rarely causes numerical issues (except very deep networks)\n:::\n\n:::{.callout-warning}\n## Mixed Precision Gotchas\n\n**Gradient underflow**: Very small gradients (< 1e-7) round to zero in FP16. Gradient scaling addresses this, but extreme cases may need:\n\n- Larger learning rates\n- Loss scaling adjustments\n- FP32 for sensitive layers (layer norm, softmax)\n\n**Batch normalization**: BatchNorm statistics in FP16 can be unstable. Use FP32 for BatchNorm layers:\n```python\nmodel = model.half()  # Convert to FP16\n# Keep BatchNorm in FP32\nfor module in model.modules():\n    if isinstance(module, nn.BatchNorm1d):\n        module.float()\n```\n:::\n\n## Memory Optimization Techniques\n\nBeyond mixed precision and gradient accumulation, several techniques reduce memory footprint, enabling larger models and batch sizes:\n\n### Gradient Checkpointing\n\nTrade computation for memory by recomputing activations during backward pass instead of storing them:\n\n::: {.cell execution_count=5}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show Gradient Checkpointing\"}\nimport torch\nimport torch.nn as nn\nfrom torch.utils.checkpoint import checkpoint\n\n\nclass CheckpointedTransformerLayer(nn.Module):\n    \"\"\"Transformer layer with gradient checkpointing for memory efficiency.\"\"\"\n\n    def __init__(self, hidden_dim, num_heads):\n        super().__init__()\n        self.attention = nn.MultiheadAttention(hidden_dim, num_heads)\n        self.ffn = nn.Sequential(nn.Linear(hidden_dim, hidden_dim * 4), nn.ReLU(), nn.Linear(hidden_dim * 4, hidden_dim))\n        self.norm1 = nn.LayerNorm(hidden_dim)\n        self.norm2 = nn.LayerNorm(hidden_dim)\n\n    def forward(self, x):\n        \"\"\"Forward with gradient checkpointing to save memory.\"\"\"\n        def attention_forward(x):\n            attn_out, _ = self.attention(x, x, x)\n            return self.norm1(x + attn_out)\n\n        x = checkpoint(attention_forward, x)\n\n        def ffn_forward(x):\n            return self.norm2(x + self.ffn(x))\n\n        x = checkpoint(ffn_forward, x)\n        return x\n\n\n# Usage example\n# layer = CheckpointedTransformerLayer(hidden_dim=512, num_heads=8)\nprint(\"Gradient checkpointing: 10-50x memory reduction\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nGradient checkpointing: 10-50x memory reduction\n```\n:::\n:::\n\n\n### Optimizer State Optimization\n\nAdam optimizer stores momentum and variance for each parameter, tripling memory usage. Optimizations:\n\n::: {.cell execution_count=6}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show Memory-Efficient Optimizer\"}\nimport torch\nimport torch.optim as optim\n\n\nclass MemoryEfficientOptimizer:\n    \"\"\"Optimize memory usage for large models using efficient optimizers.\"\"\"\n\n    @staticmethod\n    def get_optimizer(parameters, optimizer_type=\"adamw\", lr=0.001):\n        \"\"\"Get memory-efficient optimizer.\"\"\"\n        if optimizer_type == \"adamw\":\n            return optim.AdamW(parameters, lr=lr, fused=True)\n        elif optimizer_type == \"sgd\":\n            return optim.SGD(parameters, lr=lr, momentum=0.9, nesterov=True)\n        elif optimizer_type == \"8bit_adam\":\n            try:\n                import bitsandbytes as bnb\n                return bnb.optim.Adam8bit(parameters, lr=lr)\n            except ImportError:\n                print(\"bitsandbytes not installed, using AdamW\")\n                return optim.AdamW(parameters, lr=lr)\n\n\n# Usage example\n# params = model.parameters()\n# optimizer = MemoryEfficientOptimizer.get_optimizer(params, \"8bit_adam\")\nprint(\"8-bit optimizers: 4x memory reduction vs standard Adam\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n8-bit optimizers: 4x memory reduction vs standard Adam\n```\n:::\n:::\n\n\n:::{.callout-tip}\n## Memory Optimization Checklist\n\nWhen hitting memory limits, apply optimizations in this order:\n\n1. **Mixed precision (FP16)**: 2× memory reduction, 1.5-2× speedup\n2. **Gradient accumulation**: Enables larger effective batch sizes\n3. **Gradient checkpointing**: 10-50× activation memory reduction\n4. **Optimizer state optimization**: 8-bit Adam or SGD\n5. **Model parallelism**: Split model across GPUs\n6. **Batch size reduction**: Last resort (hurts contrastive learning)\n\nTypical savings:\n\n- FP16: 40GB → 20GB\n- + Checkpointing: 20GB → 8GB\n- + 8-bit optimizer: 8GB → 5GB\n- Result: Fit on single A100 (80GB) with large batch\n:::\n\n## Multi-GPU and Multi-Node Strategies\n\nScaling beyond single GPU requires coordination across devices. This section covers practical strategies for multi-GPU (single node) and multi-node (multiple machines) training.\n\n### Multi-GPU Training on Single Node\n\nSingle-node multi-GPU is the most common setup (8× A100 or V100 GPUs on one machine):\n\n::: {.cell execution_count=7}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show Distributed Data Loading\"}\nimport torch\nfrom torch.utils.data import Dataset, DataLoader, DistributedSampler\n\n\nclass EmbeddingDataset(Dataset):\n    \"\"\"Dataset for efficient distributed embedding training.\"\"\"\n\n    def __init__(self, data_path, sequence_length=512):\n        self.data_path = data_path\n        self.sequence_length = sequence_length\n        self.data = []  # Load from data_path in practice\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        return {\"anchor_ids\": torch.randint(0, 50000, (self.sequence_length,)),\n                \"positive_ids\": torch.randint(0, 50000, (self.sequence_length,))}\n\n\ndef setup_distributed_dataloaders(dataset, batch_size, world_size, rank):\n    \"\"\"Create distributed dataloaders with proper sharding.\"\"\"\n    sampler = DistributedSampler(dataset, num_replicas=world_size, rank=rank, shuffle=True)\n\n    dataloader = DataLoader(dataset, batch_size=batch_size, sampler=sampler,\n                            num_workers=4, pin_memory=True, prefetch_factor=2)\n    return dataloader\n\n\n# Usage example\n# dataset = EmbeddingDataset(\"data.parquet\")\n# loader = setup_distributed_dataloaders(dataset, batch_size=256, world_size=8, rank=0)\nprint(\"Distributed dataloaders ensure each GPU sees unique data\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDistributed dataloaders ensure each GPU sees unique data\n```\n:::\n:::\n\n\n### Multi-Node Training\n\nMulti-node training scales to hundreds of GPUs across dozens of machines:\n\n::: {.cell execution_count=8}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show Multi-Node Training Setup\"}\nimport os\nimport torch\nimport torch.distributed as dist\n\n\ndef setup_multi_node():\n    \"\"\"Initialize multi-node distributed training environment.\"\"\"\n    rank = int(os.environ.get('RANK', 0))\n    world_size = int(os.environ.get('WORLD_SIZE', 1))\n    local_rank = int(os.environ.get('LOCAL_RANK', 0))\n    master_addr = os.environ.get('MASTER_ADDR', 'localhost')\n    master_port = os.environ.get('MASTER_PORT', '12355')\n\n    os.environ['MASTER_ADDR'] = master_addr\n    os.environ['MASTER_PORT'] = master_port\n\n    print(f\"Initializing process group: rank={rank}, world_size={world_size}, local_rank={local_rank}\")\n\n    dist.init_process_group(backend='nccl', init_method='env://', world_size=world_size, rank=rank)\n\n    torch.cuda.set_device(local_rank)\n    return rank, world_size, local_rank\n\n\n# Usage example - run with torchrun:\n# torchrun --nproc_per_node=8 --nnodes=4 --node_rank=$NODE_RANK train.py\n# rank, world_size, local_rank = setup_multi_node()\nprint(\"Multi-node setup: coordinate training across multiple machines\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMulti-node setup: coordinate training across multiple machines\n```\n:::\n:::\n\n\nFor multi-node training, use SLURM or torchrun to launch across machines:\n\n```bash\n# SLURM submission\nsbatch --nodes=4 --gres=gpu:8 train_multi_node.sh\n\n# Or with torchrun on each node:\ntorchrun --nproc_per_node=8 \\\n         --nnodes=4 \\\n         --node_rank=$NODE_RANK \\\n         --master_addr=$MASTER_ADDR \\\n         --master_port=1234 \\\n         train_script.py\n```\n\n:::{.callout-tip}\n## Multi-GPU Best Practices\n\n**Data loading:**\n\n- Use `DistributedSampler` to partition data across GPUs\n- Set `num_workers=4` per GPU for async data loading\n- Use `pin_memory=True` for faster CPU→GPU transfer\n\n**Learning rate scaling:**\n\n- Scale learning rate linearly with batch size\n- 1 GPU (batch 512, lr 0.001) → 8 GPUs (batch 4096, lr 0.008)\n- May need warmup for large learning rates\n\n**Synchronization:**\n\n- Minimize `dist.barrier()` calls (blocks all GPUs)\n- Overlap communication with computation\n- Use `find_unused_parameters=False` in DDP when possible\n\n**Checkpointing:**\n\n- Only save from rank 0 to avoid duplicate writes\n- Use `dist.barrier()` after saving to synchronize\n- Consider sharded checkpointing for very large models\n:::\n\n:::{.callout-warning}\n## Multi-Node Challenges\n\n**Network bottlenecks:**\n\n- Cross-node communication 10-100× slower than NVLink\n- Use gradient compression or ZeRO optimizer\n- Consider hierarchical all-reduce (node-local first)\n\n**Fault tolerance:**\n\n- Single node failure kills entire job\n- Implement checkpointing every N steps\n- Use elastic training frameworks (TorchElastic)\n\n**Load imbalance:**\n\n- Stragglers slow down entire cluster\n- Monitor per-GPU utilization\n- Use dynamic batch sizing if variability high\n:::\n\n## Training Cost Optimization\n\nLarge-scale training is expensive. A 100-GPU training run can cost $10K-$100K. This section covers strategies to minimize cost while maintaining quality.\n\n### Cloud Cost Optimization\n\n::: {.cell execution_count=9}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show Cloud Cost Optimization\"}\nimport time\nfrom datetime import datetime\n\n\nclass CloudCostOptimizer:\n    \"\"\"Optimize training costs through instance selection and resource management.\"\"\"\n\n    def __init__(self, budget_per_hour=100.0):\n        self.budget_per_hour = budget_per_hour\n        self.instance_costs = {\n            \"p3.2xlarge\": 3.06,    # V100\n            \"p4d.24xlarge\": 32.77,  # A100\n            \"g5.xlarge\": 1.006      # T4\n        }\n\n    def select_instance_config(self, target_gpus, prefer_cost=True):\n        \"\"\"Select optimal instance configuration based on budget and requirements.\"\"\"\n        configs = []\n\n        for instance_type, hourly_cost in self.instance_costs.items():\n            gpus_per_instance = {\"p3.2xlarge\": 1, \"p4d.24xlarge\": 8, \"g5.xlarge\": 1}[instance_type]\n\n            num_instances = (target_gpus + gpus_per_instance - 1) // gpus_per_instance\n            total_cost = num_instances * hourly_cost\n\n            if total_cost <= self.budget_per_hour:\n                configs.append({\"instance_type\": instance_type, \"num_instances\": num_instances,\n                                \"total_gpus\": num_instances * gpus_per_instance, \"hourly_cost\": total_cost})\n\n        if prefer_cost:\n            configs.sort(key=lambda x: x[\"hourly_cost\"])\n        else:\n            configs.sort(key=lambda x: -x[\"total_gpus\"])\n\n        return configs[0] if configs else None\n\n\n# Usage example\noptimizer = CloudCostOptimizer(budget_per_hour=50.0)\nconfig = optimizer.select_instance_config(target_gpus=8, prefer_cost=True)\nprint(f\"Optimal config: {config}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nOptimal config: {'instance_type': 'g5.xlarge', 'num_instances': 8, 'total_gpus': 8, 'hourly_cost': 8.048}\n```\n:::\n:::\n\n\n### Spot Instance Training\n\nSpot instances offer 50-90% discounts but can be preempted. Strategies for resilient training:\n\n::: {.cell execution_count=10}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show Spot Instance Training\"}\nimport time\nimport torch\n\n\nclass SpotInstanceTrainer:\n    \"\"\"Training with spot instance resilience via frequent checkpointing.\"\"\"\n\n    def __init__(self, model, checkpoint_interval=300):\n        self.model = model\n        self.checkpoint_interval = checkpoint_interval\n        self.last_checkpoint = time.time()\n\n    def train(self, dataloader, optimizer, epochs=10):\n        \"\"\"Train with automatic checkpointing for spot instance resilience.\"\"\"\n        for epoch in range(epochs):\n            for batch_idx, batch in enumerate(dataloader):\n                try:\n                    loss = self.model(batch)\n                    loss.backward()\n                    optimizer.step()\n                    optimizer.zero_grad()\n\n                    # Checkpoint every N seconds\n                    if time.time() - self.last_checkpoint > self.checkpoint_interval:\n                        self.save_checkpoint(epoch, batch_idx)\n                        self.last_checkpoint = time.time()\n\n                except RuntimeError as e:\n                    if \"preempted\" in str(e).lower():\n                        print(\"Spot instance preempted! Checkpoint saved.\")\n                        self.save_checkpoint(epoch, batch_idx)\n                        raise\n                    else:\n                        raise\n\n    def save_checkpoint(self, epoch, batch_idx):\n        \"\"\"Save checkpoint for recovery.\"\"\"\n        checkpoint = {\"epoch\": epoch, \"batch_idx\": batch_idx, \"model_state\": self.model.state_dict()}\n        torch.save(checkpoint, f\"checkpoint_epoch{epoch}_batch{batch_idx}.pt\")\n        print(f\"Checkpoint saved: epoch {epoch}, batch {batch_idx}\")\n\n    def load_checkpoint(self, checkpoint_path):\n        \"\"\"Resume from checkpoint.\"\"\"\n        checkpoint = torch.load(checkpoint_path)\n        self.model.load_state_dict(checkpoint[\"model_state\"])\n        return checkpoint[\"epoch\"], checkpoint[\"batch_idx\"]\n\n\n# Usage example\n# trainer = SpotInstanceTrainer(model, checkpoint_interval=300)\nprint(\"Spot instance training: 50-90% cost savings with checkpointing\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSpot instance training: 50-90% cost savings with checkpointing\n```\n:::\n:::\n\n\n:::{.callout-tip}\n## Cost Optimization Strategies\n\n**Immediate savings (no quality impact):**\n1. **Spot instances**: 50-90% discount (with checkpointing)\n2. **Mixed precision**: 1.5-2× speedup → 40-60% cost reduction\n3. **Reserved instances**: 30-50% discount for long-term projects\n4. **Multi-cloud**: Compare prices across AWS/GCP/Azure\n\n**Advanced optimizations:**\n1. **Early stopping**: Halt when validation loss plateaus\n2. **Hyperparameter search efficiency**: Use Bayesian optimization, not grid search\n3. **Model distillation**: Train large model, deploy small model\n4. **Sparse training**: Train only subset of parameters\n\n**Typical cost breakdown (100-GPU training):**\n\n- Hardware: 70% (can optimize with spot instances)\n- Storage: 10% (use cheaper object storage)\n- Network: 10% (minimize cross-region transfer)\n- Other: 10% (monitoring, logging, etc.)\n:::\n\n## Key Takeaways\n\n- **Distributed training is essential at scale**: Data parallelism for throughput, model parallelism for large embedding tables, and pipeline parallelism for deep architectures combine to enable trillion-row training in reasonable time\n\n- **Gradient accumulation enables large effective batch sizes**: Split large batches into micro-batches to fit memory constraints while maintaining the benefits of large-batch contrastive learning (16K-32K samples)\n\n- **Mixed precision training provides 1.5-2× speedup**: FP16 computation on Tensor Cores with FP32 master weights maintains numerical stability while reducing memory usage and accelerating training (actual speedup is workload-dependent)\n\n- **Memory optimization unlocks larger models**: Gradient checkpointing, optimizer state quantization (8-bit Adam), and efficient activation management reduce memory footprint by 10-50×, enabling BERT-scale models on single GPUs\n\n- **Multi-node training scales to hundreds of GPUs**: Proper configuration of distributed samplers, learning rate scaling, and network topology awareness enable near-linear scaling to 64+ GPUs with 40-50× speedup\n\n- **Cost optimization is critical for sustainable training**: Spot instances (50-90% savings), mixed precision speedup, and efficient checkpointing reduce training costs from $100K to $10K-$30K for large models\n\n- **Communication is the bottleneck at scale**: Gradient synchronization, activation gathering, and cross-node communication limit speedup; overlap computation with communication and use gradient compression to mitigate\n\n## Looking Ahead\n\nThis chapter covered the computational techniques for training embedding models at scale. @sec-embedding-quality-evaluation addresses a critical question: how do you know if your embeddings are good? We explore intrinsic quality metrics (isotropy, uniformity, alignment), comprehensive retrieval metrics (Recall@K, MAP, NDCG, MRR), human evaluation frameworks, domain-specific metrics, and statistical rigor—providing the measurement foundation for continuous improvement.\n\n## Further Reading\n\n### Distributed Training\n- Li, Shen, et al. (2020). \"PyTorch Distributed: Experiences on Accelerating Data Parallel Training.\" VLDB.\n- Shoeybi, Mohammad, et al. (2019). \"Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism.\" arXiv:1909.08053.\n- Rajbhandari, Samyam, et al. (2020). \"ZeRO: Memory Optimizations Toward Training Trillion Parameter Models.\" SC20.\n\n### Mixed Precision Training\n- Micikevicius, Paulius, et al. (2018). \"Mixed Precision Training.\" ICLR.\n- Narang, Sharan, et al. (2018). \"Mixed Precision Training With 8-bit Floating Point.\" arXiv:1905.12334.\n\n### Memory Optimization\n- Chen, Tianqi, et al. (2016). \"Training Deep Nets with Sublinear Memory Cost.\" arXiv:1604.06174.\n- Sohoni, Nimit, et al. (2019). \"Low-Memory Neural Network Training.\" arXiv:1904.10631.\n- Dettmers, Tim, et al. (2022). \"8-bit Optimizers via Block-wise Quantization.\" arXiv:2110.02861.\n\n### Large-Scale Training Systems\n- Jia, Xianyan, et al. (2018). \"Highly Scalable Deep Learning Training System with Mixed-Precision.\" arXiv:1807.11205.\n- Sergeev, Alexander, and Mike Del Balso (2018). \"Horovod: Fast and Easy Distributed Deep Learning in TensorFlow.\" arXiv:1802.05799.\n- Paszke, Adam, et al. (2019). \"PyTorch: An Imperative Style, High-Performance Deep Learning Library.\" NeurIPS.\n\n### Cost Optimization\n- Chaudhary, Vinay, et al. (2020). \"Balancing Efficiency and Flexibility for DNN Acceleration via Temporal GPU-Systolic Array Integration.\" DAC.\n- Yang, Tianyi, et al. (2021). \"Toward Efficient Deep Learning in the Cloud: Resource Provisioning and Workload Scheduling.\" IEEE Cloud Computing.\n\n",
    "supporting": [
      "ch14_scaling_embedding_training_files/figure-pdf"
    ],
    "filters": []
  }
}