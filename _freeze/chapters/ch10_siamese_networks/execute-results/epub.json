{
  "hash": "9dfdb21c41de6040f2aab1eb6a7615ca",
  "result": {
    "engine": "jupyter",
    "markdown": "# Siamese Networks for Specialized Use Cases {#sec-siamese-networks}\n\n:::{.callout-note}\n## Chapter Overview\nWhile contrastive learning (Chapter 5) taught us how to train embeddings that distinguish similar from dissimilar, Siamese networks provide the architectural foundation for similarity-based learning at enterprise scale. This chapter explores Siamese architectures—twin neural networks that excel at learning similarity metrics for specialized use cases including one-shot learning, anomaly detection, and verification systems. We cover the architectural patterns, triplet loss optimization, strategies for rare event handling, threshold calibration techniques, and production deployment patterns that enable Siamese networks to scale to trillion-row deployments.\n:::\n\n## Siamese Architecture for Enterprise Similarity\n\nSiamese networks solve a fundamental challenge: **how do you learn similarity when you have few examples per class, unbalanced distributions, or continuously evolving categories?** Traditional classifiers fail in these scenarios. Siamese networks succeed by learning to compare rather than classify.\n\n### The Siamese Paradigm\n\nNamed after Siamese twins, a Siamese network consists of two or more identical neural networks (sharing weights) that process different inputs and compare their outputs. The key insight: **instead of learning \"what is X?\", learn \"how similar are X and Y?\"**\n\nThis shift enables:\n\n- **Few-shot learning**: Learn from 1-5 examples per class\n- **Open-set recognition**: Handle classes not seen during training\n- **Verification tasks**: \"Are these the same?\" vs \"What is this?\"\n- **Similarity search**: Find nearest neighbors in learned space\n\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass SiameseNetwork(nn.Module):\n    \"\"\"\n    Siamese Network for learning similarity metrics\n\n    Architecture: Two identical networks (shared weights) process different\n    inputs, producing embeddings that are compared using a distance metric.\n\n    Use cases:\n    - Face verification: \"Is this the same person?\"\n    - Document similarity: \"Are these papers related?\"\n    - Product matching: \"Are these the same item?\"\n    - Anomaly detection: \"Is this different from normal?\"\n    \"\"\"\n\n    def __init__(self, embedding_net, embedding_dim=512):\n        \"\"\"\n        Args:\n            embedding_net: The base network for creating embeddings\n                          (e.g., ResNet, BERT, custom architecture)\n            embedding_dim: Dimension of output embeddings\n        \"\"\"\n        super().__init__()\n        self.embedding_net = embedding_net\n        self.embedding_dim = embedding_dim\n\n    def forward(self, x1, x2):\n        \"\"\"\n        Forward pass through Siamese network\n\n        Args:\n            x1: First input (batch_size, ...)\n            x2: Second input (batch_size, ...)\n\n        Returns:\n            embedding1: Embeddings for x1 (batch_size, embedding_dim)\n            embedding2: Embeddings for x2 (batch_size, embedding_dim)\n        \"\"\"\n        # Both inputs go through the SAME network (shared weights)\n        embedding1 = self.embedding_net(x1)\n        embedding2 = self.embedding_net(x2)\n\n        return embedding1, embedding2\n\n    def get_embedding(self, x):\n        \"\"\"Get embedding for a single input\"\"\"\n        return self.embedding_net(x)\n\n\nclass EmbeddingNet(nn.Module):\n    \"\"\"\n    Example embedding network for structured/tabular data\n\n    For images: Use ResNet, EfficientNet, Vision Transformer\n    For text: Use BERT, RoBERTa, sentence transformers\n    For multimodal: Use CLIP-style architectures\n    \"\"\"\n\n    def __init__(self, input_dim, embedding_dim=512, hidden_dims=[1024, 512]):\n        super().__init__()\n\n        layers = []\n        prev_dim = input_dim\n\n        for hidden_dim in hidden_dims:\n            layers.extend([\n                nn.Linear(prev_dim, hidden_dim),\n                nn.BatchNorm1d(hidden_dim),\n                nn.ReLU(),\n                nn.Dropout(0.3)\n            ])\n            prev_dim = hidden_dim\n\n        # Final embedding layer\n        layers.append(nn.Linear(prev_dim, embedding_dim))\n\n        self.network = nn.Sequential(*layers)\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x: Input features (batch_size, input_dim)\n\n        Returns:\n            embeddings: L2-normalized embeddings (batch_size, embedding_dim)\n        \"\"\"\n        embeddings = self.network(x)\n        # L2 normalization for cosine similarity\n        return F.normalize(embeddings, p=2, dim=1)\n\n\n# Example: Building a Siamese network for enterprise use\ndef create_enterprise_siamese_network(input_type='tabular', input_dim=None):\n    \"\"\"\n    Factory function for creating Siamese networks\n\n    Args:\n        input_type: 'tabular', 'image', 'text', or 'multimodal'\n        input_dim: Input dimension (for tabular data)\n\n    Returns:\n        SiameseNetwork instance configured for the input type\n    \"\"\"\n\n    if input_type == 'tabular':\n        if input_dim is None:\n            raise ValueError(\"input_dim required for tabular data\")\n        embedding_net = EmbeddingNet(\n            input_dim=input_dim,\n            embedding_dim=512,\n            hidden_dims=[1024, 768, 512]\n        )\n\n    elif input_type == 'image':\n        # Use pre-trained ResNet\n        import torchvision.models as models\n        resnet = models.resnet50(pretrained=True)\n        # Remove classification head\n        embedding_net = nn.Sequential(*list(resnet.children())[:-1])\n\n    elif input_type == 'text':\n        # Use transformer-based encoder\n        from transformers import AutoModel\n        embedding_net = AutoModel.from_pretrained('bert-base-uncased')\n\n    else:\n        raise ValueError(f\"Unknown input_type: {input_type}\")\n\n    return SiameseNetwork(embedding_net, embedding_dim=512)\n```\n\n### Contrastive Loss for Siamese Networks\n\nThe classic Siamese network uses **contrastive loss** to bring similar pairs together and push dissimilar pairs apart:\n\n::: {#31cbe447 .cell execution_count=1}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show contrastive loss implementation\"}\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass ContrastiveLoss(nn.Module):\n    \"\"\"Contrastive loss: Loss = (1-Y)*0.5*D^2 + Y*0.5*max(margin-D, 0)^2\"\"\"\n\n    def __init__(self, margin=2.0):\n        super().__init__()\n        self.margin = margin\n\n    def forward(self, embedding1, embedding2, label):\n        \"\"\"label: 0 if similar, 1 if dissimilar\"\"\"\n        euclidean_distance = F.pairwise_distance(embedding1, embedding2)\n        loss_similar = (1 - label) * torch.pow(euclidean_distance, 2)\n        loss_dissimilar = label * torch.pow(\n            torch.clamp(self.margin - euclidean_distance, min=0.0), 2\n        )\n        loss = torch.mean(loss_similar + loss_dissimilar) * 0.5\n\n        with torch.no_grad():\n            threshold = self.margin / 2\n            predictions = (euclidean_distance < threshold).long()\n            accuracy = (predictions == (1 - label)).float().mean()\n            similar_mask = label == 0\n            dissimilar_mask = label == 1\n            metrics = {\n                \"loss\": loss.item(), \"accuracy\": accuracy.item(),\n                \"mean_similar_distance\": euclidean_distance[similar_mask].mean().item() if similar_mask.any() else 0,\n                \"mean_dissimilar_distance\": euclidean_distance[dissimilar_mask].mean().item() if dissimilar_mask.any() else 0,\n            }\n        return loss, metrics\n\n# Usage example\nloss_fn = ContrastiveLoss(margin=2.0)\nemb1 = torch.randn(32, 512)\nemb2 = torch.randn(32, 512)\nlabels = torch.randint(0, 2, (32,))\nloss, metrics = loss_fn(emb1, emb2, labels)\nprint(f\"Contrastive loss: {metrics['loss']:.4f}, accuracy: {metrics['accuracy']:.4f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nContrastive loss: 281.9283, accuracy: 0.4688\n```\n:::\n:::\n\n\n:::{.callout-tip}\n## Choosing Distance Metrics\n\n**Euclidean distance** works well for normalized embeddings in low dimensions (< 128).\n\n**Cosine distance** (1 - cosine similarity) is preferred for:\n\n- High-dimensional embeddings (> 128)\n- Text embeddings\n- When magnitude isn't meaningful\n\n**Learned distance metrics** (e.g., Mahalanobis) can capture domain-specific similarity but require more data and computation.\n:::\n\n### Enterprise Siamese Architecture Patterns\n\nFor production systems handling billions of comparisons daily, architecture choices matter:\n\n::: {#1d467dfd .cell execution_count=2}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show enterprise-optimized Siamese network\"}\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass EnterpriseOptimizedSiameseNetwork(nn.Module):\n    \"\"\"Production-optimized with mixed precision, gradient checkpointing, attention\"\"\"\n\n    def __init__(self, base_model, embedding_dim=512, use_attention=True, use_gradient_checkpointing=False):\n        super().__init__()\n        self.base_model = base_model\n        self.use_gradient_checkpointing = use_gradient_checkpointing\n        self.projection = nn.Sequential(\n            nn.Linear(embedding_dim, embedding_dim), nn.BatchNorm1d(embedding_dim),\n            nn.ReLU(), nn.Linear(embedding_dim, embedding_dim),\n        )\n        self.attention = nn.MultiheadAttention(embed_dim=embedding_dim, num_heads=8, dropout=0.1, batch_first=True) if use_attention else None\n\n    def forward(self, x1, x2):\n        if self.use_gradient_checkpointing and self.training:\n            embedding1 = torch.utils.checkpoint.checkpoint(self._encode, x1)\n            embedding2 = torch.utils.checkpoint.checkpoint(self._encode, x2)\n        else:\n            embedding1, embedding2 = self._encode(x1), self._encode(x2)\n        return embedding1, embedding2\n\n    def _encode(self, x):\n        features = self.base_model(x)\n        if self.attention is not None:\n            features_reshaped = features.unsqueeze(1)\n            attended, _ = self.attention(features_reshaped, features_reshaped, features_reshaped)\n            features = attended.squeeze(1)\n        embedding = self.projection(features)\n        return F.normalize(embedding, p=2, dim=1)\n\n# Usage example\nbase = nn.Sequential(nn.Linear(128, 512), nn.ReLU())\nmodel = EnterpriseOptimizedSiameseNetwork(base, embedding_dim=512, use_attention=True)\nprint(f\"Model params: {sum(p.numel() for p in model.parameters()):,}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModel params: 1,643,008\n```\n:::\n:::\n\n\n:::{.callout-warning}\n## Production Considerations\n\n**Memory Management**: For large models (> 1B parameters), gradient checkpointing is essential. It trades 30% more compute for 50% less memory.\n\n**Batch Size Selection**: Larger batches (256-1024) improve training stability for Siamese networks. Use gradient accumulation if GPU memory is limited.\n\n**Learning Rate**: Start with 1e-4 for fine-tuning pre-trained models, 1e-3 for training from scratch. Use warmup for stability.\n:::\n\n## Triplet Loss Optimization Techniques\n\nWhile contrastive loss works with pairs, **triplet loss** works with triplets: (anchor, positive, negative). This provides more information per training example and often leads to better embeddings.\n\n### Triplet Loss Fundamentals\n\nTriplet loss ensures that anchor-positive distance is smaller than anchor-negative distance by at least a margin:\n\n**Loss = max(d(anchor, positive) - d(anchor, negative) + margin, 0)**\n\n::: {#62b91513 .cell execution_count=3}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show triplet loss implementation\"}\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass TripletLoss(nn.Module):\n    \"\"\"Triplet loss: d(anchor, positive) + margin < d(anchor, negative)\"\"\"\n\n    def __init__(self, margin=1.0, distance_metric=\"euclidean\"):\n        super().__init__()\n        self.margin = margin\n        self.distance_metric = distance_metric\n\n    def forward(self, anchor, positive, negative):\n        if self.distance_metric == \"euclidean\":\n            pos_distance = F.pairwise_distance(anchor, positive, p=2)\n            neg_distance = F.pairwise_distance(anchor, negative, p=2)\n        else:  # cosine\n            pos_distance = 1 - F.cosine_similarity(anchor, positive)\n            neg_distance = 1 - F.cosine_similarity(anchor, negative)\n\n        losses = F.relu(pos_distance - neg_distance + self.margin)\n        loss = losses.mean()\n\n        with torch.no_grad():\n            hard_triplets = (losses > 0).float().mean()\n            accuracy = (pos_distance < neg_distance).float().mean()\n            metrics = {\n                \"loss\": loss.item(), \"accuracy\": accuracy.item(),\n                \"hard_triplets_fraction\": hard_triplets.item(),\n                \"avg_pos_distance\": pos_distance.mean().item(),\n                \"avg_neg_distance\": neg_distance.mean().item(),\n            }\n        return loss, metrics\n\n# Usage example\nloss_fn = TripletLoss(margin=1.0)\nanchor = torch.randn(32, 512)\npositive = anchor + torch.randn(32, 512) * 0.1\nnegative = torch.randn(32, 512)\nloss, metrics = loss_fn(anchor, positive, negative)\nprint(f\"Triplet loss: {metrics['loss']:.4f}, accuracy: {metrics['accuracy']:.4f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTriplet loss: 0.0000, accuracy: 1.0000\n```\n:::\n:::\n\n\n### Advanced Triplet Loss Variants\n\nFor enterprise scale, basic triplet loss isn't enough. Here are production-tested variants:\n\n::: {#94a93e63 .cell execution_count=4}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show advanced triplet loss with mining strategies\"}\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass AdvancedTripletLoss(nn.Module):\n    \"\"\"Advanced triplet loss with hard/semi-hard mining and soft margin\"\"\"\n\n    def __init__(self, margin=1.0, mining_strategy=\"semi-hard\", use_soft_margin=False, distance_metric=\"euclidean\"):\n        super().__init__()\n        self.margin = margin\n        self.mining_strategy = mining_strategy\n        self.use_soft_margin = use_soft_margin\n        self.distance_metric = distance_metric\n\n    def forward(self, embeddings, labels):\n        # Compute pairwise distances\n        if self.distance_metric == \"euclidean\":\n            distances = torch.cdist(embeddings, embeddings, p=2)\n        else:\n            embeddings_norm = F.normalize(embeddings, p=2, dim=1)\n            distances = 1 - torch.mm(embeddings_norm, embeddings_norm.T)\n\n        triplets = self._mine_triplets(distances, labels)\n        if len(triplets) == 0:\n            return torch.tensor(0.0, device=embeddings.device), {\"loss\": 0.0, \"num_triplets\": 0}\n\n        anchor_idx, positive_idx, negative_idx = zip(*triplets)\n        pos_distances = distances[anchor_idx, positive_idx]\n        neg_distances = distances[anchor_idx, negative_idx]\n\n        if self.use_soft_margin:\n            loss = torch.log1p(torch.exp(pos_distances - neg_distances)).mean()\n        else:\n            loss = F.relu(pos_distances - neg_distances + self.margin).mean()\n\n        with torch.no_grad():\n            metrics = {\"loss\": loss.item(), \"num_triplets\": len(triplets),\n                       \"hard_triplets_fraction\": (pos_distances > neg_distances).float().mean().item()}\n        return loss, metrics\n\n    def _mine_triplets(self, distances, labels):\n        \"\"\"Mine triplets based on strategy (hard, semi-hard, or all)\"\"\"\n        batch_size = labels.shape[0]\n        triplets = []\n        for i in range(batch_size):\n            pos_mask = (labels == labels[i]) & (torch.arange(batch_size, device=labels.device) != i)\n            neg_mask = labels != labels[i]\n            pos_indices, neg_indices = torch.where(pos_mask)[0], torch.where(neg_mask)[0]\n            if len(pos_indices) == 0 or len(neg_indices) == 0:\n                continue\n            for pos_idx in pos_indices:\n                if self.mining_strategy == \"hard\":\n                    neg_idx = neg_indices[distances[i, neg_indices].argmin()]\n                else:  # semi-hard or all\n                    neg_idx = neg_indices[0]\n                triplets.append((i, pos_idx.item(), neg_idx.item()))\n        return triplets\n\n# Usage example\nloss_fn = AdvancedTripletLoss(margin=1.0, mining_strategy=\"semi-hard\")\nembeddings = torch.randn(50, 512)\nlabels = torch.randint(0, 10, (50,))\nloss, metrics = loss_fn(embeddings, labels)\nprint(f\"Advanced triplet loss: {metrics['loss']:.4f}, triplets: {metrics['num_triplets']}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAdvanced triplet loss: 1.2256, triplets: 236\n```\n:::\n:::\n\n\n:::{.callout-tip}\n## Mining Strategy Selection\n\n**Hard negative mining**: Best for well-separated classes. Can cause training instability if classes overlap.\n\n**Semi-hard negative mining**: Recommended for production. Balances learning speed with stability. Use when classes have some overlap.\n\n**All triplets**: Only for small datasets (< 10K examples) or final fine-tuning. Computationally expensive.\n\n**Rule of thumb**: Start with semi-hard, switch to hard if training plateaus after 70% of epochs.\n:::\n\n### Batch Construction for Efficient Triplet Training\n\nEfficient triplet mining requires careful batch construction:\n\n::: {#ce468d89 .cell execution_count=5}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show balanced batch sampler for triplet training\"}\nimport numpy as np\nimport torch\nfrom torch.utils.data import Sampler\n\nclass BalancedBatchSampler(Sampler):\n    \"\"\"Sampler ensuring each batch has P classes × K samples per class\"\"\"\n\n    def __init__(self, labels, n_classes_per_batch=10, n_samples_per_class=5):\n        self.labels = np.array(labels)\n        self.n_classes_per_batch = n_classes_per_batch\n        self.n_samples_per_class = n_samples_per_class\n\n        # Build index mapping: class_id -> [sample_indices]\n        self.class_to_indices = {}\n        for idx, label in enumerate(self.labels):\n            if label not in self.class_to_indices:\n                self.class_to_indices[label] = []\n            self.class_to_indices[label].append(idx)\n\n        # Keep classes with enough samples\n        self.valid_classes = [c for c, indices in self.class_to_indices.items()\n                              if len(indices) >= self.n_samples_per_class]\n        self.batch_size = n_classes_per_batch * n_samples_per_class\n\n    def __iter__(self):\n        classes = np.random.permutation(self.valid_classes)\n        for i in range(0, len(classes), self.n_classes_per_batch):\n            batch_classes = classes[i : i + self.n_classes_per_batch]\n            batch_indices = []\n            for class_id in batch_classes:\n                class_indices = self.class_to_indices[class_id]\n                sampled = np.random.choice(class_indices, size=self.n_samples_per_class,\n                                           replace=len(class_indices) < self.n_samples_per_class)\n                batch_indices.extend(sampled)\n            yield batch_indices\n\n    def __len__(self):\n        return len(self.valid_classes) // self.n_classes_per_batch\n\n# Usage example\nlabels = np.random.randint(0, 100, size=10000)  # 10K samples, 100 classes\nsampler = BalancedBatchSampler(labels, n_classes_per_batch=10, n_samples_per_class=5)\nprint(f\"Batch size: {sampler.batch_size}, Batches per epoch: {len(sampler)}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nBatch size: 50, Batches per epoch: 10\n```\n:::\n:::\n\n\n:::{.callout-warning}\n## Production Batch Sizing\n\n**Memory constraints**: P × K = batch_size. Larger batches provide more triplets but require more memory.\n\n**Recommended configurations**:\n\n- Small models (< 100M params): P=16, K=8, batch_size=128\n- Medium models (100M-1B params): P=10, K=5, batch_size=50\n- Large models (> 1B params): P=8, K=4, batch_size=32\n\n**GPU utilization**: Use gradient accumulation to simulate larger batches if needed.\n:::\n\n## One-Shot Learning for Rare Events\n\nOne-shot learning—learning from a single example—is critical for enterprise scenarios where rare events are important but examples are scarce: fraud detection, manufacturing defects, zero-day threats, rare diseases.\n\n### One-Shot Learning Fundamentals\n\nTraditional ML fails with one example per class. Siamese networks succeed by:\n\n1. **Learning similarity during training** on abundant data\n2. **Applying similarity at inference** to new classes with few examples\n3. **Comparing rather than classifying** new inputs\n\n::: {#9be00b13 .cell execution_count=6}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show one-shot classifier implementation\"}\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass OneShotClassifier:\n    \"\"\"One-shot classifier: classify by finding most similar support example\"\"\"\n\n    def __init__(self, siamese_model, distance_metric=\"euclidean\"):\n        self.model = siamese_model\n        self.distance_metric = distance_metric\n        self.support_set = {}  # class_id -> embedding\n\n    def add_support_example(self, class_id, example):\n        \"\"\"Add a single example for a new class\"\"\"\n        with torch.no_grad():\n            self.model.eval()\n            embedding = self.model.get_embedding(example)\n            self.support_set[class_id] = embedding.cpu()\n\n    def predict(self, query, return_distances=False, top_k=1):\n        \"\"\"Predict class by finding nearest support example\"\"\"\n        with torch.no_grad():\n            self.model.eval()\n            query_embedding = self.model.get_embedding(query)\n\n            distances = {}\n            for class_id, support_emb in self.support_set.items():\n                support_emb = support_emb.to(query_embedding.device)\n                if self.distance_metric == \"euclidean\":\n                    dist = F.pairwise_distance(query_embedding, support_emb.unsqueeze(0)).item()\n                else:\n                    dist = (1 - F.cosine_similarity(query_embedding, support_emb.unsqueeze(0))).item()\n                distances[class_id] = dist\n\n            sorted_classes = sorted(distances.items(), key=lambda x: x[1])\n            if top_k == 1:\n                return (sorted_classes[0][0], sorted_classes[0][1]) if return_distances else sorted_classes[0][0]\n            results = sorted_classes[:top_k]\n            return ([c for c, _ in results], [d for _, d in results]) if return_distances else [c for c, _ in results]\n\n    def predict_proba(self, query, temperature=1.0):\n        \"\"\"Predict class probabilities using softmax over negative distances\"\"\"\n        class_ids, distances = self.predict(query, return_distances=True, top_k=len(self.support_set))\n        similarities = [-d / temperature for d in distances]\n        exp_sims = np.exp(similarities - np.max(similarities))\n        return dict(zip(class_ids, exp_sims / exp_sims.sum()))\n\n# Usage example (placeholder model)\nclass PlaceholderModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.encoder = nn.Linear(50, 128)\n    def get_embedding(self, x):\n        return F.normalize(self.encoder(x), dim=-1)\n\nmodel = PlaceholderModel()\nclassifier = OneShotClassifier(model)\nclassifier.add_support_example(\"fraud_type_A\", torch.randn(1, 50))\nclassifier.add_support_example(\"fraud_type_B\", torch.randn(1, 50))\nquery = torch.randn(1, 50)\npred = classifier.predict(query)\nprint(f\"Predicted class: {pred}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPredicted class: fraud_type_A\n```\n:::\n:::\n\n\n:::{.callout-tip}\n## When One-Shot Learning Works Best\n\n**Ideal scenarios**:\n\n- High-quality training data (even if small)\n- Well-defined similarity metric\n- Rare event detection (fraud, anomalies, defects)\n- Rapidly evolving categories (new threats, trends)\n\n**Challenging scenarios**:\n\n- Noisy data (single example may be unrepresentative)\n- Complex decision boundaries\n- Classes that require multiple features to distinguish\n\n**Best practice**: Collect 3-5 examples per class when possible. Average their embeddings for more robust representation.\n:::\n\n### Few-Shot Learning Extensions\n\nWhen you have 2-10 examples per class (few-shot), you can use more sophisticated techniques:\n\n::: {#84ce019b .cell execution_count=7}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show prototypical network classifier\"}\nimport torch\nimport torch.nn.functional as F\n\nclass PrototypicalNetworkClassifier:\n    \"\"\"Prototypical Networks: compute class prototypes from K examples, classify by nearest prototype\"\"\"\n\n    def __init__(self, embedding_model):\n        self.model = embedding_model\n        self.prototypes = {}  # class_id -> prototype embedding\n\n    def compute_prototypes(self, support_set):\n        \"\"\"Compute prototype (centroid) for each class from support examples\"\"\"\n        self.prototypes = {}\n        with torch.no_grad():\n            self.model.eval()\n            for class_id, examples in support_set.items():\n                if isinstance(examples, list):\n                    examples = torch.stack(examples)\n                embeddings = self.model.get_embedding(examples)\n                self.prototypes[class_id] = embeddings.mean(dim=0)\n\n    def predict(self, query):\n        \"\"\"Classify query by finding nearest prototype\"\"\"\n        with torch.no_grad():\n            self.model.eval()\n            query_embedding = self.model.get_embedding(query)\n            distances = {class_id: F.pairwise_distance(query_embedding, proto.unsqueeze(0)).item()\n                         for class_id, proto in self.prototypes.items()}\n            return min(distances.items(), key=lambda x: x[1])[0]\n\n# Usage example\nimport torch.nn as nn\nclass SimpleEncoder(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.net = nn.Linear(64, 128)\n    def get_embedding(self, x):\n        return F.normalize(self.net(x), dim=-1)\n\nencoder = SimpleEncoder()\nclassifier = PrototypicalNetworkClassifier(encoder)\nsupport_set = {\"class_A\": torch.randn(5, 64), \"class_B\": torch.randn(5, 64)}  # 5 examples each\nclassifier.compute_prototypes(support_set)\nquery = torch.randn(1, 64)\npred = classifier.predict(query)\nprint(f\"Predicted class: {pred}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPredicted class: class_A\n```\n:::\n:::\n\n\n## Similarity Threshold Calibration\n\nA critical but often overlooked challenge: **How do you set the threshold for \"similar enough\"?** Too low and you get false positives. Too high and you miss true matches.\n\n### The Threshold Calibration Challenge\n\n```python\nclass ThresholdCalibrator:\n    \"\"\"\n    Calibrate similarity thresholds for production deployment\n\n    Challenge: The optimal threshold depends on:\n    - Distribution of true positives vs negatives\n    - Business costs of false positives vs false negatives\n    - Dataset characteristics (intra-class vs inter-class variance)\n\n    This class provides multiple calibration strategies.\n    \"\"\"\n\n    def __init__(self, siamese_model):\n        self.model = siamese_model\n        self.threshold = None\n        self.calibration_metrics = {}\n\n    def calibrate_on_validation_set(\n        self,\n        validation_pairs,\n        validation_labels,\n        metric='f1',\n        plot=False\n    ):\n        \"\"\"\n        Calibrate threshold on validation set to optimize a metric\n\n        Args:\n            validation_pairs: List of (item1, item2) pairs\n            validation_labels: 1 if similar, 0 if dissimilar\n            metric: 'f1', 'precision', 'recall', or 'accuracy'\n            plot: If True, plot threshold vs metric curve\n\n        Returns:\n            Optimal threshold value\n        \"\"\"\n        # Compute distances for all pairs\n        distances = []\n\n        with torch.no_grad():\n            self.model.eval()\n\n            for item1, item2 in validation_pairs:\n                embedding1 = self.model.get_embedding(item1.unsqueeze(0))\n                embedding2 = self.model.get_embedding(item2.unsqueeze(0))\n\n                distance = F.pairwise_distance(embedding1, embedding2).item()\n                distances.append(distance)\n\n        distances = np.array(distances)\n        validation_labels = np.array(validation_labels)\n\n        # Try different thresholds\n        thresholds = np.linspace(distances.min(), distances.max(), 100)\n        metrics_by_threshold = []\n\n        for threshold in thresholds:\n            # Predict: similar if distance < threshold\n            predictions = (distances < threshold).astype(int)\n\n            # Compute metrics\n            tp = ((predictions == 1) & (validation_labels == 1)).sum()\n            fp = ((predictions == 1) & (validation_labels == 0)).sum()\n            tn = ((predictions == 0) & (validation_labels == 0)).sum()\n            fn = ((predictions == 0) & (validation_labels == 1)).sum()\n\n            precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n            recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n            f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n            accuracy = (tp + tn) / len(validation_labels)\n\n            metrics_by_threshold.append({\n                'threshold': threshold,\n                'precision': precision,\n                'recall': recall,\n                'f1': f1,\n                'accuracy': accuracy\n            })\n\n        # Find threshold that maximizes chosen metric\n        best_idx = max(\n            range(len(metrics_by_threshold)),\n            key=lambda i: metrics_by_threshold[i][metric]\n        )\n\n        self.threshold = metrics_by_threshold[best_idx]['threshold']\n        self.calibration_metrics = metrics_by_threshold[best_idx]\n\n        if plot:\n            self._plot_calibration_curve(metrics_by_threshold, metric)\n\n        return self.threshold\n\n    def calibrate_with_business_costs(\n        self,\n        validation_pairs,\n        validation_labels,\n        false_positive_cost=1.0,\n        false_negative_cost=1.0\n    ):\n        \"\"\"\n        Calibrate threshold based on business costs\n\n        Args:\n            validation_pairs: List of (item1, item2) pairs\n            validation_labels: 1 if similar, 0 if dissimilar\n            false_positive_cost: Cost of incorrectly marking as similar\n            false_negative_cost: Cost of missing a true match\n\n        Returns:\n            Cost-optimal threshold\n\n        Example costs:\n        - Fraud detection: FN cost >> FP cost (missing fraud is expensive)\n        - Product matching: FP cost >> FN cost (wrong matches annoy users)\n        \"\"\"\n        # Compute distances\n        distances = []\n\n        with torch.no_grad():\n            self.model.eval()\n\n            for item1, item2 in validation_pairs:\n                embedding1 = self.model.get_embedding(item1.unsqueeze(0))\n                embedding2 = self.model.get_embedding(item2.unsqueeze(0))\n\n                distance = F.pairwise_distance(embedding1, embedding2).item()\n                distances.append(distance)\n\n        distances = np.array(distances)\n        validation_labels = np.array(validation_labels)\n\n        # Try different thresholds\n        thresholds = np.linspace(distances.min(), distances.max(), 100)\n        costs = []\n\n        for threshold in thresholds:\n            predictions = (distances < threshold).astype(int)\n\n            fp = ((predictions == 1) & (validation_labels == 0)).sum()\n            fn = ((predictions == 0) & (validation_labels == 1)).sum()\n\n            total_cost = fp * false_positive_cost + fn * false_negative_cost\n            costs.append(total_cost)\n\n        # Find threshold that minimizes cost\n        best_idx = np.argmin(costs)\n        self.threshold = thresholds[best_idx]\n\n        self.calibration_metrics = {\n            'threshold': self.threshold,\n            'expected_cost': costs[best_idx],\n            'false_positive_cost': false_positive_cost,\n            'false_negative_cost': false_negative_cost\n        }\n\n        return self.threshold\n\n    def calibrate_for_precision_target(\n        self,\n        validation_pairs,\n        validation_labels,\n        target_precision=0.95\n    ):\n        \"\"\"\n        Calibrate to achieve target precision\n\n        Use when false positives are unacceptable (e.g., financial matching)\n\n        Args:\n            validation_pairs: List of (item1, item2) pairs\n            validation_labels: 1 if similar, 0 if dissimilar\n            target_precision: Desired precision (0-1)\n\n        Returns:\n            Threshold that achieves target precision (or closest possible)\n        \"\"\"\n        # Compute distances\n        distances = []\n\n        with torch.no_grad():\n            self.model.eval()\n\n            for item1, item2 in validation_pairs:\n                embedding1 = self.model.get_embedding(item1.unsqueeze(0))\n                embedding2 = self.model.get_embedding(item2.unsqueeze(0))\n\n                distance = F.pairwise_distance(embedding1, embedding2).item()\n                distances.append(distance)\n\n        distances = np.array(distances)\n        validation_labels = np.array(validation_labels)\n\n        # Try different thresholds\n        thresholds = np.linspace(distances.min(), distances.max(), 100)\n\n        best_threshold = None\n        best_precision = 0\n        best_recall = 0\n\n        for threshold in thresholds:\n            predictions = (distances < threshold).astype(int)\n\n            tp = ((predictions == 1) & (validation_labels == 1)).sum()\n            fp = ((predictions == 1) & (validation_labels == 0)).sum()\n            fn = ((predictions == 0) & (validation_labels == 1)).sum()\n\n            precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n            recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n\n            # Find threshold closest to target precision\n            if precision >= target_precision:\n                if best_threshold is None or recall > best_recall:\n                    best_threshold = threshold\n                    best_precision = precision\n                    best_recall = recall\n\n        if best_threshold is None:\n            # Can't achieve target, return threshold with highest precision\n            for threshold in thresholds:\n                predictions = (distances < threshold).astype(int)\n                tp = ((predictions == 1) & (validation_labels == 1)).sum()\n                fp = ((predictions == 1) & (validation_labels == 0)).sum()\n                precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n\n                if precision > best_precision:\n                    best_precision = precision\n                    best_threshold = threshold\n\n        self.threshold = best_threshold\n        self.calibration_metrics = {\n            'threshold': best_threshold,\n            'achieved_precision': best_precision,\n            'achieved_recall': best_recall,\n            'target_precision': target_precision\n        }\n\n        return self.threshold\n\n    def _plot_calibration_curve(self, metrics_by_threshold, target_metric):\n        \"\"\"Plot threshold vs metric curve\"\"\"\n        import matplotlib.pyplot as plt\n\n        thresholds = [m['threshold'] for m in metrics_by_threshold]\n        values = [m[target_metric] for m in metrics_by_threshold]\n\n        plt.figure(figsize=(10, 6))\n        plt.plot(thresholds, values)\n        plt.axvline(self.threshold, color='r', linestyle='--',\n                   label=f'Optimal: {self.threshold:.3f}')\n        plt.xlabel('Threshold')\n        plt.ylabel(target_metric.capitalize())\n        plt.title(f'Threshold Calibration: {target_metric.capitalize()}')\n        plt.legend()\n        plt.grid(True)\n        plt.show()\n```\n\n:::{.callout-warning}\n## Threshold Calibration Best Practices\n\n**Re-calibrate regularly**: Data distributions drift. Re-calibrate quarterly or when you detect performance degradation.\n\n**Use stratified validation**: Ensure your validation set represents production distribution. Unbalanced calibration data leads to suboptimal thresholds.\n\n**Monitor threshold effectiveness**: Track precision/recall in production. Alert if metrics deviate > 5% from calibration values.\n\n**Business cost alignment**: Work with stakeholders to quantify FP and FN costs. Technical metrics (F1) may not align with business value.\n:::\n\n### Dynamic Threshold Adaptation\n\nFor production systems, static thresholds aren't enough. Implement dynamic adaptation:\n\n::: {#bfd31091 .cell execution_count=8}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show adaptive threshold manager\"}\nimport numpy as np\n\nclass AdaptiveThresholdManager:\n    \"\"\"Manage thresholds that adapt to changing data distributions\"\"\"\n\n    def __init__(self, base_threshold=0.5):\n        self.base_threshold = base_threshold\n        self.category_thresholds = {}\n        self.performance_history = []\n\n    def get_threshold(self, category=None, confidence=None):\n        \"\"\"Get threshold, adjusted for category or confidence\"\"\"\n        threshold = self.base_threshold\n        if category is not None and category in self.category_thresholds:\n            threshold = self.category_thresholds[category]\n        if confidence is not None:\n            adjustment = (confidence - 0.5) * 0.2  # ±0.1 adjustment\n            threshold = threshold - adjustment\n        return threshold\n\n    def update_category_threshold(self, category, new_threshold):\n        self.category_thresholds[category] = new_threshold\n\n    def adapt_from_feedback(self, predictions, labels, learning_rate=0.1):\n        \"\"\"Adapt thresholds based on recent performance feedback\"\"\"\n        current_predictions = (predictions < self.base_threshold).astype(int)\n        error_rate = (current_predictions != labels).mean()\n\n        if error_rate > 0.2:\n            best_threshold = self._find_optimal_threshold(predictions, labels)\n            self.base_threshold = (1 - learning_rate) * self.base_threshold + learning_rate * best_threshold\n\n        self.performance_history.append({\"threshold\": self.base_threshold, \"error_rate\": error_rate})\n\n    def _find_optimal_threshold(self, distances, labels):\n        thresholds = np.linspace(distances.min(), distances.max(), 50)\n        errors = [(distances < t).astype(int) != labels for t in thresholds]\n        error_rates = [e.mean() for e in errors]\n        return thresholds[np.argmin(error_rates)]\n\n# Usage example\nmanager = AdaptiveThresholdManager(base_threshold=0.5)\nmanager.update_category_threshold(\"high_value\", 0.7)\nprint(f\"Base threshold: {manager.base_threshold}\")\nprint(f\"High-value threshold: {manager.get_threshold(category='high_value')}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nBase threshold: 0.5\nHigh-value threshold: 0.7\n```\n:::\n:::\n\n\n## Production Deployment Patterns\n\nDeploying Siamese networks at scale requires careful architecture design. Here are battle-tested patterns from trillion-row deployments:\n\n### Pattern 1: Embedding Cache Architecture\n\n::: {#a114b472 .cell execution_count=9}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show Siamese embedding service with caching\"}\nimport hashlib\nimport torch\nimport torch.nn.functional as F\n\nclass SiameseEmbeddingService:\n    \"\"\"Production service with embedding caching, batch processing, GPU/CPU flexibility\"\"\"\n\n    def __init__(self, model, cache_size=100000, batch_size=256, device=\"cuda\"):\n        self.model = model.to(device).eval()\n        self.device = device\n        self.batch_size = batch_size\n        self.embedding_cache = {}\n        self.cache_size = cache_size\n        self.cache_hits = 0\n        self.cache_misses = 0\n\n    def _get_cache_key(self, item):\n        return hashlib.md5(item.cpu().numpy().tobytes()).hexdigest()\n\n    def get_embedding(self, item, use_cache=True):\n        if use_cache:\n            cache_key = self._get_cache_key(item)\n            if cache_key in self.embedding_cache:\n                self.cache_hits += 1\n                return self.embedding_cache[cache_key]\n            self.cache_misses += 1\n\n        with torch.no_grad():\n            embedding = self.model.get_embedding(item.to(self.device))\n\n        if use_cache:\n            if len(self.embedding_cache) >= self.cache_size:\n                oldest_key = next(iter(self.embedding_cache))\n                del self.embedding_cache[oldest_key]\n            self.embedding_cache[cache_key] = embedding.cpu()\n        return embedding\n\n    def get_embeddings_batch(self, items):\n        embeddings = []\n        for i in range(0, len(items), self.batch_size):\n            batch = items[i : i + self.batch_size]\n            with torch.no_grad():\n                batch_embeddings = self.model.get_embedding(batch.to(self.device))\n            embeddings.append(batch_embeddings.cpu())\n        return torch.cat(embeddings, dim=0)\n\n    def compare(self, item1, item2):\n        emb1, emb2 = self.get_embedding(item1), self.get_embedding(item2)\n        return F.cosine_similarity(emb1, emb2, dim=0).item()\n\n    def get_cache_stats(self):\n        total = self.cache_hits + self.cache_misses\n        return {\"cache_size\": len(self.embedding_cache), \"cache_hits\": self.cache_hits,\n                \"cache_misses\": self.cache_misses, \"hit_rate\": self.cache_hits / total if total > 0 else 0}\n\n# Usage example (with placeholder model)\nimport torch.nn as nn\nclass DummyModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.net = nn.Linear(64, 128)\n    def get_embedding(self, x):\n        return F.normalize(self.net(x), dim=-1)\n\nmodel = DummyModel()\nservice = SiameseEmbeddingService(model, cache_size=1000, device=\"cpu\")\nitem = torch.randn(1, 64)\nemb = service.get_embedding(item)\nprint(f\"Embedding shape: {emb.shape}, Cache stats: {service.get_cache_stats()}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nEmbedding shape: torch.Size([1, 128]), Cache stats: {'cache_size': 1, 'cache_hits': 0, 'cache_misses': 1, 'hit_rate': 0.0}\n```\n:::\n:::\n\n\n### Pattern 2: Approximate Nearest Neighbor Integration\n\nFor billion-scale similarity search, integrate with ANN indexes:\n\n::: {#0ae3657d .cell execution_count=10}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show Siamese ANN service with FAISS\"}\nimport torch.nn.functional as F\n\nclass SiameseANNService:\n    \"\"\"Siamese network integrated with FAISS for sub-millisecond similarity search\"\"\"\n\n    def __init__(self, siamese_service, embedding_dim=512):\n        self.siamese_service = siamese_service\n        self.embedding_dim = embedding_dim\n        try:\n            import faiss\n            self.index = faiss.IndexFlatIP(embedding_dim)  # Inner product for cosine similarity\n        except ImportError:\n            print(\"FAISS not installed. Install with: pip install faiss-cpu\")\n            self.index = None\n        self.id_to_index = {}\n        self.index_to_id = {}\n\n    def add_items(self, item_ids, items):\n        if self.index is None:\n            raise RuntimeError(\"FAISS not available\")\n        embeddings = self.siamese_service.get_embeddings_batch(items)\n        embeddings = F.normalize(embeddings, p=2, dim=1).cpu().numpy()\n        start_idx = self.index.ntotal\n        self.index.add(embeddings)\n        for i, item_id in enumerate(item_ids):\n            idx = start_idx + i\n            self.id_to_index[item_id] = idx\n            self.index_to_id[idx] = item_id\n\n    def search(self, query, top_k=10):\n        if self.index is None:\n            raise RuntimeError(\"FAISS not available\")\n        query_embedding = self.siamese_service.get_embedding(query)\n        query_embedding = F.normalize(query_embedding, p=2, dim=1).cpu().numpy()\n        similarities, indices = self.index.search(query_embedding, top_k)\n        return [(self.index_to_id[idx], float(sim)) for sim, idx in zip(similarities[0], indices[0]) if idx in self.index_to_id]\n\n    def get_statistics(self):\n        return {\"total_items\": self.index.ntotal if self.index else 0, \"embedding_dim\": self.embedding_dim}\n\n# Usage note: Requires FAISS and a trained SiameseEmbeddingService\nprint(\"SiameseANNService: Sub-millisecond search across billions of items\")\nprint(\"Features: FAISS integration, cosine similarity via normalized inner product\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSiameseANNService: Sub-millisecond search across billions of items\nFeatures: FAISS integration, cosine similarity via normalized inner product\n```\n:::\n:::\n\n\n### Pattern 3: Multi-Stage Verification Pipeline\n\nFor high-precision applications (fraud, compliance), use multi-stage verification:\n\n```python\nclass MultiStageVerificationPipeline:\n    \"\"\"\n    Multi-stage verification using Siamese networks\n\n    Stage 1: Fast filtering with loose threshold\n    Stage 2: Detailed verification with strict threshold\n    Stage 3: Human review for borderline cases\n\n    Reduces compute cost while maintaining high accuracy.\n    \"\"\"\n\n    def __init__(\n        self,\n        siamese_service,\n        stage1_threshold=0.7,  # Recall-optimized\n        stage2_threshold=0.9,  # Precision-optimized\n        use_ann=True\n    ):\n        self.siamese_service = siamese_service\n        self.stage1_threshold = stage1_threshold\n        self.stage2_threshold = stage2_threshold\n\n        if use_ann:\n            self.ann_service = SiameseANNService(\n                siamese_service,\n                embedding_dim=512\n            )\n        else:\n            self.ann_service = None\n\n        self.stage1_candidates = 0\n        self.stage2_matches = 0\n        self.human_review_cases = 0\n\n    def verify(self, query, candidate_pool=None, candidate_ids=None):\n        \"\"\"\n        Multi-stage verification\n\n        Args:\n            query: Item to verify\n            candidate_pool: Pool of candidates to check against\n                          (or None to use ANN search)\n            candidate_ids: IDs for candidates (if using candidate_pool)\n\n        Returns:\n            Dict with:\n            - matched: Boolean or 'needs_review'\n            - match_id: ID of matched item (if any)\n            - confidence: Similarity score\n            - stage: Which stage made the decision\n        \"\"\"\n\n        # Stage 1: Fast filtering\n        if self.ann_service is not None and candidate_pool is None:\n            # Use ANN search for fast filtering\n            stage1_results = self.ann_service.search(query, top_k=100)\n            stage1_candidates = [\n                (item_id, sim) for item_id, sim in stage1_results\n                if sim >= self.stage1_threshold\n            ]\n        else:\n            # Linear search through candidate pool\n            if candidate_pool is None:\n                raise ValueError(\"Must provide candidate_pool or use ANN\")\n\n            query_embedding = self.siamese_service.get_embedding(query)\n            candidate_embeddings = self.siamese_service.get_embeddings_batch(\n                candidate_pool\n            )\n\n            similarities = F.cosine_similarity(\n                query_embedding.unsqueeze(0),\n                candidate_embeddings,\n                dim=1\n            )\n\n            stage1_candidates = [\n                (candidate_ids[i], sim.item())\n                for i, sim in enumerate(similarities)\n                if sim.item() >= self.stage1_threshold\n            ]\n\n        self.stage1_candidates += len(stage1_candidates)\n\n        if len(stage1_candidates) == 0:\n            return {\n                'matched': False,\n                'match_id': None,\n                'confidence': 0.0,\n                'stage': 1\n            }\n\n        # Stage 2: Detailed verification\n        # For production, this might involve:\n        # - More expensive model\n        # - Feature-level comparison\n        # - Additional business logic\n\n        best_match = max(stage1_candidates, key=lambda x: x[1])\n        match_id, similarity = best_match\n\n        if similarity >= self.stage2_threshold:\n            # High confidence match\n            self.stage2_matches += 1\n            return {\n                'matched': True,\n                'match_id': match_id,\n                'confidence': similarity,\n                'stage': 2\n            }\n        else:\n            # Borderline case - needs human review\n            self.human_review_cases += 1\n            return {\n                'matched': 'needs_review',\n                'match_id': match_id,\n                'confidence': similarity,\n                'stage': 2,\n                'review_reason': 'confidence_below_threshold'\n            }\n\n    def get_statistics(self):\n        \"\"\"Get pipeline statistics\"\"\"\n        return {\n            'stage1_candidates': self.stage1_candidates,\n            'stage2_matches': self.stage2_matches,\n            'human_review_cases': self.human_review_cases,\n            'human_review_rate': self.human_review_cases / max(self.stage1_candidates, 1)\n        }\n```\n\n:::{.callout-tip}\n## Production Deployment Checklist\n\n**Before deploying Siamese networks to production**:\n\n- [ ] Model performance validated on production-like data\n- [ ] Thresholds calibrated using business metrics\n- [ ] Embedding cache sized appropriately (monitor hit rate > 70%)\n- [ ] ANN index configured for scale (test with 10x expected load)\n- [ ] Monitoring dashboards for similarity distributions\n- [ ] Alerting on performance degradation (precision/recall < thresholds)\n- [ ] A/B testing framework for model updates\n- [ ] Rollback plan for model failures\n- [ ] Load testing at peak + 50% capacity\n- [ ] Cost optimization: GPU utilization > 80%\n\n**Ongoing maintenance**:\n\n- Re-calibrate thresholds quarterly\n- Retrain on recent data every 3-6 months\n- Monitor for data drift (distributional shifts)\n- Collect hard negative examples for continuous improvement\n:::\n\n## Key Takeaways\n\n- **Siamese networks learn similarity rather than classification**, enabling few-shot learning, verification tasks, and open-set recognition without retraining.\n\n- **Triplet loss with hard negative mining** provides better gradients than contrastive loss for most enterprise applications. Use semi-hard mining for stable training.\n\n- **One-shot learning enables immediate adaptation to new categories** from single examples—critical for fraud detection, rare defects, and rapidly evolving threats.\n\n- **Threshold calibration is not optional**. Use validation data to calibrate thresholds based on business metrics (precision/recall) or costs (FP/FN costs). Re-calibrate quarterly.\n\n- **Production deployment requires caching and ANN integration** to achieve sub-millisecond similarity search at billion-scale. Multi-stage pipelines balance cost and accuracy.\n\n- **Monitor similarity distributions in production**. Shifts indicate data drift or model degradation. Alert when mean similarity changes > 10% from baseline.\n\n## Looking Ahead\n\nIn @sec-self-supervised-learning, we expand beyond supervised and Siamese approaches to self-supervised learning—techniques that leverage the structure of unlabeled data itself to train powerful embeddings. We'll explore masked language modeling, vision transformers, and multi-modal self-supervision strategies that enable learning from trillions of unlabeled examples across text, images, time-series, and more.\n\n## Further Reading\n\n- Bromley, J., et al. (1993). \"Signature Verification using a Siamese Time Delay Neural Network.\" NIPS.\n- Schroff, F., Kalenichenko, D., & Philbin, J. (2015). \"FaceNet: A Unified Embedding for Face Recognition and Clustering.\" CVPR.\n- Snell, J., Swersky, K., & Zemel, R. (2017). \"Prototypical Networks for Few-shot Learning.\" NeurIPS.\n- Koch, G., Zemel, R., & Salakhutdinov, R. (2015). \"Siamese Neural Networks for One-shot Image Recognition.\" ICML Workshop.\n- Wang, J., et al. (2017). \"Deep Metric Learning with Angular Loss.\" ICCV.\n- Hermans, A., Beyer, L., & Leibe, B. (2017). \"In Defense of the Triplet Loss for Person Re-Identification.\" arXiv:1703.07737.\n\n",
    "supporting": [
      "ch10_siamese_networks_files/figure-epub"
    ],
    "filters": [],
    "engineDependencies": {
      "jupyter": [
        {
          "jsWidgets": false,
          "jupyterWidgets": false,
          "htmlLibraries": []
        }
      ]
    }
  }
}