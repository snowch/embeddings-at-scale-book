{
  "hash": "81faefb2b48b9e5e493c0876b8991b0f",
  "result": {
    "engine": "jupyter",
    "markdown": "# Performance Optimization Mastery {#sec-performance-optimization}\n\n:::{.callout-note}\n## Chapter Overview\nPerformance optimization—from sub-50ms query response to efficient resource utilization to cost-effective scaling—determines whether embedding systems deliver value or disappoint users. This chapter covers performance optimization mastery: query optimization strategies that reduce latency from 500ms to <50ms through intelligent query planning, result caching, and parallel execution, index tuning for specific workloads that adapts HNSW, IVF, and LSH parameters to access patterns and enables 10-100× throughput improvements, caching strategies for hot embeddings that reduce database load by 70-90% through multi-tier caching and intelligent invalidation, compression techniques for storage efficiency that reduce costs by 75%+ while maintaining 95%+ accuracy through quantization and dimensionality reduction, and network optimization for distributed queries that minimizes cross-datacenter latency and bandwidth through intelligent sharding, replication, and query routing. These techniques transform embedding systems from expensive, slow prototypes to production systems that serve millions of queries per second at pennies per million queries.\n:::\n\nAfter transforming media and entertainment (@sec-media-entertainment), **performance optimization becomes critical for production deployment**. Early embedding prototypes often work beautifully at small scale—1M embeddings, 100 queries per minute, single datacenter—but fail catastrophically at production scale: 256+ trillion embeddings, 1M+ queries per second, global distribution. **Performance optimization** transforms research prototypes into production systems through systematic query optimization (reduce unnecessary computation), index tuning (adapt data structures to workload patterns), caching (avoid repeated work), compression (reduce storage and bandwidth), and network optimization (minimize latency and maximize throughput)—enabling 100-1000× cost reduction while maintaining or improving quality.\n\n## Query Optimization Strategies\n\nVector similarity search—finding k nearest neighbors in high-dimensional space—appears deceptively simple but hides tremendous complexity. Naive approaches (scan all embeddings, compute all similarities, sort, return top-k) work at small scale but collapse at production scale. **Query optimization strategies** transform expensive full scans into intelligent searches that examine <0.01% of embeddings while maintaining 95%+ recall, reducing latency from 500ms to <50ms and enabling throughput scaling from 100 to 100,000+ queries per second.\n\n### The Query Performance Challenge\n\nProduction vector queries face limitations:\n\n- **Dimensionality curse**: Euclidean distance loses meaning in 768+ dimensions\n- **Scale explosion**: 256 trillion embeddings × 768 dimensions = 200+ petabytes\n- **Latency requirements**: Users expect <50ms response, recommendation systems need <10ms\n- **Throughput demands**: 1M+ queries per second during peak hours\n- **Accuracy requirements**: <95% recall is unacceptable for many applications\n- **Cost constraints**: Full scans cost $1000+ per million queries, unsustainable\n- **Dynamic data**: New embeddings arrive continuously, requiring real-time indexing\n\n**Optimization approach**: Multi-stage retrieval that progressively narrows candidates, uses approximate methods for initial filtering, exact methods for final ranking, leverages index structures (HNSW, IVF, product quantization), applies query-specific optimizations (early termination, result reranking, batch processing), and adapts strategies based on query characteristics (k value, filtering constraints, quality requirements).\n\n::: {.cell execution_count=1}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show multi-stage vector retrieval architecture\"}\nfrom dataclasses import dataclass, field\nfrom typing import Optional, List\nfrom enum import Enum\nimport torch\nimport torch.nn as nn\n\nclass RetrievalStage(Enum):\n    COARSE = \"coarse\"\n    FINE = \"fine\"\n    RERANK = \"rerank\"\n\n@dataclass\nclass QueryConfig:\n    k: int = 10\n    ef_search: int = 50\n    n_probe: int = 10\n    use_cache: bool = True\n    max_latency_ms: float = 50.0\n\nclass MultiStageRetriever(nn.Module):\n    \"\"\"Multi-stage vector retrieval with progressive refinement.\"\"\"\n    def __init__(self, config: QueryConfig, embedding_dim: int = 768):\n        super().__init__()\n        self.config = config\n        self.coarse_projector = nn.Linear(embedding_dim, 128)\n        self.reranker = nn.Linear(embedding_dim * 2, 1)\n\n    def coarse_search(self, query: torch.Tensor, candidates: torch.Tensor) -> torch.Tensor:\n        q_proj = self.coarse_projector(query)\n        c_proj = self.coarse_projector(candidates)\n        scores = torch.matmul(q_proj, c_proj.T)\n        return scores.topk(self.config.k * 10, dim=-1).indices\n\n    def rerank(self, query: torch.Tensor, candidates: torch.Tensor) -> torch.Tensor:\n        query_exp = query.unsqueeze(1).expand(-1, candidates.size(1), -1)\n        combined = torch.cat([query_exp, candidates], dim=-1)\n        scores = self.reranker(combined).squeeze(-1)\n        return scores.topk(self.config.k, dim=-1)\n\n# Usage example\nconfig = QueryConfig(k=10, ef_search=100)\nretriever = MultiStageRetriever(config)\nquery = torch.randn(1, 768)\ncandidates = torch.randn(1000, 768)\ncoarse_idx = retriever.coarse_search(query, candidates)\nprint(f\"Coarse candidates: {coarse_idx.shape}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCoarse candidates: torch.Size([1, 100])\n```\n:::\n:::\n\n\n### Query Planning and Optimization\n\n**Query planning** analyzes query characteristics and selects optimal execution strategy:\n\n\n:::{.callout-tip}\n## Query Planning Heuristics\n\n**Small k (< 10)**:\n\n- Use HNSW for best precision\n- ef_search = max(k × 2, 50)\n- Single-stage retrieval sufficient\n\n**Medium k (10-100)**:\n\n- Use IVF-PQ for efficiency\n- n_probe = 10-50 clusters\n- Two-stage: coarse → refine\n\n**Large k (> 100)**:\n\n- Hybrid approach\n- IVF coarse → HNSW refine → exact rerank\n- Consider distributed execution\n\n**High-selectivity filters (> 90% filtered)**:\n\n- Filter first, search filtered subset\n- Traditional database index + vector scan\n- May be faster than ANN with post-filtering\n\n**Low-selectivity filters (< 50% filtered)**:\n\n- Search first, filter results\n- Standard ANN + post-filter\n- Less overhead than pre-filtering\n\n**Time-sensitive queries (< 10ms budget)**:\n\n- Use fastest index (HNSW)\n- Accept slightly lower recall\n- Enable aggressive caching\n- Consider approximate distances (PQ, LSH)\n\n**Batch queries**:\n\n- Group by similarity (same filters, similar k)\n- Share index structure reads\n- Amortize query planning overhead\n- GPU batch processing for large batches\n:::\n\n**Performance characteristics:**\n\n| Strategy | Latency (p50) | Recall | Throughput | Cost/1M queries |\n|----------|---------------|--------|------------|-----------------|\n| Exact scan | 500ms | 100% | 10 QPS | $50.00 |\n| HNSW | 15ms | 97% | 5,000 QPS | $0.30 |\n| IVF-PQ | 8ms | 94% | 10,000 QPS | $0.15 |\n| Hybrid | 25ms | 98% | 3,000 QPS | $0.45 |\n| Filtered scan | 100ms | 100% | 100 QPS | $5.00 |\n\n## Index Tuning for Specific Workloads\n\nVector indexes—HNSW, IVF, Product Quantization, LSH—have dozens of tuning parameters that dramatically impact performance. Default parameters work reasonably at small scale but fail catastrophically at production scale. **Index tuning for specific workloads** adapts index parameters to access patterns, data characteristics, and hardware constraints, enabling 10-100× throughput improvements while maintaining quality.\n\n### The Index Tuning Challenge\n\nVector indexes face diverse workload characteristics:\n\n- **Query patterns**: Top-10 vs top-1000, single queries vs batches\n- **Data distribution**: Clustered vs uniform, high vs low dimensionality\n- **Update patterns**: Static vs continuously growing, bulk vs streaming\n- **Quality requirements**: 90% recall acceptable vs 99%+ required\n- **Latency constraints**: <5ms for real-time vs <100ms for batch\n- **Hardware**: CPU-only vs GPU-accelerated, RAM vs SSD\n- **Scale**: 1M vectors vs 256 trillion vectors\n\n**Tuning approach**: Measure actual workload characteristics (query distribution, access patterns, quality requirements), benchmark index variants on representative data, optimize index parameters through systematic search or learned tuning, validate on production-like traffic, and continuously monitor and adapt as workload evolves.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show index tuning configuration\"}\nfrom dataclasses import dataclass\nfrom typing import Optional, Dict\nfrom enum import Enum\nimport torch\nimport torch.nn as nn\n\nclass IndexType(Enum):\n    HNSW = \"hnsw\"\n    IVF = \"ivf\"\n    IVF_PQ = \"ivf_pq\"\n    FLAT = \"flat\"\n\n@dataclass\nclass HNSWConfig:\n    M: int = 32\n    ef_construction: int = 200\n    ef_search: int = 100\n\n@dataclass\nclass IVFConfig:\n    n_clusters: int = 1024\n    n_probe: int = 32\n    use_pq: bool = True\n    n_subvectors: int = 8\n\nclass AdaptiveIndexTuner(nn.Module):\n    \"\"\"Learns optimal index parameters for workload patterns.\"\"\"\n    def __init__(self, feature_dim: int = 16, hidden_dim: int = 64):\n        super().__init__()\n        self.workload_encoder = nn.Sequential(\n            nn.Linear(feature_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim)\n        )\n        self.hnsw_head = nn.Linear(hidden_dim, 3)  # M, ef_construction, ef_search\n        self.ivf_head = nn.Linear(hidden_dim, 3)   # n_clusters, n_probe, n_subvectors\n\n    def forward(self, workload_features: torch.Tensor) -> Dict[str, torch.Tensor]:\n        encoded = self.workload_encoder(workload_features)\n        return {\n            \"hnsw_params\": torch.sigmoid(self.hnsw_head(encoded)),\n            \"ivf_params\": torch.sigmoid(self.ivf_head(encoded))\n        }\n\n# Usage example\ntuner = AdaptiveIndexTuner()\nworkload = torch.randn(1, 16)  # Query rate, k distribution, filter selectivity, etc.\nparams = tuner(workload)\nprint(f\"HNSW params: {params['hnsw_params'].shape}, IVF params: {params['ivf_params'].shape}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nHNSW params: torch.Size([1, 3]), IVF params: torch.Size([1, 3])\n```\n:::\n:::\n\n\n### Index-Specific Tuning Guidelines\n\n:::{.callout-tip}\n## HNSW Tuning Guidelines\n\n**For high recall (>98%)**:\n\n- M = 48-64 (more connections)\n- ef_construction = 400-500\n- ef_search = k × 4 to k × 8\n- Expect: 20-40ms p50, 98-99% recall\n\n**For balanced performance (95-98% recall)**:\n\n- M = 24-32\n- ef_construction = 200-300\n- ef_search = k × 2 to k × 4\n- Expect: 10-20ms p50, 95-97% recall\n\n**For speed (<10ms p50)**:\n\n- M = 16-24\n- ef_construction = 100-200\n- ef_search = k × 1.5 to k × 2\n- Expect: 5-10ms p50, 90-95% recall\n\n**Memory optimization**:\n\n- Lower M reduces graph size (proportional savings)\n- Store vectors on SSD, keep graph in RAM\n- Use memory-mapped files for large datasets\n\n**Build-time optimization**:\n\n- Lower ef_construction speeds build (linear)\n- Parallel construction with graph merging\n- Incremental updates for streaming data\n:::\n\n:::{.callout-tip}\n## IVF Tuning Guidelines\n\n**For high recall (>98%)**:\n\n- n_clusters = sqrt(N)\n- n_probe = 5-10% of clusters\n- Expect: 15-30ms p50, 97-99% recall\n\n**For balanced performance (95-98% recall)**:\n\n- n_clusters = 2× sqrt(N)\n- n_probe = 2-5% of clusters\n- Expect: 8-15ms p50, 95-97% recall\n\n**For speed (<10ms p50)**:\n\n- n_clusters = 4× sqrt(N)\n- n_probe = 1-2% of clusters\n- Expect: 3-8ms p50, 90-95% recall\n\n**With Product Quantization**:\n\n- Combine IVF with PQ for 10-20× compression\n- n_subvectors = 8-16 for 768-dim vectors\n- n_bits = 8 for good accuracy, 4 for compression\n- Expect: 50-75% accuracy loss, 5-10× speedup\n\n**GPU acceleration**:\n\n- Batch queries (32-256 per batch)\n- Use GPU IVF-PQ for 10-100× throughput\n- Expect: 0.5-2ms per query on V100 GPU\n:::\n\n\n## Caching Strategies for Hot Embeddings\n\nVector similarity search involves reading embeddings from storage (RAM, SSD, network), computing similarities, and sorting results. At scale, **storage access dominates cost**—reading 100B 768-dimensional vectors requires 300+ TB of data transfer. **Caching strategies** exploit access patterns (20% of embeddings receive 80% of queries) to reduce database load by 70-90% through multi-tier caching, intelligent prefetching, and adaptive eviction policies.\n\n### The Caching Challenge\n\nProduction vector workloads exhibit skewed access patterns:\n\n- **Popularity skew**: 1% of content receives 50%+ of queries (viral videos, trending products)\n- **Temporal locality**: Recent content accessed more frequently than old content\n- **Spatial locality**: Similar queries access similar embeddings (related products, semantic clusters)\n- **Query patterns**: Repeated queries (homepage recommendations), batch processing\n- **Cold starts**: New embeddings with no access history yet\n- **Cache invalidation**: Embeddings updated, requiring cache refresh\n- **Multi-tenancy**: Different users have different access patterns\n\n**Caching approach**: Multi-tier cache hierarchy (L1: CPU cache, L2: RAM, L3: SSD, L4: network), adaptive replacement policies (LRU, LFU, ARC), query-aware prefetching (predict likely next queries), result caching (cache full query results, not just embeddings), and intelligent invalidation (lazy vs eager, version-based, TTL).\n\n::: {.cell execution_count=3}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show multi-tier caching architecture\"}\nfrom dataclasses import dataclass, field\nfrom typing import Optional, Dict, List\nfrom enum import Enum\nimport torch\nimport torch.nn as nn\n\nclass CacheTier(Enum):\n    L1_HOT = \"l1_hot\"      # <0.1ms, 1-10GB\n    L2_WARM = \"l2_warm\"    # <1ms, 10-100GB\n    L3_COLD = \"l3_cold\"    # <5ms, 100GB-1TB\n    STORAGE = \"storage\"    # 10-100ms, PB scale\n\n@dataclass\nclass CacheConfig:\n    l1_size_gb: float = 5.0\n    l2_size_gb: float = 50.0\n    l3_size_gb: float = 500.0\n    ttl_seconds: int = 3600\n    promotion_threshold: int = 3\n\nclass AdaptiveCacheManager(nn.Module):\n    \"\"\"Learns optimal cache placement based on access patterns.\"\"\"\n    def __init__(self, embedding_dim: int = 768, hidden_dim: int = 128):\n        super().__init__()\n        self.access_encoder = nn.LSTM(embedding_dim + 8, hidden_dim, batch_first=True)\n        self.tier_predictor = nn.Linear(hidden_dim, len(CacheTier))\n\n    def predict_tier(self, embedding: torch.Tensor, access_features: torch.Tensor) -> torch.Tensor:\n        combined = torch.cat([embedding, access_features], dim=-1)\n        _, (hidden, _) = self.access_encoder(combined.unsqueeze(1))\n        tier_logits = self.tier_predictor(hidden.squeeze(0))\n        return torch.softmax(tier_logits, dim=-1)\n\n# Usage example\nconfig = CacheConfig(l1_size_gb=10.0)\ncache_manager = AdaptiveCacheManager()\nembedding = torch.randn(1, 768)\naccess_features = torch.randn(1, 8)  # frequency, recency, size, etc.\ntier_probs = cache_manager.predict_tier(embedding, access_features)\nprint(f\"Cache tier probabilities: {tier_probs.shape}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCache tier probabilities: torch.Size([1, 4])\n```\n:::\n:::\n\n\n### Caching Best Practices\n\n:::{.callout-tip}\n## Cache Tier Selection\n\n**L1 cache (1-10GB, <0.1ms)**:\n\n- Most frequently accessed embeddings (top 0.1%)\n- Active query results\n- User session data\n- Real-time recommendations\n\n**L2 cache (10-100GB, <1ms)**:\n\n- Frequently accessed embeddings (top 1-10%)\n- Recent query results\n- Popular content\n- Warm data for active users\n\n**L3 cache (100GB-1TB, <5ms)**:\n\n- Occasionally accessed embeddings (top 10-50%)\n- Historical query results\n- Compressed older data\n- Prefetched candidates\n\n**Storage (PB scale, 10-100ms)**:\n\n- Full dataset\n- Cold embeddings (accessed <1/day)\n- Historical archives\n:::\n\n\n## Compression Techniques for Storage Efficiency\n\nAt 256+ trillion vectors × 768 dimensions × 4 bytes (float32), embedding storage requires 768+ petabytes—costing $15M+ annually at $0.02/GB/month. **Compression techniques** reduce storage by 75-95% while maintaining 95%+ accuracy through quantization, dimensionality reduction, and intelligent encoding, transforming unaffordable PB-scale systems into practical TB-scale deployments.\n\n### The Storage Cost Challenge\n\nStorage costs dominate at scale:\n\n- **Raw storage**: 256T vectors × 768 dims × 4 bytes = 768 PB\n- **Cloud storage**: $0.02/GB/month = $15M/month = $180M/year\n- **Bandwidth**: Reading 1% daily = 7.6 PB transferred = $100K+/day\n- **Memory limits**: Cannot fit entire dataset in RAM\n- **Backup/replication**: 3× redundancy → 2.3 EB total\n- **Network transfer**: Cross-region replication costs\n\n**Compression approach**: Product quantization (4-32× compression, <5% accuracy loss), dimensionality reduction via PCA/random projection (2-4× compression), scalar quantization (2-4× compression, minimal accuracy loss), learned compression (neural network encoders), and sparse embeddings (exploit natural sparsity in high-dimensional spaces).\n\n::: {.cell execution_count=4}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show compression techniques architecture\"}\nfrom dataclasses import dataclass\nfrom typing import Optional, Tuple\nfrom enum import Enum\nimport torch\nimport torch.nn as nn\n\nclass CompressionMethod(Enum):\n    SCALAR_QUANT = \"scalar_quantization\"\n    PRODUCT_QUANT = \"product_quantization\"\n    BINARY_QUANT = \"binary_quantization\"\n    PCA = \"pca_reduction\"\n\n@dataclass\nclass CompressionConfig:\n    method: CompressionMethod = CompressionMethod.SCALAR_QUANT\n    target_bits: int = 8\n    n_subvectors: int = 8\n    target_dim: Optional[int] = None\n\nclass ProductQuantizer(nn.Module):\n    \"\"\"Product quantization for high compression with searchable codes.\"\"\"\n    def __init__(self, dim: int = 768, n_subvectors: int = 8, n_centroids: int = 256):\n        super().__init__()\n        self.n_subvectors = n_subvectors\n        self.subvector_dim = dim // n_subvectors\n        self.codebooks = nn.Parameter(torch.randn(n_subvectors, n_centroids, self.subvector_dim))\n\n    def encode(self, vectors: torch.Tensor) -> torch.Tensor:\n        batch_size = vectors.size(0)\n        codes = []\n        for i in range(self.n_subvectors):\n            subvector = vectors[:, i*self.subvector_dim:(i+1)*self.subvector_dim]\n            distances = torch.cdist(subvector, self.codebooks[i])\n            codes.append(distances.argmin(dim=-1))\n        return torch.stack(codes, dim=-1)\n\n    def decode(self, codes: torch.Tensor) -> torch.Tensor:\n        reconstructed = []\n        for i in range(self.n_subvectors):\n            reconstructed.append(self.codebooks[i, codes[:, i]])\n        return torch.cat(reconstructed, dim=-1)\n\n# Usage example\npq = ProductQuantizer(dim=768, n_subvectors=8, n_centroids=256)\nvectors = torch.randn(100, 768)\ncodes = pq.encode(vectors)\nreconstructed = pq.decode(codes)\nprint(f\"Original: {vectors.shape}, Codes: {codes.shape} (uint8), Reconstructed: {reconstructed.shape}\")\nprint(f\"Compression ratio: {768 * 4 / 8:.1f}x\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nOriginal: torch.Size([100, 768]), Codes: torch.Size([100, 8]) (uint8), Reconstructed: torch.Size([100, 768])\nCompression ratio: 384.0x\n```\n:::\n:::\n\n\n### Compression Method Selection\n\n:::{.callout-tip}\n## Choosing Compression Method\n\n**For maximum compression (10-32×)**:\n\n- Binary quantization: 32× but 15-25% accuracy loss\n- Product quantization: 8-32× with 3-10% accuracy loss\n- Use when: Storage cost critical, accuracy tolerance high\n\n**For balanced compression (4-8×)**:\n\n- Scalar quantization (int8): 4× with <2% accuracy loss\n- PQ with 8 subvectors: 8× with 3-5% accuracy loss\n- Use when: Need both compression and accuracy\n\n**For minimal accuracy loss (<2%)**:\n\n- Scalar quantization (int8): 4× compression\n- Dimensionality reduction (768→384): 2× compression\n- PQ with 4 subvectors: 4× with <2% loss\n- Use when: Accuracy is critical\n\n**For searchable compression**:\n\n- Product quantization: Can search without decompression\n- Binary quantization: Very fast Hamming distance\n- Use when: Query speed matters more than storage\n\n**Combined approaches**:\n\n- PCA (768→384) + PQ (8 subvectors) = 16× compression\n- PCA + int8 quantization = 8× compression\n- Achieves better compression/accuracy trade-off\n:::\n\n\n## Network Optimization for Distributed Queries\n\nGlobal-scale embedding systems distribute across datacenters for latency, reliability, and regulatory compliance. **Network optimization** minimizes cross-datacenter latency (50-200ms) and bandwidth costs ($0.01-0.12/GB) through intelligent sharding, query routing, and replication strategies, enabling sub-100ms global query response while reducing bandwidth costs by 80%+.\n\n### The Distributed Query Challenge\n\nGlobal embedding systems face network constraints:\n\n- **Cross-datacenter latency**: 50-200ms (US-EU), 150-300ms (US-Asia)\n- **Bandwidth costs**: $0.01-0.12/GB between regions\n- **Query routing**: Which datacenter serves which query?\n- **Data sharding**: How to partition 256T embeddings?\n- **Replication**: Which data to replicate where?\n- **Consistency**: Keep replicas synchronized\n- **Failover**: Handle datacenter outages\n- **Regulatory**: GDPR, data residency requirements\n\n**Optimization approach**: Geo-distributed query routing (send queries to nearest datacenter with relevant data), intelligent sharding (co-locate frequently accessed embeddings), selective replication (hot data everywhere, cold data sharded), query aggregation (combine multiple queries to amortize latency), and compression (reduce bandwidth for cross-region transfers).\n\n::: {.cell execution_count=5}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show distributed query routing architecture\"}\nfrom dataclasses import dataclass, field\nfrom typing import Optional, Dict, List\nfrom enum import Enum\nimport torch\nimport torch.nn as nn\n\nclass ShardingStrategy(Enum):\n    HASH = \"hash\"\n    GEOGRAPHIC = \"geographic\"\n    SEMANTIC = \"semantic\"\n    HYBRID = \"hybrid\"\n\n@dataclass\nclass DatacenterConfig:\n    name: str\n    region: str\n    latency_matrix: Dict[str, float] = field(default_factory=dict)\n    capacity_gb: float = 1000.0\n\nclass GeoDistributedRouter(nn.Module):\n    \"\"\"Routes queries to optimal datacenter based on latency and data locality.\"\"\"\n    def __init__(self, n_datacenters: int = 8, embedding_dim: int = 768):\n        super().__init__()\n        self.query_encoder = nn.Linear(embedding_dim, 128)\n        self.datacenter_embeddings = nn.Parameter(torch.randn(n_datacenters, 128))\n        self.latency_predictor = nn.Linear(128 + 128, 1)\n\n    def route_query(self, query: torch.Tensor, user_location: torch.Tensor) -> torch.Tensor:\n        q_encoded = self.query_encoder(query)  # [batch, 128]\n        scores = torch.matmul(q_encoded, self.datacenter_embeddings.T)  # [batch, n_dc]\n        # Expand tensors for combining: [batch, n_dc, 128] each\n        q_expanded = q_encoded.unsqueeze(1).expand(-1, self.datacenter_embeddings.size(0), -1)\n        dc_expanded = self.datacenter_embeddings.unsqueeze(0).expand(query.size(0), -1, -1)\n        combined = torch.cat([q_expanded, dc_expanded], dim=-1)  # [batch, n_dc, 256]\n        latencies = self.latency_predictor(combined).squeeze(-1)  # [batch, n_dc]\n        routing_scores = scores - 0.1 * latencies\n        return torch.softmax(routing_scores, dim=-1)\n\n# Usage example\nrouter = GeoDistributedRouter(n_datacenters=5)\nquery = torch.randn(1, 768)\nuser_loc = torch.randn(1, 3)  # lat, lon, region_id\nrouting_probs = router.route_query(query, user_loc)\nprint(f\"Routing probabilities across 5 datacenters: {routing_probs.shape}\")\nprint(f\"Selected datacenter: {routing_probs.argmax().item()}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRouting probabilities across 5 datacenters: torch.Size([1, 5])\nSelected datacenter: 1\n```\n:::\n:::\n\n\n### Network Optimization Best Practices\n\n:::{.callout-tip}\n## Sharding Strategies\n\n**Hash-based sharding**:\n\n- Use: Uniform access patterns\n- Pros: Even distribution, simple\n- Cons: Can't colocate related embeddings\n\n**Geographic sharding**:\n\n- Use: Regional user bases (e.g., GDPR compliance)\n- Pros: Low latency, regulatory compliance\n- Cons: Uneven load distribution\n\n**Semantic sharding**:\n\n- Use: Queries access related embeddings\n- Pros: Locality, fewer cross-shard queries\n- Cons: Complex, requires clustering\n\n**Hybrid sharding**:\n\n- Hot data: Replicate globally\n- Warm data: Geographic sharding\n- Cold data: Hash-based sharding\n- Best of all approaches\n:::\n\n:::{.callout-tip}\n## Replication Policies\n\n**Full replication**:\n\n- Replicate all data to all datacenters\n- Use: <10TB datasets, low update rate\n- Pros: Lowest latency, simple\n- Cons: High storage/bandwidth cost\n\n**Hot data replication**:\n\n- Replicate top 1-10% globally\n- Shard remaining data\n- Use: Skewed access patterns (typical)\n- Pros: 80%+ queries local, 90%+ cost savings\n\n**On-demand replication**:\n\n- Replicate when access rate exceeds threshold\n- Gradually evict cold data\n- Use: Changing access patterns\n- Pros: Adaptive, efficient\n:::\n\n## Key Takeaways\n\n- **Multi-stage query optimization reduces latency 10-50× through intelligent filtering**: Query analysis selects optimal execution strategy (HNSW for k<10, IVF-PQ for k<100, hybrid for complex queries), multi-stage retrieval progressively narrows candidates from 100M+ to k final results in <50ms, parallel execution distributes work across cores achieving 10,000+ QPS per node, and adaptive caching captures 70-90% of queries reducing database load proportionally while maintaining <1ms cache hit latency\n\n- **Index tuning adapts data structures to workload patterns enabling 10-100× throughput improvements**: HNSW tuning (M=16-64, ef_search=k×1.5-4×) optimizes recall/latency trade-off achieving 95-98% recall at 10-40ms p50, IVF tuning (n_clusters=sqrt(N) to 4×sqrt(N), n_probe=1-10% of clusters) enables sub-10ms queries at billion-vector scale, and workload-specific configuration considers query distribution, filter selectivity, and quality requirements to select optimal index type and parameters\n\n- **Multi-tier caching exploits access skew reducing storage costs by 70-90%**: L1 hot cache (1-10GB, <0.1ms) serves top 0.1% of embeddings receiving 50%+ of queries, L2 warm cache (10-100GB, <1ms) handles frequent access patterns, L3 cold cache (100GB-1TB, <5ms) with compression provides cost-effective buffer, intelligent promotion/demotion based on access frequency maintains optimal tier assignments, and query result caching avoids repeated similarity computations for identical or similar queries\n\n- **Compression reduces storage costs 75-95% while maintaining 95%+ accuracy through quantization**: Product quantization achieves 8-32× compression with 3-10% accuracy loss by splitting vectors into subvectors and clustering each independently, scalar quantization (float32→int8) provides 4× compression with <2% loss through per-dimension linear mapping, binary quantization enables 32× compression for LSH-style applications, dimensionality reduction (PCA 768→256) provides 3× compression with 1-5% variance loss, and combined approaches (PCA + PQ) achieve 16× compression with <5% accuracy degradation\n\n- **Network optimization for distributed queries minimizes latency and bandwidth costs**: Geo-distributed query routing directs queries to nearest datacenter with required data achieving sub-100ms global latency, intelligent sharding strategies (hash, geographic, semantic, hybrid) balance load distribution with data locality, selective hot data replication places frequently accessed embeddings globally (top 1-10%) while sharding cold data reduces replication costs 80%+, query batching amortizes network latency across multiple queries, and compression reduces cross-region bandwidth by 4-8× lowering egress costs proportionally\n\n- **Performance optimization is system-wide requiring coordinated query, index, cache, compression, and network strategies**: No single optimization achieves production performance—query optimization provides 10× improvement, index tuning 5-10×, caching 3-5×, compression 4-8× on storage, and network optimization 2-5× on distributed latency, requiring coordinated deployment where caching benefits query optimization, compression enables larger caches, and optimized queries reduce replication bandwidth\n\n- **Continuous monitoring and adaptive tuning maintain optimal performance as workloads evolve**: Query patterns shift (viral content, trending topics, seasonal effects), data distributions change (new embeddings, concept drift), hardware characteristics vary (CPU/GPU availability, network conditions), and cost structures fluctuate (storage/bandwidth pricing), necessitating automated performance monitoring, workload analysis, A/B testing of optimization strategies, and gradual rollout of configuration changes with rollback capabilities\n\n## Looking Ahead\n\n@sec-security-privacy addresses critical security and privacy considerations: embedding encryption and secure computation for protecting sensitive embeddings, privacy-preserving similarity search enabling queries without revealing query vectors or database contents, differential privacy for embeddings providing formal privacy guarantees, access control and audit trails for regulatory compliance, and GDPR and data sovereignty compliance for global deployments.\n\n## Further Reading\n\n### Query Optimization\n- Malkov, Yury A., and Dmitry A. Yashunin (2018). \"Efficient and Robust Approximate Nearest Neighbor Search Using Hierarchical Navigable Small World Graphs.\" IEEE Transactions on Pattern Analysis and Machine Intelligence.\n- Jégou, Hervé, Matthijs Douze, and Cordelia Schmid (2011). \"Product Quantization for Nearest Neighbor Search.\" IEEE Transactions on Pattern Analysis and Machine Intelligence.\n- Babenko, Artem, and Victor Lempitsky (2014). \"The Inverted Multi-Index.\" IEEE Conference on Computer Vision and Pattern Recognition.\n- Guo, Ruiqi, et al. (2020). \"Accelerating Large-Scale Inference with Anisotropic Vector Quantization.\" International Conference on Machine Learning.\n\n### Index Structures and Tuning\n- Aumüller, Martin, et al. (2020). \"ANN-Benchmarks: A Benchmarking Tool for Approximate Nearest Neighbor Algorithms.\" Information Systems.\n- Douze, Matthijs, et al. (2024). \"The Faiss Library.\" arXiv:2401.08281.\n- Johnson, Jeff, Matthijs Douze, and Hervé Jégou (2019). \"Billion-Scale Similarity Search with GPUs.\" IEEE Transactions on Big Data.\n- Andoni, Alexandr, and Piotr Indyk (2008). \"Near-Optimal Hashing Algorithms for Approximate Nearest Neighbor in High Dimensions.\" Communications of the ACM.\n\n### Caching Systems\n- Berger, Daniel S., et al. (2018). \"Adaptive Software-Defined Storage.\" ACM Transactions on Storage.\n- Cidon, Asaf, et al. (2016). \"Dynacache: Dynamic Cloud Caching.\" Usenix Conference on File and Storage Technologies.\n- Waldspurger, Carl A., et al. (2015). \"Efficient MRC Construction with SHARDS.\" Usenix Conference on File and Storage Technologies.\n- Beckmann, Nathan, and Daniel Sanchez (2017). \"Maximizing Cache Performance Under Uncertainty.\" IEEE International Symposium on High Performance Computer Architecture.\n\n### Compression Techniques\n- Jégou, Hervé, Matthijs Douze, and Cordelia Schmid (2011). \"Product Quantization for Nearest Neighbor Search.\" IEEE Transactions on Pattern Analysis and Machine Intelligence.\n- Ge, Tiezheng, et al. (2014). \"Optimized Product Quantization.\" IEEE Transactions on Pattern Analysis and Machine Intelligence.\n- Gong, Yunchao, and Svetlana Lazebnik (2011). \"Iterative Quantization: A Procrustean Approach to Learning Binary Codes.\" IEEE Conference on Computer Vision and Pattern Recognition.\n- Martinez, Julieta, et al. (2016). \"Revisiting Additive Quantization.\" European Conference on Computer Vision.\n- Norouzi, Mohammad, and David J. Fleet (2013). \"Cartesian K-Means.\" IEEE Conference on Computer Vision and Pattern Recognition.\n\n### Distributed Systems and Networking\n- Kraska, Tim, et al. (2018). \"The Case for Learned Index Structures.\" ACM SIGMOD International Conference on Management of Data.\n- Recht, Benjamin, et al. (2011). \"Hogwild: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent.\" Advances in Neural Information Processing Systems.\n- Dean, Jeffrey, and Luiz André Barroso (2013). \"The Tail at Scale.\" Communications of the ACM.\n- Zaharia, Matei, et al. (2012). \"Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing.\" Usenix Symposium on Networked Systems Design and Implementation.\n\n### Performance Optimization and Benchmarking\n- Guo, Ruiqi, et al. (2020). \"Accelerating Large-Scale Inference with Anisotropic Vector Quantization.\" International Conference on Machine Learning.\n- Johnson, Jeff, Matthijs Douze, and Hervé Jégou (2019). \"Billion-Scale Similarity Search with GPUs.\" IEEE Transactions on Big Data.\n- Li, Wen, et al. (2020). \"Approximate Nearest Neighbor Search on High Dimensional Data—Experiments, Analyses, and Improvement.\" IEEE Transactions on Knowledge and Data Engineering.\n- Aumüller, Martin, et al. (2020). \"ANN-Benchmarks: A Benchmarking Tool for Approximate Nearest Neighbor Algorithms.\" Information Systems.\n\n### Hardware Acceleration\n- Jia, Zhe, et al. (2019). \"Dissecting the Graphcore IPU Architecture via Microbenchmarking.\" arXiv:1912.03413.\n- Chen, Tianqi, et al. (2018). \"TVM: An Automated End-to-End Optimizing Compiler for Deep Learning.\" USENIX Symposium on Operating Systems Design and Implementation.\n- Rhu, Minsoo, et al. (2016). \"vDNN: Virtualized Deep Neural Networks for Scalable, Memory-Efficient Neural Network Design.\" IEEE/ACM International Symposium on Microarchitecture.\n- Jouppi, Norman P., et al. (2017). \"In-Datacenter Performance Analysis of a Tensor Processing Unit.\" ACM/IEEE International Symposium on Computer Architecture.\n\n",
    "supporting": [
      "ch32_performance_optimization_files/figure-pdf"
    ],
    "filters": []
  }
}