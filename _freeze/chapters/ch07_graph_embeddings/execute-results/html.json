{
  "hash": "250c5599dc65f06f6ad4480863446043",
  "result": {
    "engine": "jupyter",
    "markdown": "# Graph Embeddings {#sec-graph-embeddings}\n\n::: callout-note\n## Chapter Overview\n\nThis chapter covers graph embeddings—representations that convert nodes, edges, and subgraphs into vectors capturing structural relationships. Unlike text or images where data is sequential or grid-like, graphs have arbitrary connectivity. We explore how graph embeddings learn representations where connected nodes (or nodes with similar neighborhoods) have similar vectors.\n:::\n\n## What Are Graph Embeddings?\n\nGraph embeddings convert nodes, edges, and subgraphs into vectors that capture structural relationships. A social network node might have 3 friends or 3,000—graph embeddings handle this arbitrary connectivity by learning that nodes with similar neighborhoods should have similar vectors.\n\nThe key insight: **a node's meaning comes from its connections**. In a social network, people with similar friends likely have similar interests. In a molecule, atoms with similar bonding patterns have similar chemical properties.\n\n## Visualizing Graph Embeddings\n\n::: {#cell-fig-graph-embedding .cell execution_count=1}\n\n::: {.cell-output .cell-output-display}\n![Graph embeddings: nodes in the same community (densely connected) map to nearby points in embedding space.](ch07_graph_embeddings_files/figure-html/fig-graph-embedding-output-1.png){#fig-graph-embedding width=950 height=373}\n:::\n:::\n\n\n## Creating Graph Embeddings\n\n::: {#8570176c .cell execution_count=2}\n``` {.python .cell-code code-fold=\"false\"}\n\"\"\"\nGraph Embeddings: Network Structure as Vectors\n\"\"\"\n\nimport numpy as np\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Simulated node embeddings from a social network\n# Real systems use Node2Vec, GraphSAGE, or GNN-based approaches\nnp.random.seed(42)\n\n# Two friend groups: nodes in the same group have similar embeddings\nnode_embeddings = {\n    # Community 1: Alice, Bob, Carol (densely connected)\n    'Alice': np.random.randn(64) + np.array([1, 0] + [0]*62),\n    'Bob': np.random.randn(64) + np.array([0.9, 0.1] + [0]*62),\n    'Carol': np.random.randn(64) + np.array([0.8, 0.2] + [0]*62),\n    # Community 2: Xavier, Yuki, Zara (densely connected)\n    'Xavier': np.random.randn(64) + np.array([0, 1] + [0]*62),\n    'Yuki': np.random.randn(64) + np.array([0.1, 0.9] + [0]*62),\n    'Zara': np.random.randn(64) + np.array([0.2, 0.8] + [0]*62),\n}\n\nprint(\"Graph embedding similarities:\\n\")\nprint(\"Within community (friends):\")\nab = cosine_similarity([node_embeddings['Alice']], [node_embeddings['Bob']])[0][0]\nac = cosine_similarity([node_embeddings['Alice']], [node_embeddings['Carol']])[0][0]\nprint(f\"  Alice ↔ Bob:   {ab:.3f}\")\nprint(f\"  Alice ↔ Carol: {ac:.3f}\")\n\nprint(\"\\nAcross communities (not connected):\")\nax = cosine_similarity([node_embeddings['Alice']], [node_embeddings['Xavier']])[0][0]\nprint(f\"  Alice ↔ Xavier: {ax:.3f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nGraph embedding similarities:\n\nWithin community (friends):\n  Alice ↔ Bob:   0.056\n  Alice ↔ Carol: 0.000\n\nAcross communities (not connected):\n  Alice ↔ Xavier: -0.035\n```\n:::\n:::\n\n\nNodes in the same community have high similarity because they share connections. Alice, Bob, and Carol are all friends with each other, so their embeddings cluster together. Xavier is in a different friend group with no direct connection to Alice, resulting in lower similarity.\n\n## When to Use Graph Embeddings {#sec-graph-embedding-types}\n\n**When to use graph embeddings:** Social network analysis (community detection, influence prediction, friend recommendation), recommendation systems with user-item graphs (see @sec-recommendation-systems), knowledge graph completion to predict missing relationships (see @sec-entity-resolution), fraud detection to identify suspicious patterns in transaction graphs (see @sec-financial-services), drug discovery for molecule property prediction from molecular graphs (see @sec-healthcare-life-sciences), and supply chain analysis to identify dependencies and bottlenecks.\n\n## Popular Graph Architectures\n\n| Architecture | Type | Strengths | Use Cases |\n|-------------|------|-----------|-----------|\n| [Node2Vec](https://github.com/aditya-grover/node2vec) | Random walks | Simple, scalable | Homogeneous graphs |\n| [GraphSAGE](https://github.com/williamleif/GraphSAGE) | Neighborhood aggregation | Inductive learning | New nodes |\n| [GAT](https://github.com/PetarV-/GAT) | Graph attention | Weighted neighbors | Heterogeneous graphs |\n| [TransE](https://github.com/thunlp/OpenKE) | Translation-based | Link prediction | Knowledge graphs |\n\n: Graph embedding architectures {.striped}\n\n## Advanced: How Graph Models Learn\n\n::: {.callout-note}\n## Optional Section\nThis section explains how graph embedding models learn structural patterns. Skip if you just need to use pre-built embeddings.\n:::\n\n### Node2Vec: Random Walks\n\nNode2Vec [@mikolov2013efficient] generates embeddings by performing random walks on the graph, then applying Word2Vec. The intuition: nodes that appear in similar \"context\" (nearby in random walks) should have similar embeddings.\n\n```python\ndef node2vec_walk(graph, start_node, walk_length, p=1, q=1):\n    \"\"\"Generate a random walk starting from a node.\"\"\"\n    walk = [start_node]\n    while len(walk) < walk_length:\n        cur = walk[-1]\n        neighbors = list(graph.neighbors(cur))\n        if len(neighbors) == 0:\n            break\n        # Biased sampling based on p (return) and q (in-out) parameters\n        if len(walk) == 1:\n            walk.append(random.choice(neighbors))\n        else:\n            prev = walk[-2]\n            probs = []\n            for neighbor in neighbors:\n                if neighbor == prev:\n                    probs.append(1/p)  # Return to previous\n                elif graph.has_edge(neighbor, prev):\n                    probs.append(1)  # Same neighborhood\n                else:\n                    probs.append(1/q)  # Explore\n            probs = np.array(probs) / sum(probs)\n            walk.append(np.random.choice(neighbors, p=probs))\n    return walk\n```\n\n### GraphSAGE: Neighborhood Aggregation\n\nGraphSAGE learns embeddings by aggregating features from a node's neighbors:\n\n1. Sample a fixed number of neighbors for each node\n2. Aggregate neighbor embeddings (mean, max, or LSTM)\n3. Concatenate with the node's own embedding\n4. Apply a neural network layer\n\nThis is **inductive**: it can generate embeddings for new nodes not seen during training.\n\n### Graph Attention Networks (GAT)\n\nGATs learn to weight neighbors differently based on their importance:\n\n```python\ndef gat_attention(node_emb, neighbor_embs, attention_weights):\n    \"\"\"Compute attention-weighted neighbor aggregation.\"\"\"\n    # Compute attention scores for each neighbor\n    scores = []\n    for neighbor_emb in neighbor_embs:\n        combined = torch.cat([node_emb, neighbor_emb])\n        score = torch.exp(attention_weights @ combined)\n        scores.append(score)\n\n    # Normalize to get attention weights\n    attention = torch.softmax(torch.tensor(scores), dim=0)\n\n    # Weighted aggregation\n    aggregated = sum(a * emb for a, emb in zip(attention, neighbor_embs))\n    return aggregated\n```\n\n## Key Takeaways\n\n- **Graph embeddings** capture network structure—connected nodes and nodes with similar neighborhoods have similar vectors\n- **The core principle**: a node's meaning comes from its connections, not just its features\n- **Random walks** (Node2Vec) treat graphs like text, generating \"sentences\" of nodes to train Word2Vec\n- **Neighborhood aggregation** (GraphSAGE, GAT) directly combines neighbor information, enabling inductive learning\n- **Applications** span social networks, recommendations, fraud detection, and molecular property prediction\n\n## Looking Ahead\n\nNow that you understand graph embeddings, @sec-timeseries-embeddings explores time-series embeddings—representations that capture temporal patterns and dynamics.\n\n## Further Reading\n\n- Grover, A. & Leskovec, J. (2016). \"node2vec: Scalable Feature Learning for Networks.\" *KDD*\n- Hamilton, W., et al. (2017). \"Inductive Representation Learning on Large Graphs.\" *NeurIPS*\n- Veličković, P., et al. (2018). \"Graph Attention Networks.\" *ICLR*\n\n",
    "supporting": [
      "ch07_graph_embeddings_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}