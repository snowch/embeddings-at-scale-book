{
  "hash": "18e72e0bea5b3a558cc2a09b91bf47f3",
  "result": {
    "engine": "jupyter",
    "markdown": "# Similarity and Distance Metrics {#sec-similarity-distance-metrics}\n\n::: callout-note\n## Chapter Overview\n\nChoosing the right similarity or distance metric fundamentally affects embedding system performance. This chapter covers the major metrics—cosine similarity, Euclidean distance, dot product, and others—explaining when to use each, their mathematical properties, and practical implications for vector databases and retrieval quality.\n:::\n\n## Why Metric Choice Matters\n\nThe metric you choose determines:\n\n- **What \"similar\" means** for your application\n- **Index performance** in your vector database\n- **Retrieval quality** for your use case\n- **Computational cost** at query time\n\nDifferent metrics capture different notions of similarity. Two embeddings might be \"close\" by one metric and \"far\" by another. Understanding these differences is essential for building effective embedding systems.\n\n## Cosine Similarity {#sec-cosine-similarity}\n\nWe introduced cosine similarity briefly in @sec-embedding-revolution; here we cover it in depth alongside other metrics so you can make informed choices.\n\nThink of cosine similarity as asking \"are these vectors pointing in the same direction?\" regardless of how long they are. Two documents about machine learning will point in a similar direction in embedding space whether one is a tweet or a textbook—their *lengths* differ, but their *meaning* aligns. This makes cosine similarity ideal for text, where document length shouldn't affect semantic similarity.\n\nCosine similarity measures the angle between two vectors, ignoring their magnitudes:\n\n$$\\text{cosine\\_similarity}(\\mathbf{A}, \\mathbf{B}) = \\frac{\\mathbf{A} \\cdot \\mathbf{B}}{||\\mathbf{A}|| \\times ||\\mathbf{B}||} = \\frac{\\sum_{i=1}^{n} A_i B_i}{\\sqrt{\\sum_{i=1}^{n} A_i^2} \\times \\sqrt{\\sum_{i=1}^{n} B_i^2}}$$\n\n::: {#cell-fig-cosine-similarity .cell execution_count=1}\n\n::: {.cell-output .cell-output-display}\n![Cosine similarity measures the angle θ between vectors, ignoring magnitude. Vectors A and B point in similar directions (high similarity) while C points differently (low similarity).](ch02_similarity_distance_metrics_files/figure-html/fig-cosine-similarity-output-1.png){#fig-cosine-similarity width=547 height=469}\n:::\n:::\n\n\n::: {#d9134de2 .cell execution_count=2}\n``` {.python .cell-code code-fold=\"false\"}\n\"\"\"\nCosine Similarity: Angle-Based Comparison\n\nMeasures the cosine of the angle between vectors.\nRange: -1 (opposite) to 1 (identical direction)\n\"\"\"\n\nimport numpy as np\nfrom scipy.spatial.distance import cosine\n\ndef cosine_similarity(a, b):\n    \"\"\"Calculate cosine similarity (1 = identical, -1 = opposite).\"\"\"\n    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n\n# Example: Same direction, different magnitudes\nv1 = np.array([1.0, 2.0, 3.0])\nv2 = np.array([2.0, 4.0, 6.0])  # Same direction, 2x magnitude\nv3 = np.array([3.0, 2.0, 1.0])  # Different direction\n\nprint(\"Cosine similarity examples:\")\nprint(f\"  v1 ↔ v2 (same direction, different magnitude): {cosine_similarity(v1, v2):.4f}\")\nprint(f\"  v1 ↔ v3 (different direction): {cosine_similarity(v1, v3):.4f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCosine similarity examples:\n  v1 ↔ v2 (same direction, different magnitude): 1.0000\n  v1 ↔ v3 (different direction): 0.7143\n```\n:::\n:::\n\n\n**When to use cosine similarity:**\n\n- **Text embeddings**: Sentence transformers produce vectors where direction encodes meaning\n- **High-dimensional spaces** (100+ dimensions): More stable than Euclidean distance\n- **When magnitude isn't meaningful**: Document length shouldn't affect similarity\n- **Normalized embeddings**: Most embedding models normalize output vectors\n\n**Cosine distance** is simply `1 - cosine_similarity`, converting similarity to distance where 0 = identical.\n\n## Euclidean Distance (L2) {#sec-euclidean-distance}\n\nThink of Euclidean distance as \"how far would I walk in a straight line?\" It measures absolute position in space—if you plotted embeddings as points on a map, Euclidean distance is the crow-flies distance between them. Unlike cosine similarity, magnitude matters: a short document and a long document will be far apart even if they discuss identical topics, simply because their embedding magnitudes differ.\n\nEuclidean distance measures the straight-line distance between two points:\n\n$$\\text{euclidean}(\\mathbf{A}, \\mathbf{B}) = \\sqrt{\\sum_{i=1}^{n} (A_i - B_i)^2} = ||\\mathbf{A} - \\mathbf{B}||_2$$\n\n::: {#cell-fig-euclidean-distance .cell execution_count=3}\n\n::: {.cell-output .cell-output-display}\n![Euclidean distance measures the straight-line distance between points. Unlike cosine similarity, magnitude matters—point B is far from A despite pointing in a similar direction.](ch02_similarity_distance_metrics_files/figure-html/fig-euclidean-distance-output-1.png){#fig-euclidean-distance width=464 height=469}\n:::\n:::\n\n\n::: {#2ec08238 .cell execution_count=4}\n``` {.python .cell-code code-fold=\"false\"}\n\"\"\"\nEuclidean Distance: Straight-Line Distance\n\nMeasures absolute separation in space.\nRange: 0 (identical) to infinity\n\"\"\"\n\nimport numpy as np\n\ndef euclidean_distance(a, b):\n    \"\"\"Calculate Euclidean (L2) distance.\"\"\"\n    return np.linalg.norm(a - b)\n\n# Same vectors as before\nv1 = np.array([1.0, 2.0, 3.0])\nv2 = np.array([2.0, 4.0, 6.0])  # Same direction, 2x magnitude\nv3 = np.array([3.0, 2.0, 1.0])  # Different direction\n\nprint(\"Euclidean distance examples:\")\nprint(f\"  v1 ↔ v2 (same direction, different magnitude): {euclidean_distance(v1, v2):.4f}\")\nprint(f\"  v1 ↔ v3 (different direction): {euclidean_distance(v1, v3):.4f}\")\nprint(\"\\nNote: v1 and v2 are FAR by Euclidean but IDENTICAL by cosine!\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nEuclidean distance examples:\n  v1 ↔ v2 (same direction, different magnitude): 3.7417\n  v1 ↔ v3 (different direction): 2.8284\n\nNote: v1 and v2 are FAR by Euclidean but IDENTICAL by cosine!\n```\n:::\n:::\n\n\n**When to use Euclidean distance:**\n\n- **Image embeddings**: When pixel-level differences matter\n- **Low-dimensional spaces** (< 50 dimensions): Works well\n- **When magnitude matters**: Larger vectors should be \"farther\"\n- **Clustering applications**: K-means uses Euclidean distance\n\n**Warning**: Euclidean distance suffers from the **curse of dimensionality**. In high dimensions (768+), all points tend to become equidistant, reducing discriminative power.\n\n## Dot Product (Inner Product) {#sec-dot-product}\n\nThe dot product captures both direction *and* magnitude—think of it as \"how much do these vectors agree, weighted by their strengths?\" A user embedding strongly pointing toward \"sci-fi\" matched with a blockbuster sci-fi movie (large magnitude) scores higher than the same user matched with an obscure indie sci-fi film (small magnitude). This is why recommendation systems often use dot product: magnitude encodes confidence or importance.\n\nThe dot product is the unnormalized version of cosine similarity:\n\n$$\\text{dot\\_product}(\\mathbf{A}, \\mathbf{B}) = \\mathbf{A} \\cdot \\mathbf{B} = \\sum_{i=1}^{n} A_i B_i$$\n\n::: {#cell-fig-dot-product .cell execution_count=5}\n\n::: {.cell-output .cell-output-display}\n![Dot product combines angle AND magnitude. Vectors B and C point in the same direction as A, but B (longer) has a higher dot product. Cosine similarity rates both pairs equally since it ignores magnitude.](ch02_similarity_distance_metrics_files/figure-html/fig-dot-product-output-1.png){#fig-dot-product width=974 height=390}\n:::\n:::\n\n\n::: {#0015fb6f .cell execution_count=6}\n``` {.python .cell-code code-fold=\"false\"}\n\"\"\"\nDot Product: Direction + Magnitude\n\nCombines directional similarity with magnitude.\nRange: -infinity to +infinity\n\"\"\"\n\nimport numpy as np\n\ndef dot_product(a, b):\n    \"\"\"Calculate dot product.\"\"\"\n    return np.dot(a, b)\n\n# Vectors with different magnitudes\nv1 = np.array([1.0, 2.0, 3.0])\nv2 = np.array([2.0, 4.0, 6.0])    # Same direction, 2x magnitude\nv3 = np.array([0.5, 1.0, 1.5])    # Same direction, 0.5x magnitude\n\nprint(\"Dot product examples:\")\nprint(f\"  v1 · v1: {dot_product(v1, v1):.4f}\")\nprint(f\"  v1 · v2 (2x magnitude): {dot_product(v1, v2):.4f}\")\nprint(f\"  v1 · v3 (0.5x magnitude): {dot_product(v1, v3):.4f}\")\nprint(\"\\nDot product rewards both alignment AND magnitude\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDot product examples:\n  v1 · v1: 14.0000\n  v1 · v2 (2x magnitude): 28.0000\n  v1 · v3 (0.5x magnitude): 7.0000\n\nDot product rewards both alignment AND magnitude\n```\n:::\n:::\n\n\n**When to use dot product:**\n\n- **Recommendation systems**: User-item relevance often uses dot product scoring\n- **When magnitude encodes importance**: Higher-magnitude vectors are \"stronger\" matches\n- **Maximum Inner Product Search (MIPS)**: Some vector DBs optimize for this\n- **Pre-normalized embeddings**: Equivalent to cosine similarity when vectors are unit length\n\n**Relationship to cosine similarity**: For unit-normalized vectors, dot product equals cosine similarity.\n\n## Manhattan Distance (L1) {#sec-manhattan-distance}\n\nImagine navigating a city grid—you can't walk diagonally through buildings, only along streets. Manhattan distance measures how many blocks you'd walk: 3 blocks east plus 4 blocks north, not the 5-block diagonal shortcut. This axis-aligned measurement is more robust to outliers than Euclidean distance: one extreme dimension doesn't dominate the total like it would when squared.\n\nManhattan distance sums the absolute differences along each dimension:\n\n$$\\text{manhattan}(\\mathbf{A}, \\mathbf{B}) = \\sum_{i=1}^{n} |A_i - B_i| = ||\\mathbf{A} - \\mathbf{B}||_1$$\n\n::: {#cell-fig-manhattan-distance .cell execution_count=7}\n\n::: {.cell-output .cell-output-display}\n![Manhattan distance follows a 'city block' path along axes (orange), while Euclidean takes the direct route (blue dashed). Manhattan = |4-1| + |3-1| = 5, while Euclidean = 3.61.](ch02_similarity_distance_metrics_files/figure-html/fig-manhattan-distance-output-1.png){#fig-manhattan-distance width=578 height=469}\n:::\n:::\n\n\n::: {#c29e18e1 .cell execution_count=8}\n``` {.python .cell-code code-fold=\"false\"}\n\"\"\"\nManhattan Distance: City-Block Distance\n\nSum of absolute differences along each axis.\nRange: 0 (identical) to infinity\n\"\"\"\n\nimport numpy as np\n\ndef manhattan_distance(a, b):\n    \"\"\"Calculate Manhattan (L1) distance.\"\"\"\n    return np.sum(np.abs(a - b))\n\nv1 = np.array([1.0, 2.0, 3.0])\nv2 = np.array([4.0, 6.0, 3.0])\n\neuclidean = np.linalg.norm(v1 - v2)\nmanhattan = manhattan_distance(v1, v2)\n\nprint(\"Comparing L1 vs L2 distance:\")\nprint(f\"  Euclidean (L2): {euclidean:.4f}\")\nprint(f\"  Manhattan (L1): {manhattan:.4f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nComparing L1 vs L2 distance:\n  Euclidean (L2): 5.0000\n  Manhattan (L1): 7.0000\n```\n:::\n:::\n\n\n**When to use Manhattan distance:**\n\n- **Sparse data**: Less sensitive to outliers than Euclidean\n- **Grid-like domains**: When movement is constrained to axes\n- **Feature independence**: When dimensions represent independent attributes\n- **Robust similarity**: Less affected by a single large difference\n\n## Hamming Distance {#sec-hamming-distance}\n\nHamming distance answers a simple question: \"how many bits need to flip to transform one vector into the other?\" If you compress embeddings to binary codes (0s and 1s), comparing them becomes blazingly fast—just XOR the bit strings and count the 1s. This makes Hamming distance essential for billion-scale search where you trade some accuracy for 32× storage savings and hardware-accelerated comparison.\n\nHamming distance counts the number of positions where values differ. For binary embeddings:\n\n$$\\text{hamming}(\\mathbf{A}, \\mathbf{B}) = \\sum_{i=1}^{n} \\mathbf{1}[A_i \\neq B_i]$$\n\n::: {#cell-fig-hamming-distance .cell execution_count=9}\n\n::: {.cell-output .cell-output-display}\n![Hamming distance counts differing positions. Vectors A and B differ in 2 positions (red), giving Hamming distance = 2. Used for binary embeddings where comparison is a fast XOR operation.](ch02_similarity_distance_metrics_files/figure-html/fig-hamming-distance-output-1.png){#fig-hamming-distance width=723 height=278}\n:::\n:::\n\n\n::: {#0badab9f .cell execution_count=10}\n``` {.python .cell-code code-fold=\"false\"}\n\"\"\"\nHamming Distance: Bit-Level Comparison\n\nCounts positions where values differ.\nEssential for binary/quantized embeddings.\n\"\"\"\n\nimport numpy as np\n\ndef hamming_distance(a, b):\n    \"\"\"Calculate Hamming distance for binary vectors.\"\"\"\n    return np.sum(a != b)\n\ndef hamming_similarity(a, b):\n    \"\"\"Normalized Hamming similarity (0 to 1).\"\"\"\n    return 1 - hamming_distance(a, b) / len(a)\n\n# Binary embeddings (e.g., from quantization)\nb1 = np.array([1, 0, 1, 1, 0, 1, 0, 0])\nb2 = np.array([1, 0, 1, 0, 0, 1, 0, 1])  # 2 bits different\nb3 = np.array([0, 1, 0, 0, 1, 0, 1, 1])  # 8 bits different\n\nprint(\"Hamming distance (binary embeddings):\")\nprint(f\"  b1 ↔ b2 (similar): {hamming_distance(b1, b2)} bits differ, similarity: {hamming_similarity(b1, b2):.3f}\")\nprint(f\"  b1 ↔ b3 (opposite): {hamming_distance(b1, b3)} bits differ, similarity: {hamming_similarity(b1, b3):.3f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nHamming distance (binary embeddings):\n  b1 ↔ b2 (similar): 2 bits differ, similarity: 0.750\n  b1 ↔ b3 (opposite): 8 bits differ, similarity: 0.000\n```\n:::\n:::\n\n\n**When to use Hamming distance:**\n\n- **Binary embeddings**: From binarization or locality-sensitive hashing\n- **Quantized vectors**: After product quantization\n- **Extreme scale**: Binary comparison is very fast (XOR + popcount)\n- **Memory-constrained**: Binary vectors use 32x less storage than float32\n\nSee @sec-quantized-embeddings for more on binary and quantized embeddings.\n\n## Jaccard Similarity {#sec-jaccard-similarity}\n\nJaccard similarity asks \"what fraction of items do these two sets share?\" If two users have watched 10 movies total between them, and 6 of those are the same, their Jaccard similarity is 6/10 = 0.6. It's intuitive for sparse, binary features like tags, categories, or bag-of-words representations where you care about presence/absence rather than counts or magnitudes.\n\nJaccard similarity measures overlap between sets:\n\n$$\\text{jaccard}(\\mathbf{A}, \\mathbf{B}) = \\frac{|A \\cap B|}{|A \\cup B|}$$\n\n::: {#cell-fig-jaccard-similarity .cell execution_count=11}\n\n::: {.cell-output .cell-output-display}\n![Jaccard similarity = intersection / union. Sets A and B share 3 elements (green overlap) out of 5 total unique elements, giving Jaccard = 3/5 = 0.60.](ch02_similarity_distance_metrics_files/figure-html/fig-jaccard-similarity-output-1.png){#fig-jaccard-similarity width=438 height=374}\n:::\n:::\n\n\n::: {#9929ab34 .cell execution_count=12}\n``` {.python .cell-code code-fold=\"false\"}\n\"\"\"\nJaccard Similarity: Set Overlap\n\nMeasures intersection over union.\nRange: 0 (no overlap) to 1 (identical sets)\n\"\"\"\n\nimport numpy as np\n\ndef jaccard_similarity(a, b):\n    \"\"\"Calculate Jaccard similarity for binary/set vectors.\"\"\"\n    intersection = np.sum(np.logical_and(a, b))\n    union = np.sum(np.logical_or(a, b))\n    return intersection / union if union > 0 else 0\n\n# Binary feature vectors (e.g., document has word or not)\ndoc1 = np.array([1, 1, 1, 0, 0, 1, 0, 0])  # Has words: 0, 1, 2, 5\ndoc2 = np.array([1, 1, 0, 0, 0, 1, 1, 0])  # Has words: 0, 1, 5, 6\ndoc3 = np.array([0, 0, 0, 1, 1, 0, 0, 1])  # Has words: 3, 4, 7\n\nprint(\"Jaccard similarity (set overlap):\")\nprint(f\"  doc1 ↔ doc2 (3 shared, 5 total): {jaccard_similarity(doc1, doc2):.3f}\")\nprint(f\"  doc1 ↔ doc3 (0 shared): {jaccard_similarity(doc1, doc3):.3f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nJaccard similarity (set overlap):\n  doc1 ↔ doc2 (3 shared, 5 total): 0.600\n  doc1 ↔ doc3 (0 shared): 0.000\n```\n:::\n:::\n\n\n**When to use Jaccard similarity:**\n\n- **Sparse binary features**: Bag-of-words, tag sets\n- **Set membership**: When presence/absence matters, not magnitude\n- **Near-duplicate detection**: MinHash approximates Jaccard efficiently\n- **Categorical data**: When features are one-hot encoded\n\n## Metric Comparison Summary\n\n| Metric | Range | Magnitude Sensitive | Best For | Vector DB Support |\n|--------|-------|---------------------|----------|-------------------|\n| **Cosine** | -1 to 1 | No | Text, normalized embeddings | Universal |\n| **Euclidean (L2)** | 0 to ∞ | Yes | Images, low-dimensional | Universal |\n| **Dot Product** | -∞ to ∞ | Yes | Recommendations, MIPS | Most |\n| **Manhattan (L1)** | 0 to ∞ | Yes | Sparse data, outlier-robust | Some |\n| **Hamming** | 0 to n | N/A (binary) | Binary embeddings | Some |\n| **Jaccard** | 0 to 1 | N/A (sets) | Sparse sets, tags | Limited |\n\n: Similarity and distance metrics comparison {.striped}\n\n## Choosing the Right Metric\n\n| Use Case | Embedding Type | Normalized? | Recommended Metric |\n|----------|---------------|-------------|-------------------|\n| Text (sentence transformers) | Dense | Yes | Cosine / Dot product |\n| Image (CNN features) | Dense | No | Dot product |\n| Recommendations (user-item) | Dense | No | Dot product |\n| Binary hash codes | Binary | N/A | Hamming |\n| Document tags | Sparse binary | N/A | Jaccard |\n\n: Metric recommendations by use case {.striped}\n\n### Decision Tree\n\n```\nIs your embedding binary?\n├── Yes → Hamming distance\n└── No → Is it sparse binary (sets/tags)?\n    ├── Yes → Jaccard similarity\n    └── No → Are vectors normalized?\n        ├── Yes → Cosine similarity (fastest)\n        └── No → Does magnitude encode importance?\n            ├── Yes → Dot product or Euclidean\n            └── No → Cosine similarity\n```\n\n## Impact on Vector Database Performance\n\nYour metric choice affects index structure, query latency, and storage requirements:\n\n| Metric | Index Type | HNSW Support | Storage | Query Overhead |\n|--------|-----------|--------------|---------|----------------|\n| **Cosine** | Normalize + L2 | ✓ Native | 1x | Normalize query |\n| **Euclidean (L2)** | Native L2 | ✓ Native | 1x | None |\n| **Dot Product** | MIPS or augmented L2 | ✓ With transform | 1x-1.01x | May need augmentation |\n| **Hamming** | Binary index | Specialized | 0.03x (32x smaller) | Bitwise ops only |\n\n: Vector database index compatibility by metric {.striped}\n\n### Why This Matters\n\n**Cosine vs. L2 equivalence**: For normalized vectors, cosine similarity and L2 distance produce identical rankings. Most databases exploit this—they normalize vectors once at insertion, then use fast L2 indexes:\n\n```python\n# These produce the same ranking for normalized vectors:\n# cosine_sim(a, b) = 1 - (L2_dist(a, b)² / 2)\n```\n\n**Dot product challenges**: Unlike cosine and L2, dot product (MIPS—Maximum Inner Product Search) doesn't satisfy the triangle inequality. Some databases handle this by:\n\n1. Appending a dimension to convert MIPS → L2 (slight storage overhead)\n2. Using specialized MIPS indexes (less common)\n\n**Binary embeddings**: Hamming distance enables 32x storage reduction (float32 → 1 bit per dimension) with specialized binary indexes. Ideal for large-scale deduplication where some quality loss is acceptable.\n\n::: callout-tip\n## Performance Tip\n\nIf using cosine similarity, **pre-normalize your embeddings** before insertion. This avoids redundant normalization at query time and lets you use faster L2 indexes directly.\n:::\n\n## Practical Considerations\n\n### Normalization\n\n::: {#f573b1f9 .cell execution_count=13}\n``` {.python .cell-code code-fold=\"false\"}\n\"\"\"\nL2 Normalization: Making Cosine = Dot Product\n\"\"\"\n\nimport numpy as np\n\ndef l2_normalize(vectors):\n    \"\"\"Normalize vectors to unit length.\"\"\"\n    norms = np.linalg.norm(vectors, axis=1, keepdims=True)\n    return vectors / norms\n\n# Original vectors\nvectors = np.array([\n    [3.0, 4.0],      # Magnitude 5\n    [1.0, 1.0],      # Magnitude √2\n    [10.0, 0.0],     # Magnitude 10\n])\n\nnormalized = l2_normalize(vectors)\n\nprint(\"Before normalization:\")\nprint(f\"  Magnitudes: {np.linalg.norm(vectors, axis=1)}\")\n\nprint(\"\\nAfter L2 normalization:\")\nprint(f\"  Magnitudes: {np.linalg.norm(normalized, axis=1)}\")\n\n# Now dot product = cosine similarity\nv1, v2 = normalized[0], normalized[1]\ndot = np.dot(v1, v2)\ncos = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\nprint(f\"\\nFor normalized vectors: dot product = {dot:.4f}, cosine = {cos:.4f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nBefore normalization:\n  Magnitudes: [ 5.          1.41421356 10.        ]\n\nAfter L2 normalization:\n  Magnitudes: [1. 1. 1.]\n\nFor normalized vectors: dot product = 0.9899, cosine = 0.9899\n```\n:::\n:::\n\n\n### Metric Selection by Domain\n\n| Domain | Typical Metric | Reason |\n|--------|---------------|--------|\n| **Semantic search** | Cosine | Text embeddings are normalized |\n| **Image retrieval** | Cosine or L2 | Depends on model output |\n| **Recommendations** | Dot product | Magnitude = confidence |\n| **Face recognition** | Cosine | Normalized face embeddings |\n| **Document dedup** | Jaccard or Cosine | Depending on representation |\n| **Binary codes** | Hamming | Fast bitwise operations |\n\n: Metric selection by application domain {.striped}\n\n## Emerging and Future Metrics {#sec-emerging-metrics}\n\nThe standard metrics above handle most use cases, but research continues to develop specialized approaches for challenging scenarios.\n\n### Hyperbolic Distance\n\nFor hierarchical data (taxonomies, org charts, knowledge graphs), Euclidean space is inefficient—it can't naturally represent tree structures. **Hyperbolic space** has negative curvature that matches hierarchical growth patterns:\n\n::: {#673d9cfe .cell execution_count=14}\n``` {.python .cell-code code-fold=\"false\"}\n\"\"\"\nHyperbolic Distance (Poincaré Ball Model)\n\nHyperbolic space naturally represents hierarchies.\nPoints near the center are \"general\"; points near the edge are \"specific\".\n\"\"\"\n\nimport numpy as np\n\ndef poincare_distance(u, v):\n    \"\"\"\n    Distance in the Poincaré ball model of hyperbolic space.\n\n    As points approach the boundary (norm → 1), distances grow rapidly,\n    creating \"room\" for exponentially many nodes at each level.\n    \"\"\"\n    norm_u_sq = np.sum(u ** 2)\n    norm_v_sq = np.sum(v ** 2)\n    norm_diff_sq = np.sum((u - v) ** 2)\n\n    # Hyperbolic distance formula\n    return np.arccosh(\n        1 + 2 * norm_diff_sq / ((1 - norm_u_sq) * (1 - norm_v_sq))\n    )\n\n# Example: Points in 2D Poincaré ball\ncenter = np.array([0.0, 0.0])      # Root of hierarchy\nmid_level = np.array([0.5, 0.0])   # Middle of tree\nleaf = np.array([0.9, 0.0])        # Leaf node (near boundary)\n\nprint(\"Hyperbolic distances (Poincaré ball):\")\nprint(f\"  Center ↔ Mid-level: {poincare_distance(center, mid_level):.3f}\")\nprint(f\"  Mid-level ↔ Leaf: {poincare_distance(mid_level, leaf):.3f}\")\nprint(f\"  Center ↔ Leaf: {poincare_distance(center, leaf):.3f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nHyperbolic distances (Poincaré ball):\n  Center ↔ Mid-level: 1.099\n  Mid-level ↔ Leaf: 1.846\n  Center ↔ Leaf: 2.944\n```\n:::\n:::\n\n\n**Use cases**: Product taxonomies, organizational hierarchies, knowledge graphs. Hyperbolic embeddings can represent hierarchies in 5-20 dimensions that would require 100-500 dimensions in Euclidean space.\n\n### Learned Similarity Functions\n\nInstead of choosing a fixed metric, **learn the similarity function** from your data:\n\n::: {#69b592ee .cell execution_count=15}\n``` {.python .cell-code code-fold=\"false\"}\n\"\"\"\nLearned Similarity: Let the model decide what \"similar\" means.\n\nApproaches:\n1. Mahalanobis distance (learns covariance structure)\n2. Siamese networks (learn embedding + comparison jointly)\n3. Cross-encoders (attend across both inputs)\n\"\"\"\n\nimport numpy as np\n\nclass LearnedMahalanobis:\n    \"\"\"\n    Mahalanobis distance with learned transformation matrix.\n\n    Learns which dimensions matter and how they correlate.\n    Equivalent to: d(x,y) = sqrt((x-y)^T M (x-y)) where M is learned.\n    \"\"\"\n\n    def __init__(self, dim):\n        # Initialize as identity (reduces to Euclidean)\n        self.L = np.eye(dim)  # M = L^T L ensures positive semi-definite\n\n    def distance(self, x, y):\n        \"\"\"Compute Mahalanobis distance with learned metric.\"\"\"\n        diff = x - y\n        transformed = self.L @ diff\n        return np.sqrt(np.sum(transformed ** 2))\n\n    def fit(self, similar_pairs, dissimilar_pairs, learning_rate=0.01):\n        \"\"\"\n        Learn metric from supervision (simplified).\n        Real implementation uses gradient descent on triplet/contrastive loss.\n        \"\"\"\n        # Pull similar pairs closer, push dissimilar pairs apart\n        pass  # Actual training loop omitted for brevity\n\n# Example usage\nmetric = LearnedMahalanobis(dim=128)\nx = np.random.randn(128)\ny = np.random.randn(128)\nprint(f\"Learned Mahalanobis distance: {metric.distance(x, y):.3f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLearned Mahalanobis distance: 16.261\n```\n:::\n:::\n\n\n**When to use learned metrics:**\n- Domain-specific similarity (what's \"similar\" in your domain isn't captured by cosine)\n- Few-shot learning (learn from limited examples)\n- When you have supervision signal (click data, ratings, labels)\n\n### Approximate Metrics at Scale\n\nAt billion-scale, even computing exact similarity becomes expensive. **Approximate metrics** trade accuracy for speed:\n\n::: {#316bf673 .cell execution_count=16}\n``` {.python .cell-code code-fold=\"false\"}\n\"\"\"\nApproximate Similarity for Extreme Scale\n\nTechniques:\n1. Locality-Sensitive Hashing (LSH): Hash similar items to same bucket\n2. Product Quantization (PQ): Compress vectors, approximate distance\n3. Random Projections: Preserve relative distances approximately\n\"\"\"\n\nimport numpy as np\n\ndef random_projection_similarity(a, b, n_projections=100, seed=42):\n    \"\"\"\n    Approximate cosine similarity using random projections.\n\n    Project to random hyperplanes, count sign agreements.\n    More agreements = more similar (probabilistically).\n    \"\"\"\n    np.random.seed(seed)\n    dim = len(a)\n\n    # Generate random projection vectors\n    projections = np.random.randn(n_projections, dim)\n\n    # Project both vectors\n    proj_a = np.sign(projections @ a)\n    proj_b = np.sign(projections @ b)\n\n    # Count agreements (same sign = similar direction)\n    agreement_rate = np.mean(proj_a == proj_b)\n\n    # Convert to approximate cosine similarity\n    # (1 - 2*theta/pi) where theta is angle\n    approx_cosine = np.cos(np.pi * (1 - agreement_rate))\n\n    return approx_cosine\n\n# Compare exact vs approximate\na = np.random.randn(768)\nb = a + np.random.randn(768) * 0.5  # Similar vector\n\nexact_cosine = np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\napprox_cosine = random_projection_similarity(a, b)\n\nprint(\"Exact vs Approximate similarity:\")\nprint(f\"  Exact cosine: {exact_cosine:.4f}\")\nprint(f\"  Approx (100 projections): {approx_cosine:.4f}\")\nprint(f\"  Error: {abs(exact_cosine - approx_cosine):.4f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nExact vs Approximate similarity:\n  Exact cosine: 0.8827\n  Approx (100 projections): 0.7705\n  Error: 0.1122\n```\n:::\n:::\n\n\n**Trade-offs:**\n- LSH: O(1) lookup but requires tuning hash functions\n- PQ: 10-100x compression, ~5% recall loss typical\n- Random projections: Simple, parallelizable, theoretical guarantees\n\n### Task-Adaptive Metrics\n\nThe best metric depends on your task. **Metric learning** optimizes the similarity function end-to-end:\n\n| Approach | How It Works | Best For |\n|----------|--------------|----------|\n| **Triplet loss** | Learn: d(anchor, positive) < d(anchor, negative) | Face recognition, retrieval |\n| **Contrastive loss** | Pull positives together, push negatives apart | Self-supervised learning |\n| **Cross-encoder** | Jointly encode both inputs, predict similarity | Reranking, high-precision |\n| **Late interaction** | Multiple vectors per item, aggregate similarities | Fine-grained matching |\n\n: Task-adaptive metric learning approaches {.striped}\n\nFor detailed coverage of these training approaches, see @sec-contrastive-learning and @sec-siamese-networks.\n\n## Key Takeaways\n\n- **Cosine similarity** is the default for most embedding applications—it ignores magnitude and works well in high dimensions\n\n- **Euclidean distance** is magnitude-sensitive and works best in lower dimensions; suffers from curse of dimensionality at 768+ dims\n\n- **Dot product** rewards both alignment and magnitude—use when larger embeddings should match more strongly\n\n- **Hamming distance** enables ultra-fast search on binary embeddings with 32x storage savings\n\n- **Metric choice affects indexing**: Most vector databases optimize for L2/cosine; other metrics may require transformation\n\n- **Pre-normalize for cosine**: If using cosine similarity, normalize vectors before insertion to avoid redundant computation\n\n- **Emerging approaches** like hyperbolic distance, learned metrics, and approximate similarity address specialized needs—hierarchical data, domain-specific similarity, and extreme scale\n\n## Looking Ahead\n\nWith similarity metrics understood, @sec-vector-database-fundamentals covers how vector databases use these metrics to build efficient indexes at scale. For binary and quantized embeddings that use Hamming distance, see @sec-quantized-embeddings in the advanced patterns chapter.\n\n## Further Reading\n\n- Aggarwal, C. C., Hinneburg, A., & Keim, D. A. (2001). \"On the Surprising Behavior of Distance Metrics in High Dimensional Space.\" *ICDT*\n- Johnson, J., Douze, M., & Jégou, H. (2019). \"Billion-scale similarity search with GPUs.\" *IEEE Transactions on Big Data*\n- Wang, J., et al. (2018). \"A Survey on Learning to Hash.\" *IEEE TPAMI*\n\n",
    "supporting": [
      "ch02_similarity_distance_metrics_files"
    ],
    "filters": [],
    "includes": {}
  }
}