{
  "hash": "0077460e0504183f30b918ff2a616c37",
  "result": {
    "engine": "jupyter",
    "markdown": "# Video Surveillance and Analytics {#sec-video-surveillance}\n\n:::{.callout-note}\n## Chapter Overview\nVideo surveillance and analytics—from retail loss prevention to smart city safety to industrial compliance monitoring—generates more embedding vectors than almost any other application domain, with a single camera producing 86,400 frame embeddings per day and enterprise deployments spanning thousands of cameras. This chapter applies embeddings to video analytics at scale: real-time video stream processing using efficient frame and clip embeddings that enable sub-second event detection across thousands of concurrent camera feeds, person re-identification tracking individuals across multiple cameras and time periods through appearance embeddings robust to pose, lighting, and occlusion changes, action and behavior recognition detecting activities of interest from temporal embeddings that capture motion patterns and human-object interactions, anomaly detection identifying unusual events without explicit training through deviation from learned normal behavior patterns, forensic video search enabling rapid retrieval of specific events, people, or objects across weeks of archived footage through semantic video embeddings, and privacy-preserving analytics that extract actionable insights while protecting individual privacy through on-device processing, face blurring, and federated learning. These techniques transform video from passive recording to active intelligence across retail (shoplifting detection, customer analytics), smart cities (traffic management, public safety), manufacturing (safety compliance, quality inspection), healthcare (patient monitoring, fall detection), and security (access control, perimeter monitoring)—enabling organizations to derive value from the petabytes of video they capture while respecting privacy and operating within resource constraints.\n:::\n\nBuilding on the cross-industry patterns in @sec-cross-industry-patterns, embeddings enable **video surveillance transformation** at unprecedented scale. Traditional video monitoring relies on human operators watching screens—an approach that fails at scale (one operator can effectively monitor 4-8 cameras) and misses critical events during lapses in attention. **Embedding-based video analytics** converts continuous video streams into searchable, analyzable vector representations, enabling automated detection, tracking, and search across camera networks that would be impossible with human review alone—while raising important considerations around privacy, bias, and appropriate use.\n\n## Real-Time Video Stream Processing\n\nProcessing live video at scale requires efficient embedding generation that balances accuracy with throughput. **Real-time video processing** extracts embeddings from frames or clips fast enough to enable immediate detection and alerting across thousands of concurrent streams.\n\n### The Real-Time Processing Challenge\n\nTraditional video analytics faces limitations:\n\n- **Throughput**: Processing thousands of concurrent HD/4K streams\n- **Latency**: Detection must occur within seconds for actionable alerts\n- **Resource constraints**: GPU compute is expensive; efficiency matters\n- **Variable content**: Cameras span indoor/outdoor, day/night, crowded/empty scenes\n- **24/7 operation**: Systems must run continuously without degradation\n\n**Embedding approach**: Extract lightweight frame embeddings for rapid scene understanding, with deeper clip embeddings for detected events. Hierarchical processing prioritizes compute on interesting regions and time periods.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show real-time video processing architecture\"}\nfrom dataclasses import dataclass\nfrom typing import Optional\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n@dataclass\nclass VideoConfig:\n    frame_size: int = 224\n    clip_length: int = 16\n    embedding_dim: int = 512\n\nclass FrameEncoder(nn.Module):\n    \"\"\"Efficient frame encoder for real-time processing.\"\"\"\n    def __init__(self, config: VideoConfig):\n        super().__init__()\n        self.backbone = nn.Sequential(\n            nn.Conv2d(3, 32, 3, stride=2, padding=1), nn.BatchNorm2d(32), nn.ReLU6(),\n            nn.Conv2d(32, 64, 3, stride=2, padding=1), nn.BatchNorm2d(64), nn.ReLU6(),\n            nn.Conv2d(64, 128, 3, stride=2, padding=1), nn.BatchNorm2d(128), nn.ReLU6(),\n            nn.Conv2d(128, 256, 3, stride=2, padding=1), nn.BatchNorm2d(256), nn.ReLU6(),\n            nn.AdaptiveAvgPool2d(1))\n        self.proj = nn.Linear(256, config.embedding_dim)\n\n    def forward(self, frames: torch.Tensor) -> torch.Tensor:\n        features = self.backbone(frames).squeeze(-1).squeeze(-1)\n        return F.normalize(self.proj(features), dim=-1)\n\nclass ClipEncoder(nn.Module):\n    \"\"\"Temporal clip encoder for action understanding.\"\"\"\n    def __init__(self, config: VideoConfig):\n        super().__init__()\n        self.conv3d = nn.Sequential(\n            nn.Conv3d(3, 64, (3, 7, 7), stride=(1, 2, 2), padding=(1, 3, 3)), nn.BatchNorm3d(64), nn.ReLU(),\n            nn.MaxPool3d((1, 3, 3), (1, 2, 2), (0, 1, 1)),\n            nn.Conv3d(64, 128, (3, 3, 3), stride=(2, 2, 2), padding=1), nn.BatchNorm3d(128), nn.ReLU(),\n            nn.Conv3d(128, 256, (3, 3, 3), stride=(2, 2, 2), padding=1), nn.BatchNorm3d(256), nn.ReLU(),\n            nn.AdaptiveAvgPool3d(1))\n        self.proj = nn.Linear(256, config.embedding_dim)\n\n    def forward(self, clips: torch.Tensor) -> torch.Tensor:\n        features = self.conv3d(clips).squeeze(-1).squeeze(-1).squeeze(-1)\n        return F.normalize(self.proj(features), dim=-1)\n\nclass HierarchicalProcessor(nn.Module):\n    \"\"\"Hierarchical video processing: fast frame detection triggers clip analysis.\"\"\"\n    def __init__(self, config: VideoConfig):\n        super().__init__()\n        self.frame_encoder = FrameEncoder(config)\n        self.clip_encoder = ClipEncoder(config)\n        self.event_detector = nn.Sequential(\n            nn.Linear(config.embedding_dim, 256), nn.ReLU(), nn.Linear(256, 1), nn.Sigmoid())\n        self.event_classifier = nn.Linear(config.embedding_dim, 20)\n\n    def process_frame(self, frame: torch.Tensor) -> tuple:\n        emb = self.frame_encoder(frame)\n        return emb, self.event_detector(emb)\n\n    def process_clip(self, clip: torch.Tensor) -> tuple:\n        emb = self.clip_encoder(clip)\n        return emb, self.event_classifier(emb)\n\n# Usage example\nconfig = VideoConfig()\nprocessor = HierarchicalProcessor(config)\n\n# Process individual frames for fast event detection\nframes = torch.randn(8, 3, 224, 224)\nframe_embs, event_scores = processor.process_frame(frames)\nprint(f\"Frame embeddings: {frame_embs.shape}, Event scores: {event_scores.shape}\")\n\n# Process video clips for detailed classification\nclips = torch.randn(4, 3, 16, 224, 224)  # [batch, channels, time, H, W]\nclip_embs, event_logits = processor.process_clip(clips)\nprint(f\"Clip embeddings: {clip_embs.shape}, Event logits: {event_logits.shape}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFrame embeddings: torch.Size([8, 512]), Event scores: torch.Size([8, 1])\nClip embeddings: torch.Size([4, 512]), Event logits: torch.Size([4, 20])\n```\n:::\n:::\n\n\n:::{.callout-tip}\n## Real-Time Processing Best Practices\n\n**Architecture:**\n\n- **Edge-cloud hybrid**: Initial processing at edge, detailed analysis in cloud\n- **Hierarchical models**: Fast detector triggers slower, accurate classifier\n- **Batch processing**: Aggregate frames across cameras for GPU efficiency\n- **Keyframe extraction**: Process representative frames, not every frame\n- **Region of interest**: Focus compute on relevant image areas\n\n**Efficiency:**\n\n- **Model quantization**: INT8 inference for 2-4× speedup with minimal accuracy loss\n- **Knowledge distillation**: Train small models to mimic large ones\n- **Temporal redundancy**: Skip similar consecutive frames\n- **Resolution adaptation**: Process at lower resolution when sufficient\n- **Hardware acceleration**: TensorRT, OpenVINO for optimized inference\n\n**Scalability:**\n\n- **Horizontal scaling**: Add processing nodes as camera count grows\n- **Load balancing**: Distribute streams across available compute\n- **Priority queuing**: Process high-priority cameras first\n- **Graceful degradation**: Reduce frame rate under load vs dropping streams\n- **Auto-scaling**: Spin up resources during peak activity\n\n**Reliability:**\n\n- **Stream reconnection**: Handle camera disconnects gracefully\n- **Failover**: Redundant processing for critical cameras\n- **Health monitoring**: Track processing latency and queue depth\n- **Alerting**: Notify operators of system issues\n- **Graceful shutdown**: Complete in-flight processing before restart\n:::\n\n## Person Re-Identification\n\nPerson re-identification (Re-ID) tracks individuals across multiple cameras without relying on face recognition. **Embedding-based Re-ID** learns appearance representations that remain consistent across viewpoints, lighting conditions, and time.\n\n### The Re-ID Challenge\n\nTraditional person tracking faces limitations:\n\n- **Camera gaps**: People disappear between camera fields of view\n- **Appearance changes**: Lighting, pose, and occlusion vary across cameras\n- **Scale**: Large venues may have hundreds of cameras\n- **Time gaps**: Need to match across minutes to hours\n- **Privacy**: Face recognition raises significant privacy concerns\n\n**Embedding approach**: Learn person embeddings from full-body appearance (clothing, body shape, gait) that generalize across cameras. Similar embeddings indicate the same person; enable tracking without biometric identification.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show person re-identification architecture\"}\n@dataclass\nclass ReIDConfig:\n    image_height: int = 256\n    image_width: int = 128\n    embedding_dim: int = 512\n    n_parts: int = 6\n\nclass PartBasedReIDEncoder(nn.Module):\n    \"\"\"Part-based person re-identification encoder.\"\"\"\n    def __init__(self, config: ReIDConfig):\n        super().__init__()\n        self.config = config\n        self.backbone = nn.Sequential(\n            nn.Conv2d(3, 64, 7, stride=2, padding=3), nn.BatchNorm2d(64), nn.ReLU(), nn.MaxPool2d(3, 2, 1),\n            nn.Conv2d(64, 128, 3, stride=2, padding=1), nn.BatchNorm2d(128), nn.ReLU(),\n            nn.Conv2d(128, 256, 3, stride=2, padding=1), nn.BatchNorm2d(256), nn.ReLU(),\n            nn.Conv2d(256, 512, 3, padding=1), nn.BatchNorm2d(512), nn.ReLU())\n        self.part_pool = nn.AdaptiveAvgPool2d((config.n_parts, 1))\n        self.part_embeddings = nn.ModuleList([\n            nn.Linear(512, config.embedding_dim // config.n_parts) for _ in range(config.n_parts)])\n        self.global_pool = nn.AdaptiveAvgPool2d(1)\n        self.global_embedding = nn.Linear(512, config.embedding_dim)\n\n    def forward(self, images: torch.Tensor) -> tuple:\n        features = self.backbone(images)\n        global_feat = self.global_pool(features).squeeze(-1).squeeze(-1)\n        global_emb = F.normalize(self.global_embedding(global_feat), dim=-1)\n        part_feats = self.part_pool(features).squeeze(-1)\n        part_embs = [F.normalize(self.part_embeddings[i](part_feats[:, :, i]), dim=-1)\n                     for i in range(self.config.n_parts)]\n        return global_emb, torch.stack(part_embs, dim=1)\n\nclass TripletLoss(nn.Module):\n    \"\"\"Triplet loss for re-identification training.\"\"\"\n    def __init__(self, margin: float = 0.3):\n        super().__init__()\n        self.margin = margin\n\n    def forward(self, anchor: torch.Tensor, positive: torch.Tensor, negative: torch.Tensor) -> torch.Tensor:\n        pos_dist = F.pairwise_distance(anchor, positive)\n        neg_dist = F.pairwise_distance(anchor, negative)\n        return F.relu(pos_dist - neg_dist + self.margin).mean()\n\n# Usage example\nreid_config = ReIDConfig()\nreid_encoder = PartBasedReIDEncoder(reid_config)\n\n# Encode person crops for re-identification\nperson_crops = torch.randn(4, 3, 256, 128)  # [batch, channels, H, W]\nglobal_emb, part_embs = reid_encoder(person_crops)\nprint(f\"Global embeddings: {global_emb.shape}\")  # [4, 512]\nprint(f\"Part embeddings: {part_embs.shape}\")  # [4, 6, 85]\n\n# Train with triplet loss\nanchor, positive, negative = torch.randn(4, 512), torch.randn(4, 512), torch.randn(4, 512)\nloss = TripletLoss()(anchor, positive, negative)\nprint(f\"Triplet loss: {loss.item():.4f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nGlobal embeddings: torch.Size([4, 512])\nPart embeddings: torch.Size([4, 6, 85])\nTriplet loss: 1.4191\n```\n:::\n:::\n\n\n:::{.callout-tip}\n## Person Re-ID Best Practices\n\n**Feature extraction:**\n\n- **Part-based models**: Encode head, torso, legs separately for robustness\n- **Attention mechanisms**: Focus on discriminative regions\n- **Multi-scale features**: Capture both fine details and global appearance\n- **Temporal pooling**: Aggregate features across multiple frames\n- **Occlusion handling**: Learn to ignore occluded body parts\n\n**Training:**\n\n- **Triplet loss**: Pull same-person embeddings together, push different apart\n- **Hard mining**: Focus on difficult examples (similar different people)\n- **Domain adaptation**: Fine-tune on target camera network\n- **Data augmentation**: Random erasing, color jitter, pose variation\n- **Cross-camera pairs**: Train on same person across different cameras\n\n**Deployment:**\n\n- **Gallery management**: Maintain embeddings for tracked individuals\n- **Matching threshold**: Balance precision (false matches) vs recall (missed matches)\n- **Temporal constraints**: Weight recent observations higher\n- **Spatial constraints**: Use camera topology to prune impossible matches\n- **Batch matching**: Efficient similarity search across large galleries\n\n**Evaluation:**\n\n- **Rank-1 accuracy**: Correct match in top result\n- **mAP**: Mean average precision across queries\n- **Cross-camera**: Separate evaluation per camera pair\n- **Time gap**: Performance vs time between observations\n- **Occlusion robustness**: Performance on partially visible persons\n:::\n\n:::{.callout-warning}\n## Re-ID Privacy Considerations\n\nPerson re-identification enables tracking without explicit consent:\n\n- **Scope limitation**: Only track within defined areas with notice\n- **Retention limits**: Delete tracking data after defined period\n- **Purpose restriction**: Use only for stated security purposes\n- **Audit trails**: Log all re-identification queries and results\n- **Opt-out mechanisms**: Provide ways to request non-tracking where feasible\n- **Bias testing**: Evaluate accuracy across demographic groups\n- **Human review**: Require human confirmation for consequential actions\n:::\n\n## Action and Behavior Recognition\n\nAction recognition detects activities of interest in video—from safety violations to suspicious behavior to customer interactions. **Embedding-based action recognition** learns temporal representations that capture motion patterns and human-object interactions.\n\n### The Action Recognition Challenge\n\nTraditional rule-based detection faces limitations:\n\n- **Complexity**: Human actions are highly variable and context-dependent\n- **Subtlety**: Important behaviors may be brief or partially occluded\n- **Context**: Same motion means different things in different contexts\n- **Scale**: Need to detect across many action categories\n- **Novelty**: New behaviors emerge that weren't anticipated\n\n**Embedding approach**: Learn clip embeddings that capture spatiotemporal patterns. Similar actions cluster in embedding space; enable both classification of known actions and detection of anomalous behaviors.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show action recognition architecture\"}\nclass SlowFastEncoder(nn.Module):\n    \"\"\"SlowFast-style action recognition with dual pathways.\"\"\"\n    def __init__(self, embedding_dim: int = 512, n_actions: int = 50):\n        super().__init__()\n        # Slow pathway: high resolution, low frame rate\n        self.slow_conv = nn.Sequential(\n            nn.Conv3d(3, 64, (1, 7, 7), stride=(1, 2, 2), padding=(0, 3, 3)), nn.BatchNorm3d(64), nn.ReLU(),\n            nn.Conv3d(64, 128, (1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1)), nn.BatchNorm3d(128), nn.ReLU(),\n            nn.Conv3d(128, 256, (3, 3, 3), stride=(2, 2, 2), padding=1), nn.BatchNorm3d(256), nn.ReLU(),\n            nn.AdaptiveAvgPool3d(1))\n        # Fast pathway: low resolution, high frame rate\n        self.fast_conv = nn.Sequential(\n            nn.Conv3d(3, 8, (5, 7, 7), stride=(1, 2, 2), padding=(2, 3, 3)), nn.BatchNorm3d(8), nn.ReLU(),\n            nn.Conv3d(8, 32, (3, 3, 3), stride=(1, 2, 2), padding=(1, 1, 1)), nn.BatchNorm3d(32), nn.ReLU(),\n            nn.Conv3d(32, 64, (3, 3, 3), stride=(2, 2, 2), padding=1), nn.BatchNorm3d(64), nn.ReLU(),\n            nn.AdaptiveAvgPool3d(1))\n        self.fusion = nn.Sequential(nn.Linear(256 + 64, 512), nn.ReLU(), nn.Linear(512, embedding_dim))\n        self.classifier = nn.Linear(embedding_dim, n_actions)\n\n    def forward(self, slow_clip: torch.Tensor, fast_clip: torch.Tensor) -> tuple:\n        slow_feat = self.slow_conv(slow_clip).flatten(1)\n        fast_feat = self.fast_conv(fast_clip).flatten(1)\n        emb = F.normalize(self.fusion(torch.cat([slow_feat, fast_feat], dim=-1)), dim=-1)\n        return emb, self.classifier(emb)\n\nclass TemporalActionDetector(nn.Module):\n    \"\"\"Detect when actions occur in long videos.\"\"\"\n    def __init__(self, embedding_dim: int = 512, n_actions: int = 50):\n        super().__init__()\n        self.segment_encoder = nn.LSTM(embedding_dim, 256, batch_first=True, bidirectional=True)\n        self.action_classifier = nn.Linear(512, n_actions)\n        self.boundary_predictor = nn.Linear(512, 2)  # start, end\n\n    def forward(self, segment_embeddings: torch.Tensor) -> tuple:\n        lstm_out, _ = self.segment_encoder(segment_embeddings)\n        action_logits = self.action_classifier(lstm_out)\n        boundaries = torch.sigmoid(self.boundary_predictor(lstm_out))\n        return action_logits, boundaries\n\n# Usage example\naction_encoder = SlowFastEncoder(embedding_dim=512, n_actions=50)\n\n# Encode video with slow and fast pathways\nslow_clip = torch.randn(4, 3, 8, 224, 224)   # 8 frames at full resolution\nfast_clip = torch.randn(4, 3, 32, 224, 224)  # 32 frames for motion\naction_emb, action_logits = action_encoder(slow_clip, fast_clip)\nprint(f\"Action embeddings: {action_emb.shape}, logits: {action_logits.shape}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAction embeddings: torch.Size([4, 512]), logits: torch.Size([4, 50])\n```\n:::\n:::\n\n\n:::{.callout-tip}\n## Action Recognition Best Practices\n\n**Temporal modeling:**\n\n- **3D convolutions**: Capture spatiotemporal patterns directly\n- **Two-stream**: Separate RGB (appearance) and optical flow (motion) networks\n- **Temporal transformers**: Attention across frames for long-range dependencies\n- **Recurrent models**: LSTM/GRU for sequential action modeling\n- **Temporal segment networks**: Sample frames across action duration\n\n**Application-specific:**\n\n- **Retail**: Concealment detection, checkout behavior, customer service interactions\n- **Safety**: PPE compliance, unsafe actions, fall detection\n- **Security**: Loitering, tailgating, perimeter breach\n- **Healthcare**: Patient mobility, fall risk behaviors, staff compliance\n- **Traffic**: Accidents, wrong-way driving, pedestrian violations\n\n**Training strategies:**\n\n- **Clip sampling**: Random temporal crops during training\n- **Multi-scale**: Detect actions at different temporal granularities\n- **Weakly supervised**: Learn from video-level labels without frame annotations\n- **Self-supervised**: Pre-train on unlabeled video (temporal order, speed prediction)\n- **Transfer learning**: Fine-tune from Kinetics, AVA, or similar large datasets\n\n**Deployment:**\n\n- **Sliding window**: Apply classifier across video with overlap\n- **Action proposals**: First detect when actions occur, then classify\n- **Streaming inference**: Process video as it arrives without buffering\n- **Confidence calibration**: Reliable uncertainty for alerting decisions\n- **Contextual filtering**: Reduce false positives using scene context\n:::\n\n## Anomaly Detection in Video\n\nAnomaly detection identifies unusual events without requiring explicit training examples. **Embedding-based video anomaly detection** learns representations of normal behavior and flags deviations.\n\n### The Anomaly Detection Challenge\n\nTraditional supervised detection faces limitations:\n\n- **Rare events**: Anomalies are by definition uncommon; limited training data\n- **Unknown unknowns**: Can't train for events never seen before\n- **Context dependence**: Normal varies by time, location, and situation\n- **False positives**: Unusual but benign events trigger alerts\n- **Concept drift**: Normal behavior evolves over time\n\n**Embedding approach**: Learn compressed representations of normal video; anomalies have high reconstruction error or low likelihood under the learned model. No explicit anomaly labels required.\n\n::: {.cell execution_count=4}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show video anomaly detection architecture\"}\nclass VideoAutoencoder(nn.Module):\n    \"\"\"Autoencoder for learning normal video patterns.\"\"\"\n    def __init__(self, embedding_dim: int = 256):\n        super().__init__()\n        self.encoder = nn.Sequential(\n            nn.Conv3d(3, 64, (3, 4, 4), stride=(1, 2, 2), padding=(1, 1, 1)), nn.BatchNorm3d(64), nn.ReLU(),\n            nn.Conv3d(64, 128, (3, 4, 4), stride=(2, 2, 2), padding=(1, 1, 1)), nn.BatchNorm3d(128), nn.ReLU(),\n            nn.Conv3d(128, 256, (3, 4, 4), stride=(2, 2, 2), padding=(1, 1, 1)), nn.BatchNorm3d(256), nn.ReLU(),\n            nn.AdaptiveAvgPool3d(1))\n        self.latent_proj = nn.Linear(256, embedding_dim)\n        self.decoder_proj = nn.Linear(embedding_dim, 256 * 2 * 4 * 4)\n        self.decoder = nn.Sequential(\n            nn.ConvTranspose3d(256, 128, (3, 4, 4), stride=(2, 2, 2), padding=(1, 1, 1)), nn.BatchNorm3d(128), nn.ReLU(),\n            nn.ConvTranspose3d(128, 64, (3, 4, 4), stride=(2, 2, 2), padding=(1, 1, 1)), nn.BatchNorm3d(64), nn.ReLU(),\n            nn.ConvTranspose3d(64, 3, (3, 4, 4), stride=(1, 2, 2), padding=(1, 1, 1)), nn.Sigmoid())\n\n    def encode(self, video: torch.Tensor) -> torch.Tensor:\n        features = self.encoder(video).flatten(1)\n        return self.latent_proj(features)\n\n    def decode(self, latent: torch.Tensor, target_shape: tuple) -> torch.Tensor:\n        x = self.decoder_proj(latent).view(-1, 256, 2, 4, 4)\n        return F.interpolate(self.decoder(x), size=target_shape[2:], mode='trilinear')\n\n    def forward(self, video: torch.Tensor) -> tuple:\n        latent = self.encode(video)\n        reconstructed = self.decode(latent, video.shape)\n        return reconstructed, latent\n\nclass FramePredictionModel(nn.Module):\n    \"\"\"Predict future frames - anomalies are unpredictable.\"\"\"\n    def __init__(self, embedding_dim: int = 256):\n        super().__init__()\n        self.frame_encoder = nn.Sequential(\n            nn.Conv2d(3, 64, 4, stride=2, padding=1), nn.ReLU(),\n            nn.Conv2d(64, 128, 4, stride=2, padding=1), nn.ReLU(),\n            nn.Conv2d(128, 256, 4, stride=2, padding=1), nn.ReLU(),\n            nn.AdaptiveAvgPool2d(4))\n        self.temporal = nn.LSTM(256 * 16, embedding_dim, batch_first=True)\n        self.frame_decoder = nn.Sequential(\n            nn.ConvTranspose2d(embedding_dim, 128, 4, stride=2, padding=1), nn.ReLU(),\n            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1), nn.ReLU(),\n            nn.ConvTranspose2d(64, 3, 4, stride=2, padding=1), nn.Sigmoid())\n\n    def forward(self, frame_sequence: torch.Tensor) -> tuple:\n        batch, seq_len = frame_sequence.shape[:2]\n        frames_flat = frame_sequence.flatten(0, 1)\n        frame_feats = self.frame_encoder(frames_flat).flatten(1).view(batch, seq_len, -1)\n        lstm_out, _ = self.temporal(frame_feats)\n        pred_feats = lstm_out[:, -1].view(-1, lstm_out.shape[-1], 1, 1)\n        pred_feats = F.interpolate(pred_feats, size=(28, 28))\n        return self.frame_decoder(pred_feats), lstm_out[:, -1]\n\n# Usage example\nautoencoder = VideoAutoencoder(embedding_dim=256)\n\n# Detect anomalies by reconstruction error\nvideo_clip = torch.randn(4, 3, 16, 112, 112)  # [batch, C, T, H, W]\nreconstructed, latent = autoencoder(video_clip)\nrecon_error = F.mse_loss(reconstructed, video_clip, reduction='none').mean(dim=[1,2,3,4])\nprint(f\"Reconstruction errors: {recon_error}\")\nprint(f\"Latent embeddings: {latent.shape}\")  # [4, 256]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nReconstruction errors: tensor([1.2215, 1.2229, 1.2206, 1.2225], grad_fn=<MeanBackward1>)\nLatent embeddings: torch.Size([4, 256])\n```\n:::\n:::\n\n\n:::{.callout-tip}\n## Video Anomaly Detection Best Practices\n\n**Learning normal:**\n\n- **Autoencoders**: Reconstruct normal video; anomalies have high error\n- **Predictive models**: Predict future frames; anomalies are unpredictable\n- **Density estimation**: Model distribution of normal embeddings\n- **Memory networks**: Store prototypes of normal patterns\n- **Contrastive learning**: Learn features that distinguish normal variations\n\n**Anomaly scoring:**\n\n- **Reconstruction error**: Pixel or feature-level reconstruction loss\n- **Prediction error**: Difference between predicted and actual future\n- **Likelihood**: Probability under learned normal distribution\n- **Distance to normal**: Nearest neighbor distance in embedding space\n- **Ensemble**: Combine multiple scoring methods for robustness\n\n**Contextual adaptation:**\n\n- **Time-of-day**: Different normal patterns for day vs night\n- **Day-of-week**: Weekend vs weekday differences\n- **Camera-specific**: Learn separate models per camera\n- **Seasonal**: Adapt to weather and seasonal changes\n- **Event-aware**: Adjust thresholds during known events\n\n**Operational:**\n\n- **Threshold tuning**: Balance sensitivity vs false positive rate\n- **Alert fatigue**: Aggregate and prioritize alerts\n- **Human review**: Efficient interfaces for validating anomalies\n- **Feedback loops**: Learn from operator accept/reject decisions\n- **Continuous learning**: Update models as normal evolves\n:::\n\n:::{.callout-important}\n## Initializing Video Anomaly Detection: The First Week\n\nWhen deploying anomaly detection on a new camera, you face the bootstrap problem: what's \"normal\" for this specific view?\n\n**Day 1-2: Observation Mode**\n\n- Collect video without generating alerts\n- Extract frame embeddings every 1-5 seconds\n- Build initial embedding distribution\n- **Do not alert**—you're learning, not detecting\n\n**Day 3-5: Baseline Calibration**\n\n- Train autoencoder or density model on collected normal data\n- Set initial threshold at 99th percentile of reconstruction error (very conservative)\n- Generate \"silent alerts\" for internal review\n- Have operators label obvious anomalies and false positives\n\n**Day 6-7: Soft Launch**\n\n- Enable alerts with high threshold (low sensitivity)\n- Collect operator feedback (accept/reject)\n- Adjust threshold based on acceptable false positive rate\n\n**Minimum data requirements:**\n\n| Model Type | Minimum Video | Notes |\n|------------|---------------|-------|\n| Autoencoder | 24-48 hours | Need full day/night cycle |\n| Prediction model | 48-72 hours | Need temporal patterns |\n| Density estimation | 72+ hours | Need robust distribution |\n\n**Handling time-of-day variations:**\n\nOption 1: Train separate models for day/night/shift changes\nOption 2: Include time features in embedding (hour-of-day, day-of-week)\nOption 3: Use time-aware thresholds (tighter at night when less activity expected)\n\n**What if anomalies occur during baseline collection?**\n\n- Rare anomalies won't significantly impact model (<<1% of frames)\n- Model learns the dominant pattern, not outliers\n- After deployment, retroactively flag baseline anomalies using trained model\n\n**Camera-specific vs. shared models:**\n\n| Approach | Pros | Cons |\n|----------|------|------|\n| Per-camera model | Optimal accuracy | More training time per camera |\n| Shared backbone + camera head | Faster deployment | May miss camera-specific patterns |\n| Transfer learning | Best of both | Requires model architecture design |\n\nFor large deployments (100+ cameras), start with shared backbone trained on diverse cameras, then fine-tune per-camera heads.\n:::\n\n## Forensic Video Search\n\nForensic search enables rapid retrieval of specific events, people, or objects across large video archives. **Embedding-based video search** indexes footage for semantic queries across weeks or months of recordings.\n\n### The Forensic Search Challenge\n\nTraditional video review faces limitations:\n\n- **Volume**: Reviewing hours of footage manually is impractical\n- **Speed**: Investigations need answers in minutes, not days\n- **Precision**: Finding specific moments in vast archives\n- **Multi-camera**: Events may span multiple camera views\n- **Retention**: Archives may span weeks to years\n\n**Embedding approach**: Index video with frame and clip embeddings; enable semantic search by example (find similar events), by description (natural language queries), or by structured attributes (person wearing red, vehicle type).\n\n::: {.cell execution_count=5}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show forensic video search architecture\"}\nclass VideoIndexer(nn.Module):\n    \"\"\"Index video for semantic search.\"\"\"\n    def __init__(self, embedding_dim: int = 512):\n        super().__init__()\n        self.frame_encoder = FrameEncoder(VideoConfig(embedding_dim=embedding_dim))\n        self.clip_encoder = ClipEncoder(VideoConfig(embedding_dim=embedding_dim))\n        self.attribute_head = nn.Sequential(\n            nn.Linear(embedding_dim, 256), nn.ReLU(),\n            nn.Linear(256, 100))  # color, object type, etc.\n\n    def index_frame(self, frame: torch.Tensor) -> tuple:\n        emb = self.frame_encoder(frame)\n        attrs = torch.sigmoid(self.attribute_head(emb))\n        return emb, attrs\n\n    def index_clip(self, clip: torch.Tensor) -> torch.Tensor:\n        return self.clip_encoder(clip)\n\nclass TextToVideoEncoder(nn.Module):\n    \"\"\"Encode text queries for video search.\"\"\"\n    def __init__(self, vocab_size: int = 30000, embedding_dim: int = 512):\n        super().__init__()\n        self.token_embed = nn.Embedding(vocab_size, 256)\n        self.encoder = nn.LSTM(256, 256, batch_first=True, bidirectional=True)\n        self.proj = nn.Linear(512, embedding_dim)\n\n    def forward(self, token_ids: torch.Tensor) -> torch.Tensor:\n        x = self.token_embed(token_ids)\n        _, (hidden, _) = self.encoder(x)\n        hidden = torch.cat([hidden[-2], hidden[-1]], dim=-1)\n        return F.normalize(self.proj(hidden), dim=-1)\n\nclass ForensicSearchSystem:\n    \"\"\"Search video archives by example, text, or attributes.\"\"\"\n    def __init__(self, embedding_dim: int = 512):\n        self.indexer = VideoIndexer(embedding_dim)\n        self.text_encoder = TextToVideoEncoder(embedding_dim=embedding_dim)\n        self.index_embeddings: list[torch.Tensor] = []\n        self.index_metadata: list[dict] = []\n\n    def search_by_example(self, query_frame: torch.Tensor, k: int = 10) -> list[dict]:\n        query_emb, _ = self.indexer.index_frame(query_frame)\n        if not self.index_embeddings:\n            return []\n        index = torch.stack(self.index_embeddings)\n        sims = F.cosine_similarity(query_emb.unsqueeze(0), index)\n        top_k = torch.topk(sims, k=min(k, len(self.index_embeddings)))\n        return [{\"idx\": i.item(), \"sim\": s.item()} for i, s in zip(top_k.indices, top_k.values)]\n\n# Usage example\nsearch_system = ForensicSearchSystem(embedding_dim=512)\n\n# Index a query frame\nquery_frame = torch.randn(1, 3, 224, 224)\nquery_emb, attributes = search_system.indexer.index_frame(query_frame)\nprint(f\"Query embedding: {query_emb.shape}, Attributes: {attributes.shape}\")\n\n# Encode text query\ntext_encoder = TextToVideoEncoder(embedding_dim=512)\nquery_tokens = torch.randint(0, 30000, (1, 10))  # \"person in red shirt\"\ntext_emb = text_encoder(query_tokens)\nprint(f\"Text query embedding: {text_emb.shape}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nQuery embedding: torch.Size([1, 512]), Attributes: torch.Size([1, 100])\nText query embedding: torch.Size([1, 512])\n```\n:::\n:::\n\n\n:::{.callout-tip}\n## Forensic Search Best Practices\n\n**Indexing:**\n\n- **Keyframe selection**: Index representative frames, not every frame\n- **Multi-granularity**: Frame embeddings for appearance, clip embeddings for action\n- **Attribute extraction**: Structured metadata (colors, object types, counts)\n- **Scene segmentation**: Detect shot boundaries and scene changes\n- **Incremental indexing**: Add new footage without full re-index\n\n**Query types:**\n\n- **Query by example**: Find similar to this image/clip\n- **Attribute search**: \"Person in red shirt\", \"white sedan\"\n- **Natural language**: \"Person running through parking lot\"\n- **Composite queries**: Combine multiple constraints\n- **Temporal queries**: \"What happened before/after this event\"\n\n**Search efficiency:**\n\n- **Approximate nearest neighbor**: HNSW, IVF for sub-second search\n- **Temporal pruning**: Limit search to relevant time windows\n- **Camera filtering**: Search only relevant camera subset\n- **Progressive refinement**: Fast initial filter, detailed re-ranking\n- **Result clustering**: Group similar results for efficient review\n\n**User interface:**\n\n- **Timeline visualization**: Show result distribution over time\n- **Multi-camera view**: Synchronized playback across cameras\n- **Result preview**: Quick thumbnails before full video load\n- **Relevance feedback**: Refine search based on user selections\n- **Export**: Extract clips for evidence or sharing\n:::\n\n## Industry Applications\n\nVideo surveillance embeddings enable diverse applications across industries, each with specific requirements and use cases.\n\n### Retail Loss Prevention\n\nRetail environments use video analytics for loss prevention, customer experience, and operations optimization.\n\n::: {.cell execution_count=6}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show retail analytics architecture\"}\nclass RetailBehaviorEncoder(nn.Module):\n    \"\"\"Encode shopper behavior for loss prevention and analytics.\"\"\"\n    def __init__(self, embedding_dim: int = 256, n_behaviors: int = 20):\n        super().__init__()\n        self.pose_encoder = nn.Sequential(\n            nn.Linear(34, 128), nn.ReLU(), nn.Linear(128, 128))  # 17 keypoints x 2\n        self.trajectory_encoder = nn.LSTM(2, 64, batch_first=True, bidirectional=True)\n        self.scene_encoder = nn.Sequential(\n            nn.Conv2d(3, 64, 3, stride=2, padding=1), nn.ReLU(),\n            nn.Conv2d(64, 128, 3, stride=2, padding=1), nn.ReLU(),\n            nn.AdaptiveAvgPool2d(1))\n        self.fusion = nn.Sequential(\n            nn.Linear(128 + 128 + 128, 256), nn.ReLU(), nn.Linear(256, embedding_dim))\n        self.behavior_classifier = nn.Linear(embedding_dim, n_behaviors)\n\n    def forward(self, pose: torch.Tensor, trajectory: torch.Tensor, scene: torch.Tensor) -> tuple:\n        pose_emb = self.pose_encoder(pose.flatten(1))\n        _, (traj_hidden, _) = self.trajectory_encoder(trajectory)\n        # Transpose bidirectional LSTM hidden: (num_directions, batch, hidden) -> (batch, directions*hidden)\n        traj_emb = traj_hidden.transpose(0, 1).flatten(1)\n        scene_emb = self.scene_encoder(scene).flatten(1)\n        fused = self.fusion(torch.cat([pose_emb, traj_emb, scene_emb], dim=-1))\n        emb = F.normalize(fused, dim=-1)\n        return emb, self.behavior_classifier(emb)\n\nclass ProductInteractionEncoder(nn.Module):\n    \"\"\"Encode customer-product interactions for conversion analysis.\"\"\"\n    def __init__(self, embedding_dim: int = 128):\n        super().__init__()\n        self.hand_encoder = nn.Sequential(nn.Linear(42, 64), nn.ReLU(), nn.Linear(64, 64))\n        self.product_encoder = nn.Sequential(nn.Linear(256, 128), nn.ReLU(), nn.Linear(128, 64))\n        self.fusion = nn.Sequential(nn.Linear(128, 128), nn.ReLU(), nn.Linear(128, embedding_dim))\n\n    def forward(self, hand_pose: torch.Tensor, product_features: torch.Tensor) -> torch.Tensor:\n        hand_emb = self.hand_encoder(hand_pose.flatten(1))\n        prod_emb = self.product_encoder(product_features)\n        return F.normalize(self.fusion(torch.cat([hand_emb, prod_emb], dim=-1)), dim=-1)\n\n# Usage example\nbehavior_encoder = RetailBehaviorEncoder(embedding_dim=256, n_behaviors=20)\n\n# Analyze shopper behavior\npose_keypoints = torch.randn(4, 17, 2)  # 17 body keypoints\ntrajectory = torch.randn(4, 30, 2)  # 30 timesteps of x,y positions\nscene_crop = torch.randn(4, 3, 64, 64)\nbehavior_emb, behavior_logits = behavior_encoder(pose_keypoints, trajectory, scene_crop)\nprint(f\"Behavior embeddings: {behavior_emb.shape}\")  # [4, 256]\n\n# Behavior classification (concealment, browsing, etc.)\npredicted_behavior = torch.argmax(behavior_logits, dim=-1)\nprint(f\"Predicted behaviors: {predicted_behavior}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nBehavior embeddings: torch.Size([4, 256])\nPredicted behaviors: tensor([18,  2,  2, 16])\n```\n:::\n:::\n\n\n:::{.callout-tip}\n## Retail Video Analytics\n\n**Loss prevention:**\n\n- **Concealment detection**: Identify potential shoplifting behavior\n- **Checkout exceptions**: Detect scan avoidance, sweethearting\n- **Fitting room monitoring**: Track items in vs out (respecting privacy)\n- **Exit alerts**: Match items leaving with purchases\n- **Evidence retrieval**: Rapid search for incident documentation\n\n**Customer analytics:**\n\n- **Traffic patterns**: Understand store flow and congestion\n- **Dwell time**: Measure engagement at displays\n- **Queue management**: Monitor wait times, open registers proactively\n- **Demographics**: Aggregate (not individual) customer composition\n- **Conversion analysis**: Correlate behavior with purchases\n\n**Operations:**\n\n- **Staffing optimization**: Align staff with traffic patterns\n- **Planogram compliance**: Verify display setup\n- **Cleanliness monitoring**: Detect spills, maintenance needs\n- **Delivery verification**: Confirm vendor deliveries\n- **Safety compliance**: Employee safety behaviors\n:::\n\n### Smart City Public Safety\n\nSmart cities deploy video analytics for traffic management, public safety, and urban planning.\n\n::: {.cell execution_count=7}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show smart city analytics architecture\"}\nclass VehicleEncoder(nn.Module):\n    \"\"\"Encode vehicles for traffic analysis and tracking.\"\"\"\n    def __init__(self, embedding_dim: int = 256, n_vehicle_types: int = 10):\n        super().__init__()\n        self.appearance_encoder = nn.Sequential(\n            nn.Conv2d(3, 64, 3, stride=2, padding=1), nn.BatchNorm2d(64), nn.ReLU(),\n            nn.Conv2d(64, 128, 3, stride=2, padding=1), nn.BatchNorm2d(128), nn.ReLU(),\n            nn.Conv2d(128, 256, 3, stride=2, padding=1), nn.BatchNorm2d(256), nn.ReLU(),\n            nn.AdaptiveAvgPool2d(1))\n        self.proj = nn.Linear(256, embedding_dim)\n        self.type_classifier = nn.Linear(embedding_dim, n_vehicle_types)\n        self.color_classifier = nn.Linear(embedding_dim, 12)  # common colors\n\n    def forward(self, vehicle_crops: torch.Tensor) -> tuple:\n        features = self.appearance_encoder(vehicle_crops).flatten(1)\n        emb = F.normalize(self.proj(features), dim=-1)\n        return emb, self.type_classifier(emb), self.color_classifier(emb)\n\nclass TrafficFlowEncoder(nn.Module):\n    \"\"\"Encode traffic patterns for congestion analysis.\"\"\"\n    def __init__(self, embedding_dim: int = 256):\n        super().__init__()\n        self.spatial_encoder = nn.Sequential(\n            nn.Conv2d(3, 64, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n            nn.Conv2d(64, 128, 3, padding=1), nn.ReLU(), nn.AdaptiveAvgPool2d(8))\n        self.temporal_encoder = nn.LSTM(128 * 64, 256, batch_first=True)\n        self.proj = nn.Linear(256, embedding_dim)\n\n    def forward(self, frame_sequence: torch.Tensor) -> torch.Tensor:\n        batch, seq_len = frame_sequence.shape[:2]\n        frames_flat = frame_sequence.flatten(0, 1)\n        spatial_feats = self.spatial_encoder(frames_flat).flatten(1).view(batch, seq_len, -1)\n        _, (hidden, _) = self.temporal_encoder(spatial_feats)\n        return F.normalize(self.proj(hidden[-1]), dim=-1)\n\nclass CrowdDensityEncoder(nn.Module):\n    \"\"\"Encode crowd density for public safety monitoring.\"\"\"\n    def __init__(self, embedding_dim: int = 128):\n        super().__init__()\n        self.density_cnn = nn.Sequential(\n            nn.Conv2d(3, 64, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n            nn.Conv2d(64, 128, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n            nn.Conv2d(128, 256, 3, padding=1), nn.ReLU())\n        self.density_regressor = nn.Conv2d(256, 1, 1)  # density map\n        self.embedding = nn.Sequential(nn.AdaptiveAvgPool2d(1), nn.Flatten(), nn.Linear(256, embedding_dim))\n\n    def forward(self, scene: torch.Tensor) -> tuple:\n        features = self.density_cnn(scene)\n        density_map = F.relu(self.density_regressor(features))\n        emb = F.normalize(self.embedding(features), dim=-1)\n        return emb, density_map\n\n# Usage example\nvehicle_encoder = VehicleEncoder(embedding_dim=256)\ntraffic_encoder = TrafficFlowEncoder(embedding_dim=256)\n\n# Encode vehicle crops for tracking\nvehicle_crops = torch.randn(4, 3, 128, 256)\nvehicle_emb, type_logits, color_logits = vehicle_encoder(vehicle_crops)\nprint(f\"Vehicle embeddings: {vehicle_emb.shape}\")  # [4, 256]\n\n# Encode traffic flow over time\ntraffic_frames = torch.randn(2, 10, 3, 480, 640)  # 10 frames\ntraffic_emb = traffic_encoder(traffic_frames)\nprint(f\"Traffic flow embeddings: {traffic_emb.shape}\")  # [2, 256]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nVehicle embeddings: torch.Size([4, 256])\nTraffic flow embeddings: torch.Size([2, 256])\n```\n:::\n:::\n\n\n:::{.callout-tip}\n## Smart City Video Analytics\n\n**Traffic management:**\n\n- **Vehicle counting**: Traffic volume by time and location\n- **Speed estimation**: Detect speeding, traffic flow\n- **Incident detection**: Accidents, breakdowns, debris\n- **Parking management**: Occupancy, violations, guidance\n- **Signal optimization**: Adaptive timing based on real-time flow\n\n**Public safety:**\n\n- **Crowd monitoring**: Density, flow, anomalies\n- **Incident detection**: Fights, falls, medical emergencies\n- **Abandoned objects**: Unattended bags, packages\n- **Perimeter security**: Intrusion detection at restricted areas\n- **Emergency response**: Rapid situation assessment\n\n**Urban planning:**\n\n- **Pedestrian patterns**: Sidewalk usage, crossing behavior\n- **Public space utilization**: Park, plaza usage patterns\n- **Infrastructure monitoring**: Bridge, tunnel conditions\n- **Environmental monitoring**: Flooding, smoke detection\n- **Accessibility assessment**: Mobility aid usage patterns\n:::\n\n### Manufacturing Safety Compliance\n\nManufacturing facilities use video analytics for safety monitoring, quality control, and process optimization.\n\n::: {.cell execution_count=8}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show manufacturing safety architecture\"}\nclass PPEDetector(nn.Module):\n    \"\"\"Detect personal protective equipment compliance.\"\"\"\n    def __init__(self, embedding_dim: int = 256, n_ppe_types: int = 6):\n        super().__init__()\n        self.backbone = nn.Sequential(\n            nn.Conv2d(3, 64, 3, stride=2, padding=1), nn.BatchNorm2d(64), nn.ReLU(),\n            nn.Conv2d(64, 128, 3, stride=2, padding=1), nn.BatchNorm2d(128), nn.ReLU(),\n            nn.Conv2d(128, 256, 3, stride=2, padding=1), nn.BatchNorm2d(256), nn.ReLU(),\n            nn.AdaptiveAvgPool2d(1))\n        self.embedding = nn.Linear(256, embedding_dim)\n        self.ppe_classifier = nn.Linear(embedding_dim, n_ppe_types)  # hard hat, vest, goggles, etc.\n\n    def forward(self, person_crops: torch.Tensor) -> tuple:\n        features = self.backbone(person_crops).flatten(1)\n        emb = F.normalize(self.embedding(features), dim=-1)\n        ppe_logits = self.ppe_classifier(emb)\n        return emb, torch.sigmoid(ppe_logits)\n\nclass SafeZoneMonitor(nn.Module):\n    \"\"\"Monitor restricted zones and safe distances.\"\"\"\n    def __init__(self, embedding_dim: int = 128):\n        super().__init__()\n        self.scene_encoder = nn.Sequential(\n            nn.Conv2d(3, 64, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n            nn.Conv2d(64, 128, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n            nn.Conv2d(128, 256, 3, padding=1), nn.ReLU(), nn.AdaptiveAvgPool2d(4))\n        self.position_encoder = nn.Sequential(nn.Linear(2, 64), nn.ReLU(), nn.Linear(64, 64))\n        self.fusion = nn.Sequential(nn.Linear(256 * 16 + 64, 256), nn.ReLU(), nn.Linear(256, embedding_dim))\n        self.zone_classifier = nn.Linear(embedding_dim, 5)  # zone types\n        self.violation_detector = nn.Linear(embedding_dim, 1)\n\n    def forward(self, scene: torch.Tensor, positions: torch.Tensor) -> tuple:\n        scene_feat = self.scene_encoder(scene).flatten(1)\n        pos_feat = self.position_encoder(positions)\n        fused = self.fusion(torch.cat([scene_feat, pos_feat], dim=-1))\n        emb = F.normalize(fused, dim=-1)\n        return emb, self.zone_classifier(emb), torch.sigmoid(self.violation_detector(emb))\n\n# Usage example\nppe_detector = PPEDetector(embedding_dim=256, n_ppe_types=6)\nzone_monitor = SafeZoneMonitor(embedding_dim=128)\n\n# Detect PPE compliance\nworker_crops = torch.randn(4, 3, 128, 64)\nppe_emb, ppe_probs = ppe_detector(worker_crops)\nprint(f\"PPE embeddings: {ppe_emb.shape}\")  # [4, 256]\nprint(f\"PPE detection (hard hat, vest, goggles...): {ppe_probs[0]}\")\n\n# Monitor zone compliance\nscene_frame = torch.randn(1, 3, 480, 640)\nworker_positions = torch.randn(1, 2)  # normalized x, y\nzone_emb, zone_logits, violation_prob = zone_monitor(scene_frame, worker_positions)\nprint(f\"Violation probability: {violation_prob.item():.3f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPPE embeddings: torch.Size([4, 256])\nPPE detection (hard hat, vest, goggles...): tensor([0.4915, 0.5102, 0.4795, 0.4965, 0.4976, 0.5100],\n       grad_fn=<SelectBackward0>)\nViolation probability: 0.484\n```\n:::\n:::\n\n\n:::{.callout-tip}\n## Manufacturing Video Analytics\n\n**Safety compliance:**\n\n- **PPE detection**: Hard hats, safety vests, goggles, gloves\n- **Zone monitoring**: Restricted area access, safe distances\n- **Unsafe behavior**: Running, improper lifting, horseplay\n- **Emergency detection**: Falls, injuries, equipment incidents\n- **Compliance reporting**: Automated safety audits\n\n**Quality control:**\n\n- **Defect detection**: Visual inspection of products\n- **Assembly verification**: Correct parts, proper installation\n- **Process monitoring**: Adherence to standard procedures\n- **Measurement**: Dimensional verification via vision\n- **Traceability**: Link video to production records\n\n**Operations:**\n\n- **Equipment monitoring**: Abnormal operation detection\n- **Workflow analysis**: Cycle time, bottleneck identification\n- **Inventory tracking**: Material movement, levels\n- **Maintenance**: Predictive maintenance from visual indicators\n- **Training**: Capture best practices, identify coaching opportunities\n:::\n\n### Healthcare Patient Safety\n\nHealthcare facilities use video analytics for patient safety, operational efficiency, and quality improvement.\n\n:::{.callout-tip}\n## Healthcare Video Analytics\n\n**Patient safety:**\n\n- **Fall detection**: Immediate alerts for patient falls\n- **Wandering prevention**: Dementia patient monitoring\n- **Bed exit detection**: Alert when at-risk patients attempt to leave bed\n- **Patient activity**: Mobility tracking for recovery assessment\n- **Emergency detection**: Rapid response to medical emergencies\n\n**Infection control:**\n\n- **Hand hygiene**: Monitor compliance with wash requirements\n- **PPE compliance**: Mask, gown, glove usage in appropriate areas\n- **Contact tracing**: Retrospective tracking for outbreak investigation\n- **Isolation compliance**: Monitor isolation room protocols\n- **Visitor management**: Enforce visiting policies\n\n**Operations:**\n\n- **Wait time monitoring**: Emergency department, clinic queues\n- **Room utilization**: OR, exam room efficiency\n- **Staff workflow**: Movement patterns, task analysis\n- **Equipment tracking**: Locate mobile equipment\n- **Capacity management**: Real-time bed availability\n:::\n\n## Privacy-Preserving Video Analytics\n\nPrivacy concerns require techniques that extract value from video while protecting individual privacy.\n\n### Privacy Protection Techniques\n\n::: {.cell execution_count=9}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show privacy-preserving analytics architecture\"}\nclass FaceAnonymizer(nn.Module):\n    \"\"\"Detect and blur faces for privacy protection.\"\"\"\n    def __init__(self, detection_threshold: float = 0.8):\n        super().__init__()\n        self.face_detector = nn.Sequential(\n            nn.Conv2d(3, 32, 3, stride=2, padding=1), nn.ReLU(),\n            nn.Conv2d(32, 64, 3, stride=2, padding=1), nn.ReLU(),\n            nn.Conv2d(64, 128, 3, stride=2, padding=1), nn.ReLU(),\n            nn.Conv2d(128, 5, 1))  # 4 bbox coords + confidence\n        self.threshold = detection_threshold\n\n    def detect_faces(self, image: torch.Tensor) -> torch.Tensor:\n        detections = self.face_detector(image)\n        return detections.permute(0, 2, 3, 1)  # [batch, H, W, 5]\n\n    def blur_faces(self, image: torch.Tensor, detections: torch.Tensor) -> torch.Tensor:\n        # Simplified: in practice would apply Gaussian blur to detected regions\n        return image  # Return original for demo\n\nclass PrivacyPreservingEncoder(nn.Module):\n    \"\"\"Extract embeddings without identifiable features.\"\"\"\n    def __init__(self, embedding_dim: int = 256):\n        super().__init__()\n        # Encode only motion and pose, not appearance\n        self.pose_encoder = nn.Sequential(\n            nn.Linear(34, 128), nn.ReLU(), nn.Linear(128, embedding_dim))\n        self.motion_encoder = nn.Sequential(\n            nn.Linear(34 * 2, 128), nn.ReLU(), nn.Linear(128, embedding_dim))\n        self.fusion = nn.Sequential(\n            nn.Linear(embedding_dim * 2, 256), nn.ReLU(), nn.Linear(256, embedding_dim))\n\n    def forward(self, pose: torch.Tensor, motion: torch.Tensor) -> torch.Tensor:\n        pose_emb = self.pose_encoder(pose.flatten(1))\n        motion_emb = self.motion_encoder(motion.flatten(1))\n        return F.normalize(self.fusion(torch.cat([pose_emb, motion_emb], dim=-1)), dim=-1)\n\nclass DifferentialPrivacyWrapper(nn.Module):\n    \"\"\"Add differential privacy noise to embeddings.\"\"\"\n    def __init__(self, base_encoder: nn.Module, epsilon: float = 1.0, delta: float = 1e-5):\n        super().__init__()\n        self.encoder = base_encoder\n        self.epsilon = epsilon\n        self.delta = delta\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        embedding = self.encoder(x)\n        noise_scale = 2.0 / self.epsilon  # Simplified Laplace mechanism\n        noise = torch.zeros_like(embedding).uniform_(-noise_scale, noise_scale)\n        return F.normalize(embedding + noise, dim=-1)\n\n# Usage example\nprivacy_encoder = PrivacyPreservingEncoder(embedding_dim=256)\nanonymizer = FaceAnonymizer()\n\n# Encode behavior without identifying appearance\npose_keypoints = torch.randn(4, 17, 2)  # Skeleton only\nmotion_flow = torch.randn(4, 17, 2, 2)  # Pose change over time\nprivate_emb = privacy_encoder(pose_keypoints, motion_flow.flatten(-2))\nprint(f\"Privacy-preserving embeddings: {private_emb.shape}\")\n\n# Detect and anonymize faces\nimage = torch.randn(1, 3, 480, 640)\nface_detections = anonymizer.detect_faces(image)\nprint(f\"Face detections shape: {face_detections.shape}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPrivacy-preserving embeddings: torch.Size([4, 256])\nFace detections shape: torch.Size([1, 60, 80, 5])\n```\n:::\n:::\n\n\n:::{.callout-tip}\n## Privacy-Preserving Techniques\n\n**Data minimization:**\n\n- **Edge processing**: Analyze on-camera, transmit only metadata\n- **Face blurring**: Automatic face detection and anonymization\n- **Body abstraction**: Replace people with silhouettes or skeletons\n- **Selective recording**: Only record when events detected\n- **Retention limits**: Automatic deletion after defined period\n\n**Technical measures:**\n\n- **Differential privacy**: Add noise to aggregate statistics\n- **Federated learning**: Train models without centralizing video\n- **Secure computation**: Encrypted video analysis\n- **Access controls**: Role-based access to video and analytics\n- **Audit logging**: Track all video access and queries\n\n**Policy measures:**\n\n- **Notice**: Clear signage about video monitoring\n- **Purpose limitation**: Define and enforce allowed use cases\n- **Data governance**: Policies for access, retention, sharing\n- **Impact assessments**: Evaluate privacy implications\n- **Regular audits**: Verify compliance with policies\n\n**Bias mitigation:**\n\n- **Demographic testing**: Evaluate accuracy across groups\n- **Training data diversity**: Representative training sets\n- **Threshold calibration**: Equal error rates across demographics\n- **Human review**: Require human confirmation for consequential actions\n- **Continuous monitoring**: Track disparate impact in production\n:::\n\n## Key Takeaways\n\n:::{.callout-note}\nThe performance metrics in the takeaways below are illustrative based on published research and industry benchmarks. They represent achievable performance but are not verified results from specific deployments.\n:::\n\n- **Real-time video processing at scale requires hierarchical, edge-cloud architectures**: Processing thousands of concurrent streams demands efficient frame embedding extraction (>100 fps per GPU), edge preprocessing to reduce bandwidth, hierarchical detection (fast filter then accurate classifier), and horizontal scaling with load balancing—achieving sub-second detection latency while managing compute costs\n\n- **Person re-identification enables tracking without biometric identification**: Appearance-based embeddings capture clothing, body shape, and gait patterns robust to pose and lighting changes, achieving 80-95% rank-1 accuracy across camera networks while avoiding face recognition privacy concerns—though still requiring careful governance around tracking scope and retention\n\n- **Action recognition detects behaviors through temporal embeddings**: 3D convolutions, two-stream networks, and temporal transformers capture spatiotemporal patterns for detecting activities from shoplifting behaviors to safety violations to customer interactions, with domain-specific fine-tuning achieving 85-95% accuracy on targeted action sets\n\n- **Anomaly detection identifies unusual events without explicit training examples**: Learning normal behavior patterns through autoencoders, prediction models, and density estimation enables detection of arbitrary anomalies—achieving 70-90% detection with <5% false positive rates when properly tuned to specific camera contexts and time patterns\n\n- **Forensic video search transforms archives into queryable databases**: Indexing keyframes and clips with embeddings enables semantic search across weeks of footage in seconds—finding specific people, objects, or events through query-by-example, attribute search, or natural language without manual review of hours of video\n\n- **Industry applications share common technical foundations with domain-specific requirements**: Retail (loss prevention, customer analytics), smart cities (traffic, public safety), manufacturing (safety compliance, quality), and healthcare (patient safety, infection control) all leverage the same core embedding techniques with specialized models, thresholds, and integration requirements\n\n- **Privacy-preserving analytics must be designed in from the start**: Edge processing, face blurring, purpose limitation, retention policies, access controls, and bias testing are not afterthoughts—they determine whether video analytics deployments are legally compliant, ethically acceptable, and trusted by the people being monitored\n\n## Looking Ahead\n\nThe next chapter, @sec-entity-resolution, addresses a fundamental cross-industry challenge: identifying and linking records that refer to the same real-world entities across disparate data sources—a problem that scales to trillions of comparison pairs and underpins applications from customer deduplication to fraud detection to knowledge graph construction.\n\n## Further Reading\n\n### Video Understanding and Recognition\n- Carreira, Joao, and Andrew Zisserman (2017). \"Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset.\" CVPR.\n- Feichtenhofer, Christoph, et al. (2019). \"SlowFast Networks for Video Recognition.\" ICCV.\n- Arnab, Anurag, et al. (2021). \"ViViT: A Video Vision Transformer.\" ICCV.\n- Tran, Du, et al. (2015). \"Learning Spatiotemporal Features with 3D Convolutional Networks.\" ICCV.\n- Wang, Limin, et al. (2016). \"Temporal Segment Networks: Towards Good Practices for Deep Action Recognition.\" ECCV.\n\n### Person Re-Identification\n- Ye, Mang, et al. (2021). \"Deep Learning for Person Re-identification: A Survey and Outlook.\" IEEE TPAMI.\n- Luo, Hao, et al. (2019). \"Bag of Tricks and a Strong Baseline for Deep Person Re-identification.\" CVPR Workshops.\n- Sun, Yifan, et al. (2018). \"Beyond Part Models: Person Retrieval with Refined Part Pooling.\" ECCV.\n- He, Shuting, et al. (2021). \"TransReID: Transformer-based Object Re-Identification.\" ICCV.\n- Zheng, Liang, et al. (2015). \"Scalable Person Re-identification: A Benchmark.\" ICCV.\n\n### Video Anomaly Detection\n- Liu, Wen, et al. (2018). \"Future Frame Prediction for Anomaly Detection – A New Baseline.\" CVPR.\n- Sultani, Waqas, et al. (2018). \"Real-World Anomaly Detection in Surveillance Videos.\" CVPR.\n- Park, Hyunjong, et al. (2020). \"Learning Memory-guided Normality for Anomaly Detection.\" CVPR.\n- Georgescu, Mariana-Iuliana, et al. (2021). \"Anomaly Detection in Video via Self-Supervised and Multi-Task Learning.\" CVPR.\n- Ramachandra, Bharathkumar, and Michael Jones (2020). \"Street Scene: A New Dataset and Evaluation Protocol for Video Anomaly Detection.\" WACV.\n\n### Video Retrieval and Search\n- Miech, Antoine, et al. (2019). \"HowTo100M: Learning a Text-Video Embedding by Watching Hundred Million Narrated Video Clips.\" ICCV.\n- Bain, Max, et al. (2021). \"Frozen in Time: A Joint Video and Image Encoder for End-to-End Retrieval.\" ICCV.\n- Xu, Jun, et al. (2016). \"MSR-VTT: A Large Video Description Dataset for Bridging Video and Language.\" CVPR.\n- Lei, Jie, et al. (2021). \"Less is More: ClipBERT for Video-and-Language Learning via Sparse Sampling.\" CVPR.\n- Gabeur, Valentin, et al. (2020). \"Multi-modal Transformer for Video Retrieval.\" ECCV.\n\n### Retail and Smart City Analytics\n- Hampapur, Arun, et al. (2005). \"Smart Video Surveillance: Exploring the Concept of Multiscale Spatiotemporal Tracking.\" IEEE Signal Processing Magazine.\n- Collins, Robert T., et al. (2000). \"A System for Video Surveillance and Monitoring.\" Carnegie Mellon University Technical Report.\n- Senior, Andrew, et al. (2006). \"Appearance Models for Occlusion Handling.\" Image and Vision Computing.\n- Yilmaz, Alper, et al. (2006). \"Object Tracking: A Survey.\" ACM Computing Surveys.\n- Zhang, Shanshan, et al. (2016). \"How Far are We from Solving Pedestrian Detection?\" CVPR.\n\n### Privacy and Ethics in Video Surveillance\n- Cavallaro, Andrea (2007). \"Privacy in Video Surveillance.\" IEEE Signal Processing Magazine.\n- Senior, Andrew, et al. (2005). \"Enabling Video Privacy through Computer Vision.\" IEEE Security & Privacy.\n- Winkler, Thomas, and Bernhard Rinner (2014). \"Security and Privacy Protection in Visual Sensor Networks: A Survey.\" ACM Computing Surveys.\n- Dwork, Cynthia, and Aaron Roth (2014). \"The Algorithmic Foundations of Differential Privacy.\" Foundations and Trends in Theoretical Computer Science.\n- Buolamwini, Joy, and Timnit Gebru (2018). \"Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification.\" FAT*.\n\n",
    "supporting": [
      "ch27_video_surveillance_analytics_files/figure-pdf"
    ],
    "filters": []
  }
}