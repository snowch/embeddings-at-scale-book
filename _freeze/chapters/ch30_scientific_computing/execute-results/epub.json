{
  "hash": "bf224e9b9c1464376de14d35696cafcc",
  "result": {
    "engine": "jupyter",
    "markdown": "# Scientific Computing and Research {#sec-scientific-computing}\n\n:::{.callout-note}\n## Chapter Overview\nScientific computing—from astrophysics to climate science to materials discovery—faces challenges of extreme data scales, complex physical constraints, and multi-modal observations spanning instruments worldwide. This chapter applies embeddings to scientific frontiers: astrophysics applications using image and spectral embeddings to classify galaxies, detect gravitational waves, and discover exoplanets from telescope data at petabyte scale, climate and earth science with spatio-temporal embeddings for weather prediction, climate modeling, and satellite imagery analysis, materials science acceleration using atomic graph embeddings to predict material properties and discover novel compounds, particle physics analysis with point cloud embeddings for collision reconstruction and anomaly detection at the Large Hadron Collider, and ecology and biodiversity monitoring through audio, image, and DNA sequence embeddings for species identification and ecosystem health assessment. These techniques transform scientific discovery from manual analysis and limited sampling to automated pattern recognition across the full scale of observational data.\n:::\n\nAfter transforming media and entertainment (@sec-media-entertainment), embeddings enable **scientific computing breakthroughs** at unprecedented scale. Traditional scientific analysis relies on domain expert interpretation, physics-based simulations, and manual feature engineering. **Embedding-based scientific computing** learns representations directly from observational data—telescope images, sensor networks, particle detectors, genomic sequences—discovering patterns that complement and extend physics-based understanding while scaling to the petabyte datasets modern instruments generate.\n\n## Astrophysics and Astronomy\n\nAstronomy generates massive observational datasets—the Vera C. Rubin Observatory will produce 20 terabytes per night, while the Square Kilometre Array will generate more data than the global internet. **Embedding-based astrophysics** enables automated classification, anomaly detection, and discovery across these datasets.\n\n### The Astrophysics Challenge\n\nTraditional astronomical analysis faces limitations:\n\n- **Data volume**: Human experts cannot review billions of galaxy images\n- **Rare events**: Transient phenomena (supernovae, gravitational waves) require real-time detection\n- **Multi-wavelength**: Combining radio, optical, X-ray, and gamma-ray observations\n- **Spectral complexity**: High-dimensional spectra require sophisticated analysis\n- **Simulation gaps**: Physics simulations cannot cover full parameter space\n\n**Embedding approach**: Learn representations of celestial objects from images, spectra, and light curves. Similar objects cluster in embedding space; anomalies appear as outliers. Enable cross-survey matching and rapid classification of new observations.\n\n::: {#5b5c76e8 .cell execution_count=1}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show astrophysics embedding architecture\"}\nfrom dataclasses import dataclass\nfrom typing import Optional\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n@dataclass\nclass AstroConfig:\n    image_size: int = 64\n    n_bands: int = 5  # Multi-band imaging (u, g, r, i, z)\n    embedding_dim: int = 256\n    n_spectral_bins: int = 4096\n\nclass GalaxyMorphologyEncoder(nn.Module):\n    \"\"\"Encode galaxy images for morphological classification.\"\"\"\n    def __init__(self, config: AstroConfig):\n        super().__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(config.n_bands, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(), nn.MaxPool2d(2),\n            nn.Conv2d(64, 128, 3, padding=1), nn.BatchNorm2d(128), nn.ReLU(), nn.MaxPool2d(2),\n            nn.Conv2d(128, 256, 3, padding=1), nn.BatchNorm2d(256), nn.ReLU(), nn.AdaptiveAvgPool2d(1))\n        self.proj = nn.Linear(256, config.embedding_dim)\n\n    def forward(self, images: torch.Tensor) -> torch.Tensor:\n        features = self.conv(images).squeeze(-1).squeeze(-1)\n        return F.normalize(self.proj(features), dim=-1)\n\nclass TransientLightCurveEncoder(nn.Module):\n    \"\"\"Encode light curves for transient classification (supernovae, variable stars).\"\"\"\n    def __init__(self, config: AstroConfig):\n        super().__init__()\n        self.input_proj = nn.Linear(3, 64)  # (time, magnitude, error)\n        encoder_layer = nn.TransformerEncoderLayer(d_model=64, nhead=4, batch_first=True)\n        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=4)\n        self.cls_token = nn.Parameter(torch.randn(1, 1, 64))\n        self.proj = nn.Linear(64, config.embedding_dim)\n\n    def forward(self, times: torch.Tensor, mags: torch.Tensor, errors: torch.Tensor) -> torch.Tensor:\n        x = self.input_proj(torch.stack([times, mags, errors], dim=-1))\n        x = torch.cat([self.cls_token.expand(x.size(0), -1, -1), x], dim=1)\n        x = self.transformer(x)\n        return F.normalize(self.proj(x[:, 0]), dim=-1)\n\n# Usage example\nconfig = AstroConfig()\ngalaxy_encoder = GalaxyMorphologyEncoder(config)\nlightcurve_encoder = TransientLightCurveEncoder(config)\n\n# Encode a batch of galaxy images (5-band, 64x64 pixels)\ngalaxy_images = torch.randn(4, 5, 64, 64)\ngalaxy_embeddings = galaxy_encoder(galaxy_images)\nprint(f\"Galaxy embeddings: {galaxy_embeddings.shape}\")  # [4, 256]\n\n# Encode a batch of light curves (20 observations each)\ntimes = torch.rand(4, 20) * 100  # Days\nmags = torch.randn(4, 20) * 0.5 + 18  # Magnitudes\nerrors = torch.rand(4, 20) * 0.1\ntransient_embeddings = lightcurve_encoder(times, mags, errors)\nprint(f\"Transient embeddings: {transient_embeddings.shape}\")  # [4, 256]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nGalaxy embeddings: torch.Size([4, 256])\nTransient embeddings: torch.Size([4, 256])\n```\n:::\n:::\n\n\n:::{.callout-tip}\n## Astrophysics Best Practices\n\n**Image processing:**\n\n- **Multi-band fusion**: Combine observations across wavelengths (u, g, r, i, z bands)\n- **Point spread function**: Account for atmospheric/instrumental effects\n- **Background subtraction**: Remove sky background and artifacts\n- **Augmentation**: Rotation invariance critical for galaxy morphology\n- **Transfer learning**: Pre-train on simulations, fine-tune on real data\n\n**Spectral analysis:**\n\n- **Wavelength normalization**: Standardize to rest frame (redshift correction)\n- **Continuum fitting**: Separate emission/absorption lines from continuum\n- **Resolution matching**: Handle varying spectral resolutions across instruments\n- **Missing data**: Interpolate gaps from atmospheric absorption\n\n**Time-series (light curves):**\n\n- **Irregular sampling**: Use attention or Gaussian processes for non-uniform cadence\n- **Period finding**: Encode periodic structure for variable stars\n- **Event detection**: Real-time classification of transients\n- **Multi-scale**: Capture both short-term variability and long-term trends\n\n**Production:**\n\n- **Real-time pipelines**: Sub-second classification for alert brokers\n- **Cross-matching**: Link observations across surveys (Gaia, SDSS, ZTF)\n- **Uncertainty quantification**: Calibrated confidence for scientific conclusions\n- **Explainability**: Highlight features driving classification\n:::\n\n## Climate and Earth Science\n\nClimate science requires understanding complex Earth systems across spatial scales (local to global) and temporal scales (hours to millennia). **Embedding-based climate science** learns representations of atmospheric patterns, ocean dynamics, and Earth observations to improve prediction and understanding.\n\n### The Climate Science Challenge\n\nTraditional climate modeling faces limitations:\n\n- **Computational cost**: High-resolution simulations require supercomputers for months\n- **Parameterization**: Sub-grid processes must be approximated\n- **Ensemble size**: Limited ensemble members for uncertainty quantification\n- **Observation integration**: Heterogeneous data sources difficult to combine\n- **Extreme events**: Rare events poorly sampled in historical record\n\n**Embedding approach**: Learn compressed representations of atmospheric and oceanic states. Use embeddings for efficient emulation of physics models, pattern recognition in observations, and downscaling coarse simulations to high resolution.\n\n::: {#93d1368f .cell execution_count=2}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show climate embedding architecture\"}\n@dataclass\nclass ClimateConfig:\n    n_pressure_levels: int = 13\n    n_surface_vars: int = 4  # 2m temp, 10m wind u/v, mslp\n    n_atmos_vars: int = 5  # T, u, v, q, z per level\n    lat_size: int = 181\n    lon_size: int = 360\n    embedding_dim: int = 512\n    patch_size: int = 8\n\nclass WeatherStateEncoder(nn.Module):\n    \"\"\"Encode atmospheric state for weather prediction (GraphCast-style).\"\"\"\n    def __init__(self, config: ClimateConfig):\n        super().__init__()\n        n_input = config.n_surface_vars + config.n_atmos_vars * config.n_pressure_levels\n        self.patch_embed = nn.Conv2d(n_input, 512, kernel_size=config.patch_size, stride=config.patch_size)\n        n_patches = (config.lat_size // config.patch_size) * (config.lon_size // config.patch_size)\n        self.pos_embed = nn.Parameter(torch.randn(1, n_patches, 512) * 0.02)\n        encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8, batch_first=True)\n        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=8)\n        self.proj = nn.Linear(512, config.embedding_dim)\n\n    def forward(self, surface: torch.Tensor, atmos: torch.Tensor) -> torch.Tensor:\n        atmos_flat = atmos.flatten(1, 2)\n        x = torch.cat([surface, atmos_flat], dim=1)\n        x = self.patch_embed(x).flatten(2).transpose(1, 2)\n        x = self.transformer(x + self.pos_embed)\n        return F.normalize(self.proj(x.mean(dim=1)), dim=-1)\n\nclass SatelliteImageEncoder(nn.Module):\n    \"\"\"Encode multi-spectral satellite imagery (Sentinel-2 style).\"\"\"\n    def __init__(self, n_channels: int = 13, embedding_dim: int = 256):\n        super().__init__()\n        self.encoder = nn.Sequential(\n            nn.Conv2d(n_channels, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(), nn.MaxPool2d(2),\n            nn.Conv2d(64, 128, 3, padding=1), nn.BatchNorm2d(128), nn.ReLU(), nn.MaxPool2d(2),\n            nn.Conv2d(128, 256, 3, padding=1), nn.BatchNorm2d(256), nn.ReLU(), nn.AdaptiveAvgPool2d(1))\n        self.proj = nn.Linear(256, embedding_dim)\n\n    def forward(self, imagery: torch.Tensor) -> torch.Tensor:\n        features = self.encoder(imagery).squeeze(-1).squeeze(-1)\n        return F.normalize(self.proj(features), dim=-1)\n\n# Usage example\nsat_encoder = SatelliteImageEncoder(n_channels=13, embedding_dim=256)\nsatellite_images = torch.randn(4, 13, 128, 128)  # 13-band Sentinel-2 imagery\nsat_embeddings = sat_encoder(satellite_images)\nprint(f\"Satellite embeddings: {sat_embeddings.shape}\")  # [4, 256]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSatellite embeddings: torch.Size([4, 256])\n```\n:::\n:::\n\n\n:::{.callout-tip}\n## Climate Science Best Practices\n\n**Spatial representations:**\n\n- **Spherical geometry**: Use appropriate coordinates for global data (not flat projections)\n- **Multi-resolution**: Hierarchical representations for local-to-global patterns\n- **Graph neural networks**: Model irregular grids and mesh-based simulations\n- **Physical constraints**: Embed conservation laws (mass, energy, momentum)\n\n**Temporal modeling:**\n\n- **Multi-scale**: Capture diurnal, seasonal, interannual, and decadal patterns\n- **Autoregressive**: Roll out predictions iteratively for long horizons\n- **Ensemble methods**: Generate probabilistic forecasts\n- **Memory**: Long-term dependencies (ocean heat content, ice dynamics)\n\n**Satellite imagery:**\n\n- **Multi-spectral fusion**: Combine visible, infrared, and microwave channels\n- **Cloud masking**: Handle missing data from cloud cover\n- **Temporal compositing**: Aggregate observations over time windows\n- **Super-resolution**: Downscale coarse observations to fine grid\n\n**Hybrid physics-ML:**\n\n- **Physics-informed loss**: Penalize violations of conservation laws\n- **Neural parameterization**: Replace sub-grid approximations with learned models\n- **Bias correction**: Learn systematic errors in physics models\n- **Emulation**: Fast surrogate models for expensive simulations\n:::\n\n:::{.callout-warning}\n## Climate Model Uncertainty\n\nClimate embeddings must handle multiple sources of uncertainty:\n\n- **Initial condition uncertainty**: Chaotic dynamics amplify small perturbations\n- **Model structural uncertainty**: Different models give different projections\n- **Scenario uncertainty**: Future emissions depend on human choices\n- **Internal variability**: Natural fluctuations mask forced trends\n\n**Best practices:**\n\n- **Ensemble training**: Train on multiple models and scenarios\n- **Uncertainty quantification**: Provide confidence intervals, not point predictions\n- **Out-of-distribution detection**: Flag predictions extrapolating beyond training\n- **Domain expert validation**: Verify physical plausibility of learned patterns\n:::\n\n## Materials Science and Chemistry\n\nMaterials science seeks to discover new materials with desired properties—stronger alloys, better batteries, efficient catalysts. **Embedding-based materials science** learns representations of atomic structures to predict properties and accelerate discovery.\n\n### The Materials Discovery Challenge\n\nTraditional materials discovery faces limitations:\n\n- **Combinatorial space**: Billions of possible compositions and structures\n- **Expensive experiments**: Synthesis and characterization are slow and costly\n- **Simulation bottleneck**: Quantum mechanical calculations scale poorly\n- **Property prediction**: Structure-property relationships are complex\n- **Synthesizability**: Not all computationally stable materials can be made\n\n**Embedding approach**: Represent materials as graphs (atoms = nodes, bonds = edges) and learn embeddings that predict properties. Screen virtual libraries computationally before expensive synthesis.\n\n::: {#290b74e9 .cell execution_count=3}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show materials science embedding architecture\"}\n@dataclass\nclass MaterialsConfig:\n    atom_features: int = 92  # One-hot for elements\n    bond_features: int = 10\n    hidden_dim: int = 256\n    embedding_dim: int = 128\n    n_conv_layers: int = 4\n\nclass CrystalGraphConv(nn.Module):\n    \"\"\"Graph convolution for crystal structures (CGCNN-style).\"\"\"\n    def __init__(self, hidden_dim: int, edge_dim: int):\n        super().__init__()\n        self.edge_mlp = nn.Sequential(\n            nn.Linear(2 * hidden_dim + edge_dim, hidden_dim), nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim), nn.Sigmoid())\n        self.node_mlp = nn.Sequential(\n            nn.Linear(2 * hidden_dim, hidden_dim), nn.ReLU(), nn.Linear(hidden_dim, hidden_dim))\n\n    def forward(self, x: torch.Tensor, edge_index: torch.Tensor, edge_attr: torch.Tensor) -> torch.Tensor:\n        src, dst = edge_index\n        edge_input = torch.cat([x[src], x[dst], edge_attr], dim=-1)\n        messages = x[src] * self.edge_mlp(edge_input)\n        aggregated = torch.zeros_like(x)\n        aggregated.index_add_(0, dst, messages)\n        return x + self.node_mlp(torch.cat([x, aggregated], dim=-1))\n\nclass CrystalGraphEncoder(nn.Module):\n    \"\"\"Encode crystal structures for property prediction.\"\"\"\n    def __init__(self, config: MaterialsConfig):\n        super().__init__()\n        self.atom_embed = nn.Embedding(config.atom_features, config.hidden_dim)\n        self.edge_embed = nn.Linear(config.bond_features, config.hidden_dim)\n        self.convs = nn.ModuleList([\n            CrystalGraphConv(config.hidden_dim, config.hidden_dim)\n            for _ in range(config.n_conv_layers)])\n        self.readout = nn.Sequential(\n            nn.Linear(config.hidden_dim, config.hidden_dim), nn.ReLU(),\n            nn.Linear(config.hidden_dim, config.embedding_dim))\n\n    def forward(self, atomic_numbers: torch.Tensor, edge_index: torch.Tensor,\n                edge_features: torch.Tensor, batch: torch.Tensor) -> torch.Tensor:\n        x = self.atom_embed(atomic_numbers - 1)\n        edge_attr = self.edge_embed(edge_features)\n        for conv in self.convs:\n            x = conv(x, edge_index, edge_attr)\n        # Global mean pooling per crystal\n        batch_size = batch.max().item() + 1\n        pooled = torch.zeros(batch_size, x.shape[-1], device=x.device)\n        counts = torch.zeros(batch_size, device=x.device)\n        for i in range(x.shape[0]):\n            pooled[batch[i]] += x[i]\n            counts[batch[i]] += 1\n        pooled = pooled / counts.unsqueeze(-1).clamp(min=1)\n        return F.normalize(self.readout(pooled), dim=-1)\n\n# Usage example\nmat_config = MaterialsConfig()\ncrystal_encoder = CrystalGraphEncoder(mat_config)\n\n# Encode a small crystal (8 atoms, 24 bonds)\natomic_nums = torch.tensor([14, 14, 8, 8, 8, 8, 8, 8])  # Silicon dioxide-like\nedge_index = torch.randint(0, 8, (2, 24))\nedge_features = torch.randn(24, 10)  # Bond distances, angles\nbatch = torch.zeros(8, dtype=torch.long)  # All atoms in one crystal\n\ncrystal_embedding = crystal_encoder(atomic_nums, edge_index, edge_features, batch)\nprint(f\"Crystal embedding: {crystal_embedding.shape}\")  # [1, 128]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCrystal embedding: torch.Size([1, 128])\n```\n:::\n:::\n\n\n:::{.callout-tip}\n## Materials Science Best Practices\n\n**Atomic representations:**\n\n- **Graph neural networks**: Encode local atomic environments\n- **Equivariance**: Respect rotational and translational symmetry\n- **Periodic boundaries**: Handle crystal structures appropriately\n- **Multi-fidelity**: Combine cheap (force fields) and expensive (DFT) data\n- **Pre-training**: Large-scale pre-training on crystal databases (Materials Project, OQMD)\n\n**Property prediction:**\n\n- **Multi-task learning**: Predict multiple properties jointly\n- **Uncertainty quantification**: Bayesian methods for confidence\n- **Active learning**: Iteratively select most informative experiments\n- **Transfer learning**: Fine-tune on small datasets for specific properties\n\n**Generative design:**\n\n- **Variational autoencoders**: Sample novel materials from latent space\n- **Diffusion models**: Generate crystal structures with desired properties\n- **Constraint satisfaction**: Enforce charge neutrality, stoichiometry\n- **Synthesizability scoring**: Predict whether generated materials can be made\n\n**Validation:**\n\n- **Hold-out testing**: Strict train/test splits by composition or structure type\n- **Experimental verification**: Close the loop with synthesis and characterization\n- **Domain knowledge**: Sanity check predictions against chemical intuition\n- **Uncertainty calibration**: Verify confidence intervals are well-calibrated\n:::\n\n## Particle Physics\n\nParticle physics experiments like the Large Hadron Collider generate petabytes of collision data, searching for rare events that reveal new physics. **Embedding-based particle physics** enables efficient event reconstruction, classification, and anomaly detection.\n\n### The Particle Physics Challenge\n\nTraditional particle physics analysis faces limitations:\n\n- **Data volume**: LHC generates 1 petabyte per second (before filtering)\n- **Trigger systems**: Must decide in microseconds which events to keep\n- **Reconstruction**: Converting detector hits to particle tracks is complex\n- **Background rejection**: Rare signals buried in overwhelming backgrounds\n- **New physics search**: Unknown signatures cannot be explicitly targeted\n\n**Embedding approach**: Learn representations of collision events from detector data. Similar physics processes cluster in embedding space; anomalies may indicate new particles or interactions.\n\n::: {#9c660e24 .cell execution_count=4}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show particle physics embedding architecture\"}\n@dataclass\nclass ParticleConfig:\n    particle_features: int = 7  # pt, eta, phi, E, charge, pid, etc.\n    max_particles: int = 128\n    hidden_dim: int = 256\n    embedding_dim: int = 128\n    n_heads: int = 8\n    n_layers: int = 6\n\nclass ParticleCloudEncoder(nn.Module):\n    \"\"\"Encode collision events as particle clouds (ParticleNet-style).\"\"\"\n    def __init__(self, config: ParticleConfig):\n        super().__init__()\n        self.kin_embed = nn.Linear(4, config.hidden_dim // 2)  # pt, eta, phi, E\n        self.pid_embed = nn.Embedding(20, config.hidden_dim // 4)\n        self.charge_embed = nn.Embedding(3, config.hidden_dim // 4)\n        self.cls_token = nn.Parameter(torch.randn(1, 1, config.hidden_dim))\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=config.hidden_dim, nhead=config.n_heads, batch_first=True)\n        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=config.n_layers)\n        self.proj = nn.Linear(config.hidden_dim, config.embedding_dim)\n\n    def forward(self, kinematics: torch.Tensor, particle_ids: torch.Tensor,\n                charges: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n        batch_size = kinematics.shape[0]\n        x = torch.cat([self.kin_embed(kinematics), self.pid_embed(particle_ids),\n                       self.charge_embed(charges + 1)], dim=-1)\n        x = torch.cat([self.cls_token.expand(batch_size, -1, -1), x], dim=1)\n        if mask is not None:\n            mask = torch.cat([torch.ones(batch_size, 1, device=mask.device), mask], dim=1)\n            x = self.transformer(x, src_key_padding_mask=~mask.bool())\n        else:\n            x = self.transformer(x)\n        return F.normalize(self.proj(x[:, 0]), dim=-1)\n\nclass JetEncoder(nn.Module):\n    \"\"\"Encode hadronic jets for tagging (b-jet, top-jet identification).\"\"\"\n    def __init__(self, config: ParticleConfig):\n        super().__init__()\n        self.constituent_embed = nn.Linear(config.particle_features, config.hidden_dim)\n        self.attention = nn.MultiheadAttention(config.hidden_dim, config.n_heads, batch_first=True)\n        self.readout = nn.Sequential(\n            nn.Linear(config.hidden_dim, config.hidden_dim), nn.ReLU(),\n            nn.Linear(config.hidden_dim, config.embedding_dim))\n\n    def forward(self, constituents: torch.Tensor) -> torch.Tensor:\n        x = self.constituent_embed(constituents)\n        x, _ = self.attention(x, x, x)\n        return F.normalize(self.readout(x.mean(dim=1)), dim=-1)\n\n# Usage example\nphys_config = ParticleConfig()\nevent_encoder = ParticleCloudEncoder(phys_config)\n\n# Encode a batch of collision events (up to 50 particles each)\nkinematics = torch.randn(4, 50, 4)  # pt, eta, phi, E\nparticle_ids = torch.randint(0, 15, (4, 50))  # electron, muon, photon, etc.\ncharges = torch.randint(-1, 2, (4, 50))  # -1, 0, +1\nmask = torch.ones(4, 50)\nmask[:, 30:] = 0  # Last 20 positions are padding\n\nevent_embeddings = event_encoder(kinematics, particle_ids, charges, mask)\nprint(f\"Event embeddings: {event_embeddings.shape}\")  # [4, 128]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nEvent embeddings: torch.Size([4, 128])\n```\n:::\n:::\n\n\n:::{.callout-tip}\n## Particle Physics Best Practices\n\n**Event representation:**\n\n- **Point clouds**: Variable-length sets of particles with features (momentum, charge, type)\n- **Graphs**: Connect particles with edges based on physics (jets, vertices)\n- **Images**: Project calorimeter data to images for CNN processing\n- **Sequences**: Order particles by energy or angular position\n- **Permutation invariance**: Events unchanged by particle ordering\n\n**Architecture choices:**\n\n- **Set transformers**: Handle variable-length particle collections\n- **Graph neural networks**: Model particle interactions\n- **Attention mechanisms**: Learn which particles are relevant\n- **Physics-informed**: Encode Lorentz invariance and conservation laws\n\n**Training strategies:**\n\n- **Simulation-based**: Train on Monte Carlo simulations\n- **Domain adaptation**: Transfer from simulation to real data\n- **Weakly supervised**: Use sideband regions and data-driven labels\n- **Anomaly detection**: Unsupervised methods for new physics search\n\n**Deployment:**\n\n- **Real-time inference**: Microsecond latency for trigger systems\n- **FPGA implementation**: Hardware acceleration for online selection\n- **Calibration**: Account for detector response and simulation mismodeling\n- **Systematic uncertainties**: Propagate detector and theory uncertainties\n:::\n\n## Ecology and Biodiversity\n\nBiodiversity monitoring requires tracking millions of species across global ecosystems. **Embedding-based ecology** enables automated species identification, population monitoring, and ecosystem health assessment from images, audio, and DNA.\n\n### The Biodiversity Challenge\n\nTraditional biodiversity monitoring faces limitations:\n\n- **Expert bottleneck**: Taxonomic expertise is rare and expensive\n- **Spatial coverage**: Cannot physically survey all habitats\n- **Temporal resolution**: Infrequent surveys miss dynamics\n- **Cryptic species**: Many species look similar (require molecular ID)\n- **Scale mismatch**: Local observations must inform global assessments\n\n**Embedding approach**: Learn embeddings from species images (camera traps, drones), audio recordings (bioacoustics), and DNA sequences (metabarcoding). Similar species cluster together; invasive species and ecosystem changes appear as distribution shifts.\n\n::: {#ba455a68 .cell execution_count=5}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show ecology embedding architecture\"}\n@dataclass\nclass EcologyConfig:\n    image_size: int = 224\n    n_mels: int = 128\n    sequence_length: int = 256  # DNA barcode length\n    embedding_dim: int = 256\n    n_species: int = 10000\n\nclass SpeciesImageEncoder(nn.Module):\n    \"\"\"Encode species images for identification (camera traps, citizen science).\"\"\"\n    def __init__(self, config: EcologyConfig):\n        super().__init__()\n        self.backbone = nn.Sequential(\n            nn.Conv2d(3, 64, 7, stride=2, padding=3), nn.BatchNorm2d(64), nn.ReLU(), nn.MaxPool2d(3, 2, 1),\n            nn.Conv2d(64, 128, 3, padding=1), nn.BatchNorm2d(128), nn.ReLU(), nn.MaxPool2d(2),\n            nn.Conv2d(128, 256, 3, padding=1), nn.BatchNorm2d(256), nn.ReLU(), nn.AdaptiveAvgPool2d(1))\n        self.proj = nn.Linear(256, config.embedding_dim)\n        self.species_head = nn.Linear(config.embedding_dim, config.n_species)\n\n    def forward(self, images: torch.Tensor) -> tuple:\n        features = self.backbone(images).squeeze(-1).squeeze(-1)\n        embeddings = F.normalize(self.proj(features), dim=-1)\n        return embeddings, self.species_head(embeddings)\n\nclass BioacousticEncoder(nn.Module):\n    \"\"\"Encode audio spectrograms for species identification (bird songs, whale calls).\"\"\"\n    def __init__(self, config: EcologyConfig):\n        super().__init__()\n        self.encoder = nn.Sequential(\n            nn.Conv2d(1, 32, 3, padding=1), nn.BatchNorm2d(32), nn.ReLU(), nn.MaxPool2d(2),\n            nn.Conv2d(32, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(), nn.MaxPool2d(2),\n            nn.Conv2d(64, 128, 3, padding=1), nn.BatchNorm2d(128), nn.ReLU(), nn.AdaptiveAvgPool2d(4))\n        self.proj = nn.Sequential(nn.Linear(128 * 16, 512), nn.ReLU(), nn.Linear(512, config.embedding_dim))\n\n    def forward(self, spectrograms: torch.Tensor) -> torch.Tensor:\n        features = self.encoder(spectrograms.unsqueeze(1)).flatten(1)\n        return F.normalize(self.proj(features), dim=-1)\n\nclass DNABarcodeEncoder(nn.Module):\n    \"\"\"Encode DNA barcode sequences for species identification (eDNA, metabarcoding).\"\"\"\n    def __init__(self, config: EcologyConfig):\n        super().__init__()\n        self.nucleotide_embed = nn.Embedding(5, 64)  # A, C, G, T, N\n        self.conv = nn.Sequential(\n            nn.Conv1d(64, 128, 7, padding=3), nn.BatchNorm1d(128), nn.ReLU(), nn.MaxPool1d(2),\n            nn.Conv1d(128, 256, 5, padding=2), nn.BatchNorm1d(256), nn.ReLU(), nn.AdaptiveAvgPool1d(16))\n        self.proj = nn.Linear(256 * 16, config.embedding_dim)\n\n    def forward(self, sequences: torch.Tensor) -> torch.Tensor:\n        x = self.nucleotide_embed(sequences).transpose(1, 2)\n        x = self.conv(x).flatten(1)\n        return F.normalize(self.proj(x), dim=-1)\n\n# Usage example\neco_config = EcologyConfig()\nspecies_encoder = SpeciesImageEncoder(eco_config)\naudio_encoder = BioacousticEncoder(eco_config)\ndna_encoder = DNABarcodeEncoder(eco_config)\n\n# Encode camera trap images\nwildlife_images = torch.randn(4, 3, 224, 224)\nspecies_emb, species_logits = species_encoder(wildlife_images)\nprint(f\"Species embeddings: {species_emb.shape}\")  # [4, 256]\n\n# Encode bird song spectrograms\nspectrograms = torch.randn(4, 128, 200)  # 128 mel bins, 200 time frames\naudio_emb = audio_encoder(spectrograms)\nprint(f\"Audio embeddings: {audio_emb.shape}\")  # [4, 256]\n\n# Encode DNA barcodes\ndna_seqs = torch.randint(0, 5, (4, 256))  # COI barcode sequences\ndna_emb = dna_encoder(dna_seqs)\nprint(f\"DNA embeddings: {dna_emb.shape}\")  # [4, 256]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSpecies embeddings: torch.Size([4, 256])\nAudio embeddings: torch.Size([4, 256])\nDNA embeddings: torch.Size([4, 256])\n```\n:::\n:::\n\n\n:::{.callout-tip}\n## Ecology Best Practices\n\n**Image-based monitoring:**\n\n- **Camera traps**: Automated wildlife detection and identification\n- **Drone imagery**: Vegetation mapping and animal counts\n- **Citizen science**: Leverage iNaturalist and eBird observations\n- **Few-shot learning**: Handle rare species with limited examples\n- **Hierarchical classification**: Genus/family when species uncertain\n\n**Bioacoustic analysis:**\n\n- **Spectrogram embeddings**: Convert audio to time-frequency representations\n- **Species detection**: Identify calls in continuous recordings\n- **Soundscape ecology**: Characterize ecosystem health from acoustic diversity\n- **Noise robustness**: Handle wind, rain, and anthropogenic sounds\n- **Multi-label**: Multiple species vocalizing simultaneously\n\n**DNA-based methods:**\n\n- **Metabarcoding**: Identify all species in environmental samples\n- **Sequence embeddings**: Learn representations of barcode genes\n- **Phylogenetic awareness**: Incorporate evolutionary relationships\n- **Novel species**: Detect sequences not matching reference databases\n- **Quantification**: Estimate relative abundance from read counts\n\n**Integration:**\n\n- **Multi-modal fusion**: Combine image, audio, and DNA evidence\n- **Spatial modeling**: Map species distributions from point observations\n- **Temporal dynamics**: Track population trends and phenology\n- **Uncertainty quantification**: Propagate identification uncertainty to assessments\n:::\n\n## Key Takeaways\n\n:::{.callout-note}\nThe specific performance metrics in the takeaways below are illustrative examples based on published research and hypothetical scenarios. They represent the order of magnitude of improvements achievable but are not verified results from specific deployments.\n:::\n\n- **Astrophysics at petabyte scale requires automated classification**: Galaxy morphology classification achieves 95%+ accuracy with CNNs, gravitational wave detection enables multi-messenger astronomy, and anomaly detection discovers new transient phenomena—transforming surveys from targeted observations to comprehensive sky monitoring\n\n- **Climate and earth science benefit from embedding-based emulators**: Neural weather prediction (GraphCast, Pangu-Weather) matches or exceeds traditional models at 1000x lower computational cost, satellite imagery embeddings enable real-time monitoring of deforestation and ice extent, and hybrid physics-ML models improve sub-grid parameterizations\n\n- **Materials discovery accelerates through atomic graph embeddings**: Property prediction from structure enables virtual screening of millions of candidates, generative models propose novel materials with desired properties, and active learning guides experiments—reducing discovery timelines from decades to years\n\n- **Particle physics handles extreme data rates with learned representations**: Real-time trigger systems using neural networks achieve microsecond inference, anomaly detection provides model-independent searches for new physics, and graph neural networks improve jet reconstruction accuracy by 20-40%\n\n- **Biodiversity monitoring scales through multi-modal embeddings**: Camera trap analysis automates wildlife surveys across millions of images, bioacoustic monitoring enables continuous ecosystem assessment, and DNA metabarcoding with sequence embeddings identifies entire communities from environmental samples\n\n- **Scientific embeddings require domain-specific architectures**: Spherical geometry for climate data, equivariance for molecular structures, permutation invariance for particle sets, and hierarchical classification for taxonomic trees—off-the-shelf models fail without incorporating domain structure\n\n- **Uncertainty quantification is critical for scientific conclusions**: Calibrated confidence intervals enable proper statistical inference, out-of-distribution detection flags extrapolation beyond training data, and ensemble methods capture both aleatoric and epistemic uncertainty\n\n## Looking Ahead\n\nPart V (Industry Applications) continues with @sec-defense-intelligence, which applies embeddings to defense and intelligence applications: geospatial intelligence using satellite and aerial imagery analysis for object detection and change monitoring, signals intelligence with embeddings for communication analysis and pattern recognition, open-source intelligence aggregating and analyzing public information at scale, autonomous systems leveraging embeddings for navigation, perception, and decision-making, and command and control decision support synthesizing multi-source intelligence into actionable insights.\n\n## Further Reading\n\n### Astrophysics and Astronomy\n- Huertas-Company, Marc, and François Lanusse (2023). \"The Dawes Review 10: The Impact of Deep Learning for the Analysis of Galaxy Surveys.\" PASA.\n- Walmsley, Mike, et al. (2022). \"Galaxy Zoo DECaLS: Detailed Visual Morphology Measurements from Volunteers and Deep Learning.\" MNRAS.\n- George, Daniel, and E.A. Huerta (2018). \"Deep Learning for Real-time Gravitational Wave Detection and Parameter Estimation.\" Physics Letters B.\n- Shallue, Christopher J., and Andrew Vanderburg (2018). \"Identifying Exoplanets with Deep Learning.\" The Astronomical Journal.\n- Villar, V. Ashley, et al. (2020). \"A Deep-learning Approach for Live Anomaly Detection of Extragalactic Transients.\" ApJS.\n\n### Climate and Earth Science\n- Lam, Remi, et al. (2023). \"Learning Skillful Medium-range Global Weather Forecasting.\" Science.\n- Bi, Kaifeng, et al. (2023). \"Accurate Medium-range Global Weather Forecasting with 3D Neural Networks.\" Nature.\n- Reichstein, Markus, et al. (2019). \"Deep Learning and Process Understanding for Data-Driven Earth System Science.\" Nature.\n- Beucler, Tom, et al. (2021). \"Enforcing Analytic Constraints in Neural Networks Emulating Physical Systems.\" Physical Review Letters.\n- Nguyen, Tung, et al. (2023). \"ClimaX: A Foundation Model for Weather and Climate.\" ICML.\n\n### Materials Science\n- Xie, Tian, and Jeffrey C. Grossman (2018). \"Crystal Graph Convolutional Neural Networks for an Accurate and Interpretable Prediction of Material Properties.\" Physical Review Letters.\n- Chen, Chi, et al. (2019). \"Graph Networks as a Universal Machine Learning Framework for Molecules and Crystals.\" Chemistry of Materials.\n- Merchant, Amil, et al. (2023). \"Scaling Deep Learning for Materials Discovery.\" Nature.\n- Batzner, Simon, et al. (2022). \"E(3)-Equivariant Graph Neural Networks for Data-Efficient and Accurate Interatomic Potentials.\" Nature Communications.\n- Jain, Anubhav, et al. (2013). \"Commentary: The Materials Project: A Materials Genome Approach to Accelerating Materials Innovation.\" APL Materials.\n\n### Particle Physics\n- Qu, Huilin, and Loukas Gouskos (2020). \"Jet Tagging via Particle Clouds.\" Physical Review D.\n- Mikuni, Vinicius, and Florencia Canelli (2021). \"Point Cloud Transformers Applied to Collider Physics.\" Machine Learning: Science and Technology.\n- Kasieczka, Gregor, et al. (2021). \"The Machine Learning Landscape of Top Taggers.\" SciPost Physics.\n- Baldi, Pierre, et al. (2014). \"Searching for Exotic Particles in High-Energy Physics with Deep Learning.\" Nature Communications.\n- Butter, Anja, et al. (2022). \"Machine Learning and LHC Event Generation.\" SciPost Physics.\n\n### Ecology and Biodiversity\n- Beery, Sara, et al. (2021). \"Species Distribution Modeling for Machine Learning Practitioners: A Review.\" ACM SIGCAS Conference.\n- Kahl, Stefan, et al. (2021). \"BirdNET: A Deep Learning Solution for Avian Diversity Monitoring.\" Ecological Informatics.\n- Christin, Sylvain, et al. (2019). \"Applications for Deep Learning in Ecology.\" Methods in Ecology and Evolution.\n- Wäldchen, Jana, and Patrick Mäder (2018). \"Machine Learning for Image Based Species Identification.\" Methods in Ecology and Evolution.\n- Tuia, Devis, et al. (2022). \"Perspectives in Machine Learning for Wildlife Conservation.\" Nature Communications.\n\n### Scientific Machine Learning\n- Karniadakis, George Em, et al. (2021). \"Physics-informed Machine Learning.\" Nature Reviews Physics.\n- Willard, Jared, et al. (2022). \"Integrating Scientific Knowledge with Machine Learning for Engineering and Environmental Systems.\" ACM Computing Surveys.\n- Cranmer, Miles, et al. (2020). \"Discovering Symbolic Models from Deep Learning with Inductive Biases.\" NeurIPS.\n- Jumper, John, et al. (2021). \"Highly Accurate Protein Structure Prediction with AlphaFold.\" Nature.\n- Davies, Alex, et al. (2021). \"Advancing Mathematics by Guiding Human Intuition with AI.\" Nature.\n\n",
    "supporting": [
      "ch30_scientific_computing_files/figure-epub"
    ],
    "filters": [],
    "engineDependencies": {
      "jupyter": [
        {
          "jsWidgets": false,
          "jupyterWidgets": false,
          "htmlLibraries": []
        }
      ]
    }
  }
}