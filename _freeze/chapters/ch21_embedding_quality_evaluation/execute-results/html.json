{
  "hash": "e73c81dd37752bdf49de1c636958cd39",
  "result": {
    "engine": "jupyter",
    "markdown": "# Embedding Quality Evaluation {#sec-embedding-quality-evaluation}\n\n:::{.callout-note}\n## Chapter Overview\nMeasuring embedding quality is deceptively difficult. Unlike classification accuracy or regression error, embedding quality is multidimensional: retrieval performance, representation quality, downstream task accuracy, and user satisfaction all matter but may not correlate. This chapter provides a comprehensive framework for evaluating embeddings across intrinsic quality metrics (isotropy, uniformity, alignment), retrieval metrics (Recall@K, MAP, NDCG, MRR), human evaluation protocols, domain-specific metrics, and statistical rigor. We cover evaluation at trillion-row scale including sampling strategies, efficient computation, and continuous monitoring—enabling you to measure what matters and catch degradation before users notice.\n:::\n\nEmbedding evaluation differs fundamentally from traditional ML evaluation. A classifier has a clear target: predict the correct label. An embedding has no single correct answer—quality depends on how well the embedding supports downstream tasks, which may not be known at training time. This creates a challenging evaluation landscape requiring multiple complementary metrics, careful experimental design, and statistical rigor.\n\n## Intrinsic Quality Metrics {#sec-intrinsic-quality-metrics}\n\nIntrinsic metrics measure embedding quality without reference to a specific downstream task. They capture properties of the embedding space itself—how well-distributed vectors are, how much of the space is utilized, and whether semantic relationships are preserved. These metrics detect problems even without labeled evaluation data.\n\n### Isotropy: Are Embeddings Well-Distributed?\n\n**Isotropy** measures how uniformly embeddings are distributed across the vector space. Perfectly isotropic embeddings have equal variance in all directions—no dimension dominates, and vectors aren't clustered in a narrow cone.\n\nWhy isotropy matters:\n\n- **Low isotropy** means embeddings cluster in a small region, reducing discriminative power\n- **Highly anisotropic** embeddings waste dimensions on directions with little variance\n- **Similarity search** becomes unreliable when all vectors are similar to each other\n\n::: {#b3a069e1 .cell execution_count=1}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show isotropy measurement implementation\"}\nimport torch\nimport torch.nn.functional as F\nimport numpy as np\n\ndef compute_isotropy(embeddings: torch.Tensor) -> dict:\n    \"\"\"\n    Compute isotropy metrics for a set of embeddings.\n\n    Isotropy measures how uniformly embeddings are distributed in the space.\n    Perfect isotropy = 1.0, all vectors identical = 0.0\n\n    Args:\n        embeddings: Tensor of shape (n_samples, embedding_dim)\n\n    Returns:\n        Dictionary with isotropy metrics\n    \"\"\"\n    # Center embeddings\n    centered = embeddings - embeddings.mean(dim=0)\n\n    # Compute covariance matrix\n    n = embeddings.shape[0]\n    cov = (centered.T @ centered) / (n - 1)\n\n    # Eigenvalue decomposition\n    eigenvalues = torch.linalg.eigvalsh(cov)\n    eigenvalues = eigenvalues.clamp(min=1e-10)  # Numerical stability\n\n    # Sort descending\n    eigenvalues = eigenvalues.flip(0)\n\n    # Isotropy metrics\n    # 1. Partition function isotropy (Mu et al., 2018)\n    #    Measures how much the eigenvalue distribution deviates from uniform\n    Z = eigenvalues.sum()\n    partition_isotropy = (eigenvalues.min() * len(eigenvalues)) / Z\n\n    # 2. Effective dimensionality (participation ratio)\n    #    How many dimensions are \"active\"\n    effective_dim = (eigenvalues.sum() ** 2) / (eigenvalues ** 2).sum()\n\n    # 3. Explained variance ratio\n    #    What fraction of variance is in top-k dimensions\n    total_var = eigenvalues.sum()\n    top_10_var = eigenvalues[:10].sum() / total_var\n    top_50_var = eigenvalues[:50].sum() / total_var\n\n    # 4. Average cosine similarity (should be ~0 for isotropic)\n    normalized = F.normalize(embeddings, dim=1)\n    cos_sim_matrix = normalized @ normalized.T\n    # Exclude diagonal\n    mask = ~torch.eye(n, dtype=torch.bool, device=embeddings.device)\n    avg_cos_sim = cos_sim_matrix[mask].mean()\n\n    return {\n        \"partition_isotropy\": partition_isotropy.item(),\n        \"effective_dimensionality\": effective_dim.item(),\n        \"effective_dim_ratio\": effective_dim.item() / embeddings.shape[1],\n        \"top_10_variance_ratio\": top_10_var.item(),\n        \"top_50_variance_ratio\": top_50_var.item(),\n        \"avg_cosine_similarity\": avg_cos_sim.item(),\n        \"embedding_dim\": embeddings.shape[1]\n    }\n\n\n# Example: Compare isotropic vs anisotropic embeddings\ntorch.manual_seed(42)\n\n# Well-distributed embeddings (more isotropic)\nisotropic_emb = torch.randn(1000, 256)\n\n# Poorly distributed (anisotropic - most variance in few dimensions)\nanisotropic_emb = torch.randn(1000, 256)\nanisotropic_emb[:, :10] *= 10  # First 10 dims dominate\n\nprint(\"Isotropic embeddings:\")\niso_metrics = compute_isotropy(isotropic_emb)\nprint(f\"  Partition isotropy: {iso_metrics['partition_isotropy']:.4f}\")\nprint(f\"  Effective dim ratio: {iso_metrics['effective_dim_ratio']:.2%}\")\nprint(f\"  Avg cosine similarity: {iso_metrics['avg_cosine_similarity']:.4f}\")\n\nprint(\"\\nAnisotropic embeddings:\")\naniso_metrics = compute_isotropy(anisotropic_emb)\nprint(f\"  Partition isotropy: {aniso_metrics['partition_isotropy']:.4f}\")\nprint(f\"  Effective dim ratio: {aniso_metrics['effective_dim_ratio']:.2%}\")\nprint(f\"  Avg cosine similarity: {aniso_metrics['avg_cosine_similarity']:.4f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nIsotropic embeddings:\n  Partition isotropy: 0.2481\n  Effective dim ratio: 79.38%\n  Avg cosine similarity: 0.0001\n\nAnisotropic embeddings:\n  Partition isotropy: 0.0518\n  Effective dim ratio: 5.91%\n  Avg cosine similarity: -0.0001\n```\n:::\n:::\n\n\n:::{.callout-tip}\n## Isotropy Benchmarks\n\n**Good isotropy indicators:**\n\n- Partition isotropy > 0.5 (higher is better, max 1.0)\n- Effective dimensionality > 50% of embedding dimension\n- Average cosine similarity close to 0 (typically -0.01 to 0.05)\n- Top 10 dimensions explain < 20% of variance\n\n**Warning signs:**\n\n- Average cosine similarity > 0.3 (vectors too similar)\n- Effective dimensionality < 20% of embedding dimension\n- Top 10 dimensions explain > 50% of variance\n\nNote: Some anisotropy is expected and even desirable—it reflects the structure of your data. The key is ensuring useful dimensions aren't wasted on noise.\n:::\n\n### Uniformity and Alignment\n\nWang & Isola (2020) introduced **uniformity** and **alignment** as complementary metrics for contrastive embeddings:\n\n- **Alignment**: Similar items should have similar embeddings (low distance between positives)\n- **Uniformity**: Embeddings should be uniformly distributed on the unit hypersphere (maximize use of space)\n\n::: {#d42a9e71 .cell execution_count=2}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show uniformity and alignment metrics\"}\nimport torch\nimport torch.nn.functional as F\n\ndef compute_alignment(\n    embeddings: torch.Tensor,\n    positive_pairs: torch.Tensor,\n    alpha: float = 2.0\n) -> float:\n    \"\"\"\n    Compute alignment metric: expected distance between positive pairs.\n\n    Lower is better - positive pairs should be close.\n\n    Args:\n        embeddings: (n_samples, dim) normalized embeddings\n        positive_pairs: (n_pairs, 2) indices of positive pairs\n        alpha: exponent for distance (default 2 = squared distance)\n    \"\"\"\n    emb1 = embeddings[positive_pairs[:, 0]]\n    emb2 = embeddings[positive_pairs[:, 1]]\n\n    # Squared L2 distance for normalized vectors = 2 - 2*cos_sim\n    distances = (emb1 - emb2).pow(2).sum(dim=1)\n\n    alignment = distances.pow(alpha / 2).mean()\n    return alignment.item()\n\n\ndef compute_uniformity(\n    embeddings: torch.Tensor,\n    t: float = 2.0,\n    sample_size: int = 10000\n) -> float:\n    \"\"\"\n    Compute uniformity metric: how uniformly distributed embeddings are.\n\n    Lower is better - embeddings should spread across the hypersphere.\n    Based on Wang & Isola (2020).\n\n    Args:\n        embeddings: (n_samples, dim) normalized embeddings\n        t: temperature parameter (default 2)\n        sample_size: number of pairs to sample for efficiency\n    \"\"\"\n    n = embeddings.shape[0]\n\n    if n * (n - 1) // 2 > sample_size:\n        # Sample pairs for efficiency\n        idx1 = torch.randint(0, n, (sample_size,))\n        idx2 = torch.randint(0, n, (sample_size,))\n        # Ensure different indices\n        mask = idx1 != idx2\n        idx1, idx2 = idx1[mask], idx2[mask]\n        emb1, emb2 = embeddings[idx1], embeddings[idx2]\n    else:\n        # Compute all pairs\n        emb1 = embeddings.unsqueeze(1)  # (n, 1, dim)\n        emb2 = embeddings.unsqueeze(0)  # (1, n, dim)\n\n    # Squared L2 distance\n    sq_distances = (emb1 - emb2).pow(2).sum(dim=-1)\n\n    # Uniformity loss (log of average Gaussian kernel)\n    uniformity = torch.log(torch.exp(-t * sq_distances).mean())\n\n    return uniformity.item()\n\n\ndef compute_alignment_uniformity(\n    embeddings: torch.Tensor,\n    positive_pairs: torch.Tensor = None,\n    labels: torch.Tensor = None\n) -> dict:\n    \"\"\"\n    Compute both alignment and uniformity metrics.\n\n    Args:\n        embeddings: (n_samples, dim) embeddings (will be normalized)\n        positive_pairs: (n_pairs, 2) indices of positive pairs, OR\n        labels: (n_samples,) class labels to generate positive pairs\n    \"\"\"\n    # Normalize embeddings\n    embeddings = F.normalize(embeddings, dim=1)\n\n    # Generate positive pairs from labels if needed\n    if positive_pairs is None and labels is not None:\n        positive_pairs = []\n        for label in labels.unique():\n            indices = (labels == label).nonzero().squeeze()\n            if len(indices) > 1:\n                # Sample pairs within class\n                for i in range(min(len(indices), 50)):\n                    for j in range(i + 1, min(len(indices), 50)):\n                        positive_pairs.append([indices[i].item(), indices[j].item()])\n        positive_pairs = torch.tensor(positive_pairs)\n\n    alignment = compute_alignment(embeddings, positive_pairs) if positive_pairs is not None else None\n    uniformity = compute_uniformity(embeddings)\n\n    return {\n        \"alignment\": alignment,\n        \"uniformity\": uniformity,\n        \"alignment_uniformity_sum\": (alignment + uniformity) if alignment else None\n    }\n\n\n# Example\ntorch.manual_seed(42)\nembeddings = torch.randn(500, 128)\nlabels = torch.randint(0, 10, (500,))  # 10 classes\n\nmetrics = compute_alignment_uniformity(embeddings, labels=labels)\nprint(f\"Alignment: {metrics['alignment']:.4f} (lower = positive pairs closer)\")\nprint(f\"Uniformity: {metrics['uniformity']:.4f} (lower = more spread out)\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAlignment: 1.9989 (lower = positive pairs closer)\nUniformity: -3.9338 (lower = more spread out)\n```\n:::\n:::\n\n\n:::{.callout-note}\n## Alignment vs Uniformity Trade-off\n\nPerfect alignment (all positives identical) destroys uniformity (everything clustered). Good embeddings balance both:\n\n- **High alignment + low uniformity**: Over-clustered, losing discriminative power\n- **Low alignment + high uniformity**: Good spread but positives too far apart\n- **Target**: Low alignment AND low uniformity (both close to optimal)\n\nTypical good values: alignment < 0.5, uniformity < -2.0\n:::\n\n### Dimension Utilization and Collapse\n\n**Representation collapse** occurs when embeddings fail to use the full dimensionality—a common failure mode in self-supervised learning.\n\n::: {#26c33659 .cell execution_count=3}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show dimension collapse detection\"}\nimport torch\n\ndef detect_dimension_collapse(embeddings: torch.Tensor, threshold: float = 0.01) -> dict:\n    \"\"\"\n    Detect dimension collapse in embeddings.\n\n    Collapse occurs when:\n    - Many dimensions have near-zero variance\n    - Embeddings are constant along certain dimensions\n    - Effective rank is much lower than nominal dimension\n\n    Args:\n        embeddings: (n_samples, dim) embeddings\n        threshold: variance threshold for \"dead\" dimensions\n    \"\"\"\n    # Per-dimension statistics\n    dim_means = embeddings.mean(dim=0)\n    dim_vars = embeddings.var(dim=0)\n    dim_stds = dim_vars.sqrt()\n\n    # Dead dimensions (near-zero variance)\n    dead_dims = (dim_vars < threshold).sum().item()\n    dead_ratio = dead_dims / embeddings.shape[1]\n\n    # Dimension variance distribution\n    var_percentiles = {\n        \"min\": dim_vars.min().item(),\n        \"p25\": dim_vars.quantile(0.25).item(),\n        \"median\": dim_vars.median().item(),\n        \"p75\": dim_vars.quantile(0.75).item(),\n        \"max\": dim_vars.max().item()\n    }\n\n    # SVD-based rank estimation\n    _, singular_values, _ = torch.svd(embeddings - embeddings.mean(dim=0))\n\n    # Effective rank (Roy & Bhattacharya, 2007)\n    normalized_sv = singular_values / singular_values.sum()\n    entropy = -(normalized_sv * normalized_sv.log()).sum()\n    effective_rank = entropy.exp().item()\n\n    # Stable rank\n    stable_rank = (singular_values.sum() ** 2) / (singular_values ** 2).sum()\n\n    return {\n        \"dead_dimensions\": dead_dims,\n        \"dead_ratio\": dead_ratio,\n        \"variance_distribution\": var_percentiles,\n        \"effective_rank\": effective_rank,\n        \"stable_rank\": stable_rank.item(),\n        \"nominal_dimension\": embeddings.shape[1],\n        \"collapse_detected\": dead_ratio > 0.1 or effective_rank < embeddings.shape[1] * 0.3\n    }\n\n\n# Example: Detect collapse\ntorch.manual_seed(42)\n\n# Healthy embeddings\nhealthy = torch.randn(1000, 256)\n\n# Collapsed embeddings (many dimensions unused)\ncollapsed = torch.randn(1000, 256)\ncollapsed[:, 50:] = 0.01 * torch.randn(1000, 206)  # Last 206 dims nearly dead\n\nprint(\"Healthy embeddings:\")\nh_metrics = detect_dimension_collapse(healthy)\nprint(f\"  Dead dimensions: {h_metrics['dead_dimensions']}/{h_metrics['nominal_dimension']}\")\nprint(f\"  Effective rank: {h_metrics['effective_rank']:.1f}\")\nprint(f\"  Collapse detected: {h_metrics['collapse_detected']}\")\n\nprint(\"\\nCollapsed embeddings:\")\nc_metrics = detect_dimension_collapse(collapsed)\nprint(f\"  Dead dimensions: {c_metrics['dead_dimensions']}/{c_metrics['nominal_dimension']}\")\nprint(f\"  Effective rank: {c_metrics['effective_rank']:.1f}\")\nprint(f\"  Collapse detected: {c_metrics['collapse_detected']}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nHealthy embeddings:\n  Dead dimensions: 0/256\n  Effective rank: 247.0\n  Collapse detected: False\n\nCollapsed embeddings:\n  Dead dimensions: 206/256\n  Effective rank: 61.5\n  Collapse detected: True\n```\n:::\n:::\n\n\n## Retrieval Metrics {#sec-retrieval-metrics}\n\nRetrieval metrics measure how well embeddings support similarity search—the most common downstream task. Understanding when to use each metric and how they differ is crucial for meaningful evaluation.\n\n### Recall@K: Did We Find the Relevant Items?\n\n**Recall@K** measures the fraction of relevant items found in the top K results. It answers: \"Of all the things I should find, how many did I actually find?\"\n\n$$\\text{Recall@K} = \\frac{|\\text{Relevant items in top K}|}{|\\text{Total relevant items}|}$$\n\n::: {#a096be0c .cell execution_count=4}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show Recall@K implementation\"}\nimport torch\nimport numpy as np\n\ndef recall_at_k(\n    query_embeddings: torch.Tensor,\n    corpus_embeddings: torch.Tensor,\n    relevance_labels: torch.Tensor,\n    k_values: list = [1, 5, 10, 50, 100]\n) -> dict:\n    \"\"\"\n    Compute Recall@K for embedding retrieval.\n\n    Args:\n        query_embeddings: (n_queries, dim)\n        corpus_embeddings: (n_corpus, dim)\n        relevance_labels: (n_queries, n_corpus) binary relevance matrix\n                         or (n_queries,) with corpus index of single relevant item\n        k_values: list of K values to compute\n\n    Returns:\n        Dictionary with Recall@K for each K\n    \"\"\"\n    # Compute similarities\n    query_norm = query_embeddings / query_embeddings.norm(dim=1, keepdim=True)\n    corpus_norm = corpus_embeddings / corpus_embeddings.norm(dim=1, keepdim=True)\n    similarities = query_norm @ corpus_norm.T  # (n_queries, n_corpus)\n\n    # Get rankings\n    rankings = similarities.argsort(dim=1, descending=True)\n\n    results = {}\n\n    # Handle single relevant item case\n    if relevance_labels.dim() == 1:\n        for k in k_values:\n            top_k = rankings[:, :k]\n            hits = (top_k == relevance_labels.unsqueeze(1)).any(dim=1)\n            results[f\"recall@{k}\"] = hits.float().mean().item()\n    else:\n        # Multiple relevant items case\n        for k in k_values:\n            top_k = rankings[:, :k]\n            recalls = []\n            for i in range(len(query_embeddings)):\n                relevant = relevance_labels[i].nonzero().squeeze(-1)\n                if len(relevant) == 0:\n                    continue\n                found = (top_k[i].unsqueeze(1) == relevant.unsqueeze(0)).any(dim=1).sum()\n                recalls.append(found.item() / len(relevant))\n            results[f\"recall@{k}\"] = np.mean(recalls) if recalls else 0.0\n\n    return results\n\n\n# Example\ntorch.manual_seed(42)\nn_queries, n_corpus, dim = 100, 10000, 256\n\nqueries = torch.randn(n_queries, dim)\ncorpus = torch.randn(n_corpus, dim)\n# Each query has one relevant document\nrelevant_indices = torch.randint(0, n_corpus, (n_queries,))\n\nresults = recall_at_k(queries, corpus, relevant_indices)\nfor k, v in results.items():\n    print(f\"{k}: {v:.4f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nrecall@1: 0.0000\nrecall@5: 0.0000\nrecall@10: 0.0000\nrecall@50: 0.0000\nrecall@100: 0.0100\n```\n:::\n:::\n\n\n### Precision@K: How Many Results Are Relevant?\n\n**Precision@K** measures the fraction of top K results that are relevant. It answers: \"Of what I returned, how much is useful?\"\n\n$$\\text{Precision@K} = \\frac{|\\text{Relevant items in top K}|}{K}$$\n\n::: {#49d6d743 .cell execution_count=5}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show Precision@K implementation\"}\nimport torch\n\ndef precision_at_k(\n    similarities: torch.Tensor,\n    relevance: torch.Tensor,\n    k_values: list = [1, 5, 10]\n) -> dict:\n    \"\"\"\n    Compute Precision@K.\n\n    Args:\n        similarities: (n_queries, n_corpus) similarity scores\n        relevance: (n_queries, n_corpus) binary relevance matrix\n        k_values: list of K values\n    \"\"\"\n    rankings = similarities.argsort(dim=1, descending=True)\n\n    results = {}\n    for k in k_values:\n        top_k_indices = rankings[:, :k]\n        # Gather relevance for top-k items\n        precisions = []\n        for i in range(len(similarities)):\n            relevant_in_topk = relevance[i, top_k_indices[i]].sum().item()\n            precisions.append(relevant_in_topk / k)\n        results[f\"precision@{k}\"] = np.mean(precisions)\n\n    return results\n\n\n# Example with multiple relevant items per query\ntorch.manual_seed(42)\nn_queries, n_corpus = 50, 1000\n\nsimilarities = torch.randn(n_queries, n_corpus)\n# Each query has ~10 relevant documents\nrelevance = (torch.rand(n_queries, n_corpus) < 0.01).float()\n\nresults = precision_at_k(similarities, relevance)\nfor k, v in results.items():\n    print(f\"{k}: {v:.4f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nprecision@1: 0.0000\nprecision@5: 0.0000\nprecision@10: 0.0020\n```\n:::\n:::\n\n\n### Mean Average Precision (MAP)\n\n**MAP** summarizes precision across all recall levels, rewarding systems that rank relevant items higher:\n\n$$\\text{AP} = \\frac{1}{|\\text{Relevant}|} \\sum_{k=1}^{N} P@k \\cdot \\text{rel}(k)$$\n\n$$\\text{MAP} = \\frac{1}{|Q|} \\sum_{q \\in Q} \\text{AP}(q)$$\n\n::: {#64ef05de .cell execution_count=6}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show MAP implementation\"}\nimport torch\nimport numpy as np\n\ndef average_precision(ranked_relevance: torch.Tensor) -> float:\n    \"\"\"\n    Compute Average Precision for a single query.\n\n    Args:\n        ranked_relevance: (n_items,) binary relevance in rank order\n    \"\"\"\n    relevant_mask = ranked_relevance.bool()\n    n_relevant = relevant_mask.sum().item()\n\n    if n_relevant == 0:\n        return 0.0\n\n    # Cumulative sum of relevant items up to each position\n    cum_relevant = ranked_relevance.cumsum(dim=0)\n\n    # Precision at each position\n    positions = torch.arange(1, len(ranked_relevance) + 1, dtype=torch.float32)\n    precisions = cum_relevant / positions\n\n    # AP = mean of precisions at relevant positions\n    ap = (precisions * ranked_relevance).sum() / n_relevant\n\n    return ap.item()\n\n\ndef mean_average_precision(\n    similarities: torch.Tensor,\n    relevance: torch.Tensor,\n    cutoff: int = None\n) -> dict:\n    \"\"\"\n    Compute Mean Average Precision.\n\n    Args:\n        similarities: (n_queries, n_corpus) similarity scores\n        relevance: (n_queries, n_corpus) binary relevance\n        cutoff: optional cutoff for ranking (MAP@K)\n    \"\"\"\n    rankings = similarities.argsort(dim=1, descending=True)\n\n    if cutoff:\n        rankings = rankings[:, :cutoff]\n\n    aps = []\n    for i in range(len(similarities)):\n        ranked_rel = relevance[i, rankings[i]]\n        aps.append(average_precision(ranked_rel))\n\n    return {\n        \"map\": np.mean(aps),\n        \"map_std\": np.std(aps),\n        \"min_ap\": np.min(aps),\n        \"max_ap\": np.max(aps)\n    }\n\n\n# Example\ntorch.manual_seed(42)\nsimilarities = torch.randn(100, 1000)\nrelevance = (torch.rand(100, 1000) < 0.02).float()\n\nmap_results = mean_average_precision(similarities, relevance)\nprint(f\"MAP: {map_results['map']:.4f} (std: {map_results['map_std']:.4f})\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMAP: 0.0259 (std: 0.0102)\n```\n:::\n:::\n\n\n### Mean Reciprocal Rank (MRR)\n\n**MRR** measures how high the first relevant result appears:\n\n$$\\text{MRR} = \\frac{1}{|Q|} \\sum_{q \\in Q} \\frac{1}{\\text{rank of first relevant}}$$\n\nMRR is particularly useful for navigational queries where users want one specific result.\n\n::: {#8f8c6d65 .cell execution_count=7}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show MRR implementation\"}\nimport torch\nimport numpy as np\n\ndef mean_reciprocal_rank(\n    similarities: torch.Tensor,\n    relevance: torch.Tensor\n) -> dict:\n    \"\"\"\n    Compute Mean Reciprocal Rank.\n\n    Args:\n        similarities: (n_queries, n_corpus) similarity scores\n        relevance: (n_queries, n_corpus) binary relevance\n    \"\"\"\n    rankings = similarities.argsort(dim=1, descending=True)\n\n    reciprocal_ranks = []\n\n    for i in range(len(similarities)):\n        ranked_rel = relevance[i, rankings[i]]\n        # Find first relevant item\n        first_relevant = (ranked_rel == 1).nonzero()\n\n        if len(first_relevant) > 0:\n            rank = first_relevant[0].item() + 1  # 1-indexed\n            reciprocal_ranks.append(1.0 / rank)\n        else:\n            reciprocal_ranks.append(0.0)\n\n    return {\n        \"mrr\": np.mean(reciprocal_ranks),\n        \"mrr_std\": np.std(reciprocal_ranks),\n        \"queries_with_relevant\": sum(1 for rr in reciprocal_ranks if rr > 0)\n    }\n\n\n# Example\ntorch.manual_seed(42)\nsimilarities = torch.randn(100, 1000)\nrelevance = (torch.rand(100, 1000) < 0.01).float()\n\nmrr_results = mean_reciprocal_rank(similarities, relevance)\nprint(f\"MRR: {mrr_results['mrr']:.4f}\")\nprint(f\"Queries with relevant results: {mrr_results['queries_with_relevant']}/100\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMRR: 0.0434\nQueries with relevant results: 100/100\n```\n:::\n:::\n\n\n### Normalized Discounted Cumulative Gain (NDCG)\n\n**NDCG** handles graded relevance (not just binary) and discounts the value of results lower in the ranking:\n\n$$\\text{DCG@K} = \\sum_{i=1}^{K} \\frac{2^{\\text{rel}_i} - 1}{\\log_2(i + 1)}$$\n\n$$\\text{NDCG@K} = \\frac{\\text{DCG@K}}{\\text{IDCG@K}}$$\n\nwhere IDCG is the DCG of the ideal (perfect) ranking.\n\n::: {#bb14a3ac .cell execution_count=8}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show NDCG implementation\"}\nimport torch\nimport numpy as np\n\ndef dcg_at_k(relevance_scores: torch.Tensor, k: int) -> float:\n    \"\"\"Compute DCG@K for graded relevance.\"\"\"\n    relevance_scores = relevance_scores[:k]\n    gains = 2 ** relevance_scores - 1\n    discounts = torch.log2(torch.arange(2, len(relevance_scores) + 2, dtype=torch.float32))\n    return (gains / discounts).sum().item()\n\n\ndef ndcg_at_k(\n    similarities: torch.Tensor,\n    relevance: torch.Tensor,\n    k_values: list = [5, 10, 20]\n) -> dict:\n    \"\"\"\n    Compute NDCG@K for graded relevance.\n\n    Args:\n        similarities: (n_queries, n_corpus) similarity scores\n        relevance: (n_queries, n_corpus) graded relevance (0, 1, 2, 3, ...)\n        k_values: list of K values\n    \"\"\"\n    rankings = similarities.argsort(dim=1, descending=True)\n\n    results = {}\n\n    for k in k_values:\n        ndcgs = []\n\n        for i in range(len(similarities)):\n            # Get relevance in predicted order\n            predicted_rel = relevance[i, rankings[i]]\n            dcg = dcg_at_k(predicted_rel, k)\n\n            # Get ideal relevance (sorted descending)\n            ideal_rel = relevance[i].sort(descending=True).values\n            idcg = dcg_at_k(ideal_rel, k)\n\n            ndcg = dcg / idcg if idcg > 0 else 0.0\n            ndcgs.append(ndcg)\n\n        results[f\"ndcg@{k}\"] = np.mean(ndcgs)\n\n    return results\n\n\n# Example with graded relevance (0=not relevant, 1=somewhat, 2=relevant, 3=highly relevant)\ntorch.manual_seed(42)\nsimilarities = torch.randn(100, 1000)\n# Graded relevance\nrelevance = torch.zeros(100, 1000)\nrelevance[torch.rand(100, 1000) < 0.01] = 1\nrelevance[torch.rand(100, 1000) < 0.005] = 2\nrelevance[torch.rand(100, 1000) < 0.002] = 3\n\nndcg_results = ndcg_at_k(similarities, relevance.float())\nfor k, v in ndcg_results.items():\n    print(f\"{k}: {v:.4f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nndcg@5: 0.0032\nndcg@10: 0.0066\nndcg@20: 0.0132\n```\n:::\n:::\n\n\n### When to Use Which Metric\n\n| Metric | Best For | Limitations |\n|--------|----------|-------------|\n| **Recall@K** | Measuring coverage, ensuring relevant items aren't missed | Ignores precision, treats all relevant items equally |\n| **Precision@K** | When false positives are costly (e.g., legal, medical) | Ignores items outside top K |\n| **MAP** | Comprehensive ranking quality, comparing systems | Assumes binary relevance |\n| **MRR** | Navigational queries with single correct answer | Only considers first relevant item |\n| **NDCG** | Graded relevance, nuanced quality assessment | Requires graded judgments, harder to interpret |\n\n:::{.callout-tip}\n## Metric Selection Guidelines\n\n**For product search**: Use NDCG (users prefer more relevant products) + Recall@100 (coverage)\n\n**For document retrieval**: Use MAP (comprehensive) + MRR (navigational queries)\n\n**For recommendations**: Use NDCG@10 (top matters most) + Precision@10 (quality of shown items)\n\n**For fraud detection**: Use Recall@K (can't miss fraud) + Precision (avoid alert fatigue)\n\nAlways report multiple metrics to get a complete picture.\n:::\n\n## Human Evaluation Framework {#sec-human-evaluation}\n\nAutomated metrics have limitations. Human evaluation provides ground truth that algorithms can't capture: subjective relevance, contextual appropriateness, and user satisfaction. This section covers how to collect high-quality human judgments at scale.\n\n### Designing Evaluation Tasks\n\nEffective human evaluation requires clear task design:\n\n::: {#10b8ad52 .cell execution_count=9}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show human evaluation task framework\"}\nfrom dataclasses import dataclass\nfrom typing import List, Optional\nfrom enum import Enum\n\nclass RelevanceScale(Enum):\n    \"\"\"Standard graded relevance scale (TREC-style).\"\"\"\n    NOT_RELEVANT = 0      # Completely irrelevant\n    MARGINALLY = 1        # Marginally relevant\n    FAIRLY = 2            # Fairly relevant\n    HIGHLY = 3            # Highly relevant\n    PERFECTLY = 4         # Perfect match\n\n\n@dataclass\nclass EvaluationTask:\n    \"\"\"A single human evaluation task.\"\"\"\n    task_id: str\n    query: str\n    candidate: str\n    context: Optional[str] = None\n    instructions: str = \"\"\n\n    def to_annotation_format(self) -> dict:\n        return {\n            \"id\": self.task_id,\n            \"query\": self.query,\n            \"candidate\": self.candidate,\n            \"context\": self.context,\n            \"instructions\": self.instructions,\n            \"scale\": [s.name for s in RelevanceScale]\n        }\n\n\n@dataclass\nclass AnnotationGuidelines:\n    \"\"\"Guidelines for human annotators.\"\"\"\n    task_description: str\n    relevance_definitions: dict\n    examples: List[dict]\n    edge_cases: List[str]\n\n    @staticmethod\n    def create_search_relevance_guidelines():\n        return AnnotationGuidelines(\n            task_description=\"\"\"\n            Rate how well each document answers the given query.\n            Consider: Does it answer the question? Is the information accurate?\n            Would a user be satisfied with this result?\n            \"\"\",\n            relevance_definitions={\n                \"NOT_RELEVANT\": \"Document has no useful information for the query\",\n                \"MARGINALLY\": \"Document is tangentially related but doesn't answer the query\",\n                \"FAIRLY\": \"Document partially answers the query or provides related info\",\n                \"HIGHLY\": \"Document substantially answers the query\",\n                \"PERFECTLY\": \"Document is an ideal answer to the query\"\n            },\n            examples=[\n                {\n                    \"query\": \"How to make sourdough bread\",\n                    \"document\": \"Sourdough bread recipe: Mix flour, water, starter...\",\n                    \"rating\": \"PERFECTLY\",\n                    \"reason\": \"Direct recipe for the query\"\n                },\n                {\n                    \"query\": \"How to make sourdough bread\",\n                    \"document\": \"The history of bread dates back 10,000 years...\",\n                    \"rating\": \"NOT_RELEVANT\",\n                    \"reason\": \"About bread history, not how to make sourdough\"\n                }\n            ],\n            edge_cases=[\n                \"If document is relevant but outdated, rate MARGINALLY\",\n                \"If document answers a related but different question, rate FAIRLY\",\n                \"If unsure between two ratings, choose the lower one\"\n            ]\n        )\n\n\nclass HumanEvaluationPipeline:\n    \"\"\"Pipeline for collecting and analyzing human judgments.\"\"\"\n\n    def __init__(self, guidelines: AnnotationGuidelines):\n        self.guidelines = guidelines\n        self.annotations = []\n        self.annotator_stats = {}\n\n    def create_task_batch(\n        self,\n        queries: List[str],\n        candidates: List[List[str]],\n        n_per_query: int = 10\n    ) -> List[EvaluationTask]:\n        \"\"\"Create a batch of evaluation tasks.\"\"\"\n        tasks = []\n        for i, (query, cands) in enumerate(zip(queries, candidates)):\n            for j, cand in enumerate(cands[:n_per_query]):\n                tasks.append(EvaluationTask(\n                    task_id=f\"q{i}_c{j}\",\n                    query=query,\n                    candidate=cand,\n                    instructions=self.guidelines.task_description\n                ))\n        return tasks\n\n    def compute_inter_annotator_agreement(\n        self,\n        annotations: List[dict]\n    ) -> dict:\n        \"\"\"\n        Compute inter-annotator agreement metrics.\n\n        Returns Cohen's Kappa for pairs and Fleiss' Kappa for groups.\n        \"\"\"\n        # Group by task\n        task_annotations = {}\n        for ann in annotations:\n            tid = ann[\"task_id\"]\n            if tid not in task_annotations:\n                task_annotations[tid] = []\n            task_annotations[tid].append(ann[\"rating\"])\n\n        # Only tasks with multiple annotations\n        multi = {k: v for k, v in task_annotations.items() if len(v) >= 2}\n\n        if not multi:\n            return {\"error\": \"No tasks with multiple annotations\"}\n\n        # Simple agreement rate\n        agreements = []\n        for ratings in multi.values():\n            # Check if all annotators agree\n            agreements.append(1.0 if len(set(ratings)) == 1 else 0.0)\n\n        # Pairwise agreement\n        pairwise = []\n        for ratings in multi.values():\n            for i in range(len(ratings)):\n                for j in range(i + 1, len(ratings)):\n                    pairwise.append(1.0 if ratings[i] == ratings[j] else 0.0)\n\n        return {\n            \"exact_agreement_rate\": sum(agreements) / len(agreements),\n            \"pairwise_agreement_rate\": sum(pairwise) / len(pairwise) if pairwise else 0,\n            \"tasks_with_multiple_annotations\": len(multi)\n        }\n\n\n# Example usage\nguidelines = AnnotationGuidelines.create_search_relevance_guidelines()\npipeline = HumanEvaluationPipeline(guidelines)\n\nprint(\"Task description:\")\nprint(guidelines.task_description)\nprint(\"\\nRelevance scale:\")\nfor level, desc in guidelines.relevance_definitions.items():\n    print(f\"  {level}: {desc}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTask description:\n\n            Rate how well each document answers the given query.\n            Consider: Does it answer the question? Is the information accurate?\n            Would a user be satisfied with this result?\n            \n\nRelevance scale:\n  NOT_RELEVANT: Document has no useful information for the query\n  MARGINALLY: Document is tangentially related but doesn't answer the query\n  FAIRLY: Document partially answers the query or provides related info\n  HIGHLY: Document substantially answers the query\n  PERFECTLY: Document is an ideal answer to the query\n```\n:::\n:::\n\n\n### Quality Assurance for Annotations\n\nAnnotation quality varies. Implement quality controls:\n\n::: {#ce552474 .cell execution_count=10}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show annotation quality assurance\"}\nimport numpy as np\nfrom collections import defaultdict\n\nclass AnnotationQualityMonitor:\n    \"\"\"Monitor and filter annotation quality.\"\"\"\n\n    def __init__(self, gold_standard_tasks: dict):\n        \"\"\"\n        Args:\n            gold_standard_tasks: {task_id: expected_rating} for quality checks\n        \"\"\"\n        self.gold_standard = gold_standard_tasks\n        self.annotator_performance = defaultdict(lambda: {\"correct\": 0, \"total\": 0})\n\n    def check_annotation(self, annotator_id: str, task_id: str, rating: int) -> dict:\n        \"\"\"Check annotation against gold standard if available.\"\"\"\n        result = {\"is_gold\": False, \"correct\": None}\n\n        if task_id in self.gold_standard:\n            result[\"is_gold\"] = True\n            expected = self.gold_standard[task_id]\n            result[\"correct\"] = (rating == expected)\n\n            self.annotator_performance[annotator_id][\"total\"] += 1\n            if result[\"correct\"]:\n                self.annotator_performance[annotator_id][\"correct\"] += 1\n\n        return result\n\n    def get_annotator_accuracy(self, annotator_id: str) -> float:\n        \"\"\"Get annotator's accuracy on gold standard tasks.\"\"\"\n        perf = self.annotator_performance[annotator_id]\n        if perf[\"total\"] == 0:\n            return None\n        return perf[\"correct\"] / perf[\"total\"]\n\n    def get_reliable_annotators(self, min_accuracy: float = 0.7, min_tasks: int = 10) -> List[str]:\n        \"\"\"Get list of annotators meeting quality threshold.\"\"\"\n        reliable = []\n        for annotator_id, perf in self.annotator_performance.items():\n            if perf[\"total\"] >= min_tasks:\n                accuracy = perf[\"correct\"] / perf[\"total\"]\n                if accuracy >= min_accuracy:\n                    reliable.append(annotator_id)\n        return reliable\n\n    def filter_annotations(\n        self,\n        annotations: List[dict],\n        require_agreement: bool = True,\n        min_annotator_accuracy: float = 0.7\n    ) -> List[dict]:\n        \"\"\"Filter annotations based on quality criteria.\"\"\"\n        reliable_annotators = set(self.get_reliable_annotators(min_annotator_accuracy))\n\n        # First filter: annotator quality\n        quality_filtered = [\n            a for a in annotations\n            if a[\"annotator_id\"] in reliable_annotators\n        ]\n\n        if not require_agreement:\n            return quality_filtered\n\n        # Second filter: annotation agreement\n        task_ratings = defaultdict(list)\n        for a in quality_filtered:\n            task_ratings[a[\"task_id\"]].append(a)\n\n        final_annotations = []\n        for task_id, ratings in task_ratings.items():\n            if len(ratings) < 2:\n                continue\n\n            # Check for agreement (allow ±1 difference)\n            values = [r[\"rating\"] for r in ratings]\n            if max(values) - min(values) <= 1:\n                # Use median rating\n                median_rating = int(np.median(values))\n                final_annotations.append({\n                    \"task_id\": task_id,\n                    \"rating\": median_rating,\n                    \"confidence\": 1.0 - (max(values) - min(values)) / 4,\n                    \"n_annotators\": len(ratings)\n                })\n\n        return final_annotations\n\n\n# Example\ngold_standard = {\"q0_c0\": 3, \"q1_c0\": 1, \"q2_c0\": 4}\nmonitor = AnnotationQualityMonitor(gold_standard)\n\n# Simulate annotations\nfor i, (tid, expected) in enumerate(gold_standard.items()):\n    # Good annotator gets most right\n    monitor.check_annotation(\"annotator_1\", tid, expected)\n    # Poor annotator gets some wrong\n    monitor.check_annotation(\"annotator_2\", tid, expected if i % 2 == 0 else expected - 1)\n\nprint(\"Annotator accuracy on gold standard:\")\nprint(f\"  Annotator 1: {monitor.get_annotator_accuracy('annotator_1'):.0%}\")\nprint(f\"  Annotator 2: {monitor.get_annotator_accuracy('annotator_2'):.0%}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnnotator accuracy on gold standard:\n  Annotator 1: 100%\n  Annotator 2: 67%\n```\n:::\n:::\n\n\n:::{.callout-warning}\n## Common Annotation Pitfalls\n\n1. **Position bias**: Annotators rate earlier items higher—randomize order\n2. **Fatigue**: Quality drops after many annotations—limit session length\n3. **Anchoring**: First example influences all subsequent ratings—vary examples\n4. **Scale confusion**: Annotators interpret scales differently—provide clear examples\n5. **Speed-accuracy trade-off**: Fast annotators often less accurate—monitor speed\n\n**Mitigation strategies:**\n- Include 10-15% gold standard tasks for quality monitoring\n- Require minimum time per task (e.g., 10 seconds)\n- Use attention check questions\n- Collect 3+ annotations per task for agreement filtering\n:::\n\n## Domain-Specific Metrics {#sec-domain-specific-metrics}\n\nDifferent applications require specialized metrics. This section covers evaluation frameworks for common embedding use cases.\n\n### E-Commerce and Product Search\n\n::: {#1e490b36 .cell execution_count=11}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show e-commerce metrics\"}\nimport numpy as np\nfrom collections import Counter\n\nclass ECommerceMetrics:\n    \"\"\"Evaluation metrics specific to e-commerce search and recommendations.\"\"\"\n\n    @staticmethod\n    def zero_result_rate(queries: list, results: list) -> float:\n        \"\"\"Fraction of queries returning no results.\"\"\"\n        zero_results = sum(1 for r in results if len(r) == 0)\n        return zero_results / len(queries)\n\n    @staticmethod\n    def query_abandonment_rate(\n        queries: list,\n        results: list,\n        clicks: list\n    ) -> float:\n        \"\"\"Fraction of queries where user didn't click any result.\"\"\"\n        abandoned = sum(\n            1 for r, c in zip(results, clicks)\n            if len(r) > 0 and len(c) == 0\n        )\n        queries_with_results = sum(1 for r in results if len(r) > 0)\n        return abandoned / queries_with_results if queries_with_results > 0 else 0\n\n    @staticmethod\n    def catalog_coverage(\n        recommended_items: list,\n        total_catalog_size: int\n    ) -> float:\n        \"\"\"Fraction of catalog that appears in recommendations.\"\"\"\n        unique_recommended = len(set(item for items in recommended_items for item in items))\n        return unique_recommended / total_catalog_size\n\n    @staticmethod\n    def diversity_at_k(results: list, item_categories: dict, k: int = 10) -> float:\n        \"\"\"\n        Intra-list diversity: variety of categories in top-K results.\n        Higher = more diverse recommendations.\n        \"\"\"\n        diversities = []\n        for result in results:\n            top_k = result[:k]\n            categories = [item_categories.get(item, \"unknown\") for item in top_k]\n            unique_categories = len(set(categories))\n            diversities.append(unique_categories / k if k > 0 else 0)\n        return np.mean(diversities)\n\n    @staticmethod\n    def novelty(\n        recommendations: list,\n        item_popularity: dict,\n        k: int = 10\n    ) -> float:\n        \"\"\"\n        Novelty: tendency to recommend less popular (long-tail) items.\n        Higher = recommending more novel items.\n        \"\"\"\n        novelties = []\n        max_pop = max(item_popularity.values()) if item_popularity else 1\n\n        for rec in recommendations:\n            top_k = rec[:k]\n            # Novelty = -log(popularity), normalized\n            item_novelties = []\n            for item in top_k:\n                pop = item_popularity.get(item, 1) / max_pop\n                item_novelties.append(-np.log(pop + 1e-10))\n            novelties.append(np.mean(item_novelties) if item_novelties else 0)\n\n        return np.mean(novelties)\n\n    @staticmethod\n    def revenue_per_search(\n        queries: list,\n        clicks: list,\n        purchases: list,\n        item_prices: dict\n    ) -> float:\n        \"\"\"Average revenue generated per search query.\"\"\"\n        total_revenue = sum(\n            item_prices.get(item, 0)\n            for purchase_list in purchases\n            for item in purchase_list\n        )\n        return total_revenue / len(queries) if queries else 0\n\n\n# Example\nmetrics = ECommerceMetrics()\n\n# Simulate search results\nqueries = [\"wireless headphones\", \"running shoes\", \"laptop case\"]\nresults = [\n    [\"item_1\", \"item_2\", \"item_3\"],\n    [\"item_4\", \"item_5\"],\n    []  # Zero result query\n]\n\nprint(f\"Zero result rate: {metrics.zero_result_rate(queries, results):.1%}\")\n\n# Catalog coverage\nall_recommended = [[\"item_1\", \"item_2\"], [\"item_3\", \"item_4\"], [\"item_1\", \"item_5\"]]\nprint(f\"Catalog coverage: {metrics.catalog_coverage(all_recommended, 100):.1%}\")\n\n# Diversity\nitem_categories = {f\"item_{i}\": f\"cat_{i % 3}\" for i in range(10)}\nprint(f\"Diversity@3: {metrics.diversity_at_k(results[:2], item_categories, k=3):.2f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nZero result rate: 33.3%\nCatalog coverage: 5.0%\nDiversity@3: 0.83\n```\n:::\n:::\n\n\n### Recommendation Systems\n\n::: {#0561af98 .cell execution_count=12}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show recommendation metrics\"}\nimport numpy as np\n\nclass RecommendationMetrics:\n    \"\"\"Metrics for evaluating recommendation systems.\"\"\"\n\n    @staticmethod\n    def hit_rate(\n        recommendations: list,\n        ground_truth: list,\n        k: int = 10\n    ) -> float:\n        \"\"\"Fraction of users with at least one relevant item in top-K.\"\"\"\n        hits = 0\n        for recs, truth in zip(recommendations, ground_truth):\n            if set(recs[:k]) & set(truth):\n                hits += 1\n        return hits / len(recommendations)\n\n    @staticmethod\n    def serendipity(\n        recommendations: list,\n        user_history: list,\n        item_similarity: dict,\n        k: int = 10\n    ) -> float:\n        \"\"\"\n        Serendipity: relevant recommendations that are unexpected.\n        Balances relevance with surprise.\n        \"\"\"\n        serendipities = []\n\n        for recs, history in zip(recommendations, user_history):\n            top_k = recs[:k]\n            rec_serendipity = []\n\n            for rec in top_k:\n                # How different is this from user's history?\n                min_similarity = min(\n                    item_similarity.get((rec, h), item_similarity.get((h, rec), 0.5))\n                    for h in history\n                ) if history else 1.0\n\n                # Serendipity = 1 - max_similarity (higher when more different)\n                rec_serendipity.append(1 - min_similarity)\n\n            serendipities.append(np.mean(rec_serendipity) if rec_serendipity else 0)\n\n        return np.mean(serendipities)\n\n    @staticmethod\n    def gini_coefficient(item_recommendation_counts: list) -> float:\n        \"\"\"\n        Gini coefficient of recommendation distribution.\n        0 = perfect equality (all items recommended equally)\n        1 = perfect inequality (one item gets all recommendations)\n\n        Use to detect popularity bias.\n        \"\"\"\n        counts = np.array(sorted(item_recommendation_counts))\n        n = len(counts)\n        index = np.arange(1, n + 1)\n        return (2 * np.sum(index * counts) - (n + 1) * np.sum(counts)) / (n * np.sum(counts))\n\n    @staticmethod\n    def beyond_accuracy_report(\n        recommendations: list,\n        item_categories: dict,\n        item_popularity: dict,\n        k: int = 10\n    ) -> dict:\n        \"\"\"Comprehensive beyond-accuracy metrics report.\"\"\"\n        # Aggregate statistics\n        all_recs = [item for rec in recommendations for item in rec[:k]]\n        rec_counts = Counter(all_recs)\n\n        # Coverage\n        coverage = len(set(all_recs)) / len(item_popularity)\n\n        # Gini (popularity concentration)\n        popularity_counts = list(rec_counts.values())\n        gini = RecommendationMetrics.gini_coefficient(popularity_counts)\n\n        # Category coverage\n        rec_categories = set(item_categories.get(item, \"unk\") for item in set(all_recs))\n        category_coverage = len(rec_categories) / len(set(item_categories.values()))\n\n        # Popularity bias\n        avg_pop = np.mean([item_popularity.get(item, 0) for item in all_recs])\n        overall_avg_pop = np.mean(list(item_popularity.values()))\n        popularity_bias = avg_pop / overall_avg_pop\n\n        return {\n            \"catalog_coverage\": coverage,\n            \"gini_coefficient\": gini,\n            \"category_coverage\": category_coverage,\n            \"popularity_bias\": popularity_bias,  # >1 means biased toward popular\n            \"unique_items_recommended\": len(set(all_recs))\n        }\n\n\n# Example\nmetrics = RecommendationMetrics()\n\n# Simulate\nrecommendations = [\n    [\"item_1\", \"item_2\", \"item_3\"],\n    [\"item_1\", \"item_4\", \"item_5\"],\n    [\"item_1\", \"item_2\", \"item_6\"]\n]\nground_truth = [[\"item_2\", \"item_7\"], [\"item_4\"], [\"item_6\", \"item_8\"]]\n\nprint(f\"Hit rate@3: {metrics.hit_rate(recommendations, ground_truth, k=3):.2f}\")\n\n# Beyond accuracy\nitem_popularity = {f\"item_{i}\": 100 - i*10 for i in range(1, 11)}\nitem_categories = {f\"item_{i}\": f\"cat_{i % 3}\" for i in range(1, 11)}\n\nreport = metrics.beyond_accuracy_report(recommendations, item_categories, item_popularity, k=3)\nprint(f\"\\nBeyond-accuracy metrics:\")\nfor k, v in report.items():\n    print(f\"  {k}: {v:.2f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nHit rate@3: 1.00\n\nBeyond-accuracy metrics:\n  catalog_coverage: 0.60\n  gini_coefficient: 0.24\n  category_coverage: 1.00\n  popularity_bias: 1.60\n  unique_items_recommended: 6.00\n```\n:::\n:::\n\n\n### Anomaly Detection and Fraud\n\n::: {#99b0b409 .cell execution_count=13}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show anomaly detection metrics\"}\nimport numpy as np\n\nclass AnomalyDetectionMetrics:\n    \"\"\"Metrics for evaluating anomaly/fraud detection systems.\"\"\"\n\n    @staticmethod\n    def detection_rate_at_false_positive_rate(\n        scores: np.ndarray,\n        labels: np.ndarray,\n        target_fpr: float = 0.01\n    ) -> dict:\n        \"\"\"\n        Detection rate (recall) at a specific false positive rate.\n        Critical for fraud detection where FP rate must be controlled.\n        \"\"\"\n        # Sort by score descending\n        sorted_indices = np.argsort(scores)[::-1]\n        sorted_labels = labels[sorted_indices]\n\n        n_positives = labels.sum()\n        n_negatives = len(labels) - n_positives\n\n        # Find threshold achieving target FPR\n        max_fp = int(target_fpr * n_negatives)\n\n        fp = 0\n        tp = 0\n        threshold_idx = 0\n\n        for i, label in enumerate(sorted_labels):\n            if label == 1:\n                tp += 1\n            else:\n                fp += 1\n                if fp >= max_fp:\n                    threshold_idx = i\n                    break\n\n        detection_rate = tp / n_positives if n_positives > 0 else 0\n        actual_fpr = fp / n_negatives if n_negatives > 0 else 0\n\n        return {\n            \"detection_rate\": detection_rate,\n            \"actual_fpr\": actual_fpr,\n            \"target_fpr\": target_fpr,\n            \"threshold_index\": threshold_idx\n        }\n\n    @staticmethod\n    def cost_sensitive_evaluation(\n        predictions: np.ndarray,\n        labels: np.ndarray,\n        fp_cost: float,\n        fn_cost: float\n    ) -> dict:\n        \"\"\"\n        Evaluate with asymmetric costs.\n\n        Args:\n            fp_cost: Cost of false positive (e.g., investigation cost)\n            fn_cost: Cost of false negative (e.g., fraud loss)\n        \"\"\"\n        tp = ((predictions == 1) & (labels == 1)).sum()\n        fp = ((predictions == 1) & (labels == 0)).sum()\n        tn = ((predictions == 0) & (labels == 0)).sum()\n        fn = ((predictions == 0) & (labels == 1)).sum()\n\n        total_cost = fp * fp_cost + fn * fn_cost\n\n        # Compare to always-predict-negative baseline\n        baseline_cost = labels.sum() * fn_cost\n\n        return {\n            \"total_cost\": total_cost,\n            \"cost_per_prediction\": total_cost / len(labels),\n            \"cost_reduction_vs_baseline\": 1 - (total_cost / baseline_cost) if baseline_cost > 0 else 0,\n            \"confusion_matrix\": {\"tp\": tp, \"fp\": fp, \"tn\": tn, \"fn\": fn}\n        }\n\n    @staticmethod\n    def time_to_detection(\n        anomaly_timestamps: list,\n        detection_timestamps: list\n    ) -> dict:\n        \"\"\"\n        How quickly anomalies are detected after they occur.\n        \"\"\"\n        detection_times = []\n\n        for anomaly_time, detection_time in zip(anomaly_timestamps, detection_timestamps):\n            if detection_time is not None:\n                detection_times.append(detection_time - anomaly_time)\n\n        if not detection_times:\n            return {\"mean_ttd\": None, \"detection_rate\": 0}\n\n        return {\n            \"mean_ttd\": np.mean(detection_times),\n            \"median_ttd\": np.median(detection_times),\n            \"p95_ttd\": np.percentile(detection_times, 95),\n            \"detection_rate\": len(detection_times) / len(anomaly_timestamps)\n        }\n\n\n# Example\nnp.random.seed(42)\nn_samples = 10000\nfraud_rate = 0.01\n\nlabels = (np.random.random(n_samples) < fraud_rate).astype(int)\n# Scores: frauds should have higher scores on average\nscores = np.random.random(n_samples)\nscores[labels == 1] += 0.3\nscores = np.clip(scores, 0, 1)\n\nmetrics = AnomalyDetectionMetrics()\n\n# Detection at 1% FPR\nresult = metrics.detection_rate_at_false_positive_rate(scores, labels, target_fpr=0.01)\nprint(f\"Detection rate at 1% FPR: {result['detection_rate']:.1%}\")\n\n# Cost-sensitive evaluation\npredictions = (scores > 0.5).astype(int)\ncost_result = metrics.cost_sensitive_evaluation(\n    predictions, labels,\n    fp_cost=100,    # $100 investigation cost\n    fn_cost=10000   # $10,000 fraud loss\n)\nprint(f\"Cost reduction vs baseline: {cost_result['cost_reduction_vs_baseline']:.1%}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDetection rate at 1% FPR: 24.2%\nCost reduction vs baseline: 23.0%\n```\n:::\n:::\n\n\n## Statistical Rigor {#sec-statistical-rigor}\n\nEmbedding evaluation requires statistical rigor to draw valid conclusions. This section covers sample size calculation, significance testing, and multiple comparison corrections.\n\n### Sample Size and Power Analysis\n\n::: {#068e704a .cell execution_count=14}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show power analysis for A/B testing\"}\nimport numpy as np\nfrom scipy import stats\n\ndef sample_size_for_metric_change(\n    baseline_metric: float,\n    minimum_detectable_effect: float,\n    metric_variance: float,\n    alpha: float = 0.05,\n    power: float = 0.8\n) -> int:\n    \"\"\"\n    Calculate required sample size for detecting a metric change.\n\n    Args:\n        baseline_metric: Current metric value (e.g., 0.15 for 15% CTR)\n        minimum_detectable_effect: Relative change to detect (e.g., 0.05 for 5% improvement)\n        metric_variance: Variance of the metric\n        alpha: Significance level (Type I error rate)\n        power: Statistical power (1 - Type II error rate)\n\n    Returns:\n        Required sample size per group\n    \"\"\"\n    effect_size = baseline_metric * minimum_detectable_effect\n\n    z_alpha = stats.norm.ppf(1 - alpha/2)  # Two-tailed\n    z_beta = stats.norm.ppf(power)\n\n    # Sample size formula for two-sample t-test\n    n = 2 * ((z_alpha + z_beta) ** 2) * metric_variance / (effect_size ** 2)\n\n    return int(np.ceil(n))\n\n\ndef minimum_detectable_effect(\n    sample_size: int,\n    baseline_metric: float,\n    metric_variance: float,\n    alpha: float = 0.05,\n    power: float = 0.8\n) -> float:\n    \"\"\"\n    Calculate minimum detectable effect given sample size.\n    \"\"\"\n    z_alpha = stats.norm.ppf(1 - alpha/2)\n    z_beta = stats.norm.ppf(power)\n\n    effect = np.sqrt(2 * ((z_alpha + z_beta) ** 2) * metric_variance / sample_size)\n\n    return effect / baseline_metric\n\n\n# Example: CTR experiment\nbaseline_ctr = 0.15\nctr_variance = baseline_ctr * (1 - baseline_ctr)  # Bernoulli variance\n\nprint(\"Sample size requirements for CTR experiment:\")\nfor mde in [0.01, 0.02, 0.05, 0.10]:\n    n = sample_size_for_metric_change(baseline_ctr, mde, ctr_variance)\n    print(f\"  Detect {mde:.0%} change: {n:,} samples per group\")\n\nprint(\"\\nMinimum detectable effect for given sample sizes:\")\nfor n in [1000, 10000, 100000]:\n    mde = minimum_detectable_effect(n, baseline_ctr, ctr_variance)\n    print(f\"  n={n:,}: can detect {mde:.1%} change\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSample size requirements for CTR experiment:\n  Detect 1% change: 889,540 samples per group\n  Detect 2% change: 222,385 samples per group\n  Detect 5% change: 35,582 samples per group\n  Detect 10% change: 8,896 samples per group\n\nMinimum detectable effect for given sample sizes:\n  n=1,000: can detect 29.8% change\n  n=10,000: can detect 9.4% change\n  n=100,000: can detect 3.0% change\n```\n:::\n:::\n\n\n### Confidence Intervals for Metrics\n\n::: {#aa4b728a .cell execution_count=15}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show bootstrap confidence intervals\"}\nimport numpy as np\n\ndef bootstrap_confidence_interval(\n    metric_func,\n    data: np.ndarray,\n    n_bootstrap: int = 1000,\n    confidence: float = 0.95\n) -> dict:\n    \"\"\"\n    Compute bootstrap confidence interval for any metric.\n\n    Args:\n        metric_func: Function that computes metric from data\n        data: Array of data points\n        n_bootstrap: Number of bootstrap samples\n        confidence: Confidence level\n    \"\"\"\n    point_estimate = metric_func(data)\n\n    bootstrap_estimates = []\n    for _ in range(n_bootstrap):\n        # Sample with replacement\n        sample = np.random.choice(data, size=len(data), replace=True)\n        bootstrap_estimates.append(metric_func(sample))\n\n    bootstrap_estimates = np.array(bootstrap_estimates)\n\n    alpha = 1 - confidence\n    lower = np.percentile(bootstrap_estimates, 100 * alpha/2)\n    upper = np.percentile(bootstrap_estimates, 100 * (1 - alpha/2))\n\n    return {\n        \"point_estimate\": point_estimate,\n        \"ci_lower\": lower,\n        \"ci_upper\": upper,\n        \"confidence\": confidence,\n        \"std_error\": bootstrap_estimates.std()\n    }\n\n\n# Example: Confidence interval for recall@10\nnp.random.seed(42)\n# Simulate recall values for 1000 queries\nrecall_values = np.random.beta(8, 2, 1000)  # Skewed distribution\n\nresult = bootstrap_confidence_interval(np.mean, recall_values)\nprint(f\"Recall@10: {result['point_estimate']:.4f}\")\nprint(f\"95% CI: [{result['ci_lower']:.4f}, {result['ci_upper']:.4f}]\")\nprint(f\"Standard error: {result['std_error']:.4f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRecall@10: 0.7956\n95% CI: [0.7880, 0.8023]\nStandard error: 0.0036\n```\n:::\n:::\n\n\n### Multiple Testing Correction\n\nWhen evaluating multiple metrics, the chance of false positives increases. Apply corrections:\n\n::: {#0e43c8ab .cell execution_count=16}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show multiple testing correction\"}\nimport numpy as np\n\ndef bonferroni_correction(p_values: list, alpha: float = 0.05) -> dict:\n    \"\"\"\n    Bonferroni correction: most conservative.\n    Adjusted alpha = alpha / n_tests\n    \"\"\"\n    n_tests = len(p_values)\n    adjusted_alpha = alpha / n_tests\n\n    significant = [p < adjusted_alpha for p in p_values]\n\n    return {\n        \"method\": \"bonferroni\",\n        \"original_alpha\": alpha,\n        \"adjusted_alpha\": adjusted_alpha,\n        \"significant\": significant,\n        \"n_significant\": sum(significant)\n    }\n\n\ndef benjamini_hochberg_correction(p_values: list, alpha: float = 0.05) -> dict:\n    \"\"\"\n    Benjamini-Hochberg: controls False Discovery Rate.\n    Less conservative than Bonferroni, more power.\n    \"\"\"\n    n_tests = len(p_values)\n    sorted_indices = np.argsort(p_values)\n    sorted_p = np.array(p_values)[sorted_indices]\n\n    # BH threshold: p_i <= (i/n) * alpha\n    thresholds = [(i + 1) / n_tests * alpha for i in range(n_tests)]\n\n    # Find largest k where p_k <= threshold_k\n    significant_sorted = [False] * n_tests\n    max_significant = -1\n\n    for i in range(n_tests):\n        if sorted_p[i] <= thresholds[i]:\n            max_significant = i\n\n    for i in range(max_significant + 1):\n        significant_sorted[i] = True\n\n    # Map back to original order\n    significant = [False] * n_tests\n    for i, orig_idx in enumerate(sorted_indices):\n        significant[orig_idx] = significant_sorted[i]\n\n    return {\n        \"method\": \"benjamini_hochberg\",\n        \"original_alpha\": alpha,\n        \"significant\": significant,\n        \"n_significant\": sum(significant)\n    }\n\n\n# Example: Testing multiple metrics\nnp.random.seed(42)\np_values = [0.001, 0.02, 0.03, 0.04, 0.06, 0.15, 0.25]\nmetric_names = [\"NDCG@10\", \"Recall@10\", \"MRR\", \"Precision@10\", \"CTR\", \"Dwell\", \"Bounce\"]\n\nprint(\"P-values and significance (alpha=0.05):\")\nprint(\"-\" * 50)\n\nbonf = bonferroni_correction(p_values)\nbh = benjamini_hochberg_correction(p_values)\n\nfor i, (name, p) in enumerate(zip(metric_names, p_values)):\n    bonf_sig = \"✓\" if bonf[\"significant\"][i] else \"✗\"\n    bh_sig = \"✓\" if bh[\"significant\"][i] else \"✗\"\n    print(f\"{name:15} p={p:.3f}  Bonferroni: {bonf_sig}  BH: {bh_sig}\")\n\nprint(\"-\" * 50)\nprint(f\"Bonferroni significant: {bonf['n_significant']}/{len(p_values)}\")\nprint(f\"Benjamini-Hochberg significant: {bh['n_significant']}/{len(p_values)}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nP-values and significance (alpha=0.05):\n--------------------------------------------------\nNDCG@10         p=0.001  Bonferroni: ✓  BH: ✓\nRecall@10       p=0.020  Bonferroni: ✗  BH: ✗\nMRR             p=0.030  Bonferroni: ✗  BH: ✗\nPrecision@10    p=0.040  Bonferroni: ✗  BH: ✗\nCTR             p=0.060  Bonferroni: ✗  BH: ✗\nDwell           p=0.150  Bonferroni: ✗  BH: ✗\nBounce          p=0.250  Bonferroni: ✗  BH: ✗\n--------------------------------------------------\nBonferroni significant: 1/7\nBenjamini-Hochberg significant: 1/7\n```\n:::\n:::\n\n\n:::{.callout-tip}\n## When to Use Which Correction\n\n**Bonferroni**: Use when false positives are very costly (medical, financial decisions). Very conservative—may miss real effects.\n\n**Benjamini-Hochberg**: Use for exploratory analysis or when some false positives are acceptable. Controls False Discovery Rate rather than family-wise error rate.\n\n**No correction**: Only when metrics are truly independent and you're comfortable with inflated Type I error.\n\n**Rule of thumb**: If you're making decisions based on results, use correction. If exploring data for hypotheses to test later, correction may be optional.\n:::\n\n## Evaluation at Scale {#sec-evaluation-at-scale}\n\nEvaluating embeddings over trillions of items requires efficient sampling and computation strategies.\n\n### Stratified Sampling for Large Corpora\n\n::: {#e0056271 .cell execution_count=17}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show stratified sampling strategy\"}\nimport numpy as np\nfrom collections import defaultdict\n\nclass StratifiedEvaluationSampler:\n    \"\"\"Efficient stratified sampling for large-scale evaluation.\"\"\"\n\n    def __init__(self, corpus_size: int, strata_assignments: dict):\n        \"\"\"\n        Args:\n            corpus_size: Total number of items\n            strata_assignments: {stratum_name: [item_indices]}\n        \"\"\"\n        self.corpus_size = corpus_size\n        self.strata = strata_assignments\n\n    def sample_stratified(\n        self,\n        total_sample_size: int,\n        allocation: str = \"proportional\",\n        min_per_stratum: int = 100\n    ) -> dict:\n        \"\"\"\n        Draw stratified sample.\n\n        Args:\n            total_sample_size: Total samples to draw\n            allocation: 'proportional', 'equal', or 'neyman' (optimal)\n            min_per_stratum: Minimum samples per stratum\n        \"\"\"\n        stratum_sizes = {k: len(v) for k, v in self.strata.items()}\n        n_strata = len(self.strata)\n\n        # Determine allocation\n        if allocation == \"proportional\":\n            weights = {k: v / self.corpus_size for k, v in stratum_sizes.items()}\n        elif allocation == \"equal\":\n            weights = {k: 1 / n_strata for k in self.strata}\n        else:\n            raise ValueError(f\"Unknown allocation: {allocation}\")\n\n        # Allocate samples\n        samples_per_stratum = {}\n        remaining = total_sample_size - min_per_stratum * n_strata\n\n        for stratum in self.strata:\n            base = min_per_stratum\n            additional = int(remaining * weights[stratum])\n            samples_per_stratum[stratum] = min(base + additional, stratum_sizes[stratum])\n\n        # Draw samples\n        sampled_indices = {}\n        for stratum, indices in self.strata.items():\n            n_sample = samples_per_stratum[stratum]\n            sampled_indices[stratum] = np.random.choice(\n                indices, size=n_sample, replace=False\n            ).tolist()\n\n        return {\n            \"samples_per_stratum\": samples_per_stratum,\n            \"sampled_indices\": sampled_indices,\n            \"total_sampled\": sum(samples_per_stratum.values())\n        }\n\n    def oversample_rare_strata(\n        self,\n        base_sample: dict,\n        rare_strata: list,\n        oversample_factor: float = 3.0\n    ) -> dict:\n        \"\"\"Oversample rare but important strata (e.g., tail queries).\"\"\"\n        enhanced_indices = dict(base_sample[\"sampled_indices\"])\n\n        for stratum in rare_strata:\n            if stratum in self.strata:\n                current_n = len(enhanced_indices[stratum])\n                target_n = min(\n                    int(current_n * oversample_factor),\n                    len(self.strata[stratum])\n                )\n                enhanced_indices[stratum] = np.random.choice(\n                    self.strata[stratum], size=target_n, replace=False\n                ).tolist()\n\n        return {\n            \"sampled_indices\": enhanced_indices,\n            \"total_sampled\": sum(len(v) for v in enhanced_indices.values())\n        }\n\n\n# Example\nnp.random.seed(42)\ncorpus_size = 10_000_000\n\n# Define strata based on item popularity\nstrata = {\n    \"head\": list(range(0, 1000)),           # Top 1K items (0.01%)\n    \"torso\": list(range(1000, 100000)),     # Next 99K (1%)\n    \"tail\": list(range(100000, corpus_size)) # Rest (99%)\n}\n\nsampler = StratifiedEvaluationSampler(corpus_size, strata)\n\n# Draw sample\nsample = sampler.sample_stratified(total_sample_size=10000)\nprint(\"Proportional stratified sample:\")\nfor stratum, n in sample[\"samples_per_stratum\"].items():\n    print(f\"  {stratum}: {n} samples\")\n\n# Oversample tail\nenhanced = sampler.oversample_rare_strata(sample, rare_strata=[\"tail\"])\nprint(f\"\\nAfter oversampling tail: {enhanced['total_sampled']} total samples\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nProportional stratified sample:\n  head: 100 samples\n  torso: 196 samples\n  tail: 9703 samples\n\nAfter oversampling tail: 29405 total samples\n```\n:::\n:::\n\n\n### Efficient Metric Computation\n\n::: {#f8684186 .cell execution_count=18}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show efficient evaluation at scale\"}\nimport torch\nimport numpy as np\n\nclass EfficientEvaluator:\n    \"\"\"Efficient evaluation for large-scale embedding systems.\"\"\"\n\n    def __init__(self, embedding_dim: int, device: str = \"cpu\"):\n        self.embedding_dim = embedding_dim\n        self.device = device\n\n    def batch_recall_at_k(\n        self,\n        query_embeddings: torch.Tensor,\n        corpus_embeddings: torch.Tensor,\n        relevance: torch.Tensor,\n        k: int = 10,\n        batch_size: int = 1000\n    ) -> float:\n        \"\"\"\n        Compute Recall@K with batched processing for memory efficiency.\n        \"\"\"\n        n_queries = len(query_embeddings)\n        total_recall = 0.0\n\n        for i in range(0, n_queries, batch_size):\n            batch_queries = query_embeddings[i:i+batch_size].to(self.device)\n            batch_relevance = relevance[i:i+batch_size]\n\n            # Normalize\n            batch_queries = batch_queries / batch_queries.norm(dim=1, keepdim=True)\n            corpus_norm = corpus_embeddings / corpus_embeddings.norm(dim=1, keepdim=True)\n\n            # Compute similarities\n            similarities = batch_queries @ corpus_norm.T\n\n            # Get top-k\n            top_k_indices = similarities.topk(k, dim=1).indices\n\n            # Compute recall\n            for j, (topk, rel) in enumerate(zip(top_k_indices, batch_relevance)):\n                relevant_items = rel.nonzero().squeeze(-1)\n                if len(relevant_items) == 0:\n                    continue\n                found = (topk.unsqueeze(1) == relevant_items.unsqueeze(0)).any(dim=1).sum()\n                total_recall += found.item() / len(relevant_items)\n\n        return total_recall / n_queries\n\n    def approximate_evaluation(\n        self,\n        query_sample_indices: list,\n        query_embeddings: torch.Tensor,\n        corpus_embeddings: torch.Tensor,\n        relevance: torch.Tensor,\n        k: int = 10\n    ) -> dict:\n        \"\"\"\n        Evaluate on sampled queries with confidence intervals.\n        \"\"\"\n        sampled_queries = query_embeddings[query_sample_indices]\n        sampled_relevance = relevance[query_sample_indices]\n\n        # Compute metric\n        recall = self.batch_recall_at_k(\n            sampled_queries, corpus_embeddings, sampled_relevance, k\n        )\n\n        # Bootstrap confidence interval\n        n_bootstrap = 100\n        bootstrap_recalls = []\n\n        for _ in range(n_bootstrap):\n            boot_indices = np.random.choice(len(query_sample_indices), size=len(query_sample_indices), replace=True)\n            boot_queries = sampled_queries[boot_indices]\n            boot_relevance = sampled_relevance[boot_indices]\n\n            boot_recall = self.batch_recall_at_k(\n                boot_queries, corpus_embeddings, boot_relevance, k\n            )\n            bootstrap_recalls.append(boot_recall)\n\n        return {\n            f\"recall@{k}\": recall,\n            \"ci_lower\": np.percentile(bootstrap_recalls, 2.5),\n            \"ci_upper\": np.percentile(bootstrap_recalls, 97.5),\n            \"n_queries_evaluated\": len(query_sample_indices),\n            \"confidence\": 0.95\n        }\n\n\n# Example\ntorch.manual_seed(42)\nevaluator = EfficientEvaluator(embedding_dim=256)\n\n# Simulate large-scale evaluation\nn_queries, n_corpus = 10000, 100000\nqueries = torch.randn(n_queries, 256)\ncorpus = torch.randn(n_corpus, 256)\nrelevance = (torch.rand(n_queries, n_corpus) < 0.001).float()\n\n# Sample 1000 queries for evaluation\nsample_indices = np.random.choice(n_queries, size=1000, replace=False).tolist()\n\nresult = evaluator.approximate_evaluation(\n    sample_indices, queries, corpus, relevance, k=10\n)\nprint(f\"Recall@10: {result['recall@10']:.4f}\")\nprint(f\"95% CI: [{result['ci_lower']:.4f}, {result['ci_upper']:.4f}]\")\nprint(f\"Evaluated on {result['n_queries_evaluated']} queries\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRecall@10: 0.0001\n95% CI: [0.0000, 0.0002]\nEvaluated on 1000 queries\n```\n:::\n:::\n\n\n## Key Takeaways\n\n- **Intrinsic quality metrics (isotropy, uniformity, alignment)** detect embedding problems without downstream tasks—monitor them continuously to catch degradation early\n\n- **Choose retrieval metrics based on your use case**: Recall@K for coverage, Precision@K when false positives are costly, NDCG for graded relevance, MRR for navigational queries, MAP for comprehensive ranking quality\n\n- **Human evaluation provides ground truth** that automated metrics cannot capture—design clear tasks, use quality controls, and measure inter-annotator agreement\n\n- **Domain-specific metrics matter**: E-commerce needs zero-result rate and catalog coverage; recommendations need diversity and novelty; fraud detection needs cost-sensitive evaluation\n\n- **Statistical rigor is essential**: Calculate required sample sizes, report confidence intervals, and apply multiple testing corrections when evaluating many metrics\n\n- **Scale requires smart sampling**: Use stratified sampling, oversample rare but important segments, and compute confidence intervals to quantify uncertainty\n\n## Looking Ahead\n\n@sec-high-performance-vector-ops shifts focus from evaluation to serving, exploring high-performance vector operations: optimized similarity search algorithms, approximate nearest neighbor (ANN) methods, GPU acceleration for vector operations, memory-mapped storage strategies, and parallel query processing that enables sub-millisecond similarity search across billion-vector indices.\n\n## Further Reading\n\n### Intrinsic Quality Metrics\n\n- Mu, Jiaqi, et al. (2018). \"All-but-the-Top: Simple and Effective Postprocessing for Word Representations.\" ICLR.\n- Wang, Tongzhou, and Phillip Isola (2020). \"Understanding Contrastive Representation Learning through Alignment and Uniformity on the Hypersphere.\" ICML.\n- Ethayarajh, Kawin (2019). \"How Contextual are Contextualized Word Representations?\" EMNLP.\n\n### Retrieval Evaluation\n\n- Manning, Christopher D., Prabhakar Raghavan, and Hinrich Schütze (2008). \"Introduction to Information Retrieval.\" Cambridge University Press. Chapter 8.\n- Järvelin, Kalervo, and Jaana Kekäläinen (2002). \"Cumulated Gain-Based Evaluation of IR Techniques.\" ACM TOIS.\n- Craswell, Nick (2009). \"Mean Reciprocal Rank.\" Encyclopedia of Database Systems.\n\n### Human Evaluation\n\n- Voorhees, Ellen M. (2000). \"Variations in Relevance Judgments and the Measurement of Retrieval Effectiveness.\" Information Processing & Management.\n- Carterette, Ben (2011). \"System Effectiveness, User Models, and User Utility: A Conceptual Framework for Investigation.\" SIGIR.\n- Alonso, Omar, and Stefano Mizzaro (2012). \"Using Crowdsourcing for TREC Relevance Assessment.\" Information Processing & Management.\n\n### Statistical Methods\n\n- Sakai, Tetsuya (2014). \"Statistical Reform in Information Retrieval?\" SIGIR Forum.\n- Carterette, Ben (2012). \"Multiple Testing in Statistical Analysis of Systems-Based Information Retrieval Experiments.\" ACM TOIS.\n- Smucker, Mark D., James Allan, and Ben Carterette (2007). \"A Comparison of Statistical Significance Tests for Information Retrieval Evaluation.\" CIKM.\n\n### Beyond-Accuracy Evaluation\n\n- Ge, Mouzhi, Carla Delgado-Battenfeld, and Dietmar Jannach (2010). \"Beyond Accuracy: Evaluating Recommender Systems by Coverage and Serendipity.\" RecSys.\n- Kaminskas, Marius, and Derek Bridge (2016). \"Diversity, Serendipity, Novelty, and Coverage: A Survey and Empirical Analysis of Beyond-Accuracy Objectives in Recommender Systems.\" ACM TIST.\n- Castells, Pablo, Neil J. Hurley, and Saul Vargas (2015). \"Novelty and Diversity in Recommender Systems.\" Recommender Systems Handbook.\n\n",
    "supporting": [
      "ch21_embedding_quality_evaluation_files"
    ],
    "filters": [],
    "includes": {}
  }
}