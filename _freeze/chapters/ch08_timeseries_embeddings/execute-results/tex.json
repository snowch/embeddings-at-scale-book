{
  "hash": "b05b7b1f577fead9f23c70e532d27188",
  "result": {
    "engine": "jupyter",
    "markdown": "# Time-Series Embeddings {#sec-timeseries-embeddings}\n\n::: callout-note\n## Chapter Overview\n\nThis chapter covers time-series embeddings—representations that convert sequences of measurements over time into vectors capturing temporal patterns. We explore how these embeddings detect trends, seasonality, cycles, and anomalies, enabling applications from predictive maintenance to financial pattern recognition.\n:::\n\n## What Are Time-Series Embeddings?\n\nTime-series embeddings convert sequences of measurements over time into vectors that capture temporal patterns—trends, seasonality, cycles, and anomalies. Two sensors with similar behavior patterns (e.g., both showing daily cycles) will have similar embeddings, even if their absolute values differ.\n\nThe challenge: patterns exist at multiple scales—short-term fluctuations, medium-term trends, and long-term seasonality. Like audio, time-series vary in length, so embeddings must aggregate temporal information into fixed-size vectors.\n\n## Visualizing Time-Series Patterns\n\n::: {.cell execution_count=1}\n\n::: {.cell-output .cell-output-display}\n![Time-series patterns: sine waves and upward trends have distinct signatures that embeddings capture.](ch08_timeseries_embeddings_files/figure-pdf/fig-timeseries-patterns-output-1.pdf){#fig-timeseries-patterns}\n:::\n:::\n\n\n## Creating Time-Series Embeddings\n\n::: {.cell execution_count=2}\n``` {.python .cell-code code-fold=\"false\"}\n\"\"\"\nTime-Series Embeddings: Temporal Patterns as Vectors\n\"\"\"\n\nimport numpy as np\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nnp.random.seed(42)\n\ndef generate_pattern(pattern_type, length=100):\n    \"\"\"Generate synthetic time-series with specific patterns.\"\"\"\n    t = np.linspace(0, 4*np.pi, length)\n    if pattern_type == 'sine':\n        return np.sin(t) + np.random.randn(length) * 0.1\n    elif pattern_type == 'increasing':\n        return t/10 + np.random.randn(length) * 0.2\n    else:\n        return np.random.randn(length)\n\ndef timeseries_embedding(series):\n    \"\"\"Extract statistical features as a simple embedding.\"\"\"\n    return np.array([\n        np.mean(series),                              # level\n        np.std(series),                               # variability\n        np.max(series) - np.min(series),              # range\n        np.mean(np.diff(series)),                     # trend (avg change)\n        np.corrcoef(series[:-1], series[1:])[0, 1],   # autocorrelation\n    ])\n\n# Generate time-series with different patterns\npatterns = {\n    'sine_wave_1': generate_pattern('sine'),\n    'sine_wave_2': generate_pattern('sine'),\n    'trend_up_1': generate_pattern('increasing'),\n    'trend_up_2': generate_pattern('increasing'),\n}\n\nembeddings = {name: timeseries_embedding(ts) for name, ts in patterns.items()}\n\nprint(f\"Embedding dimension: {len(embeddings['sine_wave_1'])} (5 statistical features)\\n\")\nprint(\"Time-series embedding similarities:\\n\")\nprint(\"Same pattern type:\")\nsine_sim = cosine_similarity([embeddings['sine_wave_1']], [embeddings['sine_wave_2']])[0][0]\ntrend_sim = cosine_similarity([embeddings['trend_up_1']], [embeddings['trend_up_2']])[0][0]\nprint(f\"  Sine wave 1 ↔ Sine wave 2: {sine_sim:.3f}\")\nprint(f\"  Trend up 1 ↔ Trend up 2:   {trend_sim:.3f}\")\n\nprint(\"\\nDifferent pattern types:\")\ncross_sim = cosine_similarity([embeddings['sine_wave_1']], [embeddings['trend_up_1']])[0][0]\nprint(f\"  Sine wave ↔ Trend up:      {cross_sim:.3f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nEmbedding dimension: 5 (5 statistical features)\n\nTime-series embedding similarities:\n\nSame pattern type:\n  Sine wave 1 ↔ Sine wave 2: 1.000\n  Trend up 1 ↔ Trend up 2:   0.998\n\nDifferent pattern types:\n  Sine wave ↔ Trend up:      0.943\n```\n:::\n:::\n\n\nTime-series with the same pattern type have high similarity because they share statistical properties (similar autocorrelation for sine waves, similar trend for increasing patterns). Different pattern types have lower similarity because their fundamental characteristics differ.\n\nThe statistical approach above is simple but limited—it can't capture complex patterns like \"spike followed by gradual recovery.\" Modern approaches use learned embeddings from neural networks trained on large time-series datasets.\n\n## When to Use Time-Series Embeddings {#sec-timeseries-embedding-types}\n\n**When to use time-series embeddings:** Anomaly detection in sensor data, predictive maintenance, financial pattern recognition, health monitoring (ECG, EEG), and IoT device fingerprinting.\n\nThis book covers time-series applications in financial services (@sec-financial-services) and manufacturing (@sec-manufacturing-industry40). If you'd like to see other time-series applications covered in future editions, reach out to the author.\n\n## Popular Time-Series Architectures\n\n| Architecture | Type | Strengths | Use Cases |\n|-------------|------|-----------|-----------|\n| Statistical features | Hand-crafted | Simple, interpretable | Baseline, small data |\n| [TSFresh](https://github.com/blue-yonder/tsfresh) | Auto feature extraction | Comprehensive | General purpose |\n| LSTM/GRU | Recurrent | Captures sequences | Variable length |\n| [Temporal Fusion Transformer](https://github.com/google-research/google-research/tree/master/tft) | Attention | Multi-horizon | Forecasting |\n\n: Time-series embedding architectures {.striped}\n\n## Advanced: Learned Time-Series Embeddings\n\n::: {.callout-note}\n## Optional Section\nThis section covers neural network approaches for time-series embeddings. Skip if statistical features meet your needs.\n:::\n\n### Recurrent Neural Networks (LSTM/GRU)\n\nLSTMs process sequences step-by-step, maintaining a hidden state that accumulates information:\n\n```python\nimport torch.nn as nn\n\nclass TimeSeriesEncoder(nn.Module):\n    def __init__(self, input_dim, hidden_dim, num_layers=2):\n        super().__init__()\n        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n\n    def forward(self, x):\n        # x shape: (batch, seq_len, features)\n        _, (hidden, _) = self.lstm(x)\n        # Return final hidden state as embedding\n        return hidden[-1]  # (batch, hidden_dim)\n```\n\n### Temporal Convolutional Networks (TCN)\n\nTCNs use dilated convolutions to capture patterns at multiple time scales:\n\n```python\nclass TCNBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, dilation):\n        super().__init__()\n        self.conv = nn.Conv1d(in_channels, out_channels, kernel_size,\n                              padding=(kernel_size-1) * dilation, dilation=dilation)\n        self.norm = nn.BatchNorm1d(out_channels)\n\n    def forward(self, x):\n        return torch.relu(self.norm(self.conv(x)))\n```\n\n### Contrastive Learning for Time-Series\n\nTrain embeddings by making augmented views of the same time-series similar:\n\n```python\ndef time_series_augmentation(x):\n    \"\"\"Create augmented views for contrastive learning.\"\"\"\n    # Random scaling\n    scale = torch.empty(x.size(0)).uniform_(0.8, 1.2)\n    scaled = x * scale.unsqueeze(1)\n\n    # Random time shift\n    shift = torch.randint(-10, 10, (x.size(0),))\n    shifted = torch.roll(scaled, shifts=shift.tolist(), dims=1)\n\n    # Add noise\n    noise = torch.randn_like(shifted) * 0.1\n    return shifted + noise\n```\n\n## Key Takeaways\n\n- **Time-series embeddings** capture temporal patterns—trends, seasonality, and anomalies—in fixed-size vectors\n- **Statistical features** (mean, std, autocorrelation) provide simple baselines but miss complex patterns\n- **Neural approaches** (LSTM, TCN, Transformers) learn rich representations from data\n- **Multi-scale patterns** require architectures that capture both short-term and long-term dependencies\n- **Applications** include anomaly detection, predictive maintenance, financial analysis, and health monitoring\n\n## Looking Ahead\n\nNow that you understand time-series embeddings, @sec-code-embeddings explores code embeddings—representations that capture program semantics regardless of implementation details.\n\n## Further Reading\n\n- Hochreiter, S. & Schmidhuber, J. (1997). \"Long Short-Term Memory.\" *Neural Computation*\n- Bai, S., et al. (2018). \"An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling.\" *arXiv:1803.01271*\n- Yue, Z., et al. (2022). \"TS2Vec: Towards Universal Representation of Time Series.\" *AAAI*\n\n",
    "supporting": [
      "ch08_timeseries_embeddings_files/figure-pdf"
    ],
    "filters": []
  }
}