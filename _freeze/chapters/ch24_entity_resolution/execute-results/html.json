{
  "hash": "45f81ca1576f9252da8eef26201a9f74",
  "result": {
    "engine": "jupyter",
    "markdown": "# Entity Resolution and Data Quality {#sec-entity-resolution}\n\n:::{.callout-note}\n## Chapter Overview\nEntity resolution—determining when different records refer to the same real-world entity—is a foundational data quality challenge that affects every organization operating at scale. This chapter applies embeddings to entity resolution at trillion-record scale: blocking strategies that reduce quadratic matching complexity to linear, similarity scoring using learned embeddings that capture semantic equivalence beyond exact string matching, graph-based resolution that propagates match decisions through entity networks, active learning approaches that maximize human labeling efficiency, and incremental matching systems that handle streaming data. These techniques transform entity resolution from a batch data cleaning task to a real-time, continuously learning system that maintains data quality as records arrive.\n:::\n\nBuilding on the cross-industry patterns (@sec-cross-industry-patterns), **entity resolution** represents the most computationally challenging cross-industry problem: determining when two records refer to the same real-world entity. Customer databases contain duplicates; healthcare systems need patient matching; marketing platforms build identity graphs; government agencies link records across departments. At scale, the naive O(N²) approach—comparing every record to every other—becomes impossible: 1 billion records would require 10¹⁸ comparisons.\n\n**Embedding-based entity resolution** transforms this challenge by representing records as vectors, enabling approximate nearest neighbor search to find candidate matches in sub-linear time, learned similarity functions that capture semantic equivalence, and graph neural networks that propagate match decisions through entity networks.\n\n## The Entity Resolution Challenge\n\nEntity resolution appears under many names: record linkage, deduplication, entity matching, merge-purge, and identity resolution. The core problem remains the same: given two (or more) records, do they refer to the same real-world entity?\n\n### Why Entity Resolution is Hard\n\nTraditional approaches face fundamental limitations:\n\n- **Scale**: N records require O(N²) comparisons for exhaustive matching\n- **Data quality**: Typos, abbreviations, missing values, format variations\n- **Schema heterogeneity**: Different systems represent entities differently\n- **Temporal changes**: Addresses, names, relationships change over time\n- **Ambiguity**: \"John Smith\" could be millions of different people\n- **Transitivity**: If A=B and B=C, then A=C—but similarity isn't transitive\n\n**Example challenges:**\n\n```\nRecord A: \"Jon Smith, 123 Main St, NYC\"\nRecord B: \"Jonathan Smith, 123 Main Street, New York\"\nRecord C: \"John Smith, 456 Oak Ave, Los Angeles\"\n\nAre A and B the same person? (Likely yes - name variation, address normalization)\nAre A and C the same person? (Likely no - different address, but common name)\n```\n\n### The Scale Problem\n\nAt trillion-record scale, exhaustive comparison is impossible:\n\n| Records | Comparisons (N²) | Time at 1M/sec |\n|---------|------------------|----------------|\n| 1,000 | 500,000 | 0.5 seconds |\n| 1 million | 500 billion | 6 days |\n| 1 billion | 500 quintillion | 15 million years |\n\n**Embedding approach**: Use learned embeddings + approximate nearest neighbor (ANN) search to find candidate matches in O(N log N) or O(N) time, then apply detailed comparison only to candidates.\n\n## Blocking: Reducing Comparison Space\n\n**Blocking** partitions records into groups where matches are likely, comparing only within groups. Traditional blocking uses exact attribute values (same zip code, same first letter of name). **Embedding-based blocking** uses vector similarity to find candidate matches regardless of surface-form variations.\n\n::: {#635e57b0 .cell execution_count=1}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show Embedding-Based Blocking\"}\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nfrom dataclasses import dataclass, field\nfrom typing import List, Dict, Set, Tuple, Optional\n\n\n@dataclass\nclass Record:\n    \"\"\"Entity record for matching.\"\"\"\n    record_id: str\n    attributes: Dict[str, str]\n    source: str\n    timestamp: float = 0.0\n    embedding: Optional[np.ndarray] = None\n\n\nclass RecordEncoder(nn.Module):\n    \"\"\"Encode records to embeddings for blocking and matching.\"\"\"\n    def __init__(self, vocab_size: int = 50000, embedding_dim: int = 256,\n                 num_attributes: int = 10):\n        super().__init__()\n        self.char_embedding = nn.Embedding(vocab_size, 64)\n        self.char_encoder = nn.LSTM(64, 128, num_layers=2,\n                                     batch_first=True, bidirectional=True)\n        self.attribute_attention = nn.MultiheadAttention(256, num_heads=8, batch_first=True)\n        self.projection = nn.Sequential(\n            nn.Linear(256, 256),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(256, embedding_dim)\n        )\n\n    def encode_attribute(self, char_ids: torch.Tensor) -> torch.Tensor:\n        \"\"\"Encode single attribute from character IDs.\"\"\"\n        char_emb = self.char_embedding(char_ids)\n        _, (hidden, _) = self.char_encoder(char_emb)\n        # Concatenate forward and backward final states\n        return torch.cat([hidden[-2], hidden[-1]], dim=-1)\n\n    def forward(self, attribute_char_ids: List[torch.Tensor]) -> torch.Tensor:\n        \"\"\"Encode full record from multiple attributes.\"\"\"\n        # Encode each attribute\n        attr_embeddings = [self.encode_attribute(attr) for attr in attribute_char_ids]\n        attr_stack = torch.stack(attr_embeddings, dim=1)  # [batch, num_attrs, 256]\n\n        # Self-attention across attributes\n        attn_out, _ = self.attribute_attention(attr_stack, attr_stack, attr_stack)\n\n        # Pool and project\n        pooled = attn_out.mean(dim=1)\n        return F.normalize(self.projection(pooled), p=2, dim=-1)\n\n\nclass EmbeddingBlocker:\n    \"\"\"Block records using embedding similarity.\"\"\"\n    def __init__(self, encoder: RecordEncoder, similarity_threshold: float = 0.7,\n                 max_candidates: int = 100):\n        self.encoder = encoder\n        self.similarity_threshold = similarity_threshold\n        self.max_candidates = max_candidates\n        self.index = None  # Would use FAISS/ScaNN in production\n        self.records: List[Record] = []\n\n    def index_records(self, records: List[Record]) -> None:\n        \"\"\"Index records for blocking.\"\"\"\n        self.records = records\n        # In production: build ANN index (FAISS, ScaNN)\n        print(f\"Indexed {len(records)} records for blocking\")\n\n    def find_candidates(self, query_record: Record) -> List[Tuple[str, float]]:\n        \"\"\"Find candidate matches using embedding similarity.\"\"\"\n        # In production: ANN search\n        # Here: brute force for demonstration\n        candidates = []\n        query_emb = query_record.embedding\n\n        for record in self.records:\n            if record.record_id == query_record.record_id:\n                continue\n            similarity = np.dot(query_emb, record.embedding)\n            if similarity >= self.similarity_threshold:\n                candidates.append((record.record_id, similarity))\n\n        # Sort by similarity, return top candidates\n        candidates.sort(key=lambda x: x[1], reverse=True)\n        return candidates[:self.max_candidates]\n\n\n# Usage example\nencoder = RecordEncoder(vocab_size=50000, embedding_dim=256)\nblocker = EmbeddingBlocker(encoder, similarity_threshold=0.7)\n\n# Simulate records\nrecords = [\n    Record(\"r1\", {\"name\": \"John Smith\", \"address\": \"123 Main St\"}, \"crm\"),\n    Record(\"r2\", {\"name\": \"Jon Smith\", \"address\": \"123 Main Street\"}, \"sales\"),\n    Record(\"r3\", {\"name\": \"Jane Doe\", \"address\": \"456 Oak Ave\"}, \"crm\"),\n]\n\n# Assign random embeddings for demo\nfor r in records:\n    r.embedding = np.random.randn(256).astype(np.float32)\n    r.embedding /= np.linalg.norm(r.embedding)\n\nblocker.index_records(records)\nprint(f\"Blocker ready with {len(records)} records\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nIndexed 3 records for blocking\nBlocker ready with 3 records\n```\n:::\n:::\n\n\n:::{.callout-tip}\n## Blocking Best Practices\n\n**Blocking strategies:**\n\n- **LSH (Locality Sensitive Hashing)**: Hash similar embeddings to same buckets\n- **ANN indexes**: FAISS IVF, ScaNN, HNSW for approximate search\n- **Canopy clustering**: Loose clusters for initial grouping\n- **Sorted neighborhood**: Sort by blocking key, slide window\n- **Hybrid**: Combine multiple blocking strategies for coverage\n\n**Tuning for recall vs efficiency:**\n\n- **High recall (>99%)**: Lower similarity threshold, more candidates per record\n- **Efficiency**: Higher threshold, fewer candidates, risk missing matches\n- **Adaptive**: Vary threshold based on record characteristics\n\n**Production considerations:**\n\n- **Incremental blocking**: Add new records without full reindex\n- **Distributed blocking**: Partition across nodes by blocking key\n- **Monitoring**: Track blocking recall on labeled sample\n:::\n\n## Similarity Scoring with Learned Embeddings\n\nOnce blocking identifies candidate pairs, **similarity scoring** determines match probability. Traditional approaches use hand-crafted rules (Jaro-Winkler on names, edit distance on addresses). **Learned similarity scoring** trains models to predict match probability from record embeddings.\n\n::: {#b1ecfcda .cell execution_count=2}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show Learned Similarity Scoring\"}\nimport torch\nimport torch.nn as nn\n\n\nclass SiameseMatcher(nn.Module):\n    \"\"\"Siamese network for record matching.\"\"\"\n    def __init__(self, embedding_dim: int = 256):\n        super().__init__()\n        # Comparison network\n        self.comparison = nn.Sequential(\n            nn.Linear(embedding_dim * 3, 512),  # concat + element-wise diff + product\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(512, 256),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(256, 128),\n            nn.ReLU(),\n            nn.Linear(128, 1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, emb_a: torch.Tensor, emb_b: torch.Tensor) -> torch.Tensor:\n        \"\"\"Compute match probability for record pairs.\"\"\"\n        # Multiple comparison features\n        diff = torch.abs(emb_a - emb_b)\n        product = emb_a * emb_b\n        combined = torch.cat([emb_a, diff, product], dim=-1)\n\n        return self.comparison(combined)\n\n\nclass AttributeAwareMatcher(nn.Module):\n    \"\"\"Match records with attribute-level attention.\"\"\"\n    def __init__(self, embedding_dim: int = 256, num_attributes: int = 10):\n        super().__init__()\n        self.num_attributes = num_attributes\n\n        # Per-attribute comparison\n        self.attr_comparators = nn.ModuleList([\n            nn.Sequential(\n                nn.Linear(embedding_dim * 2, 128),\n                nn.ReLU(),\n                nn.Linear(128, 64)\n            ) for _ in range(num_attributes)\n        ])\n\n        # Attribute importance weights\n        self.importance_weights = nn.Linear(num_attributes * 64, num_attributes)\n\n        # Final classifier\n        self.classifier = nn.Sequential(\n            nn.Linear(num_attributes * 64, 256),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(256, 1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, attrs_a: List[torch.Tensor],\n                attrs_b: List[torch.Tensor]) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"Compare records at attribute level.\"\"\"\n        attr_comparisons = []\n\n        for i, (a, b, comparator) in enumerate(\n            zip(attrs_a, attrs_b, self.attr_comparators)\n        ):\n            combined = torch.cat([a, b], dim=-1)\n            comparison = comparator(combined)\n            attr_comparisons.append(comparison)\n\n        # Stack attribute comparisons\n        stacked = torch.cat(attr_comparisons, dim=-1)\n\n        # Compute attribute importance\n        importance = torch.softmax(self.importance_weights(stacked), dim=-1)\n\n        # Final match probability\n        match_prob = self.classifier(stacked)\n\n        return match_prob, importance\n\n\n# Usage example\nsiamese = SiameseMatcher(embedding_dim=256)\nattr_matcher = AttributeAwareMatcher(embedding_dim=64, num_attributes=5)\n\n# Test with random embeddings\nemb_a = torch.randn(1, 256)\nemb_b = torch.randn(1, 256)\n\nmatch_prob = siamese(emb_a, emb_b)\nprint(f\"Siamese match probability: {match_prob.item():.3f}\")\n\n# Attribute-level matching\nattrs_a = [torch.randn(1, 64) for _ in range(5)]\nattrs_b = [torch.randn(1, 64) for _ in range(5)]\nmatch_prob, importance = attr_matcher(attrs_a, attrs_b)\nprint(f\"Attribute-aware match probability: {match_prob.item():.3f}\")\nprint(f\"Attribute importance: {importance.squeeze().detach().numpy()}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSiamese match probability: 0.496\nAttribute-aware match probability: 0.509\nAttribute importance: [0.18549398 0.19629572 0.19387142 0.20217499 0.22216389]\n```\n:::\n:::\n\n\n:::{.callout-tip}\n## Similarity Scoring Best Practices\n\n**Model architectures:**\n\n- **Siamese networks**: Shared encoder, comparison network (see @sec-siamese-networks)\n- **Cross-encoders**: Joint encoding of record pairs (more accurate, slower)\n- **Attribute-level**: Compare attributes separately, aggregate\n- **Ensemble**: Combine multiple matchers for robustness\n\n**Training data:**\n\n- **Labeled pairs**: Gold standard matches and non-matches\n- **Active learning**: Prioritize uncertain pairs for labeling\n- **Weak supervision**: Generate labels from rules, aggregate\n- **Contrastive learning**: Learn embeddings where matches are close\n\n**Threshold selection:**\n\n- **Precision-focused**: High threshold (0.9+) for automated matching\n- **Recall-focused**: Lower threshold (0.5-0.7) + human review\n- **Cost-based**: Optimize threshold for business cost function\n:::\n\n## Graph-Based Resolution\n\nEntity resolution isn't just pairwise—matches form **connected components** where transitivity applies: if A matches B and B matches C, all three likely refer to the same entity. **Graph-based resolution** models records as nodes, potential matches as edges, and uses graph algorithms to find entity clusters.\n\n::: {#0e4d5ca7 .cell execution_count=3}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show Graph-Based Entity Resolution\"}\nfrom collections import defaultdict\nfrom dataclasses import dataclass\nfrom typing import Dict, List, Set, Tuple\nimport numpy as np\n\n\n@dataclass\nclass MatchEdge:\n    \"\"\"Edge representing potential match between records.\"\"\"\n    record_a: str\n    record_b: str\n    similarity: float\n    match_probability: float\n    evidence: Dict[str, float]  # Per-attribute match scores\n\n\nclass EntityGraph:\n    \"\"\"Graph structure for entity resolution.\"\"\"\n    def __init__(self):\n        self.nodes: Set[str] = set()\n        self.edges: Dict[Tuple[str, str], MatchEdge] = {}\n        self.adjacency: Dict[str, Set[str]] = defaultdict(set)\n\n    def add_edge(self, edge: MatchEdge) -> None:\n        \"\"\"Add potential match edge.\"\"\"\n        self.nodes.add(edge.record_a)\n        self.nodes.add(edge.record_b)\n\n        key = tuple(sorted([edge.record_a, edge.record_b]))\n        self.edges[key] = edge\n\n        self.adjacency[edge.record_a].add(edge.record_b)\n        self.adjacency[edge.record_b].add(edge.record_a)\n\n    def find_connected_components(self, threshold: float = 0.5) -> List[Set[str]]:\n        \"\"\"Find entity clusters via connected components.\"\"\"\n        visited = set()\n        components = []\n\n        for node in self.nodes:\n            if node in visited:\n                continue\n\n            # BFS to find component\n            component = set()\n            queue = [node]\n\n            while queue:\n                current = queue.pop(0)\n                if current in visited:\n                    continue\n\n                visited.add(current)\n                component.add(current)\n\n                # Add neighbors with edge above threshold\n                for neighbor in self.adjacency[current]:\n                    key = tuple(sorted([current, neighbor]))\n                    if key in self.edges:\n                        edge = self.edges[key]\n                        if edge.match_probability >= threshold:\n                            queue.append(neighbor)\n\n            if component:\n                components.append(component)\n\n        return components\n\n\nclass GraphEntityResolver:\n    \"\"\"Resolve entities using graph clustering.\"\"\"\n    def __init__(self, match_threshold: float = 0.5,\n                 min_cluster_confidence: float = 0.7):\n        self.match_threshold = match_threshold\n        self.min_cluster_confidence = min_cluster_confidence\n        self.graph = EntityGraph()\n\n    def add_match_evidence(self, edge: MatchEdge) -> None:\n        \"\"\"Add pairwise match evidence to graph.\"\"\"\n        if edge.match_probability >= self.match_threshold:\n            self.graph.add_edge(edge)\n\n    def resolve(self) -> Dict[str, str]:\n        \"\"\"Resolve all entities, return record_id -> entity_id mapping.\"\"\"\n        components = self.graph.find_connected_components(self.match_threshold)\n\n        record_to_entity = {}\n        for i, component in enumerate(components):\n            entity_id = f\"entity_{i}\"\n            for record_id in component:\n                record_to_entity[record_id] = entity_id\n\n        return record_to_entity\n\n    def get_entity_records(self, entity_id: str,\n                           record_to_entity: Dict[str, str]) -> Set[str]:\n        \"\"\"Get all records belonging to an entity.\"\"\"\n        return {r for r, e in record_to_entity.items() if e == entity_id}\n\n\n# Usage example\nresolver = GraphEntityResolver(match_threshold=0.7)\n\n# Add match evidence\nedges = [\n    MatchEdge(\"r1\", \"r2\", 0.95, 0.92, {\"name\": 0.98, \"address\": 0.85}),\n    MatchEdge(\"r2\", \"r3\", 0.88, 0.85, {\"name\": 0.90, \"address\": 0.80}),\n    MatchEdge(\"r4\", \"r5\", 0.91, 0.89, {\"name\": 0.95, \"address\": 0.82}),\n    MatchEdge(\"r1\", \"r6\", 0.45, 0.35, {\"name\": 0.60, \"address\": 0.20}),  # Below threshold\n]\n\nfor edge in edges:\n    resolver.add_match_evidence(edge)\n\n# Resolve entities\nmapping = resolver.resolve()\nprint(\"Record to Entity mapping:\")\nfor record, entity in sorted(mapping.items()):\n    print(f\"  {record} -> {entity}\")\n\n# Count entities\nentities = set(mapping.values())\nprint(f\"\\nResolved {len(mapping)} records into {len(entities)} entities\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRecord to Entity mapping:\n  r1 -> entity_0\n  r2 -> entity_0\n  r3 -> entity_0\n  r4 -> entity_1\n  r5 -> entity_1\n\nResolved 5 records into 2 entities\n```\n:::\n:::\n\n\n:::{.callout-tip}\n## Graph Resolution Best Practices\n\n**Clustering algorithms:**\n\n- **Connected components**: Simple, transitive closure\n- **Correlation clustering**: Minimize disagreements with edge weights\n- **Markov clustering**: Flow-based clustering for weighted graphs\n- **Hierarchical**: Build dendrograms, cut at desired granularity\n\n**Handling conflicts:**\n\n- **Negative evidence**: Some pairs definitely don't match (different SSN)\n- **Hard constraints**: Never merge records with conflicting unique IDs\n- **Soft constraints**: Penalize but allow merging with minor conflicts\n\n**Scalability:**\n\n- **Incremental updates**: Add new records to existing clusters\n- **Distributed clustering**: Partition graph, local clustering, merge\n- **Streaming**: Online clustering as matches arrive\n:::\n\n## Active Learning for Entity Resolution\n\nLabeling record pairs is expensive—domain experts must review each pair. **Active learning** maximizes labeling efficiency by prioritizing the most informative pairs: those where the model is uncertain, or where a label would most improve overall accuracy.\n\n::: {#d7a36662 .cell execution_count=4}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show Active Learning for Entity Resolution\"}\nfrom dataclasses import dataclass\nfrom typing import List, Tuple, Optional\nimport numpy as np\n\n\n@dataclass\nclass RecordPair:\n    \"\"\"Candidate record pair for matching.\"\"\"\n    record_a_id: str\n    record_b_id: str\n    features: np.ndarray\n    match_probability: float = 0.5\n    label: Optional[bool] = None  # None = unlabeled\n    uncertainty: float = 0.5\n\n\nclass ActiveLearningMatcher:\n    \"\"\"Active learning for entity resolution.\"\"\"\n    def __init__(self, model, initial_threshold: float = 0.5):\n        self.model = model\n        self.threshold = initial_threshold\n        self.labeled_pairs: List[RecordPair] = []\n        self.unlabeled_pairs: List[RecordPair] = []\n\n    def compute_uncertainty(self, pair: RecordPair) -> float:\n        \"\"\"Compute uncertainty for a pair (entropy-based).\"\"\"\n        p = pair.match_probability\n        if p <= 0 or p >= 1:\n            return 0.0\n        # Binary entropy\n        return -p * np.log2(p) - (1-p) * np.log2(1-p)\n\n    def select_pairs_uncertainty(self, n: int) -> List[RecordPair]:\n        \"\"\"Select most uncertain pairs for labeling.\"\"\"\n        # Update uncertainties\n        for pair in self.unlabeled_pairs:\n            pair.uncertainty = self.compute_uncertainty(pair)\n\n        # Sort by uncertainty (highest first)\n        sorted_pairs = sorted(self.unlabeled_pairs,\n                             key=lambda p: p.uncertainty, reverse=True)\n        return sorted_pairs[:n]\n\n    def select_pairs_diversity(self, n: int) -> List[RecordPair]:\n        \"\"\"Select diverse pairs using clustering.\"\"\"\n        if len(self.unlabeled_pairs) <= n:\n            return self.unlabeled_pairs\n\n        # Simple diversity: select from different similarity ranges\n        sorted_by_prob = sorted(self.unlabeled_pairs,\n                               key=lambda p: p.match_probability)\n\n        selected = []\n        step = len(sorted_by_prob) // n\n        for i in range(n):\n            idx = min(i * step, len(sorted_by_prob) - 1)\n            selected.append(sorted_by_prob[idx])\n\n        return selected\n\n    def select_pairs_hybrid(self, n: int,\n                            uncertainty_weight: float = 0.7) -> List[RecordPair]:\n        \"\"\"Hybrid selection: uncertainty + diversity.\"\"\"\n        uncertain = self.select_pairs_uncertainty(n * 2)\n        diverse = self.select_pairs_diversity(n * 2)\n\n        # Score by weighted combination\n        pair_scores = {}\n        for i, pair in enumerate(uncertain):\n            pair_scores[pair.record_a_id + pair.record_b_id] = \\\n                uncertainty_weight * (len(uncertain) - i)\n\n        for i, pair in enumerate(diverse):\n            key = pair.record_a_id + pair.record_b_id\n            pair_scores[key] = pair_scores.get(key, 0) + \\\n                (1 - uncertainty_weight) * (len(diverse) - i)\n\n        # Select top n\n        all_pairs = {p.record_a_id + p.record_b_id: p\n                    for p in uncertain + diverse}\n        sorted_keys = sorted(pair_scores.keys(),\n                            key=lambda k: pair_scores[k], reverse=True)\n\n        return [all_pairs[k] for k in sorted_keys[:n]]\n\n    def label_pair(self, pair: RecordPair, is_match: bool) -> None:\n        \"\"\"Record human label for a pair.\"\"\"\n        pair.label = is_match\n        self.labeled_pairs.append(pair)\n        if pair in self.unlabeled_pairs:\n            self.unlabeled_pairs.remove(pair)\n\n    def get_labeling_progress(self) -> dict:\n        \"\"\"Get labeling statistics.\"\"\"\n        matches = sum(1 for p in self.labeled_pairs if p.label)\n        non_matches = sum(1 for p in self.labeled_pairs if not p.label)\n\n        return {\n            \"total_labeled\": len(self.labeled_pairs),\n            \"matches\": matches,\n            \"non_matches\": non_matches,\n            \"match_rate\": matches / max(len(self.labeled_pairs), 1),\n            \"remaining_unlabeled\": len(self.unlabeled_pairs)\n        }\n\n\n# Usage example\nclass DummyModel:\n    def predict(self, features):\n        return np.random.random()\n\nmatcher = ActiveLearningMatcher(DummyModel())\n\n# Add unlabeled pairs\nfor i in range(100):\n    pair = RecordPair(\n        record_a_id=f\"a_{i}\",\n        record_b_id=f\"b_{i}\",\n        features=np.random.randn(128),\n        match_probability=np.random.random()\n    )\n    matcher.unlabeled_pairs.append(pair)\n\n# Select pairs for labeling\nuncertain_pairs = matcher.select_pairs_uncertainty(10)\nprint(f\"Selected {len(uncertain_pairs)} uncertain pairs\")\nprint(f\"Uncertainty range: {uncertain_pairs[0].uncertainty:.3f} - {uncertain_pairs[-1].uncertainty:.3f}\")\n\n# Simulate labeling\nfor pair in uncertain_pairs[:5]:\n    matcher.label_pair(pair, is_match=np.random.random() > 0.5)\n\nprogress = matcher.get_labeling_progress()\nprint(f\"\\nLabeling progress: {progress}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSelected 10 uncertain pairs\nUncertainty range: 1.000 - 0.996\n\nLabeling progress: {'total_labeled': 5, 'matches': 3, 'non_matches': 2, 'match_rate': 0.6, 'remaining_unlabeled': 95}\n```\n:::\n:::\n\n\n:::{.callout-tip}\n## Active Learning Best Practices\n\n**Selection strategies:**\n\n- **Uncertainty sampling**: Pairs where model is most uncertain (near 0.5)\n- **Query-by-committee**: Pairs where multiple models disagree\n- **Diversity sampling**: Cover different regions of feature space\n- **Expected model change**: Pairs that would most change model if labeled\n\n**Practical considerations:**\n\n- **Batch selection**: Select batches (10-100) for efficiency\n- **Human fatigue**: Mix easy and hard pairs to maintain quality\n- **Stopping criteria**: When to stop labeling (convergence, budget)\n- **Cold start**: Initial random sample before active selection\n\n**Quality control:**\n\n- **Duplicate questions**: Verify labeler consistency\n- **Gold questions**: Known-answer pairs to check quality\n- **Multiple labelers**: Aggregate labels for difficult pairs\n:::\n\n## Incremental and Streaming Resolution\n\nProduction systems receive new records continuously—customers sign up, data feeds arrive, systems sync. **Incremental entity resolution** matches new records against existing entities without re-processing the entire database.\n\n::: {#62a195a9 .cell execution_count=5}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show Incremental Entity Resolution\"}\nfrom dataclasses import dataclass, field\nfrom typing import Dict, List, Optional, Set\nfrom datetime import datetime\nimport numpy as np\n\n\n@dataclass\nclass Entity:\n    \"\"\"Resolved entity with member records.\"\"\"\n    entity_id: str\n    canonical_record: Dict[str, str]  # Best values for each attribute\n    member_records: Set[str] = field(default_factory=set)\n    embedding: Optional[np.ndarray] = None\n    created_at: datetime = field(default_factory=datetime.now)\n    updated_at: datetime = field(default_factory=datetime.now)\n\n\nclass IncrementalResolver:\n    \"\"\"Incremental entity resolution for streaming data.\"\"\"\n    def __init__(self, match_threshold: float = 0.8,\n                 max_candidates: int = 50):\n        self.match_threshold = match_threshold\n        self.max_candidates = max_candidates\n        self.entities: Dict[str, Entity] = {}\n        self.record_to_entity: Dict[str, str] = {}\n        self.entity_embeddings: List[np.ndarray] = []\n        self.entity_ids: List[str] = []\n\n    def _find_candidate_entities(self, record_embedding: np.ndarray) -> List[str]:\n        \"\"\"Find candidate entities using ANN search.\"\"\"\n        if not self.entity_embeddings:\n            return []\n\n        # Brute force for demo (use FAISS/ScaNN in production)\n        embeddings = np.stack(self.entity_embeddings)\n        similarities = embeddings @ record_embedding\n\n        # Get top candidates\n        top_indices = np.argsort(similarities)[::-1][:self.max_candidates]\n        return [self.entity_ids[i] for i in top_indices\n                if similarities[i] >= self.match_threshold]\n\n    def _compute_match_score(self, record_embedding: np.ndarray,\n                             entity: Entity) -> float:\n        \"\"\"Compute match score between record and entity.\"\"\"\n        if entity.embedding is None:\n            return 0.0\n        return float(np.dot(record_embedding, entity.embedding))\n\n    def _create_entity(self, record_id: str, attributes: Dict[str, str],\n                       embedding: np.ndarray) -> Entity:\n        \"\"\"Create new entity from record.\"\"\"\n        entity_id = f\"entity_{len(self.entities)}\"\n        entity = Entity(\n            entity_id=entity_id,\n            canonical_record=attributes.copy(),\n            member_records={record_id},\n            embedding=embedding\n        )\n\n        self.entities[entity_id] = entity\n        self.record_to_entity[record_id] = entity_id\n        self.entity_embeddings.append(embedding)\n        self.entity_ids.append(entity_id)\n\n        return entity\n\n    def _merge_into_entity(self, record_id: str, attributes: Dict[str, str],\n                           embedding: np.ndarray, entity: Entity) -> None:\n        \"\"\"Merge record into existing entity.\"\"\"\n        entity.member_records.add(record_id)\n        self.record_to_entity[record_id] = entity.entity_id\n\n        # Update entity embedding (running average)\n        n = len(entity.member_records)\n        entity.embedding = (entity.embedding * (n-1) + embedding) / n\n        entity.embedding /= np.linalg.norm(entity.embedding)\n\n        # Update canonical record (simple: keep existing)\n        # In production: more sophisticated merging\n        entity.updated_at = datetime.now()\n\n    def resolve_record(self, record_id: str, attributes: Dict[str, str],\n                       embedding: np.ndarray) -> Dict:\n        \"\"\"Resolve a single new record.\"\"\"\n        # Find candidate entities\n        candidates = self._find_candidate_entities(embedding)\n\n        if not candidates:\n            # No candidates: create new entity\n            entity = self._create_entity(record_id, attributes, embedding)\n            return {\n                \"action\": \"created\",\n                \"entity_id\": entity.entity_id,\n                \"confidence\": 1.0\n            }\n\n        # Score candidates\n        best_entity = None\n        best_score = 0.0\n\n        for entity_id in candidates:\n            entity = self.entities[entity_id]\n            score = self._compute_match_score(embedding, entity)\n            if score > best_score:\n                best_score = score\n                best_entity = entity\n\n        if best_score >= self.match_threshold:\n            # Merge into existing entity\n            self._merge_into_entity(record_id, attributes, embedding, best_entity)\n            return {\n                \"action\": \"merged\",\n                \"entity_id\": best_entity.entity_id,\n                \"confidence\": best_score\n            }\n        else:\n            # Create new entity\n            entity = self._create_entity(record_id, attributes, embedding)\n            return {\n                \"action\": \"created\",\n                \"entity_id\": entity.entity_id,\n                \"confidence\": 1.0,\n                \"closest_entity\": best_entity.entity_id if best_entity else None,\n                \"closest_score\": best_score\n            }\n\n\n# Usage example\nresolver = IncrementalResolver(match_threshold=0.8)\n\n# Process streaming records\nrecords = [\n    (\"r1\", {\"name\": \"John Smith\", \"email\": \"john@email.com\"}),\n    (\"r2\", {\"name\": \"Jon Smith\", \"email\": \"john@email.com\"}),  # Should merge with r1\n    (\"r3\", {\"name\": \"Jane Doe\", \"email\": \"jane@email.com\"}),   # New entity\n    (\"r4\", {\"name\": \"John Smith\", \"email\": \"different@email.com\"}),  # Uncertain\n]\n\nfor record_id, attrs in records:\n    # Generate embedding (would use encoder in production)\n    embedding = np.random.randn(256).astype(np.float32)\n    embedding /= np.linalg.norm(embedding)\n\n    result = resolver.resolve_record(record_id, attrs, embedding)\n    print(f\"Record {record_id}: {result['action']} -> {result['entity_id']} \"\n          f\"(confidence: {result['confidence']:.2f})\")\n\nprint(f\"\\nTotal entities: {len(resolver.entities)}\")\nprint(f\"Total records: {len(resolver.record_to_entity)}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRecord r1: created -> entity_0 (confidence: 1.00)\nRecord r2: created -> entity_1 (confidence: 1.00)\nRecord r3: created -> entity_2 (confidence: 1.00)\nRecord r4: created -> entity_3 (confidence: 1.00)\n\nTotal entities: 4\nTotal records: 4\n```\n:::\n:::\n\n\n:::{.callout-tip}\n## Incremental Resolution Best Practices\n\n**Index maintenance:**\n\n- **Incremental ANN updates**: Add vectors to index without rebuild\n- **Periodic reindexing**: Full rebuild during low-traffic windows\n- **Entity embedding updates**: Running average or periodic recomputation\n\n**Handling merges:**\n\n- **Transitive closure**: When two entities merge, update all references\n- **Split detection**: Sometimes entities should be split (wrong merge)\n- **Audit trail**: Track merge history for debugging and compliance\n\n**Latency requirements:**\n\n- **Real-time (< 100ms)**: Online matching for customer-facing applications\n- **Near-real-time (< 1s)**: Streaming pipelines, fraud detection\n- **Batch (minutes-hours)**: Nightly deduplication runs\n:::\n\n## Industry Applications\n\nEntity resolution appears across industries with domain-specific challenges:\n\n### Healthcare: Patient Matching\n\nPatient matching across healthcare systems is critical for care coordination and safety. Challenges include:\n\n- **Name variations**: Married names, nicknames, spelling errors\n- **Address changes**: Patients move frequently\n- **No universal ID**: Unlike SSN for adults, no unique patient identifier\n- **Life-or-death stakes**: Wrong patient match can be fatal\n\n**Regulatory context**: HIPAA requires accurate patient identification; ONC promotes patient matching standards.\n\n### Financial Services: Customer Identity\n\nFinancial institutions must maintain accurate customer records for:\n\n- **KYC/AML compliance**: Know Your Customer, Anti-Money Laundering\n- **Fraud prevention**: Detect accounts opened by same fraudster\n- **Cross-selling**: Unified view of customer across products\n- **Regulatory reporting**: Accurate aggregate positions\n\n### Marketing: Identity Graphs\n\nMarketing platforms build identity graphs linking:\n\n- **Cross-device**: Same person on phone, laptop, tablet\n- **Cross-channel**: Email, cookies, mobile IDs, CTV\n- **Offline-online**: Store purchases linked to digital profiles\n- **Household**: Grouping family members\n\n**Scale**: Major identity graphs contain billions of records with trillions of potential links.\n\n### Government: Record Linkage\n\nGovernment agencies link records across:\n\n- **Benefits programs**: Prevent fraud, ensure eligibility\n- **Tax administration**: Match income reports across sources\n- **Law enforcement**: Link identities across jurisdictions\n- **Census**: Deduplicate responses, link to administrative records\n\n## Key Takeaways\n\n- **Entity resolution at scale requires blocking to avoid O(N²) complexity**: Embedding-based blocking using ANN search reduces candidate pairs to O(N log N) or O(N), enabling trillion-record matching by comparing only records with similar embeddings\n\n- **Learned similarity scoring outperforms hand-crafted rules**: Siamese networks and attribute-aware matchers learn semantic similarity from labeled examples, capturing variations (nicknames, abbreviations, typos) that rule-based systems miss while providing calibrated match probabilities\n\n- **Graph-based resolution handles transitivity and conflicts**: Modeling records as nodes and matches as edges enables connected component clustering for entity groups, correlation clustering that respects negative evidence, and principled handling of conflicting match signals\n\n- **Active learning maximizes labeling efficiency**: Uncertainty sampling prioritizes pairs where the model is uncertain, reducing labeling effort by 60-80% compared to random sampling while achieving equivalent accuracy\n\n- **Incremental resolution is essential for production systems**: New records must be matched against existing entities in real-time without full reprocessing, requiring incremental ANN indexes, entity embedding updates, and merge/split handling\n\n- **Domain-specific challenges require specialized approaches**: Healthcare patient matching faces unique identifier absence and safety stakes; financial services requires KYC/AML compliance; marketing operates at billion-record scale with cross-device linking\n\n## Looking Ahead\n\n@sec-financial-services explores how financial services applies entity resolution for KYC/AML compliance, customer deduplication, and fraud detection, along with other embedding applications including trading signals, credit risk, and market sentiment.\n\n## Further Reading\n\n### Entity Resolution Foundations\n- Christen, Peter (2012). \"Data Matching: Concepts and Techniques for Record Linkage, Entity Resolution, and Duplicate Detection.\" Springer.\n- Elmagarmid, Ahmed K., Panagiotis G. Ipeirotis, and Vassilios S. Verykios (2007). \"Duplicate Record Detection: A Survey.\" IEEE TKDE.\n- Getoor, Lise, and Ashwin Machanavajjhala (2012). \"Entity Resolution: Theory, Practice & Open Challenges.\" VLDB Tutorial.\n- Papadakis, George, et al. (2020). \"Blocking and Filtering Techniques for Entity Resolution: A Survey.\" ACM Computing Surveys.\n\n### Deep Learning for Entity Resolution\n- Mudgal, Sidharth, et al. (2018). \"Deep Learning for Entity Matching: A Design Space Exploration.\" SIGMOD.\n- Ebraheem, Muhammad, et al. (2018). \"Distributed Representations of Tuples for Entity Resolution.\" VLDB.\n- Kasai, Jungo, et al. (2019). \"Low-Resource Deep Entity Resolution with Transfer and Active Learning.\" ACL.\n- Li, Yuliang, et al. (2020). \"Deep Entity Matching with Pre-Trained Language Models.\" VLDB.\n\n### Scalable Entity Resolution\n- Steorts, Rebecca C., Samuel L. Ventura, Mauricio Sadinle, and Stephen E. Fienberg (2016). \"A Comparison of Blocking Methods for Record Linkage.\" Privacy in Statistical Databases.\n- Wang, Qing, et al. (2011). \"Fast-Join: An Efficient Method for Fuzzy Token Matching Based String Similarity Join.\" ICDE.\n- Vernica, Rares, Michael J. Carey, and Chen Li (2010). \"Efficient Parallel Set-Similarity Joins Using MapReduce.\" SIGMOD.\n- Chu, Xu, et al. (2016). \"Distributed Data Deduplication.\" VLDB.\n\n### Active Learning and Human-in-the-Loop\n- Arasu, Arvind, Michaela Götz, and Raghav Kaushik (2010). \"On Active Learning of Record Matching Packages.\" SIGMOD.\n- Bellare, Kedar, et al. (2012). \"Active Learning for Crowdsourced Entity Resolution.\" Workshop on Crowdsourcing and Data Mining.\n- Wang, Jiannan, et al. (2012). \"CrowdER: Crowdsourcing Entity Resolution.\" VLDB.\n- Firmani, Donatella, et al. (2016). \"Online Entity Resolution Using an Oracle.\" VLDB.\n\n### Graph-Based Entity Resolution\n- Bhattacharya, Indrajit, and Lise Getoor (2007). \"Collective Entity Resolution in Relational Data.\" ACM TKDD.\n- Rastogi, Vibhor, et al. (2011). \"Large-Scale Collective Entity Matching.\" VLDB.\n- Vesdapunt, Norases, Kedar Bellare, and Nilesh Dalvi (2014). \"Crowdsourcing Algorithms for Entity Resolution.\" VLDB.\n- Steorts, Rebecca C., Rob Hall, and Stephen E. Fienberg (2016). \"A Bayesian Approach to Graphical Record Linkage and Deduplication.\" JASA.\n\n",
    "supporting": [
      "ch24_entity_resolution_files"
    ],
    "filters": [],
    "includes": {}
  }
}