# How Embedding Models Learn {#sec-embedding-model-fundamentals}

::: callout-note
## Chapter Overview

This chapter explains how the embedding models you use actually learn to create meaningful vector representations. We start with Word2Vec—historically important and conceptually simple—then progress to transformer-based models like BERT that dominate today. Understanding these fundamentals helps you choose the right model for your use case and diagnose issues when embeddings don't behave as expected.
:::

## The Learning Problem

Before diving into specific architectures, let's understand what embedding models are actually learning. The core insight is deceptively simple: **words (or images, or any objects) that appear in similar contexts should have similar embeddings**.

Consider these sentences:
- "The **cat** sat on the mat"
- "The **dog** sat on the rug"
- "A **kitten** played on the floor"

A model that sees millions of such sentences learns that "cat," "dog," and "kitten" appear in similar contexts (near words like "sat," "played," "mat," "rug"). Without any explicit labeling, the model discovers that these are all animals, and positions them close together in embedding space.

This is the foundation of all embedding models: **learning from context**.

## Word2Vec: The Breakthrough {#sec-word2vec}

Word2Vec, introduced by Mikolov et al. in 2013 [@mikolov2013efficient], revolutionized natural language processing by showing that simple neural networks could learn rich semantic representations from raw text. Despite its simplicity, Word2Vec remains influential and helps build intuition for more complex models.

### The Skip-Gram Architecture

Word2Vec's skip-gram model learns embeddings by predicting context words given a target word. Here's the intuition:

```
Training sentence: "The cat sat on the mat"

If target word = "sat", predict context words within a window:
  - "The" (2 words before)
  - "cat" (1 word before)
  - "on"  (1 word after)
  - "the" (2 words after)
```

The model learns embeddings such that words appearing in similar contexts get similar vectors.

```{python}
#| code-fold: false

"""
Word2Vec Skip-Gram: Simplified Implementation

This demonstrates the core concept of Word2Vec's skip-gram model.
In practice, you'd use optimized libraries like Gensim, but this
shows what's happening under the hood.
"""

import numpy as np

# Vocabulary and hyperparameters
vocab = ["the", "cat", "sat", "on", "mat", "dog", "rug", "a", "kitten", "played"]
vocab_size = len(vocab)
embedding_dim = 4  # Small for illustration; real models use 100-300
word_to_idx = {w: i for i, w in enumerate(vocab)}

# Initialize two embedding matrices:
# - W_target: embeddings for target words
# - W_context: embeddings for context words
np.random.seed(42)
W_target = np.random.randn(vocab_size, embedding_dim) * 0.1
W_context = np.random.randn(vocab_size, embedding_dim) * 0.1


def sigmoid(x):
    return 1 / (1 + np.exp(-np.clip(x, -500, 500)))


def train_skipgram_pair(target_word, context_word, negative_words, lr=0.1):
    """
    Train on one (target, context) pair with negative sampling.

    The model learns to:
    1. Maximize similarity between target and true context
    2. Minimize similarity between target and random (negative) words
    """
    global W_target, W_context

    t_idx = word_to_idx[target_word]
    c_idx = word_to_idx[context_word]

    # Get embeddings
    target_emb = W_target[t_idx]
    context_emb = W_context[c_idx]

    # Positive example: target and context should be similar
    score = np.dot(target_emb, context_emb)
    pred = sigmoid(score)

    # Gradient for positive pair (label = 1)
    grad = (pred - 1) * context_emb
    W_target[t_idx] -= lr * grad
    W_context[c_idx] -= lr * (pred - 1) * target_emb

    # Negative examples: target and random words should be dissimilar
    for neg_word in negative_words:
        n_idx = word_to_idx[neg_word]
        neg_emb = W_context[n_idx]

        score = np.dot(target_emb, neg_emb)
        pred = sigmoid(score)

        # Gradient for negative pair (label = 0)
        W_target[t_idx] -= lr * pred * neg_emb
        W_context[n_idx] -= lr * pred * target_emb


# Training corpus (simplified)
corpus = [
    ["the", "cat", "sat", "on", "the", "mat"],
    ["the", "dog", "sat", "on", "the", "rug"],
    ["a", "kitten", "played", "on", "the", "mat"],
]

# Train for a few epochs
window_size = 2
for epoch in range(100):
    for sentence in corpus:
        for i, target in enumerate(sentence):
            # Get context words within window
            start = max(0, i - window_size)
            end = min(len(sentence), i + window_size + 1)
            context_words = [sentence[j] for j in range(start, end) if j != i]

            # Get negative samples (words not in context)
            negatives = [w for w in vocab if w not in context_words and w != target][:3]

            for context in context_words:
                train_skipgram_pair(target, context, negatives)

# After training, similar words should have similar embeddings
def cosine_similarity(w1, w2):
    v1, v2 = W_target[word_to_idx[w1]], W_target[word_to_idx[w2]]
    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))

print("Learned similarities (after training on tiny corpus):")
print(f"  cat ↔ dog:    {cosine_similarity('cat', 'dog'):.3f}")
print(f"  cat ↔ kitten: {cosine_similarity('cat', 'kitten'):.3f}")
print(f"  sat ↔ played: {cosine_similarity('sat', 'played'):.3f}")
print(f"  cat ↔ mat:    {cosine_similarity('cat', 'mat'):.3f}")
```

### Why Word2Vec Works

The magic of Word2Vec comes from a simple observation: if we force the model to predict context from target words (or vice versa), the learned embeddings must encode semantic meaning. Words that can substitute for each other in sentences end up with similar embeddings.

**Key innovations:**

1. **Negative sampling**: Instead of computing softmax over the entire vocabulary (expensive), sample a few "negative" words and train the model to distinguish real context from random words.

2. **Subsampling frequent words**: Common words like "the" and "a" provide less information, so they're downsampled during training.

3. **Vector arithmetic**: A surprising emergent property—relationships are encoded as vector offsets:
   ```
   king - man + woman ≈ queen
   Paris - France + Italy ≈ Rome
   ```

### Limitations of Word2Vec

While groundbreaking, Word2Vec has significant limitations:

- **Static embeddings**: Each word has one embedding, regardless of context. "Bank" (financial) and "bank" (river) share the same vector.
- **Out-of-vocabulary words**: Words not seen during training have no embedding.
- **Limited context window**: Only considers nearby words, missing long-range dependencies.

These limitations motivated the development of contextual embeddings.

## From Words to Sentences: Extending Word2Vec

Before transformers, several approaches extended word embeddings to longer text:

### GloVe (Global Vectors)

GloVe [@pennington2014glove] improves on Word2Vec by explicitly factorizing the word co-occurrence matrix. While Word2Vec processes text sequentially, GloVe computes global statistics first, then learns embeddings that preserve co-occurrence patterns.

```python
# GloVe is typically used via pre-trained vectors
from gensim.models import KeyedVectors

# Load pre-trained GloVe (converted to Word2Vec format)
# glove = KeyedVectors.load_word2vec_format('glove.6B.100d.txt', no_header=True)
# similar = glove.most_similar('king')
```

### Doc2Vec

Doc2Vec extends Word2Vec to documents by adding a "paragraph vector" that's trained alongside word vectors. Each document gets a unique embedding that captures its overall meaning.

### Limitations of Pre-Transformer Approaches

All these approaches share a fundamental limitation: **the embedding is computed independently of the specific context**. The word "bank" gets the same embedding whether it appears in "river bank" or "bank account."

## Transformers and BERT: Contextual Embeddings {#sec-transformers-bert}

The transformer architecture [@vaswani2017attention], introduced in 2017, and BERT [@devlin2018bert] (2018) fundamentally changed how we create embeddings. The key innovation: **attention mechanisms** that allow each word to attend to all other words in the input.

### The Attention Mechanism

Attention answers the question: "When processing word X, how much should I focus on each other word in the sentence?"

```{python}
#| code-fold: false

"""
Simplified Self-Attention Mechanism

This demonstrates the core concept of transformer attention.
Real implementations include multiple heads, layer normalization,
and many optimizations—but this captures the essence.
"""

import numpy as np

def softmax(x, axis=-1):
    exp_x = np.exp(x - np.max(x, axis=axis, keepdims=True))
    return exp_x / np.sum(exp_x, axis=axis, keepdims=True)


def self_attention(embeddings, W_query, W_key, W_value):
    """
    Self-attention: each position attends to all positions.

    Args:
        embeddings: (seq_len, embed_dim) - input embeddings
        W_query, W_key, W_value: projection matrices

    Returns:
        Contextualized embeddings where each position incorporates
        information from all other positions.
    """
    # Project embeddings to queries, keys, and values
    queries = embeddings @ W_query  # What am I looking for?
    keys = embeddings @ W_key       # What do I contain?
    values = embeddings @ W_value   # What information do I provide?

    # Compute attention scores: how much should each position
    # attend to each other position?
    d_k = keys.shape[-1]
    scores = (queries @ keys.T) / np.sqrt(d_k)  # Scale for stability

    # Softmax to get attention weights (each row sums to 1)
    attention_weights = softmax(scores, axis=-1)

    # Weighted sum of values
    output = attention_weights @ values

    return output, attention_weights


# Example: process "The bank by the river"
sentence = ["The", "bank", "by", "the", "river"]
seq_len = len(sentence)
embed_dim = 4
hidden_dim = 4

# Random initial embeddings (in practice, these are learned)
np.random.seed(42)
embeddings = np.random.randn(seq_len, embed_dim)

# Random projection matrices (in practice, these are learned)
W_q = np.random.randn(embed_dim, hidden_dim) * 0.1
W_k = np.random.randn(embed_dim, hidden_dim) * 0.1
W_v = np.random.randn(embed_dim, hidden_dim) * 0.1

# Apply self-attention
output, attn_weights = self_attention(embeddings, W_q, W_k, W_v)

print("Attention weights (which words attend to which):")
print("Rows = query word, Columns = key word")
print(f"Words: {sentence}\n")
for i, word in enumerate(sentence):
    weights = [f"{w:.2f}" for w in attn_weights[i]]
    print(f"  {word:6s} attends to: {weights}")
```

### How BERT Creates Contextual Embeddings

BERT stacks multiple transformer layers, each applying self-attention. The key insight: **the same word gets different embeddings depending on its context**.

```
Input:  "I went to the bank to deposit money"
        "I sat on the bank of the river"

Word2Vec: "bank" → same embedding in both sentences

BERT:    "bank" (sentence 1) → embedding influenced by "deposit", "money"
         "bank" (sentence 2) → embedding influenced by "river", "sat"
```

### BERT's Training Objectives

BERT learns through two self-supervised tasks:

**1. Masked Language Modeling (MLM)**

Randomly mask 15% of tokens and train the model to predict them:

```
Input:  "The cat [MASK] on the mat"
Target: "sat"
```

This forces the model to understand context deeply—it must use surrounding words to infer the masked word.

**2. Next Sentence Prediction (NSP)**

Given two sentences, predict whether the second follows the first in the original text. This helps the model understand document-level coherence.

```{python}
#| code-fold: false

"""
Demonstrating BERT's Contextual Embeddings

Using a real BERT model to show how the same word gets different
embeddings in different contexts.
"""

from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

# Load a sentence transformer (built on BERT architecture)
model = SentenceTransformer('all-MiniLM-L6-v2')

# Same word, different contexts
sentences = [
    "I deposited money at the bank",      # bank = financial institution
    "The bank approved my loan",          # bank = financial institution
    "We had a picnic on the river bank",  # bank = riverside
    "Fish swim near the bank",            # bank = riverside
]

embeddings = model.encode(sentences)

print("Contextual similarity (same word, different meanings):\n")
labels = ["financial-1", "financial-2", "river-1", "river-2"]
for i in range(len(sentences)):
    for j in range(i + 1, len(sentences)):
        sim = cosine_similarity([embeddings[i]], [embeddings[j]])[0][0]
        print(f"  {labels[i]:12s} ↔ {labels[j]:12s}: {sim:.3f}")

print("\nNotice: financial contexts cluster together, river contexts cluster together")
```

### Why Transformers Dominate

Transformers offer several advantages over previous architectures:

1. **Parallelization**: Unlike RNNs, all positions can be processed simultaneously
2. **Long-range dependencies**: Attention can directly connect distant words
3. **Transfer learning**: Pre-trained models work well across many tasks
4. **Scalability**: Performance improves predictably with more data and compute

## Sentence Transformers: From BERT to Embeddings {#sec-sentence-transformers}

While BERT produces contextual word embeddings, we often need a single embedding for an entire sentence or document. Sentence Transformers [@reimers2019sentence] fine-tune BERT-like models specifically for this purpose.

### The Training Process

Sentence Transformers use **contrastive learning** (covered in depth in @sec-contrastive-learning):

1. Take pairs of similar sentences (e.g., paraphrases, question-answer pairs)
2. Train the model to produce similar embeddings for related sentences
3. Push apart embeddings of unrelated sentences

```{python}
#| code-fold: false

"""
Sentence Transformers in Action

Showing how sentence-level embeddings capture semantic similarity
even when surface forms differ significantly.
"""

from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity

model = SentenceTransformer('all-MiniLM-L6-v2')

# Semantically similar sentences with different words
sentence_pairs = [
    ("The cat is sleeping on the couch", "A feline rests on the sofa"),
    ("How do I reset my password?", "I forgot my login credentials"),
    ("The stock market crashed today", "Financial markets saw major losses"),
]

print("Semantic similarity (different words, same meaning):\n")
for s1, s2 in sentence_pairs:
    emb1, emb2 = model.encode([s1, s2])
    sim = cosine_similarity([emb1], [emb2])[0][0]
    print(f"  \"{s1[:40]}...\"")
    print(f"  \"{s2[:40]}...\"")
    print(f"  Similarity: {sim:.3f}\n")
```

## Image Embeddings: CNNs and Vision Transformers {#sec-image-embedding-models}

Image embedding models follow similar principles but with architectures suited to visual data.

### Convolutional Neural Networks (CNNs)

CNNs like ResNet learn hierarchical visual features:
- Early layers detect edges and textures
- Middle layers recognize shapes and patterns
- Later layers identify objects and scenes

The embedding comes from the layer just before the final classification head—a dense vector capturing visual semantics.

### Vision Transformers (ViT)

Vision Transformers apply the same attention mechanism to images by:
1. Splitting the image into patches (e.g., 16x16 pixels)
2. Treating each patch as a "token" (like words in text)
3. Applying transformer layers to learn relationships between patches

### Multi-Modal Models: CLIP

CLIP [@radford2021learning] learns joint embeddings for text and images by training on 400 million image-caption pairs. It learns to:
- Place images near their text descriptions
- Place text descriptions near matching images

This enables powerful zero-shot capabilities—CLIP can match images to text descriptions it's never seen during training.

## Choosing the Right Model

Different use cases call for different embedding models:

| Use Case | Recommended Model Type | Why |
|----------|----------------------|-----|
| **Keyword search** | BM25 (not embeddings) | Exact matching still wins for precise queries |
| **Semantic search** | Sentence Transformers | Captures meaning beyond keywords |
| **Multilingual** | Multilingual models (mBERT, LaBSE) | Aligned cross-lingual embeddings |
| **Domain-specific** | Fine-tuned models | Generic models miss domain vocabulary |
| **Image similarity** | ResNet, ViT, CLIP | Visual feature extraction |
| **Image + text** | CLIP, BLIP | Unified multi-modal space |

: Model selection guide {.striped}

## Key Takeaways

- **Word2Vec** learns embeddings by predicting context words, capturing semantic relationships through co-occurrence patterns

- **Transformers** use attention mechanisms to create contextual embeddings where the same word gets different representations based on surrounding context

- **BERT** learns through masked language modeling—predicting masked words forces deep contextual understanding

- **Sentence Transformers** adapt BERT for producing fixed-size embeddings for entire sentences, trained with contrastive learning

- **Image models** (CNNs, ViT) learn visual features hierarchically; CLIP extends this to joint text-image embeddings

- **Model choice** depends on your use case: domain, modality, language requirements, and whether you need contextual understanding

## Looking Ahead

Now that you understand how embedding models work internally, @sec-custom-embedding-strategies covers when and how to customize these models for your specific domain and use case.

## Further Reading

- Mikolov, T., et al. (2013). "Efficient Estimation of Word Representations in Vector Space." *arXiv:1301.3781* — The original Word2Vec paper
- Pennington, J., et al. (2014). "GloVe: Global Vectors for Word Representation." *EMNLP* — GloVe's approach to word embeddings
- Vaswani, A., et al. (2017). "Attention Is All You Need." *NeurIPS* — The transformer architecture
- Devlin, J., et al. (2018). "BERT: Pre-training of Deep Bidirectional Transformers." *arXiv:1810.04805* — BERT's training methodology
- Reimers, N. & Gurevych, I. (2019). "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks." *arXiv:1908.10084* — Adapting BERT for sentence embeddings
- Radford, A., et al. (2021). "Learning Transferable Visual Models From Natural Language Supervision." *arXiv:2103.00020* — CLIP's multi-modal approach
