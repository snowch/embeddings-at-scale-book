# Vector Database Fundamentals for Scale {#sec-vector-database-fundamentals}

:::{.callout-important}
## Chapter Update In Progress
This chapter is being updated to reflect the VAST Data Platform Vector DB architecture. Sections on sharding, replication, and distribution patterns will be revised to cover VAST-specific approaches. The foundational concepts (indexing algorithms, SLA design, benchmarking) remain applicable.
:::

:::{.callout-note}
## Chapter Overview
This chapter covers vector database architecture principles, indexing strategies for 256+ trillion rows, distributed systems considerations, performance benchmarking and SLA design, and data locality patterns for global-scale embedding deployments.
:::

## Vector Database Architecture Principles

Traditional databases were designed for exact matches: "Find customer with ID=12345" or "Return all orders where status='shipped'". Vector databases serve a fundamentally different purpose: finding semantic similarity in high-dimensional space. This section explores the architectural principles that make trillion-row vector search possible.

### Why Traditional Databases Fail for Embeddings

The scale mismatch becomes clear with a simple calculation:

```python
# Traditional database query
def find_customer(database, customer_id):
    """O(log N) with B-tree index"""
    return database.index['customer_id'].lookup(customer_id)
    # 256 trillion rows: ~48 comparisons

# Naive embedding search
def find_similar_naive(query_embedding, all_embeddings):
    """O(N * D) where N=rows, D=dimensions"""
    similarities = []
    for embedding in all_embeddings:  # 256 trillion iterations
        similarity = cosine_similarity(query_embedding, embedding)  # 768 multiplications
        similarities.append(similarity)
    return top_k(similarities, k=10)

# Cost calculation:
# 256 trillion rows × 768 dimensions = 196 quadrillion operations
# At 1 billion ops/second: 6 years per query
```

Traditional databases optimize for exact lookups and range scans. Vector databases optimize for approximate nearest neighbor (ANN) search in high-dimensional space. These are fundamentally different problems requiring different architectures.

### The Core Architectural Principles

**Principle 1: Approximate is Sufficient**

Unlike financial transactions where precision is mandatory, embedding similarity is inherently approximate. Whether an item is the 47th or 48th most similar out of 256 trillion doesn't matter—both are highly relevant.

This insight unlocks massive performance gains:

| Aspect | Traditional DB | Vector DB |
|--------|---------------|-----------|
| **Correctness** | 100% exact | 95-99% approximate |
| **Performance** | O(log N) with index | O(log N) even without perfect accuracy |
| **Use Case** | Exact match, transactions | Semantic similarity, recommendations |

The key insight: trading a small amount of accuracy for massive speed gains. Finding the top-10 most similar items from 256T vectors via exact search is infeasible—approximate search with HNSW achieves 95%+ correct results in <100ms.

**Principle 2: Geometry Matters More Than Algebra**

Vector databases leverage geometric properties of high-dimensional spaces:

```{python}
#| echo: true
#| code-fold: true
#| code-summary: "Show geometric intuition for vector similarity"
import numpy as np

class GeometricIntuition:
    """Understanding vector similarity through geometry"""

    def demonstrate_similarity_measures(self):
        """Common similarity metrics and their geometric meaning"""
        embedding_a = np.array([0.5, 0.3, 0.8, 0.1])
        embedding_b = np.array([0.6, 0.4, 0.7, 0.2])

        # Euclidean distance (L2) - straight-line distance
        l2_distance = np.linalg.norm(embedding_a - embedding_b)

        # Cosine similarity - angle between vectors (most common)
        cosine_sim = np.dot(embedding_a, embedding_b) / (
            np.linalg.norm(embedding_a) * np.linalg.norm(embedding_b)
        )

        # Inner product - projection of one vector onto another
        inner_product = np.dot(embedding_a, embedding_b)

        return {"l2_distance": l2_distance, "cosine_similarity": cosine_sim, "inner_product": inner_product}

# Usage example
geo = GeometricIntuition()
measures = geo.demonstrate_similarity_measures()
print(f"L2 distance: {measures['l2_distance']:.4f}")
print(f"Cosine similarity: {measures['cosine_similarity']:.4f}")
```

**Principle 3: Hierarchical Navigation is Key**

The breakthrough insight: you don't need to compare with all vectors, just navigate through the space efficiently.

```{python}
#| echo: true
#| code-fold: true
#| code-summary: "Show hierarchical navigation concept"
class HierarchicalNavigation:
    """How hierarchical structures enable fast search"""

    def hierarchical_solution(self):
        """Multi-level navigation reduces comparisons"""
        # Level 0 (coarsest): 1,000 centroids
        # Level 1: 1M centroids, Level 2: 1B, Level 3: 1T, Level 4: 256T vectors
        # Search: compare with 1,000 at each level, descend to best

        comparisons_per_level = 1000
        num_levels = 5
        total_comparisons = comparisons_per_level * num_levels

        return {
            "total_comparisons": total_comparisons,
            "speedup": f"{256_000_000_000_000 / total_comparisons:.2e}x faster",
            "latency": "<100ms vs 6 years",
        }

# Usage example
nav = HierarchicalNavigation()
result = nav.hierarchical_solution()
print(f"Total comparisons: {result['total_comparisons']:,}")
print(f"Speedup: {result['speedup']}")
```

**Principle 4: Index Structure is Everything**

The choice of index structure determines performance, accuracy, and scalability:

| Index | Build Time | Query Time | Memory | Accuracy | Max Scale | Use Case |
|-------|-----------|------------|--------|----------|-----------|----------|
| **Flat** | O(N) | O(N×D) | O(N×D) | 100% | ~1M | Ground truth |
| **IVF** | O(N×k) | O((N/k)×D×n_probe) | O(N×D+k×D) | 80-95% | ~1B | Balanced |
| **HNSW** | O(N×log(N)×M) | O(log(N)×M) | O(N×(D+M)) | 95-99% | 100B+ | Production (best tradeoff) |
| **LSH** | O(N×L) | O(L×bucket) | O(N×L) | 70-90% | Trillion+ | Ultra-massive scale |
| **PQ** | O(N×iter) | O(N) compressed | O(N×code) | 85-95% | 10B+ | Memory-constrained |

HNSW is the gold standard for high-performance production systems due to its best accuracy/speed tradeoff at scale.

### Production Vector Database Architecture

A production-grade vector database at trillion-row scale needs these core layers:

- **Ingestion**: API gateway, validation, batching, rate limiting (100K-1M embeddings/sec)
- **Storage**: Raw embeddings (cold/S3), indices (hot/SSD), metadata (separate DB), WAL
- **Index**: HNSW graphs, IVF, product quantization, background rebuilder
- **Query**: Parser, planner, distributed executor, result aggregator, LRU cache
- **Metadata**: Filter optimizer, join coordinator (pre-filter vs post-filter)
- **Serving**: Load balancer, router, circuit breaker, adaptive throttling
- **Monitoring**: Latency percentiles, QPS, recall@k, resource utilization, alerts

**Key design patterns for trillion-scale:**

- *Caching*: LRU query cache, hot index pages in memory (>80% hit rate target)
- *Versioning*: Multi-version indices for zero-downtime updates and A/B testing

:::{.callout-note}
## VAST Data Platform Approach
This section will cover VAST Data Platform's approach to data distribution and storage architecture, which differs from traditional sharding patterns. VAST's unified storage architecture eliminates many of the manual sharding decisions required by other vector databases.
:::

:::{.callout-important}
## Architecture First, Scaling Later
The most expensive mistake: starting with single-node architecture and retrofitting for scale. Design for distribution from day one, even if you start with one machine. The patterns are the same, only the scale changes.
:::

## Indexing Strategies for 256+ Trillion Rows

Scaling to 256 trillion embeddings requires sophisticated indexing strategies that balance accuracy, speed, memory, and build time. This section explores battle-tested approaches.

### The Indexing Challenge at Scale

Before diving into solutions, let's quantify what 256 trillion vectors actually means for memory, cost, and build time:

```{python}
#| code-fold: false

# Scale parameters
num_vectors = 256_000_000_000_000  # 256 trillion
embedding_dim = 768
bytes_per_float = 4

# Memory: How much storage do we need?
raw_petabytes = (num_vectors * embedding_dim * bytes_per_float) / (1024 ** 5)
with_index_pb = raw_petabytes * 1.5  # HNSW adds ~50% overhead

# Cost: What if we put it all in RAM?
machines_1tb = int(with_index_pb * 1024)  # AWS r6i.32xlarge has 1TB RAM
monthly_cost = machines_1tb * 8.064 * 24 * 30  # $8.064/hour per machine

# Build time: How long to index everything?
build_seconds = (num_vectors * 100) / 1_000_000  # ~100μs per vector for HNSW
build_years = build_seconds / (60 * 60 * 24 * 365)
parallel_days = (build_seconds / 10_000) / (60 * 60 * 24)  # With 10K machines

print(f"Storage:    {with_index_pb:,.0f} PB (embeddings + HNSW index)")
print(f"Machines:   {machines_1tb:,} × 1TB RAM instances")
print(f"Cost:       ${monthly_cost/1e9:.0f}B/month if all in RAM")
print(f"Build time: {build_years:,.0f} years single-machine, {parallel_days:.0f} days with 10K machines")
print(f"Query:      Must return results in <50ms (leaves no room for brute force)")
```

Clearly, naïve approaches won't work. We need sophisticated indexing strategies.

### Strategy 1: Hierarchical Navigable Small World (HNSW)

HNSW is the gold standard for high-recall, low-latency vector search. Understanding how it works is essential for trillion-scale deployments.

**Core Concept**: HNSW builds a multi-layer proximity graph where:

- Upper layers have long-range connections (for fast navigation)
- Lower layers have short-range connections (for high accuracy)
- Search starts at top layer and descends, getting more refined

```{python}
#| echo: true
#| code-fold: true
#| code-summary: "Show HNSW deep dive implementation"
import math

class HNSWDeepDive:
    """Understanding HNSW internals for scale"""

    def __init__(self, M=16, ef_construction=200, max_level=5):
        """
        M: Max connections per node (typical: 16-64)
        ef_construction: Candidates considered during build (typical: 100-500)
        """
        self.M = M
        self.ef_construction = ef_construction
        self.max_level = max_level
        self.layers = [{} for _ in range(max_level + 1)]
        self.entry_point = None

    def complexity_analysis(self, num_vectors):
        """Analyze HNSW complexity"""
        num_layers = int(math.log(num_vectors) / math.log(self.M))
        comparisons_per_layer = self.M
        total_comparisons = num_layers * comparisons_per_layer
        avg_connections = self.M * 1.5
        memory_per_vector = avg_connections * 8  # 64-bit ID

        return {
            "num_layers": num_layers,
            "comparisons_per_query": total_comparisons,
            "memory_overhead_bytes": memory_per_vector,
            "query_complexity": f"O(log N) ≈ {total_comparisons} comparisons",
        }

    def tune_for_scale(self):
        """Tuning guidelines for trillion-scale"""
        return {
            "M": {"small_1m": 16, "medium_100m": 32, "large_10b": 48, "trillion_100t": 64},
            "ef_construction": {"fast": 100, "balanced": 200, "high_quality": 400},
            "ef_search": {"fast_low_recall": 50, "balanced": 100, "high_recall": 200},
        }

# Usage example
hnsw = HNSWDeepDive(M=48, ef_construction=300)
analysis = hnsw.complexity_analysis(num_vectors=100_000_000_000)
print(f"HNSW for 100B vectors: {analysis['num_layers']} layers, {analysis['comparisons_per_query']} comparisons/query")
```

**HNSW at Trillion Scale: Practical Considerations**

**Sharding**: Use horizontal sharding with 100M-1B vectors per shard. Query all shards in parallel, merge top-k. For optimization, use IVF-HNSW hybrid: IVF for coarse partitioning (5-10x speedup), HNSW within partitions.

**Incremental Updates**:

- *Online inserts*: Add to existing graph, O(log N × M × ef_construction), 10-100ms per insert
- *Batch inserts*: Build mini-HNSW, merge with main graph
- *Deletions*: Soft delete (filter at query time) with periodic rebuild
- *Updates*: Delete + re-insert; rebuild when >20% of vectors changed

**Memory Optimization**:

- *Tiered storage*: Layer 0 in RAM, upper layers on SSD, embeddings on slow storage (60-80% reduction, <10ms latency impact)
- *Graph compression*: 30-50% savings with slight decompression overhead
- *32-bit IDs*: 50% savings on graph structure (limits to 4B vectors/shard)
- *Memory-mapping*: Let OS manage hot/cold page paging

```python
def calculate_memory_savings():
    """Memory savings from optimization techniques"""
    vectors = 1_000_000_000  # 1B vectors
    M, dim = 48, 768

    # Baseline: everything in RAM
    baseline_gb = (vectors * dim * 4 + vectors * M * 8) / (1024**3)

    # Optimized: embeddings on SSD, 32-bit IDs, compression
    optimized_gb = (vectors * M * 4 * 0.7) / (1024**3)

    return f"Baseline: {baseline_gb:.0f} GB → Optimized: {optimized_gb:.0f} GB ({(1-optimized_gb/baseline_gb)*100:.0f}% savings)"

# Output: Baseline: 3214 GB → Optimized: 125 GB (96% savings)
```

### Strategy 2: IVF (Inverted File Index) with Product Quantization

While HNSW is excellent for recall and latency, IVF-PQ excels at massive scale with memory constraints.

**Core Concept**: IVF partitions the space into Voronoi cells. Search checks only the cells nearest to query. Product Quantization compresses vectors 20-100x.

```{python}
#| echo: true
#| code-fold: true
#| code-summary: "Show IVF-PQ strategy implementation"
import numpy as np

class IVFPQStrategy:
    """IVF with Product Quantization for memory efficiency"""

    def __init__(self, num_centroids=65536, num_subquantizers=8, bits_per_sq=8):
        """
        num_centroids: Number of Voronoi cells (typical: 4096-65536)
        num_subquantizers: Divide vector into subvectors (typical: 8-16)
        bits_per_sq: Bits per subquantizer (typical: 8 = 256 clusters)
        """
        self.num_centroids = num_centroids
        self.num_subquantizers = num_subquantizers
        self.bits_per_sq = bits_per_sq

    def memory_analysis(self, num_vectors, embedding_dim):
        """Compare memory usage vs uncompressed"""
        # Uncompressed: float32
        uncompressed_bytes = num_vectors * embedding_dim * 4

        # IVF-PQ: centroids + codes
        centroid_bytes = self.num_centroids * embedding_dim * 4
        code_bytes_per_vector = self.num_subquantizers * (self.bits_per_sq // 8)
        compressed_bytes = centroid_bytes + (num_vectors * code_bytes_per_vector)

        compression_ratio = uncompressed_bytes / compressed_bytes

        return {
            "uncompressed_gb": uncompressed_bytes / (1024**3),
            "compressed_gb": compressed_bytes / (1024**3),
            "compression_ratio": f"{compression_ratio:.1f}x",
            "savings_pct": f"{(1 - compressed_bytes / uncompressed_bytes) * 100:.1f}%",
        }

# Usage example
ivf_pq = IVFPQStrategy(num_centroids=65536, num_subquantizers=96, bits_per_sq=8)
memory = ivf_pq.memory_analysis(num_vectors=100_000_000_000, embedding_dim=768)
print(f"Uncompressed: {memory['uncompressed_gb']:,.0f} GB")
print(f"Compressed: {memory['compressed_gb']:,.0f} GB ({memory['compression_ratio']} compression)")
```

**HNSW vs IVF-PQ Trade-offs**

| Dimension | HNSW | IVF-PQ | Winner |
|-----------|------|--------|--------|
| **Memory** | 1.5-2x raw data | 0.02-0.05x (20-50x compression) | IVF-PQ |
| **Recall** | 95-99% | 85-95% | HNSW |
| **Latency (p99)** | 20-100ms | 50-200ms | HNSW |
| **Build Time** | Slower (graph construction) | Faster (k-means) | IVF-PQ |
| **Updates** | Easy incremental | Must reassign centroids | HNSW |
| **Max Scale** | ~100B vectors | Trillions+ | IVF-PQ |

**When to use each:**

- **HNSW**: High recall (>95%), low latency (<100ms p99), frequent updates, sufficient memory
- **IVF-PQ**: Memory constrained, can tolerate 85-90% recall, infrequent updates, trillion+ scale
- **Hybrid IVF-HNSW**: Best of both—IVF for coarse search, HNSW within partitions

```python
def recommend_index_strategy(num_vectors, memory_budget_gb, recall_requirement, latency_p99_ms):
    """Recommend index strategy based on requirements"""
    embedding_dim = 768
    raw_data_gb = (num_vectors * embedding_dim * 4) / (1024**3)
    hnsw_memory_gb = raw_data_gb * 1.7
    ivf_pq_memory_gb = raw_data_gb * 0.03

    if memory_budget_gb >= hnsw_memory_gb and recall_requirement >= 0.95 and latency_p99_ms <= 100:
        return "HNSW"
    if memory_budget_gb < hnsw_memory_gb and recall_requirement < 0.90:
        return "IVF-PQ"
    if recall_requirement >= 0.93:
        return "Hybrid IVF-HNSW"
    return "IVF-PQ with high n_probe"
```

### Strategy 3: Data Distribution at Scale

At trillion scale, efficient data distribution is essential for performance and availability.

:::{.callout-note}
## VAST Data Platform Approach
This section will cover VAST Data Platform's approach to data distribution, which provides automatic scaling and data placement without manual sharding configuration. VAST's architecture handles data distribution transparently, eliminating the complexity of traditional sharding strategies.
:::

## Distributed Systems Considerations

Vector databases at trillion-scale are distributed systems, inheriting all the challenges of distributed computing: consistency, availability, partition tolerance, and coordination.

### The CAP Theorem for Vector Databases

Vector databases choose **AP (Availability + Partition Tolerance)** over strong consistency. This is the right tradeoff because embeddings are inherently approximate—if one replica has slightly outdated embeddings, query results are still useful.

**Consistency requirements by operation:**

- **Writes/Inserts**: Eventual consistency. Write to primary, async replicate. New embedding visible within 5 seconds.
- **Updates/Deletions**: Eventual consistency with tombstones. Deleted items filtered at query time.
- **Reads/Queries**: Read-your-writes for same session (via session affinity). May see stale data from other users—acceptable.
- **Metadata filters**: Strong consistency required. Security filters (user access) must be immediate.

**Availability techniques**: 3x replication, read from any replica, automatic failover, circuit breakers. Target: 99.99%.

**Partition tolerance**: Gracefully degrade by serving cached results, partial results from available shards, or falling back to multi-region replicas.

### Replication and Data Protection

Data replication ensures availability and durability at scale.

:::{.callout-note}
## VAST Data Platform Approach
This section will cover VAST Data Platform's built-in data protection mechanisms, including its approach to replication, erasure coding, and automatic failover. VAST provides enterprise-grade data protection without requiring manual replication configuration.
:::

### Failure Modes and Recovery

:::{.callout-warning}
## Node Failure
**Probability**: High (MTBF ~3-5 years per machine)
**Impact**: Loss of one shard or replica
**Detection**: Heartbeat timeout (10-30 seconds)
**Recovery**: Remove from load balancer → serve from replica → spawn replacement → rebuild index (5-30 minutes)
**Data Loss**: None with proper replication
:::

:::{.callout-warning}
## Disk Failure
**Probability**: Medium (MTBF ~4 years per disk)
**Detection**: I/O errors, SMART metrics
**Recovery**: Switch to replica → replace disk → restore from backup → rebuild index (1-4 hours)
:::

:::{.callout-warning}
## Network Partition
**Probability**: Medium (datacenter network issues)
**Impact**: Subset of nodes unreachable
**Recovery**: Serve from available partition with graceful degradation (partial results), resync after partition resolves
:::

:::{.callout-caution}
## Index Corruption
**Probability**: Low but critical
**Detection**: Checksums, recall monitoring (sudden drop), user reports
**Recovery**: Rollback to previous version or rebuild from raw embeddings (1-8 hours)
:::

:::{.callout-caution}
## Query of Death
**Probability**: Low but high impact (cascading failures)
**Prevention**: Query timeouts, input validation, circuit breakers, load shedding
:::

### Disaster Recovery

For region-level failures, maintain multi-region replication with automated failover:

1. Detect region failure via health checks
2. Update DNS/routing to backup region
3. Promote backup region to primary
4. Scale up if needed
5. Investigate and restore primary region
6. Resync and failback

Target RPO <5 minutes, RTO <15 minutes. Cost: 2x infrastructure for active-active deployment.

### Chaos Engineering

Test failures proactively using tools like Chaos Monkey, Gremlin, or LitmusChaos:

- Terminate random nodes (10% of fleet)
- Inject network latency (100-500ms)
- Simulate datacenter partitions
- Corrupt index files

Run weekly in staging, monthly in production. Success criteria: query success rate >99.9%, latency within SLA, automatic recovery without human intervention.

### Coordination and Cluster Management

Distributed vector databases require coordination for metadata management and cluster operations.

:::{.callout-note}
## VAST Data Platform Approach
This section will cover how VAST Data Platform handles cluster coordination and management. VAST's architecture simplifies operational complexity by providing integrated cluster management without external coordination services like ZooKeeper or etcd.
:::

## Performance Benchmarking and SLA Design

Production vector databases require rigorous SLA design and continuous performance monitoring. This section covers benchmarking methodologies and SLA patterns.

### Defining SLA Metrics

Core SLA metrics for vector databases:

| Metric | Typical Target | Business Impact |
|--------|---------------|-----------------|
| **Query Latency** | p50 <20ms, p95 <50ms, p99 <100ms | Every 100ms → 1% conversion loss |
| **Recall@K** | recall@10 >0.95, recall@100 >0.98 | Low recall → users don't find relevant items |
| **Throughput** | 1K-10K QPS/shard, 100K-1M global | Insufficient → requests queued or dropped |
| **Availability** | 99.99% (52 min downtime/year) | Downtime → lost revenue |
| **Index Freshness** | <5 minutes to queryable | Stale data → missing new items |
| **Resource Utilization** | CPU <70%, Memory <85%, Disk I/O <80% | Over-utilization → latency spikes |

**Availability budget by target:**

| Target | Allowed Downtime |
|--------|-----------------|
| 99% | 3.65 days/year |
| 99.9% | 8.76 hours/year |
| 99.99% | 52.6 minutes/year |
| 99.999% | 5.26 minutes/year |

**SLI vs SLO vs SLA:**

- **SLI (Service Level Indicator)**: Quantitative measurement (e.g., "p99 latency: 78ms")
- **SLO (Service Level Objective)**: Internal target (e.g., "p99 < 100ms")
- **SLA (Service Level Agreement)**: Contract with consequences (e.g., "p99 < 100ms or 10% credit")

### Benchmarking Methodology

Key benchmark dimensions for vector databases:

| Category | Metrics | Variables |
|----------|---------|-----------|
| **Index Build** | Build time, throughput (vec/sec), peak memory, CPU | Dataset size, dimensions, index params (M, ef) |
| **Query** | p50/p95/p99 latency, QPS, recall@10/100 | K, ef_search, query distribution, concurrency |
| **Updates** | Insert latency/throughput, recall drift | Insert rate, update fraction |
| **Scalability** | Latency/memory vs size | Test at 1M, 10M, 100M, 1B, 10B vectors |

Standard benchmark datasets include SIFT-1M (1M vectors, 128 dims), Deep1B (1B vectors, 96 dims), and LAION-5B (5B vectors, 768 dims). However, production data provides the most accurate benchmarks since query distributions differ from academic datasets.

```python
import numpy as np
import time

def measure_recall(index, embeddings, k=10, num_queries=100):
    """Measure recall@k by comparing approximate vs exact search"""
    total_recall = 0
    for _ in range(num_queries):
        query_idx = np.random.randint(0, len(embeddings))
        query = embeddings[query_idx]

        # Ground truth: brute force exact search
        distances = np.linalg.norm(embeddings - query, axis=1)
        true_top_k = set(np.argsort(distances)[:k])

        # Approximate search
        approx_top_k = set(index.search(query, k=k))

        recall = len(true_top_k & approx_top_k) / k
        total_recall += recall

    return total_recall / num_queries

def benchmark_latency(index, embedding_dim, num_queries=10000):
    """Measure query latency percentiles"""
    latencies = []
    for _ in range(num_queries):
        query = np.random.randn(embedding_dim).astype(np.float32)
        start = time.time()
        index.search(query, k=10)
        latencies.append((time.time() - start) * 1000)

    latencies = np.array(latencies)
    return {
        'p50_ms': np.percentile(latencies, 50),
        'p95_ms': np.percentile(latencies, 95),
        'p99_ms': np.percentile(latencies, 99)
    }
```

### Load Testing and Capacity Planning

Essential load test scenarios for vector databases:

- **Steady State**: Maintain target QPS (e.g., 100K) for 1 hour. Verify p99 <100ms, no errors, stable resource usage.
- **Ramp Up**: Gradually increase 0→200K QPS over 30 minutes to find breaking point and verify graceful degradation.
- **Spike**: Sudden burst (50K→500K QPS for 5 minutes) to test autoscaling—system should scale within 2 minutes.
- **Sustained Peak**: 150K QPS for 8 hours to detect memory leaks and resource exhaustion.
- **Thundering Herd**: 1M simultaneous requests to test queue depth control and load shedding.
- **Geographic**: Multi-region simultaneous load to verify routing and cross-region failover.

For capacity planning, assume ~10K QPS per shard and maintain 2x headroom for spikes. With 50% YoY growth, plan 3 years ahead: 100K QPS today requires 20 shards with headroom, growing to 68 shards by year 3.

## Data Locality and Global Distribution

For trillion-row systems serving global users, data locality and geographic distribution are critical for latency and compliance.

### Geographic Distribution Patterns

Four primary patterns exist for global vector database deployment:

**Full Replication**: Complete copy of all embeddings in each region. Provides lowest query latency and highest availability, but at 5x storage cost. Best for global consumer applications requiring <100ms latency SLA.

**Regional Sharding**: Partition data by region (US, EU, APAC). Lower storage cost and enables data sovereignty compliance, but cross-region queries are expensive. Best for inherently regional data.

**Tiered Distribution**: Hot data (top 10%) in all regions, cold data (remaining 60%) in primary region only. Balances cost and latency with 60-80% savings vs full replication. Best for Zipfian access patterns.

**Edge Caching**: CDN-style caching at 100+ edge locations for top 1% most-queried embeddings. Achieves <20ms latency globally with 85-95% cache hit rates. Best for product search and recommendations.

### Data Residency Compliance

For GDPR compliance, EU citizen data must stay in EU datacenters with replication only within EU (Paris, Frankfurt, Ireland). Implementation requires tagging embeddings with region constraints, enforcing at ingestion, maintaining audit trails, and encrypting with EU-only keys.

For CCPA, maintain user_id → embedding_ids mappings to support deletion requests within 30 days across all replicas.

For China's Cybersecurity Law, operate a completely separate China region with no cross-border data transfer.

### Latency Optimization

Key strategies for minimizing global latency:

- **Geo DNS**: Route users to nearest datacenter (100-200ms reduction)
- **Anycast**: Single IP routing to nearest PoP via CloudFlare/Fastly
- **Prefetching**: Predict and precompute likely queries (50-100ms reduction)
- **Query result caching**: Redis/Memcached with 5-60 minute TTL (60-80% hit rate)
- **Compression**: gzip/brotli for results (10-50ms reduction)

## Key Takeaways

- **Vector databases are fundamentally different from traditional databases**—optimized for approximate nearest neighbor search in high-dimensional space rather than exact matches, making approximate results and geometric reasoning core architectural principles

- **HNSW is the gold standard for high-recall, low-latency search** at billion to trillion scale, achieving O(log N) query complexity through hierarchical graph navigation, with typical configurations (M=32-64, ef_construction=200-400) delivering 95-99% recall at <100ms p99

- **IVF-PQ provides extreme memory efficiency** with 20-100x compression through coarse quantization and product quantization, making it the best choice for memory-constrained trillion-scale deployments despite slightly lower recall (85-95%)

- **Data distribution is essential at trillion-scale**—modern platforms like VAST Data Platform handle distribution automatically, while traditional approaches require manual sharding with configurations of 100M-1B vectors per partition

- **Vector databases choose AP over C in the CAP theorem**, prioritizing availability and partition tolerance with eventual consistency for embeddings (acceptable due to inherent approximation) while maintaining strong consistency for critical metadata like access controls

- **SLA design requires percentile-based latency targets** (p99 <100ms is typical), recall guarantees (>95% recall@10), and availability targets (99.99%), measured continuously with public dashboards and automated alerting on violations

- **Global distribution requires geographic strategies**—full replication for lowest latency (5x cost), regional sharding for data sovereignty (lower cost), tiered distribution for balanced cost/latency (60-80% savings), or edge caching for popular queries (85-95% hit rates)

## Looking Ahead

Part II begins with @sec-text-embeddings, exploring text embeddings—the most common type you'll encounter. You'll learn to create embeddings for words, sentences, and documents, with an optional "Advanced" section explaining how models like Word2Vec, BERT, and Sentence Transformers work under the hood. Subsequent chapters cover image, multi-modal, graph, time-series, and code embeddings.

## Further Reading

- Malkov, Y. A., & Yashunin, D. A. (2018). "Efficient and robust approximate nearest neighbor search using Hierarchical Navigable Small World graphs." *IEEE Transactions on Pattern Analysis and Machine Intelligence*
- Jégou, H., Douze, M., & Schmid, C. (2011). "Product Quantization for Nearest Neighbor Search." *IEEE Transactions on Pattern Analysis and Machine Intelligence*
- Johnson, J., Douze, M., & Jégou, H. (2019). "Billion-scale similarity search with GPUs." *IEEE Transactions on Big Data*
- Aumüller, M., Bernhardsson, E., & Faithfull, A. (2020). "ANN-Benchmarks: A Benchmarking Tool for Approximate Nearest Neighbor Algorithms." *Information Systems*
- Brewer, E. A. (2012). "CAP twelve years later: How the 'rules' have changed." *Computer*
- Gormley, C., & Tong, Z. (2015). *Elasticsearch: The Definitive Guide*. O'Reilly Media
- Kleppmann, M. (2017). *Designing Data-Intensive Applications*. O'Reilly Media
- Beyer, B., et al. (2016). *Site Reliability Engineering: How Google Runs Production Systems*. O'Reilly Media
