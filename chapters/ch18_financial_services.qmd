# Financial Services Disruption {#sec-financial-services}

:::{.callout-note}
## Chapter Overview
Financial services—from trading to lending to compliance—operate on information asymmetries, market timing, and risk assessment. This chapter applies embeddings to financial services disruption: trading signal generation using embeddings of securities, market conditions, and alternative data to identify opportunities before markets react, credit risk assessment with entity embeddings that encode creditworthiness from traditional and alternative data sources for more accurate underwriting, regulatory compliance automation through document and transaction embeddings that monitor policy adherence and detect violations, customer behavior analysis via embedding-based segmentation that enables personalized products and prevents churn, and market sentiment analysis extracting trading signals from news, social media, and earnings call embeddings. These techniques transform financial services from rule-based systems to learned representations that capture complex market dynamics and customer patterns.
:::

After building automated decision systems (@sec-automated-decision-systems), embeddings enable **financial services disruption** at scale. Traditional financial systems rely on handcrafted features (P/E ratio, debt-to-income), rigid rules (FICO score > 700), and human judgment (trader intuition, analyst reports). **Embedding-based financial systems** represent securities, customers, transactions, and market conditions as vectors, enabling discovery of non-obvious patterns, transfer learning across markets and products, and real-time adaptation to market regime changes—providing competitive advantages measured in basis points that compound to billions.

## Trading Signal Generation

Financial markets are complex adaptive systems where information propagates through securities, sectors, and geographies. **Embedding-based trading signal generation** represents securities and market conditions as vectors, identifying opportunities through learned relationships before traditional models react.

### The Trading Signal Challenge

Traditional trading signals face limitations:

- **Factor models**: Limited to known factors (value, momentum, quality), miss complex interactions
- **Technical analysis**: Hand-crafted patterns (head and shoulders), high false positive rates
- **Fundamental analysis**: Slow, requires manual interpretation, can't scale across thousands of securities
- **Alternative data**: Unstructured (satellite imagery, credit card transactions), hard to integrate

**Embedding approach**: Learn security embeddings from price history, fundamentals, news, and alternative data. Similar securities cluster together; opportunities manifest as embedding movements that predict future returns before price movements.

```python
"""
Trading Signal Generation with Embeddings

Architecture:
1. Security encoder: Maps securities to embeddings
2. Market encoder: Captures market regime (bull, bear, volatile)
3. News encoder: Extracts sentiment and topics from financial news
4. Alternative data encoders: Satellite imagery, web traffic, etc.
5. Signal generator: Predicts future returns from embeddings

Techniques:
- Time series embeddings: LSTM/Transformer over price history
- Cross-sectional learning: Securities with similar fundamentals behave similarly
- Graph embeddings: Capture supply chain, sector, geographic relationships
- Multi-modal fusion: Combine price, news, fundamentals, alternative data

Production considerations:
- Low latency: <10ms for real-time trading
- Interpretability: Explain signal sources for risk management
- Risk constraints: Sector limits, position sizing, stop losses
- Transaction costs: Model slippage, commissions, market impact
"""

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import List, Dict, Optional, Tuple, Any
from dataclasses import dataclass
from datetime import datetime
import time

@dataclass
class Security:
    """
    Security (stock, bond, commodity, etc.)
    
    Attributes:
        ticker: Symbol (AAPL, TSLA, etc.)
        name: Company name
        sector: Industry sector
        market_cap: Market capitalization
        price_history: Historical prices (open, high, low, close, volume)
        fundamentals: Financial metrics (revenue, earnings, debt, etc.)
        news: Recent news articles
        alternative_data: Non-traditional data (web traffic, sentiment, etc.)
    """
    ticker: str
    name: str
    sector: str
    market_cap: float
    price_history: Optional[np.ndarray] = None
    fundamentals: Optional[Dict[str, float]] = None
    news: Optional[List[str]] = None
    alternative_data: Optional[Dict[str, Any]] = None
    
    def __post_init__(self):
        if self.fundamentals is None:
            self.fundamentals = {}
        if self.news is None:
            self.news = []
        if self.alternative_data is None:
            self.alternative_data = {}

@dataclass
class TradingSignal:
    """
    Trading signal output
    
    Attributes:
        ticker: Security ticker
        timestamp: Signal generation time
        predicted_return: Expected return (next day, week, month)
        confidence: Signal confidence (0-1)
        factors: Contributing factors to signal
        risk_score: Risk assessment (0-1)
        position_size: Recommended position size
        explanation: Human-readable explanation
    """
    ticker: str
    timestamp: float
    predicted_return: float
    confidence: float
    factors: Dict[str, float]
    risk_score: float
    position_size: float
    explanation: str

class SecurityEncoder(nn.Module):
    """
    Encode securities to embeddings
    
    Architecture:
    - Price encoder: LSTM over historical prices
    - Fundamental encoder: MLP over financial metrics
    - News encoder: Transformer over recent news
    - Alternative data encoder: Custom encoders per data type
    - Fusion: Attention-based combination of all modalities
    
    Training:
    - Return prediction: Embedding predicts future returns
    - Contrastive: Securities in same sector closer
    - Triplet: High-correlation securities closer than low-correlation
    """
    
    def __init__(
        self,
        embedding_dim: int = 256,
        price_lookback: int = 60,  # 60 days
        num_fundamental_features: int = 50
    ):
        super().__init__()
        self.embedding_dim = embedding_dim
        self.price_lookback = price_lookback
        
        # Price encoder (LSTM over OHLCV)
        self.price_encoder = nn.LSTM(
            input_size=5,  # open, high, low, close, volume
            hidden_size=128,
            num_layers=2,
            batch_first=True,
            dropout=0.2
        )
        
        # Fundamental encoder
        self.fundamental_encoder = nn.Sequential(
            nn.Linear(num_fundamental_features, 128),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(128, 128)
        )
        
        # Fusion layer
        self.fusion = nn.Sequential(
            nn.Linear(256, 256),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(256, embedding_dim)
        )
        
    def forward(
        self,
        price_history: torch.Tensor,
        fundamentals: torch.Tensor
    ) -> torch.Tensor:
        """
        Encode securities
        
        Args:
            price_history: Price history (batch_size, seq_len, 5)
            fundamentals: Fundamental features (batch_size, num_features)
        
        Returns:
            Security embeddings (batch_size, embedding_dim)
        """
        # Encode price history
        _, (price_hidden, _) = self.price_encoder(price_history)
        price_emb = price_hidden[-1]  # Last layer hidden state
        
        # Encode fundamentals
        fundamental_emb = self.fundamental_encoder(fundamentals)
        
        # Fuse
        combined = torch.cat([price_emb, fundamental_emb], dim=1)
        security_emb = self.fusion(combined)
        
        # Normalize
        security_emb = F.normalize(security_emb, p=2, dim=1)
        
        return security_emb

class MarketRegimeEncoder(nn.Module):
    """
    Encode market regime (bull, bear, volatile, calm)
    
    Captures macro conditions affecting all securities:
    - VIX level (volatility)
    - Interest rates
    - Credit spreads
    - Market breadth
    - Sector rotation
    
    Used to condition trading signals on market state.
    """
    
    def __init__(self, embedding_dim: int = 64):
        super().__init__()
        self.embedding_dim = embedding_dim
        
        # Market indicators encoder
        self.encoder = nn.Sequential(
            nn.Linear(20, 64),  # 20 market indicators
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(64, embedding_dim)
        )
        
    def forward(self, market_indicators: torch.Tensor) -> torch.Tensor:
        """
        Encode market regime
        
        Args:
            market_indicators: Market indicators (batch_size, 20)
        
        Returns:
            Market regime embeddings (batch_size, embedding_dim)
        """
        regime_emb = self.encoder(market_indicators)
        regime_emb = F.normalize(regime_emb, p=2, dim=1)
        return regime_emb

class TradingSignalGenerator(nn.Module):
    """
    Generate trading signals from security and market embeddings
    
    Predicts future returns conditioned on:
    - Security embedding (intrinsic characteristics)
    - Market regime embedding (macro environment)
    - Recent momentum (short-term price action)
    - Cross-sectional position (relative to sector/market)
    
    Outputs:
    - Expected return (alpha)
    - Confidence (signal strength)
    - Risk score (downside risk)
    """
    
    def __init__(
        self,
        security_dim: int = 256,
        regime_dim: int = 64,
        hidden_dim: int = 256
    ):
        super().__init__()
        
        # Signal generation network
        self.signal_network = nn.Sequential(
            nn.Linear(security_dim + regime_dim + 10, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(hidden_dim, 3)  # return, confidence, risk
        )
        
    def forward(
        self,
        security_emb: torch.Tensor,
        regime_emb: torch.Tensor,
        momentum_features: torch.Tensor
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        Generate trading signals
        
        Args:
            security_emb: Security embeddings (batch_size, security_dim)
            regime_emb: Market regime embeddings (batch_size, regime_dim)
            momentum_features: Recent momentum (batch_size, 10)
        
        Returns:
            Tuple of (predicted_return, confidence, risk_score)
        """
        # Combine all inputs
        combined = torch.cat([security_emb, regime_emb, momentum_features], dim=1)
        
        # Generate signals
        outputs = self.signal_network(combined)
        
        # Split outputs
        predicted_return = outputs[:, 0]
        confidence = torch.sigmoid(outputs[:, 1])  # 0-1
        risk_score = torch.sigmoid(outputs[:, 2])  # 0-1
        
        return predicted_return, confidence, risk_score

# Example: End-to-end trading signal system
def trading_signal_example():
    """
    Complete trading signal generation pipeline
    
    Demonstrates:
    1. Encoding securities from price history and fundamentals
    2. Encoding market regime
    3. Generating trading signals
    4. Position sizing and risk management
    """
    
    print("=== Trading Signal Generation System ===")
    print("\nObjective: Generate alpha by identifying mispriced securities")
    print("Approach: Learn security embeddings from multi-modal data")
    print("         Predict future returns in embedding space")
    
    # Initialize components
    security_encoder = SecurityEncoder(embedding_dim=256)
    regime_encoder = MarketRegimeEncoder(embedding_dim=64)
    signal_generator = TradingSignalGenerator(security_dim=256, regime_dim=64)
    
    print("\n--- Example 1: Tech Growth Stock ---")
    print("Ticker: GROWTH_TECH")
    print("Sector: Technology")
    print("Recent Performance: +15% vs market +5%")
    print("Fundamentals: High revenue growth, negative earnings")
    print("News: New product launch, positive analyst coverage")
    print("Market Regime: Bull market, low volatility")
    
    # Simulate encoding
    print("\nSecurity Embedding Analysis:")
    print("  Similar to: Other high-growth tech stocks")
    print("  Cluster: Growth momentum cluster")
    print("  Distance from value stocks: 0.85 (far)")
    
    print("\nSignal:")
    print("  Predicted return (1 month): +8.5%")
    print("  Confidence: 0.72")
    print("  Risk score: 0.68 (high volatility)")
    print("  Position size: 2% of portfolio")
    print("  Explanation: Strong momentum, positive news sentiment")
    print("               But elevated risk due to negative earnings")
    
    print("\n--- Example 2: Value Stock Opportunity ---")
    print("Ticker: VALUE_IND")
    print("Sector: Industrials")
    print("Recent Performance: -10% vs market +5%")
    print("Fundamentals: Low P/E, high dividend yield, strong balance sheet")
    print("News: Temporary supply chain issues (resolved)")
    print("Market Regime: Bull market, low volatility")
    
    print("\nSecurity Embedding Analysis:")
    print("  Similar to: Other undervalued industrials")
    print("  Cluster: Value recovery cluster")
    print("  Recent shift: Moving toward growth cluster")
    
    print("\nSignal:")
    print("  Predicted return (1 month): +12.3%")
    print("  Confidence: 0.85")
    print("  Risk score: 0.35 (low volatility)")
    print("  Position size: 4% of portfolio")
    print("  Explanation: Temporary selloff created value opportunity")
    print("               Fundamentals strong, supply chain issues resolved")
    print("               Similar stocks historically recovered quickly")
    
    print("\n--- Example 3: Avoid Signal ---")
    print("Ticker: BUBBLE_STOCK")
    print("Sector: Technology")
    print("Recent Performance: +150% in 3 months")
    print("Fundamentals: No revenue, massive valuation")
    print("News: Heavy retail investor interest, social media hype")
    print("Market Regime: Bull market, increasing volatility")
    
    print("\nSecurity Embedding Analysis:")
    print("  Similar to: Past bubble stocks (2000, 2021)")
    print("  Cluster: Speculative bubble cluster")
    print("  Warning: High distance from fundamental value cluster")
    
    print("\nSignal:")
    print("  Predicted return (1 month): -15.2%")
    print("  Confidence: 0.68")
    print("  Risk score: 0.92 (extreme risk)")
    print("  Position size: 0% (AVOID)")
    print("  Explanation: Embedding similar to past bubble stocks")
    print("               Price disconnected from fundamentals")
    print("               High crash risk when sentiment reverses")
    
    print("\n--- Portfolio Construction ---")
    print("Strategy: Long-short equity")
    print("Long positions:")
    print("  VALUE_IND: 4% (high confidence, low risk)")
    print("  GROWTH_TECH: 2% (medium confidence, high risk)")
    print("  ... 10 other long positions")
    print("Short positions:")
    print("  BUBBLE_STOCK: -2% (betting on decline)")
    print("  ... 5 other short positions")
    print("Total exposure: 50% long, 20% short, 30% cash")
    print("Expected return: 12% annualized")
    print("Sharpe ratio: 1.8")

# Uncomment to run:
# trading_signal_example()
```

:::{.callout-tip}
## Trading Signal Best Practices

**Data sources:**
- **Price data**: Historical OHLCV, bid-ask spreads, order book depth
- **Fundamentals**: Earnings, revenue, margins, debt, cash flow
- **News**: Financial news, earnings calls, SEC filings
- **Alternative data**: Satellite imagery, web traffic, credit card data, social sentiment
- **Market data**: VIX, interest rates, sector indices, credit spreads

**Modeling:**
- **Time series**: LSTM/Transformer for temporal patterns
- **Cross-sectional**: Learn relationships between securities
- **Multi-modal**: Fuse price, fundamentals, news, alternative data
- **Graph embeddings**: Capture supply chain, sector relationships
- **Meta-learning**: Adapt quickly to regime changes

**Production:**
- **Low latency**: <10ms for high-frequency, <1s for daily signals
- **Risk management**: Position limits, stop losses, correlation constraints
- **Backtesting**: Out-of-sample testing on historical data
- **Transaction costs**: Model slippage, commissions, market impact
- **Monitoring**: Track signal performance, attribution, regime changes

**Challenges:**
- **Overfitting**: Easy to find spurious patterns in financial data
- **Regime changes**: Markets shift (2008 crisis, COVID), models break
- **Data quality**: Corporate actions, survivorship bias, look-ahead bias
- **Market impact**: Large orders move prices, eroding alpha
- **Competition**: Other quants use similar techniques, alpha decays
:::

## Credit Risk Assessment

Credit risk assessment determines lending decisions—approving loans, setting interest rates, determining credit limits. **Embedding-based credit risk assessment** represents borrowers, transactions, and economic conditions as vectors, enabling more accurate risk scoring from traditional and alternative data sources.

### The Credit Risk Challenge

Traditional credit scoring faces limitations:

- **Limited features**: FICO score uses only 5 factors (payment history, utilization, length, new credit, mix)
- **Sparse data**: "Credit invisibles" lack traditional credit history
- **Static models**: Don't adapt to changing economic conditions
- **Fairness concerns**: Proxy features (zip code) correlated with protected attributes

**Embedding approach**: Learn borrower embeddings from traditional credit data (payment history, utilization) plus alternative data (rent payments, utility bills, employment history, transaction patterns). Similar borrowers cluster together; risk propagates through social and transaction networks.

```python
"""
Credit Risk Assessment with Embeddings

Architecture:
1. Borrower encoder: Maps borrowers to embeddings
2. Transaction encoder: Behavioral patterns from transaction history
3. Network encoder: Social and business relationships
4. Economic encoder: Macro conditions, regional factors
5. Risk scorer: Predicts default probability from embeddings

Techniques:
- Multi-modal fusion: Credit history + alternative data
- Graph embeddings: Capture relationship networks
- Sequential modeling: Transaction patterns over time
- Transfer learning: Pre-train on large datasets, fine-tune per segment

Production considerations:
- Explainability: SHAP values, adverse action requirements
- Fairness: Monitor for disparate impact on protected groups
- Compliance: FCRA, ECOA, state-specific regulations
- Online learning: Update models as borrowers repay/default
"""

@dataclass
class Borrower:
    """
    Loan applicant or existing borrower
    
    Attributes:
        borrower_id: Unique identifier
        credit_score: Traditional credit score (if available)
        income: Annual income
        employment: Employment history
        credit_history: Payment history, utilization, etc.
        transaction_history: Bank transaction patterns
        alternative_data: Rent, utilities, etc.
        relationships: Known relationships (employer, landlord, etc.)
        application: Current loan application details
    """
    borrower_id: str
    credit_score: Optional[int] = None
    income: Optional[float] = None
    employment: Optional[Dict[str, Any]] = None
    credit_history: Optional[Dict[str, Any]] = None
    transaction_history: Optional[List[Dict[str, Any]]] = None
    alternative_data: Optional[Dict[str, Any]] = None
    relationships: Optional[List[str]] = None
    application: Optional[Dict[str, Any]] = None
    
    def __post_init__(self):
        if self.employment is None:
            self.employment = {}
        if self.credit_history is None:
            self.credit_history = {}
        if self.transaction_history is None:
            self.transaction_history = []
        if self.alternative_data is None:
            self.alternative_data = {}
        if self.relationships is None:
            self.relationships = []
        if self.application is None:
            self.application = {}

@dataclass
class CreditDecision:
    """
    Credit decision output
    
    Attributes:
        borrower_id: Applicant identifier
        decision: Approve, reject, or manual review
        interest_rate: Approved interest rate (if approved)
        credit_limit: Credit limit (if approved)
        default_probability: Predicted default probability
        confidence: Decision confidence
        explanation: Explanation for decision
        adverse_action_reasons: Reasons for rejection (if applicable)
    """
    borrower_id: str
    decision: str  # approve, reject, review
    interest_rate: Optional[float] = None
    credit_limit: Optional[float] = None
    default_probability: float = 0.0
    confidence: float = 0.0
    explanation: str = ""
    adverse_action_reasons: Optional[List[str]] = None

class BorrowerEncoder(nn.Module):
    """
    Encode borrowers to embeddings
    
    Architecture:
    - Credit history encoder: Payment patterns, utilization, age of accounts
    - Transaction encoder: LSTM over bank transactions
    - Alternative data encoder: Rent, utilities, employment stability
    - Network encoder: Graph neural network over relationships
    - Fusion: Attention-based combination
    
    Training:
    - Default prediction: Embedding predicts default probability
    - Contrastive: Good borrowers close, bad borrowers far
    - Multi-task: Predict default, prepayment, utilization
    """
    
    def __init__(
        self,
        embedding_dim: int = 128,
        num_credit_features: int = 30,
        num_alternative_features: int = 20
    ):
        super().__init__()
        self.embedding_dim = embedding_dim
        
        # Credit history encoder
        self.credit_encoder = nn.Sequential(
            nn.Linear(num_credit_features, 64),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(64, 64)
        )
        
        # Transaction pattern encoder
        self.transaction_encoder = nn.LSTM(
            input_size=10,  # transaction features
            hidden_size=64,
            num_layers=1,
            batch_first=True
        )
        
        # Alternative data encoder
        self.alternative_encoder = nn.Sequential(
            nn.Linear(num_alternative_features, 64),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(64, 64)
        )
        
        # Fusion
        self.fusion = nn.Sequential(
            nn.Linear(192, 128),  # 64 * 3
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(128, embedding_dim)
        )
        
    def forward(
        self,
        credit_features: torch.Tensor,
        transaction_history: torch.Tensor,
        alternative_features: torch.Tensor
    ) -> torch.Tensor:
        """
        Encode borrowers
        
        Args:
            credit_features: Credit history (batch_size, num_credit_features)
            transaction_history: Transactions (batch_size, seq_len, 10)
            alternative_features: Alternative data (batch_size, num_alternative_features)
        
        Returns:
            Borrower embeddings (batch_size, embedding_dim)
        """
        # Encode credit history
        credit_emb = self.credit_encoder(credit_features)
        
        # Encode transaction history
        _, (transaction_hidden, _) = self.transaction_encoder(transaction_history)
        transaction_emb = transaction_hidden[-1]
        
        # Encode alternative data
        alternative_emb = self.alternative_encoder(alternative_features)
        
        # Fuse
        combined = torch.cat([credit_emb, transaction_emb, alternative_emb], dim=1)
        borrower_emb = self.fusion(combined)
        
        # Normalize
        borrower_emb = F.normalize(borrower_emb, p=2, dim=1)
        
        return borrower_emb

class CreditRiskScorer(nn.Module):
    """
    Score credit risk from borrower embeddings
    
    Outputs:
    - Default probability
    - Expected loss (probability × loss given default)
    - Confidence score
    
    Calibrated to produce well-calibrated probabilities
    for regulatory compliance and pricing.
    """
    
    def __init__(self, embedding_dim: int = 128):
        super().__init__()
        
        # Risk scoring network
        self.scorer = nn.Sequential(
            nn.Linear(embedding_dim + 10, 128),  # +10 for loan features
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(64, 3)  # default_prob, expected_loss, confidence
        )
        
    def forward(
        self,
        borrower_emb: torch.Tensor,
        loan_features: torch.Tensor
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        Score credit risk
        
        Args:
            borrower_emb: Borrower embeddings (batch_size, embedding_dim)
            loan_features: Loan characteristics (batch_size, 10)
        
        Returns:
            Tuple of (default_prob, expected_loss, confidence)
        """
        # Combine borrower and loan features
        combined = torch.cat([borrower_emb, loan_features], dim=1)
        
        # Score risk
        outputs = self.scorer(combined)
        
        # Split outputs
        default_prob = torch.sigmoid(outputs[:, 0])  # 0-1
        expected_loss = torch.sigmoid(outputs[:, 1])  # 0-1
        confidence = torch.sigmoid(outputs[:, 2])  # 0-1
        
        return default_prob, expected_loss, confidence

# Example: Credit risk assessment
def credit_risk_example():
    """
    Credit risk assessment pipeline
    
    Demonstrates:
    1. Traditional credit scoring limitations
    2. Alternative data integration
    3. Embedding-based risk assessment
    4. Explainability for adverse actions
    """
    
    print("=== Credit Risk Assessment System ===")
    print("\nObjective: Expand access to credit while managing risk")
    print("Challenge: Credit invisibles lack traditional credit history")
    print("Solution: Alternative data + embeddings")
    
    print("\n--- Applicant 1: Traditional Credit User ---")
    print("Credit score: 750")
    print("Income: $75,000/year")
    print("Credit history: 10 years, no missed payments")
    print("Utilization: 15%")
    print("Loan request: $10,000 personal loan")
    
    print("\nAssessment:")
    print("  Traditional model: APPROVE (strong credit history)")
    print("  Default probability: 2.5%")
    print("  Interest rate: 8.5%")
    print("  Credit limit: $10,000")
    print("  Confidence: 0.92")
    print("\n→ Easy approval, standard process")
    
    print("\n--- Applicant 2: Credit Invisible ---")
    print("Credit score: None (no credit history)")
    print("Income: $55,000/year (verified)")
    print("Rent payments: 3 years, always on time")
    print("Utility payments: Never late")
    print("Transaction history: Stable income deposits, responsible spending")
    print("Employment: 4 years same employer")
    print("Loan request: $5,000 personal loan")
    
    print("\nTraditional assessment:")
    print("  REJECTED - No credit score")
    print("  → Lost customer, perpetuates credit invisibility")
    
    print("\nEmbedding-based assessment:")
    print("  Borrower embedding analysis:")
    print("    - Similar to: Prime borrowers (based on alternative data)")
    print("    - Cluster: Responsible credit invisible segment")
    print("    - Distance from high-risk cluster: 0.78 (far)")
    print("  ")
    print("  Decision: APPROVE")
    print("  Default probability: 4.2% (slightly higher due to uncertainty)")
    print("  Interest rate: 11.5% (risk-adjusted)")
    print("  Credit limit: $5,000")
    print("  Confidence: 0.78")
    print("  ")
    print("  Explanation:")
    print("    ✓ Consistent rent/utility payments (3 years)")
    print("    ✓ Stable employment (4 years)")
    print("    ✓ Responsible transaction patterns")
    print("    ! Limited by lack of traditional credit history")
    print("\n→ Expanded access while managing risk")
    
    print("\n--- Applicant 3: High Risk ---")
    print("Credit score: 620 (fair)")
    print("Income: $45,000/year")
    print("Credit history: 2 late payments last year")
    print("Utilization: 85% (high)")
    print("Transaction history: Frequent overdrafts, gambling transactions")
    print("Recent applications: 5 credit cards last 3 months")
    print("Loan request: $15,000 personal loan")
    
    print("\nEmbedding-based assessment:")
    print("  Borrower embedding analysis:")
    print("    - Similar to: High-default borrowers")
    print("    - Cluster: Financial stress segment")
    print("    - Red flags: High utilization, credit seeking, gambling")
    print("  ")
    print("  Decision: REJECT")
    print("  Default probability: 18.5%")
    print("  Confidence: 0.85")
    print("  ")
    print("  Adverse action reasons:")
    print("    1. High credit utilization (85%)")
    print("    2. Recent late payments")
    print("    3. Multiple recent credit applications")
    print("    4. Transaction patterns indicate financial stress")
    print("\n→ Responsible rejection with explanation")
    
    print("\n--- Results Across Portfolio ---")
    print("Approval rate: 72% (vs 60% traditional)")
    print("Default rate: 3.8% (vs 4.5% traditional)")
    print("Credit invisibles served: 15,000 new customers")
    print("Average interest rate: 10.2%")
    print("Portfolio ROI: 8.5% (vs 7.2% traditional)")
    print("\n→ Expanded access + better risk management")

# Uncomment to run:
# credit_risk_example()
```

:::{.callout-tip}
## Credit Risk Best Practices

**Data sources:**
- **Traditional**: Credit score, payment history, utilization, credit mix
- **Alternative**: Rent/utility payments, bank transactions, employment history
- **Behavioral**: Transaction patterns, savings behavior, bill-pay timing
- **Network**: Employer, landlord, known relationships
- **Contextual**: Income verification, regional economics, industry trends

**Modeling:**
- **Multi-modal fusion**: Combine traditional + alternative data
- **Sequential models**: LSTM over transaction/payment history
- **Graph neural networks**: Capture network effects
- **Calibration**: Well-calibrated probabilities for pricing
- **Transfer learning**: Pre-train on large datasets

**Production:**
- **Explainability**: SHAP values, adverse action requirements
- **Fairness monitoring**: Track approval/default rates by demographics
- **Compliance**: FCRA, ECOA, state regulations
- **Online learning**: Update as loans perform
- **A/B testing**: Test new models on small segments

**Challenges:**
- **Adverse selection**: Approved borrowers different from rejected
- **Label lag**: Loans take months/years to default or repay
- **Distribution shift**: Economic cycles change risk profiles
- **Fairness**: Avoid proxy variables for protected attributes
- **Cold start**: New borrowers have minimal data
:::

## Regulatory Compliance Automation

Financial institutions face extensive regulatory requirements—anti-money laundering (AML), know-your-customer (KYC), trading restrictions, privacy rules. **Embedding-based compliance automation** represents documents, transactions, and entities as vectors, enabling automated policy monitoring, violation detection, and regulatory reporting at scale.

### The Compliance Challenge

Traditional compliance systems face limitations:

- **Rule-based**: Brittle keyword matching, high false positives
- **Manual review**: Expensive, slow, inconsistent
- **Siloed**: Different systems for different regulations
- **Reactive**: Detect violations after they occur

**Embedding approach**: Learn embeddings of regulations, internal policies, transactions, and communications. Violations manifest as semantic similarity between actions and prohibited patterns, enabling proactive detection across structured and unstructured data.

```python
"""
Regulatory Compliance Automation

Architecture:
1. Document encoder: Embed regulations, policies, procedures
2. Transaction encoder: Embed financial transactions
3. Communication encoder: Embed emails, chats, calls
4. Violation detector: Identify actions similar to known violations
5. Report generator: Automated regulatory reporting

Use cases:
- AML: Detect suspicious transaction patterns
- Trading surveillance: Identify market manipulation, insider trading
- Privacy: Monitor GDPR, CCPA compliance
- KYC: Verify customer identities, screen against sanctions lists
- Recordkeeping: Ensure complete audit trails

Production considerations:
- Accuracy: Minimize false positives (costly manual review)
- Coverage: Monitor all transactions, communications
- Latency: Real-time blocking for high-risk transactions
- Auditability: Explain why violations were flagged
"""

@dataclass
class ComplianceRule:
    """
    Regulatory or internal compliance rule
    
    Attributes:
        rule_id: Unique identifier
        rule_type: Type (AML, trading, privacy, etc.)
        description: Human-readable description
        examples: Example violations
        severity: Low, medium, high, critical
        actions: Actions to take when triggered
        embedding: Learned rule embedding
    """
    rule_id: str
    rule_type: str
    description: str
    examples: List[str]
    severity: str
    actions: List[str]
    embedding: Optional[np.ndarray] = None

@dataclass
class ComplianceEvent:
    """
    Event requiring compliance review
    
    Attributes:
        event_id: Unique identifier
        event_type: Type (transaction, communication, etc.)
        timestamp: When event occurred
        entities: Involved entities (customers, employees, etc.)
        content: Event content (transaction details, message text, etc.)
        matched_rules: Rules potentially violated
        risk_score: Compliance risk score (0-1)
        requires_review: Whether manual review needed
    """
    event_id: str
    event_type: str
    timestamp: float
    entities: List[str]
    content: Dict[str, Any]
    matched_rules: List[str]
    risk_score: float
    requires_review: bool

class ComplianceEncoder(nn.Module):
    """
    Encode compliance rules and events
    
    Uses same embedding space for rules and events,
    enabling semantic similarity matching.
    
    Training:
    - Contrastive: Violations close to violated rules
    - Classification: Predict rule type from content
    - Few-shot: Learn from limited violation examples
    """
    
    def __init__(self, embedding_dim: int = 256):
        super().__init__()
        self.embedding_dim = embedding_dim
        
        # Text encoder (for rules and event descriptions)
        self.text_encoder = nn.LSTM(
            input_size=768,  # BERT embeddings
            hidden_size=256,
            num_layers=2,
            batch_first=True,
            dropout=0.2
        )
        
        # Structured data encoder (for transaction features)
        self.structured_encoder = nn.Sequential(
            nn.Linear(50, 128),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(128, 256)
        )
        
        # Fusion
        self.fusion = nn.Sequential(
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(256, embedding_dim)
        )
        
    def forward(
        self,
        text_embeddings: torch.Tensor,
        structured_features: torch.Tensor
    ) -> torch.Tensor:
        """
        Encode compliance rules or events
        
        Args:
            text_embeddings: Text embeddings (batch_size, seq_len, 768)
            structured_features: Structured features (batch_size, 50)
        
        Returns:
            Compliance embeddings (batch_size, embedding_dim)
        """
        # Encode text
        _, (text_hidden, _) = self.text_encoder(text_embeddings)
        text_emb = text_hidden[-1]
        
        # Encode structured features
        structured_emb = self.structured_encoder(structured_features)
        
        # Fuse
        combined = torch.cat([text_emb, structured_emb], dim=1)
        compliance_emb = self.fusion(combined)
        
        # Normalize
        compliance_emb = F.normalize(compliance_emb, p=2, dim=1)
        
        return compliance_emb

# Example: AML transaction monitoring
def aml_monitoring_example():
    """
    Anti-money laundering transaction monitoring
    
    Detects suspicious patterns:
    - Structuring (splitting large transactions to avoid reporting)
    - Rapid movement (funds in and out quickly)
    - Round-tripping (circular money flows)
    - Shell company usage
    - Geographic anomalies (high-risk jurisdictions)
    """
    
    print("=== AML Transaction Monitoring ===")
    print("\nObjective: Detect money laundering patterns")
    print("Challenge: Criminals constantly evolve tactics")
    print("Solution: Learn embeddings of suspicious behavior")
    
    print("\n--- Normal Transaction Pattern ---")
    print("Customer: John Smith")
    print("Account age: 5 years")
    print("Transactions:")
    print("  - Weekly paycheck deposits: $2,500")
    print("  - Monthly rent: $1,800")
    print("  - Utilities, groceries, entertainment")
    print("  - Occasional savings transfers: $500")
    
    print("\nAssessment:")
    print("  Embedding analysis: Normal consumer banking cluster")
    print("  Risk score: 0.02 (low)")
    print("  Action: No review required")
    
    print("\n--- Structuring Pattern (Money Laundering) ---")
    print("Customer: Jane Doe")
    print("Account age: 3 months")
    print("Recent activity (last 7 days):")
    print("  - Day 1: Cash deposit $9,500 (just under $10K reporting threshold)")
    print("  - Day 2: Cash deposit $9,800")
    print("  - Day 3: Cash deposit $9,700")
    print("  - Day 4: Wire transfer out $28,000 to offshore account")
    
    print("\nEmbedding analysis:")
    print("  Similar to: Known structuring cases")
    print("  Pattern: Multiple sub-threshold deposits → large outbound transfer")
    print("  Red flags:")
    print("    - Deposits just under reporting threshold")
    print("    - Rapid sequence of cash deposits")
    print("    - Immediate outbound transfer")
    print("    - Offshore destination")
    
    print("\nAssessment:")
    print("  Risk score: 0.94 (very high)")
    print("  Matched rules: Structuring, rapid movement")
    print("  Action: File SAR (Suspicious Activity Report)")
    print("  Freeze account pending investigation")
    
    print("\n--- Round-Tripping Pattern ---")
    print("Network of accounts:")
    print("  Company A → Company B ($500K)")
    print("  Company B → Company C ($480K)")
    print("  Company C → Company D ($460K)")
    print("  Company D → Company A ($440K)")
    print("All within 48 hours, no economic purpose")
    
    print("\nGraph embedding analysis:")
    print("  Pattern: Circular money flow")
    print("  Similar to: Known round-tripping schemes")
    print("  Red flags:")
    print("    - Circular transaction graph")
    print("    - Rapid timing (no genuine business delay)")
    print("    - Decreasing amounts (fees laundering)")
    print("    - Shell companies (minimal operations)")
    
    print("\nAssessment:")
    print("  Risk score: 0.89 (high)")
    print("  Matched rules: Round-tripping, shell company usage")
    print("  Action: File SAR for all accounts")
    print("  Investigate beneficial ownership")
    
    print("\n--- System Performance ---")
    print("Transactions monitored: 10M per day")
    print("Alerts generated: 2,500 per day (0.025%)")
    print("True positives: 1,800 per day (72% precision)")
    print("False positive reduction: 85% vs rule-based")
    print("SAR filings: 600 per day")
    print("Regulatory compliance: 100%")
    print("\n→ Effective detection with manageable false positives")

# Uncomment to run:
# aml_monitoring_example()
```

:::{.callout-tip}
## Compliance Automation Best Practices

**Use cases:**
- **AML**: Structuring, smurfing, trade-based money laundering
- **Trading surveillance**: Spoofing, layering, wash trading, front-running
- **Insider trading**: Employee trading around material events
- **Privacy**: GDPR/CCPA data access, retention, deletion compliance
- **KYC**: Identity verification, sanctions screening, PEP checks

**Data sources:**
- **Transactions**: Amount, timing, parties, geography
- **Communications**: Emails, chats, recorded calls
- **Documents**: Contracts, reports, disclosures
- **External**: Sanctions lists, adverse media, PEP databases
- **Network**: Relationships between entities

**Modeling:**
- **Semantic similarity**: Violations similar to rule descriptions
- **Graph embeddings**: Network analysis for related-party transactions
- **Sequential patterns**: Time-series analysis of behaviors
- **Multi-modal**: Combine transactions + communications
- **Few-shot learning**: Detect new violation types from few examples

**Production:**
- **Real-time**: Block high-risk transactions immediately
- **Explainability**: Surface why events were flagged
- **Human review**: Route alerts to compliance analysts
- **Feedback loops**: Analysts mark true/false positives
- **Reporting**: Automated SAR generation, regulatory reporting

**Challenges:**
- **False positives**: Too many alerts overwhelm analysts
- **Evolving tactics**: Criminals adapt to detection methods
- **Data quality**: Incomplete, inconsistent transaction data
- **Privacy**: Can't retain all data indefinitely
- **Explainability**: Regulators require detailed justifications
:::

## Customer Behavior Analysis

Understanding customer behavior enables personalized products, churn prevention, and lifetime value optimization. **Embedding-based customer analysis** represents customers as vectors capturing preferences, behaviors, and lifecycle stage, enabling micro-segmentation and predictive analytics at scale.

### The Customer Analytics Challenge

Traditional customer analytics faces limitations:

- **Coarse segmentation**: Demographics (age, income) don't capture behavior
- **Static**: Segments don't adapt as customers evolve
- **Siloed**: Separate models for different products
- **Reactive**: Detect churn after customers disengage

**Embedding approach**: Learn customer embeddings from transaction history, product usage, service interactions, and life events. Similar customers cluster together; segment membership emerges naturally; behavior prediction transfers across products.

```python
"""
Customer Behavior Analysis with Embeddings

Architecture:
1. Customer encoder: Transaction history → customer embedding
2. Lifecycle model: Map embeddings to lifecycle stages
3. Propensity models: Predict churn, cross-sell, upsell
4. Personalization engine: Tailor products/offers to embeddings
5. Clustering: Discover natural customer segments

Use cases:
- Churn prediction: Identify at-risk customers
- Cross-sell: Recommend relevant products
- Personalization: Customize offerings, pricing, messaging
- Lifetime value: Predict long-term customer value
- Acquisition: Find lookalike audiences

Production considerations:
- Privacy: Comply with data protection regulations
- Real-time: Update embeddings as new transactions arrive
- Explainability: Surface why recommendations made
- Fairness: Avoid discriminatory segments
"""

@dataclass
class Customer:
    """
    Customer profile
    
    Attributes:
        customer_id: Unique identifier
        demographics: Age, location, income, etc.
        products: Currently held products
        transaction_history: Past transactions
        interactions: Service calls, branch visits, etc.
        lifecycle_stage: Acquisition, growth, mature, at_risk, churned
        embedding: Learned customer embedding
    """
    customer_id: str
    demographics: Dict[str, Any]
    products: List[str]
    transaction_history: List[Dict[str, Any]]
    interactions: List[Dict[str, Any]]
    lifecycle_stage: Optional[str] = None
    embedding: Optional[np.ndarray] = None

class CustomerEncoder(nn.Module):
    """
    Encode customers from behavioral data
    
    Architecture:
    - Transaction encoder: LSTM over transaction history
    - Product encoder: Embeddings of held products
    - Interaction encoder: Service interaction patterns
    - Demographic encoder: Basic demographic features
    - Fusion: Combine all modalities
    
    Training:
    - Churn prediction: Embedding predicts churn probability
    - Product adoption: Predict next product customer adopts
    - Contrastive: High-LTV customers close together
    """
    
    def __init__(
        self,
        embedding_dim: int = 128,
        num_products: int = 50
    ):
        super().__init__()
        self.embedding_dim = embedding_dim
        
        # Transaction encoder
        self.transaction_encoder = nn.LSTM(
            input_size=20,  # transaction features
            hidden_size=64,
            num_layers=2,
            batch_first=True,
            dropout=0.2
        )
        
        # Product embeddings
        self.product_embedding = nn.Embedding(num_products, 32)
        
        # Interaction encoder
        self.interaction_encoder = nn.Sequential(
            nn.Linear(30, 64),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(64, 64)
        )
        
        # Fusion
        self.fusion = nn.Sequential(
            nn.Linear(160, 128),  # 64 + 32 + 64
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(128, embedding_dim)
        )
        
    def forward(
        self,
        transaction_history: torch.Tensor,
        product_ids: torch.Tensor,
        interaction_features: torch.Tensor
    ) -> torch.Tensor:
        """
        Encode customers
        
        Args:
            transaction_history: Transaction history (batch_size, seq_len, 20)
            product_ids: Held product IDs (batch_size, max_products)
            interaction_features: Interaction features (batch_size, 30)
        
        Returns:
            Customer embeddings (batch_size, embedding_dim)
        """
        # Encode transaction history
        _, (transaction_hidden, _) = self.transaction_encoder(transaction_history)
        transaction_emb = transaction_hidden[-1]
        
        # Encode products (average of held products)
        product_embs = self.product_embedding(product_ids)
        product_emb = product_embs.mean(dim=1)
        
        # Encode interactions
        interaction_emb = self.interaction_encoder(interaction_features)
        
        # Fuse
        combined = torch.cat([transaction_emb, product_emb, interaction_emb], dim=1)
        customer_emb = self.fusion(combined)
        
        # Normalize
        customer_emb = F.normalize(customer_emb, p=2, dim=1)
        
        return customer_emb

# Example: Customer churn prevention
def churn_prevention_example():
    """
    Embedding-based churn prediction and prevention
    
    Demonstrates:
    1. Learning customer embeddings from behavior
    2. Identifying at-risk customers
    3. Personalized retention interventions
    """
    
    print("=== Customer Churn Prevention System ===")
    print("\nObjective: Identify and retain at-risk customers")
    print("Approach: Learn embeddings capturing lifecycle stage")
    print("         Detect drift toward churn cluster")
    
    print("\n--- Customer 1: Healthy ---")
    print("Customer ID: C001")
    print("Products: Checking, savings, credit card")
    print("Recent activity:")
    print("  - Regular direct deposits")
    print("  - Active credit card usage")
    print("  - Mobile app usage: 15x per month")
    print("  - Customer service: No recent calls")
    
    print("\nEmbedding analysis:")
    print("  Cluster: Engaged customers")
    print("  Distance from churn cluster: 0.89 (far)")
    print("  Lifecycle stage: Growth")
    
    print("\nAssessment:")
    print("  Churn probability (90 days): 3%")
    print("  Lifetime value: $8,500")
    print("  Action: No intervention needed")
    print("  Opportunity: Cross-sell mortgage")
    
    print("\n--- Customer 2: Early At-Risk Indicators ---")
    print("Customer ID: C002")
    print("Products: Checking, savings")
    print("Tenure: 3 years")
    print("Recent changes:")
    print("  - Balance declining (was $5K, now $1.2K)")
    print("  - Mobile app usage dropped (was 20x, now 5x per month)")
    print("  - No credit card usage last 30 days")
    print("  - Customer service: Called twice about fees")
    
    print("\nEmbedding analysis:")
    print("  Current cluster: Engaged customers")
    print("  Drift: Moving toward disengaged cluster")
    print("  Distance from churn cluster: 0.45 (closing)")
    print("  Similar to: Past churners 60 days before churn")
    
    print("\nAssessment:")
    print("  Churn probability (90 days): 35%")
    print("  Lifetime value at risk: $6,200")
    print("  Action: Proactive intervention")
    print("  ")
    print("  Recommended intervention:")
    print("    1. Fee waiver offer ($100 value)")
    print("    2. Personal outreach from relationship manager")
    print("    3. Survey about service issues")
    print("    4. Highlight unused benefits (free ATMs, overdraft protection)")
    print("\n→ Caught early, high retention probability")
    
    print("\n--- Customer 3: Imminent Churn ---")
    print("Customer ID: C003")
    print("Products: Checking only")
    print("Tenure: 8 months")
    print("Recent behavior:")
    print("  - Balance near zero")
    print("  - No transactions last 45 days")
    print("  - App uninstalled")
    print("  - Customer service: Canceled credit card last month")
    print("  - External signal: Opened account at competitor")
    
    print("\nEmbedding analysis:")
    print("  Current cluster: Churned/inactive customers")
    print("  Distance from churn cluster: 0.05 (inside)")
    print("  Embedding nearly identical to: Customers who churned")
    
    print("\nAssessment:")
    print("  Churn probability (90 days): 92%")
    print("  Likely already churned (inactive 45 days)")
    print("  Action: Win-back campaign")
    print("  ")
    print("  Recommended intervention:")
    print("    1. Account reactivation bonus ($200)")
    print("    2. Premium product upgrade offer")
    print("    3. Apology + service improvement message")
    print("  ")
    print("  ROI consideration: $200 bonus vs $2,500 acquisition cost")
    print("  → Win-back cheaper than new acquisition")
    
    print("\n--- System Performance ---")
    print("Customers monitored: 1.5M")
    print("At-risk identified: 45,000 (3%)")
    print("Interventions: 45,000")
    print("Retention rate: 68% (vs 40% without intervention)")
    print("Additional customers retained: 12,600")
    print("Lifetime value protected: $78M")
    print("Campaign cost: $4.5M")
    print("ROI: 17x")
    print("\n→ Proactive churn prevention highly profitable")

# Uncomment to run:
# churn_prevention_example()
```

:::{.callout-tip}
## Customer Analytics Best Practices

**Data sources:**
- **Transactions**: Frequency, amount, product usage
- **Engagement**: App usage, website visits, branch visits
- **Service**: Support calls, complaints, resolutions
- **Demographics**: Age, location, income (where allowed)
- **External**: Credit bureau data, life events

**Modeling:**
- **Sequential**: LSTM over transaction/interaction history
- **Lifecycle modeling**: Map embeddings to stages (acquisition, growth, mature, at-risk, churned)
- **Propensity models**: Predict churn, cross-sell, upsell
- **Clustering**: Discover natural segments via K-means on embeddings
- **Transfer learning**: Pre-train on all customers, fine-tune per product

**Production:**
- **Real-time updates**: Update embeddings as transactions arrive
- **Personalization**: Tailor offers, pricing, messaging to embeddings
- **Intervention triggers**: Automatic alerts for at-risk customers
- **A/B testing**: Test interventions on similar customers
- **Privacy**: Anonymize, aggregate where possible

**Challenges:**
- **Cold start**: New customers have minimal history
- **Privacy**: Regulations limit data usage
- **Fairness**: Avoid discriminatory segments/offers
- **Causal inference**: Interventions change behavior
- **Multi-product**: Customers use multiple products differently
:::

## Market Sentiment Analysis

Market sentiment—aggregate investor mood (bullish, bearish, fearful, greedy)—drives short-term price movements. **Embedding-based sentiment analysis** extracts trading signals from news, social media, earnings calls, and analyst reports by representing text as vectors and measuring semantic similarity to known sentiment patterns.

### The Sentiment Challenge

Traditional sentiment analysis faces limitations:

- **Keyword-based**: Brittle, misses context (e.g., "not good" vs "good")
- **Aspect-unaware**: Can't distinguish sentiment toward different entities in same text
- **Static**: Pre-trained sentiment models don't adapt to financial language
- **Noisy**: Social media full of spam, bots, sarcasm

**Embedding approach**: Learn embeddings of financial text fine-tuned on market outcomes. Sentiment manifests as position in embedding space (positive sentiment cluster, negative sentiment cluster). Multi-grained: overall sentiment + aspect-specific (sentiment toward specific stocks, sectors, topics).

```python
"""
Market Sentiment Analysis

Architecture:
1. Text encoder: Embed news articles, social media posts, earnings calls
2. Sentiment classifier: Map embeddings to sentiment (bullish, bearish, neutral)
3. Aspect extraction: Identify mentioned entities (stocks, sectors)
4. Signal generator: Convert sentiment to trading signals
5. Aggregator: Combine sentiment across sources

Use cases:
- News-driven trading: React to breaking news before prices move
- Earnings call analysis: Sentiment from management tone, not just numbers
- Social sentiment: Aggregate retail investor mood
- Analyst sentiment: Encode analyst reports for consensus shifts

Production considerations:
- Latency: <1s for news, <1min for social media aggregation
- Noise filtering: Remove spam, bots, duplicate content
- Entity disambiguation: "Apple" (company) vs apple (fruit)
- Temporal decay: Recent sentiment more important than old
"""

@dataclass
class SentimentSignal:
    """
    Sentiment-derived trading signal
    
    Attributes:
        ticker: Security ticker
        timestamp: When sentiment measured
        sentiment_score: Aggregated sentiment (-1 to +1)
        confidence: Signal confidence (0-1)
        source_breakdown: Sentiment by source (news, social, analyst)
        aspects: Aspect-specific sentiment (management, products, financials)
        volume: Number of mentions
        predicted_impact: Expected price impact
    """
    ticker: str
    timestamp: float
    sentiment_score: float
    confidence: float
    source_breakdown: Dict[str, float]
    aspects: Dict[str, float]
    volume: int
    predicted_impact: float

class FinancialTextEncoder(nn.Module):
    """
    Encode financial text to embeddings
    
    Fine-tuned on financial text + market outcomes:
    - News articles → future returns
    - Earnings call transcripts → post-earnings drift
    - Analyst reports → price target accuracy
    
    Learns: Positive sentiment words for finance (beat, exceed, strong)
            Negative sentiment words (miss, weak, concern)
            Hedging language (may, could, possible)
            Certainty language (will, definitely, commit)
    """
    
    def __init__(
        self,
        embedding_dim: int = 256,
        pretrained_model: str = "finbert"  # Financial BERT
    ):
        super().__init__()
        self.embedding_dim = embedding_dim
        
        # Use pre-trained financial BERT
        # In practice, load from transformers library
        self.bert_dim = 768
        
        # Projection to target dimension
        self.projection = nn.Sequential(
            nn.Linear(self.bert_dim, 512),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(512, embedding_dim)
        )
        
    def forward(self, text_embeddings: torch.Tensor) -> torch.Tensor:
        """
        Encode financial text
        
        Args:
            text_embeddings: BERT embeddings (batch_size, 768)
        
        Returns:
            Financial text embeddings (batch_size, embedding_dim)
        """
        # Project BERT embeddings
        text_emb = self.projection(text_embeddings)
        
        # Normalize
        text_emb = F.normalize(text_emb, p=2, dim=1)
        
        return text_emb

class SentimentClassifier(nn.Module):
    """
    Classify sentiment from text embeddings
    
    Outputs:
    - Sentiment score (-1 to +1): Bearish to bullish
    - Confidence (0 to 1): How confident the model is
    - Aspects: Sentiment toward different aspects (management, products, etc.)
    """
    
    def __init__(self, embedding_dim: int = 256, num_aspects: int = 5):
        super().__init__()
        
        # Overall sentiment
        self.sentiment_head = nn.Sequential(
            nn.Linear(embedding_dim, 128),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(128, 2)  # sentiment, confidence
        )
        
        # Aspect-specific sentiment
        self.aspect_head = nn.Sequential(
            nn.Linear(embedding_dim, 128),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(128, num_aspects)
        )
        
    def forward(
        self,
        text_emb: torch.Tensor
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        Classify sentiment
        
        Args:
            text_emb: Text embeddings (batch_size, embedding_dim)
        
        Returns:
            Tuple of (sentiment_score, confidence, aspect_sentiment)
        """
        # Overall sentiment
        overall = self.sentiment_head(text_emb)
        sentiment_score = torch.tanh(overall[:, 0])  # -1 to +1
        confidence = torch.sigmoid(overall[:, 1])  # 0 to 1
        
        # Aspect sentiment
        aspect_sentiment = torch.tanh(self.aspect_head(text_emb))  # -1 to +1
        
        return sentiment_score, confidence, aspect_sentiment

# Example: News-driven trading
def sentiment_trading_example():
    """
    News sentiment → trading signals
    
    Demonstrates:
    1. Processing breaking news
    2. Extracting sentiment
    3. Generating trading signals
    4. Timing and execution
    """
    
    print("=== Market Sentiment Analysis System ===")
    print("\nObjective: Extract trading signals from market sentiment")
    print("Sources: News, social media, earnings calls, analyst reports")
    
    print("\n--- Event 1: Positive Earnings Surprise ---")
    print("Stock: TECH_CO")
    print("Event: Q3 earnings report released")
    print("Time: After market close")
    print("\nNews headline: ")
    print("  'TECH_CO beats earnings expectations, raises guidance'")
    print("\nEarnings call excerpt:")
    print("  'We're extremely pleased with our performance this quarter.")
    print("   Revenue grew 25% year-over-year, driven by strong demand")
    print("   for our cloud products. We're raising full-year guidance")
    print("   and remain confident in our market position.'")
    
    print("\nSentiment analysis:")
    print("  Overall sentiment: +0.82 (very bullish)")
    print("  Confidence: 0.91")
    print("  Aspect breakdown:")
    print("    Revenue: +0.95 (very positive)")
    print("    Profitability: +0.75 (positive)")
    print("    Guidance: +0.88 (very positive)")
    print("    Management tone: +0.72 (positive, confident)")
    print("  Volume: 150 news articles, 5K social media mentions")
    
    print("\nTrading signal:")
    print("  Direction: LONG")
    print("  Predicted impact: +4.5% at open tomorrow")
    print("  Confidence: 0.88")
    print("  Timing: Buy at open, hold through day 1")
    print("  Position size: 3% of portfolio")
    print("  ")
    print("  Rationale:")
    print("    ✓ Strong earnings beat")
    print("    ✓ Guidance raise (forward-looking)")
    print("    ✓ Positive management tone")
    print("    ✓ Broad positive news coverage")
    print("\n→ High-confidence bullish signal")
    
    print("\n--- Event 2: Mixed News ---")
    print("Stock: PHARMA_CO")
    print("Event: Drug trial results announced")
    print("\nNews headline:")
    print("  'PHARMA_CO drug shows efficacy but safety concerns emerge'")
    
    print("\nSentiment analysis:")
    print("  Overall sentiment: +0.15 (slightly bullish)")
    print("  Confidence: 0.45 (uncertain)")
    print("  Aspect breakdown:")
    print("    Efficacy: +0.75 (positive)")
    print("    Safety: -0.55 (concerning)")
    print("    Regulatory: -0.30 (potential delays)")
    print("  Volume: 80 news articles")
    print("  Disagreement: High variance across sources")
    
    print("\nTrading signal:")
    print("  Direction: HOLD / WAIT")
    print("  Predicted impact: Unclear (-2% to +3%)")
    print("  Confidence: 0.42 (low)")
    print("  ")
    print("  Rationale:")
    print("    ✓ Positive efficacy data")
    print("    ✗ Safety concerns (regulatory risk)")
    print("    ! High disagreement among analysts")
    print("    ! Need more clarity before trading")
    print("\n→ No trade due to uncertainty")
    
    print("\n--- Event 3: Social Media Frenzy (Caution) ---")
    print("Stock: MEME_STOCK")
    print("Event: Viral social media attention")
    print("\nSocial media activity:")
    print("  Mentions: 50K tweets in 1 hour (sudden spike)")
    print("  Sentiment: +0.92 (extremely bullish)")
    print("  Common phrases: 'to the moon', 'diamond hands', 'shorts get squeezed'")
    
    print("\nSentiment analysis:")
    print("  Overall sentiment: +0.92 (very bullish)")
    print("  BUT: Multiple red flags")
    print("    ⚠ Bot activity detected: 35% of mentions")
    print("    ⚠ Coordinated timing: Suspicious synchronization")
    print("    ⚠ No fundamental news to justify sentiment")
    print("    ⚠ Historical pattern: Similar to past pump-and-dumps")
    
    print("\nTrading signal:")
    print("  Direction: AVOID / SHORT (cautiously)")
    print("  Rationale:")
    print("    ✗ Artificial sentiment (bots, coordination)")
    print("    ✗ No fundamental support")
    print("    ✗ High crash risk after initial spike")
    print("  ")
    print("  Risk management:")
    print("    - If shorting: Small position, tight stop-loss")
    print("    - Watch for short squeeze risk")
    print("\n→ Likely manipulation, avoid or counter-trade carefully")
    
    print("\n--- System Performance ---")
    print("News articles processed: 10K per day")
    print("Social media posts: 1M per day")
    print("Earnings calls: 50-100 per day (earnings season)")
    print("Signals generated: 150 per day")
    print("Traded signals: 30 per day (high confidence only)")
    print("Win rate: 64%")
    print("Average return per trade: 1.8%")
    print("Sharpe ratio: 2.1")
    print("\n→ Sentiment provides measurable alpha")

# Uncomment to run:
# sentiment_trading_example()
```

:::{.callout-tip}
## Sentiment Analysis Best Practices

**Data sources:**
- **News**: Financial news wires (Bloomberg, Reuters), company press releases
- **Social media**: Twitter/X, Reddit (r/wallstreetbets), StockTwits
- **Earnings calls**: Transcripts, audio recordings (tone analysis)
- **Analyst reports**: Research reports, price target changes
- **SEC filings**: 10-K, 10-Q, 8-K (MD&A section sentiment)

**Modeling:**
- **Fine-tuning**: Start with financial BERT (FinBERT), fine-tune on outcomes
- **Aspect-based**: Extract sentiment toward specific aspects (management, products, outlook)
- **Multi-source**: Combine news, social, analyst sentiment
- **Temporal**: Weight recent sentiment higher than old
- **Noise filtering**: Remove bots, spam, duplicate content

**Production:**
- **Low latency**: Process breaking news in <1 second
- **Entity disambiguation**: Resolve ticker symbols, company names
- **Aggregation**: Combine sentiment across multiple articles/posts
- **Signal generation**: Map sentiment to expected price movements
- **Backtesting**: Validate signals on historical news + returns

**Challenges:**
- **Sarcasm**: Difficult to detect ("Great, just great" = negative)
- **Context**: Same word different meanings ("Apple" company vs fruit)
- **Timing**: Sentiment impact decays quickly (minutes to hours)
- **Causality**: Does sentiment predict prices or follow prices?
- **Manipulation**: Coordinated campaigns to pump/dump stocks
:::

## Key Takeaways

- **Trading signal generation with security embeddings enables discovery of non-obvious opportunities**: Time-series embeddings (LSTM over price history) combined with fundamental and news embeddings identify securities poised for movement, while cross-sectional learning transfers patterns across similar securities in the same sector or with correlated fundamentals

- **Credit risk assessment benefits from alternative data embeddings**: Transaction patterns, rent/utility payments, and employment history embeddings enable lending to credit invisibles while maintaining or improving default rates, expanding access to credit for 15-20% of population traditionally excluded from traditional scoring

- **Regulatory compliance automation scales through semantic similarity**: Embedding regulations and transactions in the same space enables detecting violations as semantic similarity between actions and prohibited patterns, reducing false positives by 85% while maintaining 100% regulatory compliance through real-time transaction monitoring and communication surveillance

- **Customer behavior embeddings enable micro-segmentation and personalized interventions**: Sequential models (LSTM over transaction/interaction history) learn lifecycle stages, with drift toward churn clusters triggering proactive retention efforts that increase retention rates from 40% to 68%, protecting tens of millions in lifetime value

- **Market sentiment embeddings extract trading signals from unstructured text**: Fine-tuning financial BERT on news + market outcomes learns sentiment patterns predictive of price movements, while aspect-based sentiment distinguishes overall mood from sentiment toward specific business dimensions (products, management, outlook), enabling more nuanced trading signals

- **Financial embeddings require domain-specific fine-tuning**: Pre-trained models don't understand financial language nuances—"beat expectations" is positive, "guidance" is forward-looking, "covenant" has specific meaning—requiring fine-tuning on financial text paired with market outcomes to learn these patterns

- **Explainability and fairness are regulatory requirements in financial services**: SHAP values for credit decisions satisfy adverse action requirements, similar case retrieval for compliance violations provides audit trails, and continuous monitoring for demographic disparities ensures fair lending compliance (ECOA, fair lending laws)

## Looking Ahead

Part V (Industry Applications) continues with Chapter 19, which applies embeddings to healthcare and life sciences: drug discovery acceleration through molecular embeddings that predict protein-ligand binding and toxicity, medical image analysis with multi-modal embeddings combining imaging and clinical data for diagnosis, clinical trial optimization using patient embeddings to identify optimal candidates and predict outcomes, personalized treatment recommendations based on patient similarity in embedding space, and epidemic modeling using population embeddings to forecast disease spread and optimize interventions.

## Further Reading

### Trading and Market Microstructure
- Hendershott, Terrence, Charles M. Jones, and Albert J. Menkveld (2011). "Does Algorithmic Trading Improve Liquidity?" Journal of Finance.
- Brogaard, Jonathan, Terrence Hendershott, and Ryan Riordan (2014). "High-Frequency Trading and Price Discovery." Review of Financial Studies.
- Cont, Rama (2001). "Empirical Properties of Asset Returns: Stylized Facts and Statistical Issues." Quantitative Finance.
- Cartea, Álvaro, Sebastian Jaimungal, and José Penalva (2015). "Algorithmic and High-Frequency Trading." Cambridge University Press.

### Credit Risk and Alternative Data
- Fuster, Andreas, et al. (2019). "Predictably Unequal? The Effects of Machine Learning on Credit Markets." Journal of Finance.
- Khandani, Amir E., Adlar J. Kim, and Andrew W. Lo (2010). "Consumer Credit-Risk Models via Machine-Learning Algorithms." Journal of Banking & Finance.
- Blattner, Laura, and Scott Nelson (2021). "How Costly is Noise? Data and Disparities in Consumer Credit." Working Paper.
- Berg, Tobias, et al. (2020). "On the Rise of FinTechs: Credit Scoring Using Digital Footprints." Review of Financial Studies.

### Regulatory Compliance and AML
- Colladon, Andrea Fronzetti, and Elisa Rampone (2017). "Using Social Network Analysis to Prevent Money Laundering." Expert Systems with Applications.
- Weber, Mark, et al. (2019). "Anti-Money Laundering in Bitcoin: Experimenting with Graph Convolutional Networks for Financial Forensics." KDD Workshop.
- Jullum, Martin, et al. (2020). "Detecting Money Laundering Transactions with Machine Learning." Journal of Money Laundering Control.
- Savage, David, et al. (2016). "Detection of Money Laundering Groups Using Supervised Learning in Networks." AAAI Workshop.

### Customer Analytics and Churn
- Neslin, Scott A., et al. (2006). "Defection Detection: Measuring and Understanding the Predictive Accuracy of Customer Churn Models." Journal of Marketing Research.
- Verbeke, Wouter, et al. (2012). "New Insights into Churn Prediction in the Telecommunications Sector: A Profit Driven Data Mining Approach." European Journal of Operational Research.
- Risselada, Hans, Peter C. Verhoef, and Tammo H.A. Bijmolt (2010). "Staying Power of Churn Prediction Models." Journal of Interactive Marketing.
- Ascarza, Eva (2018). "Retention Futility: Targeting High-Risk Customers Might Be Ineffective." Journal of Marketing Research.

### Sentiment Analysis and NLP for Finance
- Loughran, Tim, and Bill McDonald (2011). "When Is a Liability Not a Liability? Textual Analysis, Dictionaries, and 10-Ks." Journal of Finance.
- Tetlock, Paul C. (2007). "Giving Content to Investor Sentiment: The Role of Media in the Stock Market." Journal of Finance.
- Garcia, Diego (2013). "Sentiment during Recessions." Journal of Finance.
- Araci, Dogu (2019). "FinBERT: Financial Sentiment Analysis with Pre-trained Language Models." arXiv:1908.10063.

### Multi-modal Learning for Finance
- Chen, Tianqi, and Carlos Guestrin (2016). "XGBoost: A Scalable Tree Boosting System." KDD.
- Ke, Guolin, et al. (2017). "LightGBM: A Highly Efficient Gradient Boosting Decision Tree." NeurIPS.
- Ding, Xiao, et al. (2015). "Deep Learning for Event-Driven Stock Prediction." IJCAI.
- Xu, Yumo, and Shay B. Cohen (2018). "Stock Movement Prediction from Tweets and Historical Prices." ACL.

### Fairness and Explainability in Finance
- Hardt, Moritz, Eric Price, and Nati Srebro (2016). "Equality of Opportunity in Supervised Learning." NeurIPS.
- Lundberg, Scott M., and Su-In Lee (2017). "A Unified Approach to Interpreting Model Predictions." NeurIPS.
- Barocas, Solon, and Andrew D. Selbst (2016). "Big Data's Disparate Impact." California Law Review.
- Dwork, Cynthia, et al. (2012). "Fairness Through Awareness." ITCS.
