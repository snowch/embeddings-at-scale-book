# Security and Privacy {#sec-security-privacy}

:::{.callout-note}
## Chapter Overview
Security and privacy—from protecting sensitive embeddings to enabling privacy-preserving queries to ensuring regulatory compliance—determine whether embedding systems can operate on confidential data while maintaining user trust and legal compliance. This chapter covers security and privacy fundamentals: embedding encryption and secure computation protecting sensitive vectors through homomorphic encryption and secure multi-party computation that enable encrypted similarity search with <10% overhead, privacy-preserving similarity search using locality-sensitive hashing and differential privacy that prevent query vector and database content leakage while maintaining 90%+ utility, differential privacy for embeddings providing formal privacy guarantees through controlled noise injection that bound information leakage to ε≤1.0 while preserving semantic relationships, access control and audit trails implementing fine-grained permissions and comprehensive logging that ensure only authorized queries access sensitive embeddings, and GDPR and data sovereignty compliance through data residency controls, right-to-deletion workflows, and audit capabilities that satisfy regulatory requirements across jurisdictions. These techniques transform embedding systems from security-problematic prototypes to enterprise-grade platforms that protect confidential data, preserve user privacy, and satisfy regulatory mandates—enabling deployment on healthcare records, financial transactions, and personal data while maintaining 80-95% of unencrypted system performance.
:::

After optimizing performance (@sec-performance-optimization), **security and privacy become paramount for production deployment**. Embedding systems process sensitive data—customer behavior, proprietary documents, medical records, financial transactions—and generate vectors that encode private information. Traditional database security (encryption at rest, access control, audit logs) protects storage but fails during computation: similarity search requires accessing unencrypted embeddings, query vectors reveal search intent, and nearest neighbors leak database content. **Security-aware embedding systems** use cryptographic techniques (homomorphic encryption, secure enclaves, differential privacy) to protect data during computation, privacy-preserving algorithms (LSH with noise, federated learning) to prevent information leakage, and comprehensive access controls with auditing to ensure compliance—enabling deployment on confidential data while maintaining 80-95% of unencrypted performance and satisfying GDPR, HIPAA, SOC2, and other regulatory frameworks.

## Embedding Encryption and Secure Computation

Embeddings encode semantic information from source data—a customer behavior embedding reveals purchasing patterns, a document embedding exposes content themes, a medical record embedding encodes diagnosis information. **Encryption and secure computation** protect embeddings throughout their lifecycle while enabling similarity search, achieving cryptographic security guarantees (IND-CPA, semantic security) with practical performance (<10× overhead for most operations) through homomorphic encryption (compute on encrypted vectors), secure enclaves (trusted execution environments), and secure multi-party computation (distributed computation without revealing inputs).

### The Embedding Security Challenge

Embedding systems face unique security requirements:

- **At-rest encryption**: Embeddings stored encrypted, but traditional encryption prevents similarity search
- **In-transit protection**: Query vectors and results transmitted securely without revealing content
- **Computation security**: Similarity search on encrypted vectors without decryption
- **Query privacy**: Search queries don't reveal query content to database operator
- **Result privacy**: Returned neighbors don't leak database content beyond necessary
- **Performance requirements**: <10× overhead for encrypted operations, <100ms query latency
- **Key management**: Secure key distribution, rotation, revocation at scale
- **Multi-tenant isolation**: Prevent cross-tenant data leakage in shared systems

**Security approach**: Layer multiple techniques—encryption at rest protects stored vectors (AES-256), encryption in transit protects network communication (TLS 1.3), homomorphic encryption or secure enclaves enable computation on encrypted data, differential privacy bounds information leakage from query results, and access controls with audit trails ensure only authorized queries proceed.

```python
"""
Secure Embedding Computation with Homomorphic Encryption

Architecture:
1. Encryption: Encrypt embeddings with homomorphic encryption scheme
2. Secure indexing: Build encrypted index supporting similarity search
3. Query encryption: Encrypt query vectors with same scheme
4. Encrypted search: Compute encrypted similarity scores
5. Secure decryption: Return encrypted results for client-side decryption

Techniques:
- Homomorphic encryption: CKKS for approximate operations on floats
- Secure enclaves: Intel SGX, AMD SEV for trusted execution
- Functional encryption: Allow specific computations on encrypted data
- Secure multi-party computation: Distribute trust across parties
- Order-preserving encryption: Enable range queries on encrypted scalars

Security properties:
- IND-CPA security: Ciphertext reveals no information about plaintext
- Query privacy: Server learns nothing about query vector
- Result privacy: Client learns only k nearest neighbors, nothing else
- Collusion resistance: Multiple parties cannot learn individual contributions

Performance targets:
- Encryption: <10ms per 768-d vector
- Encrypted similarity: <50ms for 1M encrypted vectors
- Overhead: <10× vs plaintext for CKKS, <3× for SGX
"""

import numpy as np
from typing import List, Dict, Optional, Tuple, Any
from dataclasses import dataclass, field
from datetime import datetime
import hashlib
import hmac
import secrets
from abc import ABC, abstractmethod

@dataclass
class EncryptionConfig:
    """
    Configuration for embedding encryption
    
    Attributes:
        scheme: Encryption scheme (ckks, sgx, ope, hybrid)
        key_size: Key size in bits
        security_level: Security parameter (128, 192, 256)
        noise_budget: Noise budget for homomorphic operations
        precision: Precision bits for CKKS encoding
        enable_packing: Pack multiple vectors in single ciphertext
        enable_batching: Batch operations for efficiency
    """
    scheme: str = "ckks"  # "ckks", "sgx", "ope", "hybrid"
    key_size: int = 2048
    security_level: int = 128
    noise_budget: int = 100
    precision: int = 40
    enable_packing: bool = True
    enable_batching: bool = True

@dataclass
class EncryptedEmbedding:
    """
    Encrypted embedding representation
    
    Attributes:
        id: Embedding identifier
        ciphertext: Encrypted vector data
        public_key_id: Public key used for encryption
        encryption_metadata: Additional encryption info
        dimension: Original embedding dimension
        encrypted_at: Encryption timestamp
    """
    id: str
    ciphertext: bytes
    public_key_id: str
    encryption_metadata: Dict[str, Any] = field(default_factory=dict)
    dimension: int = 768
    encrypted_at: datetime = field(default_factory=datetime.now)

class SecureEmbeddingStore(ABC):
    """
    Abstract interface for secure embedding storage and computation
    """
    
    @abstractmethod
    def encrypt_embedding(
        self,
        embedding: np.ndarray,
        embedding_id: str
    ) -> EncryptedEmbedding:
        """Encrypt an embedding vector"""
        pass
    
    @abstractmethod
    def decrypt_embedding(
        self,
        encrypted: EncryptedEmbedding
    ) -> np.ndarray:
        """Decrypt an embedding vector"""
        pass
    
    @abstractmethod
    def encrypted_similarity(
        self,
        query_encrypted: EncryptedEmbedding,
        candidate_encrypted: EncryptedEmbedding
    ) -> float:
        """Compute similarity between encrypted embeddings"""
        pass
    
    @abstractmethod
    def secure_nearest_neighbors(
        self,
        query_encrypted: EncryptedEmbedding,
        k: int = 10
    ) -> List[Tuple[str, float]]:
        """Find k nearest neighbors in encrypted space"""
        pass

class CKKSEmbeddingStore(SecureEmbeddingStore):
    """
    CKKS homomorphic encryption for embeddings
    
    CKKS (Cheon-Kim-Kim-Song) enables approximate arithmetic on
    encrypted floating-point numbers, suitable for embedding similarity.
    
    Operations:
    - Addition: Add encrypted vectors (for aggregation)
    - Multiplication: Multiply encrypted vectors (for dot product)
    - Rotation: Rotate slots (for vector operations)
    
    Performance:
    - Encryption: O(d log d) for d-dimensional vector
    - Addition: O(d) on encrypted vectors
    - Multiplication: O(d^2) but can be optimized
    - Decryption: O(d log d)
    """
    
    def __init__(self, config: EncryptionConfig):
        self.config = config
        self.public_key = None
        self.secret_key = None
        self.encrypted_store: Dict[str, EncryptedEmbedding] = {}
        
        # Initialize CKKS parameters
        self._initialize_ckks()
    
    def _initialize_ckks(self):
        """
        Initialize CKKS encryption scheme
        
        In production, use libraries like:
        - Microsoft SEAL (C++, Python bindings)
        - OpenFHE (C++, Python bindings)
        - HElib (C++)
        - TenSEAL (Python)
        """
        # Simplified initialization for demonstration
        # In production, use proper CKKS library
        
        # Generate key pair (simplified)
        self.secret_key = secrets.token_bytes(self.config.key_size // 8)
        self.public_key = hashlib.sha256(self.secret_key).hexdigest()
        
        print(f"CKKS initialized: security={self.config.security_level} bits")
        print(f"  Key size: {self.config.key_size} bits")
        print(f"  Precision: {self.config.precision} bits")
        print(f"  Noise budget: {self.config.noise_budget}")
    
    def encrypt_embedding(
        self,
        embedding: np.ndarray,
        embedding_id: str
    ) -> EncryptedEmbedding:
        """
        Encrypt embedding with CKKS
        
        Steps:
        1. Encode: Convert float vector to CKKS plaintext
        2. Encrypt: Apply CKKS encryption with public key
        3. Pack: Optionally pack multiple vectors in single ciphertext
        
        Args:
            embedding: Float vector to encrypt
            embedding_id: Unique identifier
            
        Returns:
            Encrypted embedding
        """
        # In production, use CKKS library:
        # from tenseal import CKKS
        # context = ts.context(ts.SCHEME_TYPE.CKKS, ...)
        # encrypted = ts.ckks_vector(context, embedding)
        # ciphertext = encrypted.serialize()
        
        # Simplified encryption for demonstration
        embedding_bytes = embedding.tobytes()
        
        # Simulated CKKS encryption (use proper library in production)
        cipher = hmac.new(
            self.secret_key,
            embedding_bytes,
            hashlib.sha256
        ).digest()
        
        # Add noise based on noise budget
        noise = np.random.randint(
            0, 2**self.config.noise_budget,
            size=len(cipher)
        ).tobytes()
        ciphertext = bytes(a ^ b for a, b in zip(cipher, noise[:len(cipher)]))
        
        encrypted = EncryptedEmbedding(
            id=embedding_id,
            ciphertext=ciphertext,
            public_key_id=self.public_key,
            dimension=len(embedding),
            encryption_metadata={
                "scheme": "ckks",
                "precision": self.config.precision,
                "noise_budget": self.config.noise_budget
            }
        )
        
        self.encrypted_store[embedding_id] = encrypted
        return encrypted
    
    def decrypt_embedding(
        self,
        encrypted: EncryptedEmbedding
    ) -> np.ndarray:
        """
        Decrypt CKKS-encrypted embedding
        
        Args:
            encrypted: Encrypted embedding
            
        Returns:
            Decrypted float vector
        """
        # In production:
        # encrypted_vec = ts.ckks_vector_from(context, encrypted.ciphertext)
        # decrypted = encrypted_vec.decrypt()
        
        # Simplified decryption (demonstration only)
        # Real CKKS decryption requires secret key and proper scheme
        raise NotImplementedError(
            "Decryption requires proper CKKS library (TenSEAL, SEAL, etc.)"
        )
    
    def encrypted_similarity(
        self,
        query_encrypted: EncryptedEmbedding,
        candidate_encrypted: EncryptedEmbedding
    ) -> float:
        """
        Compute cosine similarity between encrypted vectors
        
        CKKS enables:
        1. Encrypted dot product: <x, y>_encrypted
        2. Encrypted norms: ||x||_encrypted, ||y||_encrypted
        3. Similarity: <x,y> / (||x|| * ||y||)
        
        Challenges:
        - Division not directly supported, use approximation
        - Noise accumulation from multiple operations
        - Precision loss from encoding
        
        Args:
            query_encrypted: Encrypted query vector
            candidate_encrypted: Encrypted candidate vector
            
        Returns:
            Approximate similarity score
        """
        # In production using TenSEAL:
        # dot_product = query_encrypted * candidate_encrypted
        # query_norm = query_encrypted.dot(query_encrypted).sqrt()
        # candidate_norm = candidate_encrypted.dot(candidate_encrypted).sqrt()
        # similarity = dot_product / (query_norm * candidate_norm)
        
        # Simplified similarity computation
        # Real implementation requires CKKS operations
        raise NotImplementedError(
            "Encrypted similarity requires proper CKKS library with "
            "dot product, multiplication, and approximate division"
        )
    
    def secure_nearest_neighbors(
        self,
        query_encrypted: EncryptedEmbedding,
        k: int = 10
    ) -> List[Tuple[str, float]]:
        """
        Find k nearest neighbors in encrypted space
        
        Approach:
        1. Compute encrypted similarities with all candidates
        2. Use oblivious selection to find top-k
        3. Return encrypted result for client-side decryption
        
        Or use secure index structures:
        - Encrypted LSH: Hash encrypted vectors
        - Encrypted tree: Navigate tree in encrypted space
        - Secure MPC: Distribute computation across parties
        
        Args:
            query_encrypted: Encrypted query
            k: Number of neighbors
            
        Returns:
            List of (id, encrypted_score) for top-k neighbors
        """
        # Compute encrypted similarities
        similarities = []
        for emb_id, candidate in self.encrypted_store.items():
            try:
                sim = self.encrypted_similarity(query_encrypted, candidate)
                similarities.append((emb_id, sim))
            except NotImplementedError:
                # Fallback to approximate method
                pass
        
        # Sort by similarity (obliviously in production)
        similarities.sort(key=lambda x: x[1], reverse=True)
        
        return similarities[:k]

class SGXEmbeddingStore(SecureEmbeddingStore):
    """
    Intel SGX (Software Guard Extensions) for secure embedding computation
    
    SGX creates trusted execution environments (enclaves) where code
    and data are protected from the operating system and other processes.
    
    Benefits:
    - Low overhead: 2-5× vs unencrypted (vs 10-100× for FHE)
    - Full computation: Can perform any operation inside enclave
    - Hardware-backed: Security guaranteed by CPU
    
    Limitations:
    - Limited enclave memory (128MB in SGX1, GBs in SGX2)
    - Platform-specific: Intel CPUs only
    - Side-channel vulnerabilities: Spectre, etc.
    
    Use cases:
    - Encrypted database: Store embeddings outside enclave, compute inside
    - Query privacy: Process queries without revealing to database operator
    - Multi-tenant isolation: Separate tenant data in different enclaves
    """
    
    def __init__(self, config: EncryptionConfig):
        self.config = config
        self.enclave_initialized = False
        self.encrypted_store: Dict[str, EncryptedEmbedding] = {}
        
        self._initialize_sgx()
    
    def _initialize_sgx(self):
        """
        Initialize SGX enclave
        
        Steps:
        1. Load enclave code (signed binary)
        2. Attestation: Verify enclave is genuine Intel SGX
        3. Establish secure channel for data transfer
        4. Load secret keys into enclave
        
        In production, use:
        - Intel SGX SDK
        - Gramine (formerly Graphene) for running Python in SGX
        - Microsoft Azure Confidential Computing
        - Google Cloud Confidential VMs
        """
        # Check if SGX is available
        sgx_available = self._check_sgx_support()
        
        if sgx_available:
            print("SGX enclave initialized")
            print(f"  Security level: {self.config.security_level} bits")
            print(f"  Enclave memory: Limited by hardware")
            self.enclave_initialized = True
        else:
            print("Warning: SGX not available, using simulation mode")
            print("  SGX provides hardware-backed security in production")
    
    def _check_sgx_support(self) -> bool:
        """Check if SGX is supported on current platform"""
        # In production: check CPUID for SGX support
        # import cpuinfo
        # info = cpuinfo.get_cpu_info()
        # return 'sgx' in info.get('flags', [])
        return False  # Simulation mode for demonstration
    
    def encrypt_embedding(
        self,
        embedding: np.ndarray,
        embedding_id: str
    ) -> EncryptedEmbedding:
        """
        Seal embedding for SGX enclave
        
        Data sealed with enclave's key can only be unsealed
        by the same enclave on the same platform.
        
        Args:
            embedding: Vector to seal
            embedding_id: Unique identifier
            
        Returns:
            Sealed embedding
        """
        # In production with SGX SDK:
        # sealed_data = sgx_seal_data(embedding.tobytes())
        
        # Simplified sealing for demonstration
        embedding_bytes = embedding.tobytes()
        sealed = hmac.new(
            b"sgx_sealing_key",  # In production: enclave-specific key
            embedding_bytes,
            hashlib.sha256
        ).digest()
        
        encrypted = EncryptedEmbedding(
            id=embedding_id,
            ciphertext=sealed + embedding_bytes,  # Seal + data
            public_key_id="sgx_enclave",
            dimension=len(embedding),
            encryption_metadata={
                "scheme": "sgx",
                "platform": "simulated"
            }
        )
        
        self.encrypted_store[embedding_id] = encrypted
        return encrypted
    
    def decrypt_embedding(
        self,
        encrypted: EncryptedEmbedding
    ) -> np.ndarray:
        """
        Unseal embedding within SGX enclave
        
        Args:
            encrypted: Sealed embedding
            
        Returns:
            Unsealed vector (only accessible within enclave)
        """
        # In production: sgx_unseal_data(encrypted.ciphertext)
        
        # Extract sealed data
        seal_size = 32  # SHA256 size
        embedding_bytes = encrypted.ciphertext[seal_size:]
        
        # Reconstruct vector
        embedding = np.frombuffer(embedding_bytes, dtype=np.float32)
        return embedding[:encrypted.dimension]
    
    def encrypted_similarity(
        self,
        query_encrypted: EncryptedEmbedding,
        candidate_encrypted: EncryptedEmbedding
    ) -> float:
        """
        Compute similarity within SGX enclave
        
        Steps:
        1. Unseal both vectors inside enclave
        2. Compute cosine similarity in enclave
        3. Return only result (vectors stay in enclave)
        
        Args:
            query_encrypted: Sealed query
            candidate_encrypted: Sealed candidate
            
        Returns:
            Similarity score
        """
        # Unseal within enclave
        query = self.decrypt_embedding(query_encrypted)
        candidate = self.decrypt_embedding(candidate_encrypted)
        
        # Compute similarity (protected by enclave)
        similarity = np.dot(query, candidate) / (
            np.linalg.norm(query) * np.linalg.norm(candidate)
        )
        
        return float(similarity)
    
    def secure_nearest_neighbors(
        self,
        query_encrypted: EncryptedEmbedding,
        k: int = 10
    ) -> List[Tuple[str, float]]:
        """
        Find k nearest neighbors within SGX enclave
        
        Entire search happens in enclave:
        1. Unseal query
        2. Unseal candidates (batch by batch if memory limited)
        3. Compute similarities
        4. Return top-k (scores can be unsealed for client)
        
        Args:
            query_encrypted: Sealed query
            k: Number of neighbors
            
        Returns:
            Top-k neighbors with scores
        """
        query = self.decrypt_embedding(query_encrypted)
        
        similarities = []
        for emb_id, candidate_enc in self.encrypted_store.items():
            candidate = self.decrypt_embedding(candidate_enc)
            
            sim = np.dot(query, candidate) / (
                np.linalg.norm(query) * np.linalg.norm(candidate)
            )
            similarities.append((emb_id, float(sim)))
        
        # Sort and return top-k
        similarities.sort(key=lambda x: x[1], reverse=True)
        return similarities[:k]

class HybridSecureStore(SecureEmbeddingStore):
    """
    Hybrid approach combining multiple security techniques
    
    Strategy:
    1. SGX for high-performance similarity computation
    2. CKKS for data at rest and cross-platform transfer
    3. Secure MPC for distributed queries across data silos
    4. Differential privacy for public result release
    
    Benefits:
    - Performance: Use SGX when available, CKKS otherwise
    - Portability: CKKS works on any platform
    - Privacy: DP bounds information leakage
    - Flexibility: Adapt to deployment constraints
    """
    
    def __init__(self, config: EncryptionConfig):
        self.config = config
        
        # Initialize both backends
        self.sgx_store = SGXEmbeddingStore(config)
        self.ckks_store = CKKSEmbeddingStore(config)
        
        # Choose primary based on availability
        self.primary_store = (
            self.sgx_store if self.sgx_store.enclave_initialized
            else self.ckks_store
        )
        
        print(f"Hybrid store initialized, primary: {type(self.primary_store).__name__}")
    
    def encrypt_embedding(
        self,
        embedding: np.ndarray,
        embedding_id: str
    ) -> EncryptedEmbedding:
        """Use primary store for encryption"""
        return self.primary_store.encrypt_embedding(embedding, embedding_id)
    
    def decrypt_embedding(
        self,
        encrypted: EncryptedEmbedding
    ) -> np.ndarray:
        """Route to appropriate store based on encryption scheme"""
        scheme = encrypted.encryption_metadata.get("scheme")
        if scheme == "sgx":
            return self.sgx_store.decrypt_embedding(encrypted)
        elif scheme == "ckks":
            return self.ckks_store.decrypt_embedding(encrypted)
        else:
            return self.primary_store.decrypt_embedding(encrypted)
    
    def encrypted_similarity(
        self,
        query_encrypted: EncryptedEmbedding,
        candidate_encrypted: EncryptedEmbedding
    ) -> float:
        """Use SGX if available for best performance"""
        if self.sgx_store.enclave_initialized:
            return self.sgx_store.encrypted_similarity(
                query_encrypted, candidate_encrypted
            )
        else:
            return self.ckks_store.encrypted_similarity(
                query_encrypted, candidate_encrypted
            )
    
    def secure_nearest_neighbors(
        self,
        query_encrypted: EncryptedEmbedding,
        k: int = 10
    ) -> List[Tuple[str, float]]:
        """Use primary store for search"""
        return self.primary_store.secure_nearest_neighbors(query_encrypted, k)

# Example usage
def secure_embedding_example():
    """
    Demonstrate secure embedding encryption and computation
    """
    print("=== Secure Embedding Computation ===")
    print()
    
    # Configuration
    config = EncryptionConfig(
        scheme="sgx",  # Use SGX for demonstration
        security_level=128,
        precision=40
    )
    
    # Initialize secure store
    store = HybridSecureStore(config)
    print()
    
    # Generate sample embeddings
    print("Encrypting embeddings...")
    np.random.seed(42)
    embeddings = {
        f"emb_{i}": np.random.randn(768).astype(np.float32)
        for i in range(100)
    }
    
    # Normalize embeddings
    for emb_id in embeddings:
        embeddings[emb_id] /= np.linalg.norm(embeddings[emb_id])
    
    # Encrypt all embeddings
    encrypted = {}
    for emb_id, emb in embeddings.items():
        encrypted[emb_id] = store.encrypt_embedding(emb, emb_id)
    
    print(f"Encrypted {len(encrypted)} embeddings")
    print(f"  Original size: {embeddings['emb_0'].nbytes} bytes/vector")
    print(f"  Encrypted size: {len(encrypted['emb_0'].ciphertext)} bytes/vector")
    print()
    
    # Encrypt query
    query = np.random.randn(768).astype(np.float32)
    query /= np.linalg.norm(query)
    query_encrypted = store.encrypt_embedding(query, "query")
    
    print("Performing secure nearest neighbor search...")
    results = store.secure_nearest_neighbors(query_encrypted, k=10)
    
    print(f"\nTop 10 neighbors:")
    for rank, (emb_id, score) in enumerate(results, 1):
        print(f"  {rank}. {emb_id}: {score:.4f}")
    
    # Compare with plaintext search
    print("\nValidation (plaintext search):")
    plaintext_scores = []
    for emb_id, emb in embeddings.items():
        score = np.dot(query, emb)
        plaintext_scores.append((emb_id, score))
    plaintext_scores.sort(key=lambda x: x[1], reverse=True)
    
    print("Top 10 neighbors (plaintext):")
    for rank, (emb_id, score) in enumerate(plaintext_scores[:10], 1):
        print(f"  {rank}. {emb_id}: {score:.4f}")

if __name__ == "__main__":
    secure_embedding_example()
```

:::{.callout-warning}
## Production Considerations

**Homomorphic Encryption**:
- Use established libraries: TenSEAL, Microsoft SEAL, OpenFHE
- CKKS provides approximate arithmetic, suitable for embeddings
- Performance: 10-100× slower than plaintext operations
- Memory: Ciphertexts are 10-50× larger than plaintexts
- Noise management: Track noise budget, refresh when needed

**Intel SGX**:
- Limited enclave memory: 128MB (SGX1) to 256GB (SGX2+)
- Page faults expensive: Load data in batches
- Side channels: Use constant-time operations, ORAM
- Attestation: Verify enclave authenticity before sending data
- Cloud availability: Azure DCv2/DCv3, GCP N2D

**Key Management**:
- Separate key hierarchy: Master → encryption → sealing keys
- Rotation: Support key rotation without re-encrypting all data
- Multi-tenant: Per-tenant keys for isolation
- Recovery: Secure key backup and recovery procedures

**Performance Optimization**:
- Batching: Encrypt/process multiple vectors at once
- Caching: Cache decrypted vectors (if security model allows)
- Hybrid: Use SGX for hot paths, CKKS for cold storage
- Hardware acceleration: Use GPUs for CKKS operations
:::

:::{.callout-tip}
## Security-Performance Trade-offs

**High Security, Lower Performance** (10-100× overhead):
- Full homomorphic encryption (FHE)
- All operations on encrypted data
- Use: Healthcare, financial regulatory data
- Cost: $10-50 per million queries

**Balanced** (2-5× overhead):
- Intel SGX or AMD SEV
- Computation in secure enclave
- Use: Enterprise multi-tenant systems
- Cost: $1-5 per million queries

**Privacy-Utility Trade-off** (<2× overhead):
- Differential privacy with plaintext computation
- Add calibrated noise to queries/results
- Use: Public-facing services, analytics
- Cost: $0.10-1 per million queries

Choose based on:
- Threat model: Who are you protecting against?
- Regulatory requirements: HIPAA, PCI-DSS, GDPR?
- Performance budget: What latency/throughput needed?
- Infrastructure: SGX availability, key management capability?
:::
## Privacy-Preserving Similarity Search

Similarity search reveals information—query vectors expose search intent, returned neighbors leak database content, access patterns reveal correlations. **Privacy-preserving similarity search** enables queries without revealing query content to the database operator or database content to the querier beyond the k results, using locality-sensitive hashing with noise injection, secure multi-party computation, and differential privacy to balance utility (95%+ recall) with formal privacy guarantees (ε≤1.0 differential privacy, query unlinkability, result indistinguishability).

### The Privacy-Leakage Challenge

Standard similarity search leaks information:

- **Query leakage**: Database sees query vector, learns user intent
- **Result leakage**: User sees neighbors, learns about database content
- **Access pattern leakage**: Repeated queries reveal correlations
- **Membership leakage**: Can determine if specific embedding in database
- **Model inversion**: Reconstruct training data from embeddings
- **Attribute inference**: Infer sensitive attributes from similar embeddings
- **Linkage attacks**: Connect embeddings across databases

**Privacy approach**: Private information retrieval (PIR) enables queries without revealing query to server, oblivious RAM (ORAM) hides access patterns, secure multi-party computation distributes trust, differential privacy adds calibrated noise to bound information leakage, and federated learning keeps data distributed.

```python
"""
Privacy-Preserving Similarity Search

Architecture:
1. Query protection: Encrypt/obfuscate query vector
2. Private retrieval: LSH with differential privacy
3. Secure computation: MPC for distance computation
4. Result obfuscation: Add dummy results, noise
5. Access pattern hiding: ORAM for index access

Techniques:
- Differentially private LSH: Add noise to hash functions
- Secure k-NN: MPC-based nearest neighbor search
- Private information retrieval: Query without revealing query
- Oblivious RAM: Hide access patterns
- Federated similarity: Distributed search across silos

Privacy guarantees:
- ε-differential privacy: Queries leak at most ε information
- Query unlinkability: Cannot link queries to same user
- Result indistinguishability: Cannot infer database beyond k results
- Computational security: Based on cryptographic hardness assumptions

Performance targets:
- Latency: <200ms for private search (2-4× overhead)
- Throughput: 1,000+ private queries/second
- Recall: >90% despite privacy noise
- Privacy budget: ε ≤ 1.0 for typical applications
"""

import numpy as np
from typing import List, Dict, Optional, Tuple, Set, Any
from dataclasses import dataclass, field
from datetime import datetime
import hashlib
from collections import defaultdict
import math

@dataclass
class PrivacyConfig:
    """
    Privacy configuration for similarity search
    
    Attributes:
        epsilon: Differential privacy parameter (smaller = more private)
        delta: Failure probability for (ε,δ)-DP
        sensitivity: Query sensitivity (for noise calibration)
        noise_mechanism: Noise distribution (laplace, gaussian)
        enable_query_obfuscation: Add dummy queries
        enable_result_obfuscation: Add dummy results  
        enable_access_hiding: Use ORAM for access pattern hiding
        privacy_budget_tracking: Track cumulative privacy loss
    """
    epsilon: float = 1.0  # Typical: 0.1-10.0
    delta: float = 1e-5   # Typical: 1e-5 to 1e-7
    sensitivity: float = 1.0
    noise_mechanism: str = "laplace"  # "laplace", "gaussian"
    enable_query_obfuscation: bool = True
    enable_result_obfuscation: bool = True
    enable_access_hiding: bool = False
    privacy_budget_tracking: bool = True

@dataclass
class PrivateQuery:
    """
    Privacy-preserving query representation
    
    Attributes:
        query_id: Unique identifier
        obfuscated_vector: Query with added noise/dummies
        original_norm: Original query norm (for normalization)
        privacy_budget_used: ε used for this query
        timestamp: Query timestamp
        auxiliary_data: Additional protected information
    """
    query_id: str
    obfuscated_vector: np.ndarray
    original_norm: float
    privacy_budget_used: float
    timestamp: datetime = field(default_factory=datetime.now)
    auxiliary_data: Dict[str, Any] = field(default_factory=dict)

@dataclass
class PrivateResult:
    """
    Privacy-preserving search result
    
    Attributes:
        query_id: Query identifier
        results: List of (id, noisy_score) tuples
        dummy_results: Dummy results for obfuscation
        privacy_guarantee: Achieved privacy level
        utility_metric: Result quality (recall, precision)
    """
    query_id: str
    results: List[Tuple[str, float]]
    dummy_results: List[Tuple[str, float]] = field(default_factory=list)
    privacy_guarantee: float = 1.0  # ε value
    utility_metric: Dict[str, float] = field(default_factory=dict)

class DifferentiallyPrivateLSH:
    """
    Locality-Sensitive Hashing with Differential Privacy
    
    Standard LSH leaks information through hash bucket membership.
    DP-LSH adds calibrated noise to protect privacy while maintaining
    approximate nearest neighbor guarantees.
    
    Approach:
    1. Generate LSH hash functions
    2. Add Laplace noise to hash values
    3. Query noisy hash buckets
    4. Post-process results with additional noise
    
    Privacy guarantee:
    - Each query satisfies ε-differential privacy
    - Privacy budget tracks cumulative usage
    - Composition theorems bound total leakage
    """
    
    def __init__(
        self,
        dimension: int,
        num_hashes: int,
        num_tables: int,
        privacy_config: PrivacyConfig
    ):
        self.dimension = dimension
        self.num_hashes = num_hashes
        self.num_tables = num_tables
        self.privacy_config = privacy_config
        
        # Generate LSH hash functions
        self.hash_functions = self._generate_hash_functions()
        
        # Hash tables: table_id -> bucket_hash -> [embedding_ids]
        self.hash_tables: Dict[int, Dict[int, Set[str]]] = defaultdict(
            lambda: defaultdict(set)
        )
        
        # Privacy budget tracking
        self.privacy_budget_used = 0.0
        self.query_count = 0
        
        print(f"Differentially Private LSH initialized:")
        print(f"  Hash functions: {num_hashes} × {num_tables} tables")
        print(f"  Privacy: ε={privacy_config.epsilon}, δ={privacy_config.delta}")
    
    def _generate_hash_functions(self) -> List[List[np.ndarray]]:
        """
        Generate random projection hash functions
        
        Returns:
            List of hash function matrices for each table
        """
        np.random.seed(42)
        hash_functions = []
        
        for _ in range(self.num_tables):
            table_hashes = []
            for _ in range(self.num_hashes):
                # Random projection vector
                projection = np.random.randn(self.dimension)
                projection /= np.linalg.norm(projection)
                table_hashes.append(projection)
            hash_functions.append(table_hashes)
        
        return hash_functions
    
    def _hash_vector(
        self,
        vector: np.ndarray,
        table_id: int,
        add_noise: bool = False
    ) -> int:
        """
        Hash vector using LSH with optional differential privacy noise
        
        Steps:
        1. Project vector using hash functions
        2. Quantize projections to bits
        3. Add Laplace noise (if privacy enabled)
        4. Combine bits into hash value
        
        Args:
            vector: Vector to hash
            table_id: Which hash table to use
            add_noise: Whether to add DP noise
            
        Returns:
            Hash value (integer)
        """
        hash_bits = []
        hash_functions = self.hash_functions[table_id]
        
        for projection in hash_functions:
            # Compute projection
            proj_value = np.dot(vector, projection)
            
            # Add Laplace noise for differential privacy
            if add_noise:
                noise_scale = self.privacy_config.sensitivity / self.privacy_config.epsilon
                noise = np.random.laplace(0, noise_scale)
                proj_value += noise
            
            # Quantize to bit
            bit = 1 if proj_value >= 0 else 0
            hash_bits.append(bit)
        
        # Convert bits to integer hash
        hash_value = int(''.join(map(str, hash_bits)), 2)
        return hash_value
    
    def index_embedding(
        self,
        embedding: np.ndarray,
        embedding_id: str
    ):
        """
        Index embedding in DP-LSH tables
        
        Args:
            embedding: Vector to index
            embedding_id: Unique identifier
        """
        # Hash into each table (no noise during indexing)
        for table_id in range(self.num_tables):
            hash_value = self._hash_vector(embedding, table_id, add_noise=False)
            self.hash_tables[table_id][hash_value].add(embedding_id)
    
    def private_query(
        self,
        query: np.ndarray,
        k: int = 10,
        epsilon_per_query: Optional[float] = None
    ) -> PrivateQuery:
        """
        Create privacy-preserving query
        
        Techniques:
        1. Add Laplace noise to query vector
        2. Generate dummy queries for obfuscation
        3. Track privacy budget usage
        
        Args:
            query: Original query vector
            k: Number of results desired
            epsilon_per_query: Privacy budget for this query
            
        Returns:
            Private query object
        """
        if epsilon_per_query is None:
            epsilon_per_query = self.privacy_config.epsilon
        
        # Add noise to query vector
        if self.privacy_config.noise_mechanism == "laplace":
            noise_scale = self.privacy_config.sensitivity / epsilon_per_query
            noise = np.random.laplace(0, noise_scale, size=query.shape)
        elif self.privacy_config.noise_mechanism == "gaussian":
            # Gaussian mechanism for (ε,δ)-DP
            sigma = math.sqrt(
                2 * math.log(1.25 / self.privacy_config.delta)
            ) * self.privacy_config.sensitivity / epsilon_per_query
            noise = np.random.normal(0, sigma, size=query.shape)
        else:
            noise = np.zeros_like(query)
        
        obfuscated = query + noise
        
        # Track privacy budget
        if self.privacy_config.privacy_budget_tracking:
            self.privacy_budget_used += epsilon_per_query
            self.query_count += 1
        
        return PrivateQuery(
            query_id=f"query_{self.query_count}",
            obfuscated_vector=obfuscated,
            original_norm=np.linalg.norm(query),
            privacy_budget_used=epsilon_per_query
        )
    
    def search(
        self,
        private_query: PrivateQuery,
        k: int = 10,
        embeddings: Optional[Dict[str, np.ndarray]] = None
    ) -> PrivateResult:
        """
        Privacy-preserving similarity search
        
        Steps:
        1. Hash query into each table (with noise)
        2. Retrieve candidates from matching buckets
        3. Compute noisy similarities
        4. Add dummy results for obfuscation
        5. Return top-k with privacy guarantees
        
        Args:
            private_query: Privacy-preserving query
            k: Number of results
            embeddings: Embedding store for similarity computation
            
        Returns:
            Private search results
        """
        # Retrieve candidates using noisy hashing
        candidates = set()
        for table_id in range(self.num_tables):
            hash_value = self._hash_vector(
                private_query.obfuscated_vector,
                table_id,
                add_noise=True
            )
            
            # Get candidates from this bucket
            bucket_candidates = self.hash_tables[table_id].get(hash_value, set())
            candidates.update(bucket_candidates)
        
        if not candidates or embeddings is None:
            return PrivateResult(
                query_id=private_query.query_id,
                results=[],
                privacy_guarantee=private_query.privacy_budget_used
            )
        
        # Compute noisy similarities
        similarities = []
        for emb_id in candidates:
            embedding = embeddings[emb_id]
            
            # True similarity
            true_sim = np.dot(
                private_query.obfuscated_vector, embedding
            ) / (
                np.linalg.norm(private_query.obfuscated_vector) *
                np.linalg.norm(embedding)
            )
            
            # Add noise for privacy
            noise_scale = 1.0 / private_query.privacy_budget_used
            noisy_sim = true_sim + np.random.laplace(0, noise_scale)
            
            similarities.append((emb_id, noisy_sim))
        
        # Sort and get top-k
        similarities.sort(key=lambda x: x[1], reverse=True)
        top_k_results = similarities[:k]
        
        # Add dummy results for obfuscation
        dummy_results = []
        if self.privacy_config.enable_result_obfuscation:
            num_dummies = k // 2  # Add 50% dummy results
            all_ids = set(embeddings.keys()) - set(c[0] for c in top_k_results)
            dummy_ids = np.random.choice(
                list(all_ids),
                size=min(num_dummies, len(all_ids)),
                replace=False
            )
            for dummy_id in dummy_ids:
                # Random similarity scores
                dummy_score = np.random.uniform(-1, 1)
                dummy_results.append((str(dummy_id), dummy_score))
        
        return PrivateResult(
            query_id=private_query.query_id,
            results=top_k_results,
            dummy_results=dummy_results,
            privacy_guarantee=private_query.privacy_budget_used,
            utility_metric={
                "num_candidates": len(candidates),
                "privacy_budget_used": self.privacy_budget_used,
                "query_count": self.query_count
            }
        )

class SecureMultiPartyKNN:
    """
    Secure Multi-Party Computation for k-NN
    
    Scenario: Multiple parties have embedding databases, want to jointly
    compute k-NN without revealing their embeddings to each other.
    
    Protocol:
    1. Secret sharing: Each party splits embeddings into shares
    2. Distributed computation: Compute similarities on shares
    3. Secure aggregation: Combine results without reconstruction
    4. Result revelation: Only final top-k revealed
    
    Security: Honest-but-curious adversary model, collusion resistance
    """
    
    def __init__(self, num_parties: int, privacy_config: PrivacyConfig):
        self.num_parties = num_parties
        self.privacy_config = privacy_config
        
        # Party databases: party_id -> {emb_id: embedding}
        self.party_databases: Dict[int, Dict[str, np.ndarray]] = {
            i: {} for i in range(num_parties)
        }
        
        print(f"Secure MPC k-NN initialized for {num_parties} parties")
    
    def _secret_share(
        self,
        value: float,
        num_shares: int
    ) -> List[float]:
        """
        Split value into additive secret shares
        
        Property: sum(shares) = value (modulo some large prime)
        Each party gets one share, learns nothing individually
        
        Args:
            value: Value to share
            num_shares: Number of shares
            
        Returns:
            List of shares
        """
        shares = np.random.uniform(-10, 10, size=num_shares - 1).tolist()
        final_share = value - sum(shares)
        shares.append(final_share)
        return shares
    
    def add_party_embedding(
        self,
        party_id: int,
        embedding: np.ndarray,
        embedding_id: str
    ):
        """Add embedding to party's database"""
        self.party_databases[party_id][embedding_id] = embedding
    
    def secure_similarity(
        self,
        query: np.ndarray,
        party_embeddings: List[np.ndarray]
    ) -> float:
        """
        Compute similarity using secure MPC
        
        Protocol:
        1. Each party computes partial dot product with their data
        2. Parties secret-share partial results
        3. Aggregate shares to get final similarity
        4. No party learns individual contributions
        
        Args:
            query: Query vector (can be shared)
            party_embeddings: Embedding from each party
            
        Returns:
            Aggregated similarity score
        """
        # Each party computes local dot product
        partial_sims = [
            np.dot(query, emb) for emb in party_embeddings
        ]
        
        # Secret share partial results
        shares_matrix = [
            self._secret_share(sim, self.num_parties)
            for sim in partial_sims
        ]
        
        # Each party aggregates their shares
        party_aggregates = [
            sum(shares_matrix[i][j] for i in range(len(partial_sims)))
            for j in range(self.num_parties)
        ]
        
        # Final aggregation (reveals only total)
        total_similarity = sum(party_aggregates)
        
        # Normalize by query norm and average embedding norm
        query_norm = np.linalg.norm(query)
        avg_emb_norm = np.mean([
            np.linalg.norm(emb) for emb in party_embeddings
        ])
        
        normalized_sim = total_similarity / (query_norm * avg_emb_norm)
        return normalized_sim
    
    def federated_search(
        self,
        query: np.ndarray,
        k: int = 10
    ) -> List[Tuple[int, str, float]]:
        """
        Federated k-NN across parties
        
        Protocol:
        1. Each party computes local top-k
        2. Parties securely aggregate to find global top-k
        3. Only final results revealed
        
        Args:
            query: Query vector
            k: Number of results
            
        Returns:
            Global top-k: (party_id, emb_id, score) tuples
        """
        # Each party computes local similarities
        local_results = []
        
        for party_id in range(self.num_parties):
            party_db = self.party_databases[party_id]
            
            for emb_id, embedding in party_db.items():
                similarity = np.dot(query, embedding) / (
                    np.linalg.norm(query) * np.linalg.norm(embedding)
                )
                local_results.append((party_id, emb_id, similarity))
        
        # Sort and get global top-k
        local_results.sort(key=lambda x: x[2], reverse=True)
        global_top_k = local_results[:k]
        
        return global_top_k

class PrivateInformationRetrieval:
    """
    Private Information Retrieval for embeddings
    
    Goal: Query database without revealing which item you're querying for
    
    Computational PIR:
    - Based on cryptographic assumptions (e.g., homomorphic encryption)
    - Polylogarithmic communication complexity
    - Practical for small-medium databases
    
    Information-theoretic PIR:
    - Requires database replication across non-colluding servers
    - Perfect privacy but higher communication cost
    - Used in privacy-critical applications
    """
    
    def __init__(
        self,
        database_size: int,
        dimension: int,
        privacy_config: PrivacyConfig
    ):
        self.database_size = database_size
        self.dimension = dimension
        self.privacy_config = privacy_config
        
        print(f"PIR initialized for database of {database_size} embeddings")
    
    def private_retrieve(
        self,
        query_index: int,
        database: Dict[str, np.ndarray]
    ) -> Optional[np.ndarray]:
        """
        Retrieve embedding without revealing which one
        
        Simplified approach (real PIR uses homomorphic encryption):
        1. Generate query vector that encodes index obliviously
        2. Server computes linear combination
        3. Client decodes result
        
        Args:
            query_index: Index to retrieve (hidden from server)
            database: Embedding database
            
        Returns:
            Retrieved embedding
        """
        # In production: use homomorphic PIR (e.g., SealPIR, XPIR)
        # This is a simplified demonstration
        
        db_items = list(database.items())
        if query_index >= len(db_items):
            return None
        
        # Generate oblivious query (one-hot encoded with noise)
        query = np.random.randn(len(db_items)) * 0.1
        query[query_index] = 1.0
        
        # Server computes linear combination (doesn't know index)
        result = np.zeros(self.dimension)
        for i, (_, embedding) in enumerate(db_items):
            result += query[i] * embedding
        
        return result

# Example usage
def privacy_preserving_search_example():
    """
    Demonstrate privacy-preserving similarity search techniques
    """
    print("=== Privacy-Preserving Similarity Search ===")
    print()
    
    # Configuration
    privacy_config = PrivacyConfig(
        epsilon=1.0,
        delta=1e-5,
        noise_mechanism="laplace",
        enable_query_obfuscation=True,
        enable_result_obfuscation=True
    )
    
    # Generate sample embeddings
    print("Generating sample embeddings...")
    np.random.seed(42)
    dimension = 768
    num_embeddings = 1000
    
    embeddings = {}
    for i in range(num_embeddings):
        emb = np.random.randn(dimension).astype(np.float32)
        emb /= np.linalg.norm(emb)
        embeddings[f"emb_{i}"] = emb
    
    print(f"Generated {num_embeddings} embeddings")
    print()
    
    # 1. Differentially Private LSH
    print("1. Differentially Private LSH")
    dp_lsh = DifferentiallyPrivateLSH(
        dimension=dimension,
        num_hashes=16,
        num_tables=8,
        privacy_config=privacy_config
    )
    
    # Index embeddings
    for emb_id, emb in embeddings.items():
        dp_lsh.index_embedding(emb, emb_id)
    
    # Private query
    query = np.random.randn(dimension).astype(np.float32)
    query /= np.linalg.norm(query)
    
    private_q = dp_lsh.private_query(query, k=10)
    result = dp_lsh.search(private_q, k=10, embeddings=embeddings)
    
    print(f"  Query ID: {result.query_id}")
    print(f"  Privacy guarantee: ε={result.privacy_guarantee:.2f}")
    print(f"  Candidates found: {result.utility_metric['num_candidates']}")
    print(f"  Top 5 results:")
    for emb_id, score in result.results[:5]:
        print(f"    {emb_id}: {score:.4f}")
    print(f"  Dummy results added: {len(result.dummy_results)}")
    print()
    
    # 2. Secure Multi-Party k-NN
    print("2. Secure Multi-Party k-NN")
    num_parties = 3
    mpc_knn = SecureMultiPartyKNN(num_parties, privacy_config)
    
    # Distribute embeddings across parties
    for i, (emb_id, emb) in enumerate(embeddings.items()):
        party_id = i % num_parties
        mpc_knn.add_party_embedding(party_id, emb, emb_id)
    
    # Federated search
    results = mpc_knn.federated_search(query, k=10)
    
    print(f"  Federated top 5 results across {num_parties} parties:")
    for party_id, emb_id, score in results[:5]:
        print(f"    Party {party_id}, {emb_id}: {score:.4f}")
    print()
    
    # 3. Privacy budget tracking
    print("3. Privacy Budget Tracking")
    print(f"  Total privacy budget used: ε={dp_lsh.privacy_budget_used:.2f}")
    print(f"  Number of queries: {dp_lsh.query_count}")
    print(f"  Average ε per query: {dp_lsh.privacy_budget_used/dp_lsh.query_count:.2f}")
    
    # Demonstrate privacy budget exhaustion
    budget_limit = 10.0
    remaining = budget_limit - dp_lsh.privacy_budget_used
    print(f"  Remaining budget (limit={budget_limit}): ε={remaining:.2f}")
    print(f"  Can support ~{int(remaining/privacy_config.epsilon)} more queries")

if __name__ == "__main__":
    privacy_preserving_search_example()
```

:::{.callout-warning}
## Privacy-Utility Trade-offs

**Differential Privacy Parameter Selection**:
- **ε = 0.1**: Very strong privacy, 30-50% utility loss
  - Use: Healthcare records, sensitive personal data
- **ε = 1.0**: Balanced, 10-20% utility loss
  - Use: Enterprise data, financial records (recommended)
- **ε = 10.0**: Weak privacy, <5% utility loss
  - Use: Public datasets, aggregate statistics

**LSH Privacy Enhancement**:
- Standard LSH: No privacy, reveals bucket membership
- DP-LSH: ε-differential privacy per query
- Overhead: 2-3× latency, 10-20% recall loss
- Composition: Privacy budget degrades with queries

**MPC Performance**:
- 2-party: 5-10× overhead vs plaintext
- 3+ parties: 10-50× overhead
- Communication: O(n) per similarity computation
- Best for: Federated learning, cross-silo queries
:::

:::{.callout-tip}
## Practical Privacy Deployment

**Start with Hybrid Approach**:
1. Public-facing API: Differential privacy (ε=1.0)
2. Internal trusted use: Minimal privacy overhead
3. Cross-tenant: SGX enclaves + DP
4. External federation: Secure MPC

**Privacy Budget Management**:
- Set daily/monthly privacy budget per user
- Track cumulative ε across queries
- Throttle or reject when budget exhausted
- Use privacy accounting (e.g., Rényi DP, zCDP)

**Optimize for Common Case**:
- Cache popular queries (public results)
- Use coarser privacy for exploratory queries
- Apply stronger privacy for sensitive final queries
- Batch similar queries for composition benefits

**Monitor Privacy-Utility**:
- Track recall at different ε levels
- A/B test privacy parameters
- Measure user satisfaction vs privacy cost
- Adjust based on regulatory requirements
:::
## Differential Privacy for Embeddings

Embedding models trained on sensitive data encode private information—training on medical records produces embeddings that leak diagnoses, training on private messages exposes conversation patterns. **Differential privacy for embeddings** provides formal mathematical guarantees that embeddings reveal bounded information about any individual training example, using noise injection during training (DP-SGD), output perturbation after training, and privacy accounting to track cumulative privacy loss—achieving ε≤1.0 privacy while maintaining 85-95% of non-private model utility.

### The Training Privacy Challenge

Embedding model training faces privacy risks:

- **Membership inference**: Determine if specific example was in training data
- **Attribute inference**: Infer sensitive attributes from embeddings
- **Model inversion**: Reconstruct training examples from model
- **Embedding leakage**: Similar embeddings reveal similar training data
- **Gradient leakage**: Training gradients expose training examples
- **Fine-tuning risk**: Fine-tuning on private data leaks information
- **Deployment exposure**: Serving embeddings leaks training distribution

**Differential privacy approach**: Add calibrated noise during training (DP-SGD) to prevent any single training example from significantly affecting model, bound privacy loss through privacy accounting (ε,δ), clip gradients to limit per-example influence, use private aggregation for federated learning, and apply output perturbation for additional privacy layer.

```python
"""
Differential Privacy for Embedding Training

Architecture:
1. Gradient clipping: Bound per-example gradient norm
2. Noise injection: Add Gaussian noise to gradients
3. Privacy accounting: Track cumulative privacy loss
4. Batch sampling: Use Poisson sampling for stronger privacy
5. Model validation: Verify privacy-utility trade-off

Techniques:
- DP-SGD: Stochastic gradient descent with differential privacy
- PATE: Private aggregation of teacher ensembles
- DP-FTRL: Follow-the-regularized-leader with DP
- Private federated learning: Distributed DP training
- Post-processing: Additional privacy via output perturbation

Privacy guarantees:
- (ε,δ)-differential privacy for entire training process
- Privacy amplification via sampling
- Composition theorems for multiple releases
- Rényi differential privacy for tighter accounting

Performance targets:
- Privacy: ε ≤ 1.0, δ ≤ 1e-5 for typical applications
- Utility: >85% of non-private model performance
- Training time: 2-5× longer than standard training
- Memory: 1.5-2× due to per-example gradients
"""

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import List, Dict, Optional, Tuple, Any, Callable
from dataclasses import dataclass, field
from datetime import datetime
import math

@dataclass
class DPTrainingConfig:
    """
    Configuration for differentially private training
    
    Attributes:
        target_epsilon: Target privacy parameter
        target_delta: Target failure probability
        max_grad_norm: Gradient clipping threshold
        noise_multiplier: Noise scale relative to clipping
        batch_size: Training batch size
        num_epochs: Number of training epochs
        learning_rate: Learning rate
        sampling_probability: Probability of including example
        accounting_mode: Privacy accounting method (rdp, gdp, glw)
    """
    target_epsilon: float = 1.0
    target_delta: float = 1e-5
    max_grad_norm: float = 1.0
    noise_multiplier: float = 1.0  # σ, computed from ε,δ
    batch_size: int = 256
    num_epochs: int = 10
    learning_rate: float = 0.001
    sampling_probability: Optional[float] = None  # Computed from batch_size
    accounting_mode: str = "rdp"  # "rdp", "gdp", "glw"

@dataclass
class PrivacyAccountant:
    """
    Track cumulative privacy loss during training
    
    Attributes:
        epsilon_spent: Cumulative ε used
        delta_spent: Cumulative δ used
        steps: Number of training steps
        composition_method: How to compose privacy guarantees
        history: History of privacy loss per step
    """
    epsilon_spent: float = 0.0
    delta_spent: float = 0.0
    steps: int = 0
    composition_method: str = "rdp"
    history: List[Tuple[float, float]] = field(default_factory=list)

class DPEmbeddingModel(nn.Module):
    """
    Embedding model with differential privacy support
    
    Standard embedding model with hooks for:
    - Per-example gradient computation
    - Gradient clipping
    - Noise injection
    - Privacy accounting
    """
    
    def __init__(
        self,
        vocab_size: int,
        embedding_dim: int,
        hidden_dim: int = 512,
        num_layers: int = 2
    ):
        super().__init__()
        
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        
        # Projection layers
        layers = []
        in_dim = embedding_dim
        for _ in range(num_layers - 1):
            layers.extend([
                nn.Linear(in_dim, hidden_dim),
                nn.ReLU(),
                nn.LayerNorm(hidden_dim)
            ])
            in_dim = hidden_dim
        
        layers.append(nn.Linear(in_dim, embedding_dim))
        self.projection = nn.Sequential(*layers)
        
        # Output normalization
        self.output_norm = nn.LayerNorm(embedding_dim)
    
    def forward(self, input_ids: torch.Tensor) -> torch.Tensor:
        """
        Forward pass
        
        Args:
            input_ids: [batch_size, seq_len] token IDs
            
        Returns:
            embeddings: [batch_size, embedding_dim]
        """
        # Embed tokens
        embedded = self.embedding(input_ids)  # [batch, seq_len, emb_dim]
        
        # Pool over sequence (mean pooling)
        pooled = embedded.mean(dim=1)  # [batch, emb_dim]
        
        # Project
        projected = self.projection(pooled)
        
        # Normalize
        output = self.output_norm(projected)
        
        return F.normalize(output, p=2, dim=1)

class DPSGDOptimizer:
    """
    Differentially Private SGD optimizer
    
    Implements DP-SGD algorithm:
    1. Compute per-example gradients
    2. Clip gradients to max_grad_norm
    3. Add Gaussian noise
    4. Average and apply update
    """
    
    def __init__(
        self,
        model: nn.Module,
        config: DPTrainingConfig
    ):
        self.model = model
        self.config = config
        
        # Standard optimizer for parameter updates
        self.optimizer = torch.optim.Adam(
            model.parameters(),
            lr=config.learning_rate
        )
        
        # Privacy accountant
        self.accountant = PrivacyAccountant(
            composition_method=config.accounting_mode
        )
        
        # Compute noise multiplier if not provided
        if config.noise_multiplier == 1.0:
            self.config.noise_multiplier = self._compute_noise_multiplier()
        
        print(f"DP-SGD initialized:")
        print(f"  Target privacy: ε={config.target_epsilon}, δ={config.target_delta}")
        print(f"  Noise multiplier: σ={self.config.noise_multiplier:.2f}")
        print(f"  Gradient clipping: {config.max_grad_norm}")
    
    def _compute_noise_multiplier(self) -> float:
        """
        Compute noise multiplier from target ε,δ
        
        Uses Gaussian mechanism calibration:
        σ = sqrt(2 * ln(1.25/δ)) * sensitivity / ε
        
        For DP-SGD with gradient clipping:
        sensitivity = max_grad_norm
        
        Returns:
            Noise multiplier σ
        """
        # Simplified computation
        # In production, use more accurate calibration (e.g., from Opacus)
        
        sensitivity = self.config.max_grad_norm
        delta = self.config.target_delta
        epsilon = self.config.target_epsilon
        
        sigma = math.sqrt(2 * math.log(1.25 / delta)) * sensitivity / epsilon
        
        return sigma
    
    def compute_per_example_gradients(
        self,
        loss: torch.Tensor,
        inputs: torch.Tensor
    ) -> Dict[str, torch.Tensor]:
        """
        Compute gradients for each example in batch
        
        Standard PyTorch computes mean gradient across batch.
        DP-SGD requires per-example gradients for clipping.
        
        Args:
            loss: Loss for batch
            inputs: Input batch
            
        Returns:
            Dictionary of per-example gradients for each parameter
        """
        # Enable per-example gradient computation
        # In production, use torch.func.grad or Opacus GradSampleModule
        
        per_example_grads = {}
        
        # Compute gradients for each parameter
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                # Get per-example gradient
                # This is simplified; production uses proper per-sample gradient
                grad = torch.autograd.grad(
                    loss,
                    param,
                    retain_graph=True,
                    create_graph=False
                )[0]
                
                per_example_grads[name] = grad
        
        return per_example_grads
    
    def clip_gradients(
        self,
        gradients: Dict[str, torch.Tensor]
    ) -> Dict[str, torch.Tensor]:
        """
        Clip per-example gradients to max_grad_norm
        
        Clipping bounds the influence of any single example:
        g_clipped = g * min(1, max_grad_norm / ||g||)
        
        Args:
            gradients: Per-example gradients
            
        Returns:
            Clipped gradients
        """
        clipped_grads = {}
        
        for name, grad in gradients.items():
            # Compute gradient norm
            grad_norm = torch.norm(grad)
            
            # Clip if needed
            if grad_norm > self.config.max_grad_norm:
                clipped_grads[name] = grad * (
                    self.config.max_grad_norm / grad_norm
                )
            else:
                clipped_grads[name] = grad
        
        return clipped_grads
    
    def add_noise(
        self,
        gradients: Dict[str, torch.Tensor],
        batch_size: int
    ) -> Dict[str, torch.Tensor]:
        """
        Add Gaussian noise to gradients for privacy
        
        Noise scale: σ = noise_multiplier * max_grad_norm
        
        Args:
            gradients: Clipped gradients
            batch_size: Batch size for normalization
            
        Returns:
            Noisy gradients
        """
        noisy_grads = {}
        
        noise_scale = self.config.noise_multiplier * self.config.max_grad_norm
        
        for name, grad in gradients.items():
            # Sample Gaussian noise with same shape as gradient
            noise = torch.randn_like(grad) * noise_scale
            
            # Add noise and normalize by batch size
            noisy_grads[name] = (grad + noise) / batch_size
        
        return noisy_grads
    
    def step(
        self,
        loss: torch.Tensor,
        inputs: torch.Tensor
    ):
        """
        Perform one DP-SGD optimization step
        
        Steps:
        1. Compute per-example gradients
        2. Clip each gradient
        3. Add noise
        4. Average and apply update
        5. Update privacy accountant
        
        Args:
            loss: Batch loss
            inputs: Input batch
        """
        batch_size = inputs.shape[0]
        
        # Zero existing gradients
        self.optimizer.zero_grad()
        
        # Compute per-example gradients (simplified)
        # In production: use Opacus or torch.func.grad_and_value
        for name, param in self.model.named_parameters():
            if param.grad is not None:
                # Clip gradient
                grad_norm = torch.norm(param.grad)
                if grad_norm > self.config.max_grad_norm:
                    param.grad = param.grad * (
                        self.config.max_grad_norm / grad_norm
                    )
                
                # Add noise
                noise_scale = (
                    self.config.noise_multiplier *
                    self.config.max_grad_norm /
                    batch_size
                )
                noise = torch.randn_like(param.grad) * noise_scale
                param.grad = param.grad + noise
        
        # Apply update
        self.optimizer.step()
        
        # Update privacy accounting
        self._update_privacy_accountant()
    
    def _update_privacy_accountant(self):
        """
        Update privacy loss after one step
        
        Uses composition theorems to track cumulative (ε,δ)
        """
        self.accountant.steps += 1
        
        # Simplified privacy accounting
        # In production: use opacus.privacy_analysis or TF Privacy
        
        # Per-step privacy loss (simplified Gaussian mechanism)
        sampling_prob = self.config.batch_size / 60000  # Assume 60k dataset
        
        # RDP accounting (simplified)
        step_epsilon = (
            2 * sampling_prob * self.accountant.steps /
            (self.config.noise_multiplier ** 2)
        )
        
        self.accountant.epsilon_spent = step_epsilon
        self.accountant.delta_spent = self.config.target_delta
        
        self.accountant.history.append((
            self.accountant.epsilon_spent,
            self.accountant.delta_spent
        ))
    
    def get_privacy_spent(self) -> Tuple[float, float]:
        """
        Get current privacy spent
        
        Returns:
            (epsilon, delta) tuple
        """
        return (
            self.accountant.epsilon_spent,
            self.accountant.delta_spent
        )

class PrivateAggregationOfTeacherEnsembles:
    """
    PATE: Private Aggregation of Teacher Ensembles
    
    Alternative to DP-SGD:
    1. Train multiple "teacher" models on disjoint data
    2. Label public data using noisy aggregation of teachers
    3. Train "student" model on public labeled data
    
    Benefits:
    - Student model has no direct privacy cost
    - Can achieve better utility than DP-SGD
    - Suitable when public unlabeled data available
    
    Limitations:
    - Requires partitionable private data
    - Needs public unlabeled data
    - Multiple model training overhead
    """
    
    def __init__(
        self,
        num_teachers: int,
        embedding_dim: int,
        privacy_config: DPTrainingConfig
    ):
        self.num_teachers = num_teachers
        self.embedding_dim = embedding_dim
        self.privacy_config = privacy_config
        
        # Teacher models (would be trained separately)
        self.teachers: List[nn.Module] = []
        
        # Privacy accountant for aggregation
        self.accountant = PrivacyAccountant()
        
        print(f"PATE initialized with {num_teachers} teachers")
    
    def noisy_aggregation(
        self,
        teacher_predictions: List[torch.Tensor],
        epsilon: float = 1.0
    ) -> torch.Tensor:
        """
        Aggregate teacher predictions with differential privacy
        
        Steps:
        1. Each teacher predicts on unlabeled example
        2. Take majority vote (for classification) or mean (for regression)
        3. Add Laplace noise to vote counts
        4. Return noisy consensus
        
        Args:
            teacher_predictions: List of teacher predictions
            epsilon: Privacy budget for this aggregation
            
        Returns:
            Noisy consensus prediction
        """
        # Stack predictions
        predictions = torch.stack(teacher_predictions)  # [num_teachers, ...]
        
        # Average predictions (for embeddings)
        consensus = predictions.mean(dim=0)
        
        # Add Laplace noise for privacy
        sensitivity = 2.0 / self.num_teachers  # Bounded by averaging
        noise_scale = sensitivity / epsilon
        
        noise = torch.from_numpy(
            np.random.laplace(0, noise_scale, size=consensus.shape)
        ).float()
        
        noisy_consensus = consensus + noise
        
        # Update privacy accounting
        self.accountant.epsilon_spent += epsilon
        self.accountant.steps += 1
        
        return noisy_consensus
    
    def train_student(
        self,
        public_data: torch.Tensor,
        student_model: nn.Module
    ) -> nn.Module:
        """
        Train student model on privately labeled public data
        
        Args:
            public_data: Unlabeled public data
            student_model: Student model to train
            
        Returns:
            Trained student model with privacy guarantee
        """
        # Get teacher predictions with noisy aggregation
        print("Labeling public data with private aggregation...")
        
        labels = []
        for example in public_data:
            # Each teacher predicts
            teacher_preds = [
                teacher(example.unsqueeze(0))
                for teacher in self.teachers
            ]
            
            # Noisy aggregation
            label = self.noisy_aggregation(
                teacher_preds,
                epsilon=self.privacy_config.target_epsilon / len(public_data)
            )
            labels.append(label)
        
        labels = torch.stack(labels)
        
        # Train student on labeled public data (no privacy cost!)
        print("Training student model...")
        optimizer = torch.optim.Adam(student_model.parameters(), lr=0.001)
        
        for epoch in range(10):
            optimizer.zero_grad()
            
            predictions = student_model(public_data)
            loss = F.mse_loss(predictions, labels)
            
            loss.backward()
            optimizer.step()
        
        print(f"Student trained with privacy: ε={self.accountant.epsilon_spent:.2f}")
        
        return student_model

# Example usage
def dp_embedding_training_example():
    """
    Demonstrate differentially private embedding training
    """
    print("=== Differentially Private Embedding Training ===")
    print()
    
    # Configuration
    dp_config = DPTrainingConfig(
        target_epsilon=1.0,
        target_delta=1e-5,
        max_grad_norm=1.0,
        batch_size=256,
        num_epochs=5,
        learning_rate=0.001
    )
    
    # Initialize model
    vocab_size = 10000
    embedding_dim = 256
    
    model = DPEmbeddingModel(
        vocab_size=vocab_size,
        embedding_dim=embedding_dim,
        hidden_dim=512,
        num_layers=2
    )
    
    # Initialize DP optimizer
    dp_optimizer = DPSGDOptimizer(model, dp_config)
    print()
    
    # Simulate training data
    print("Training with DP-SGD...")
    num_samples = 10000
    
    for epoch in range(dp_config.num_epochs):
        epoch_loss = 0.0
        num_batches = num_samples // dp_config.batch_size
        
        for batch_idx in range(num_batches):
            # Simulate batch
            batch_ids = torch.randint(
                0, vocab_size,
                (dp_config.batch_size, 20)
            )
            
            # Forward pass
            embeddings = model(batch_ids)
            
            # Contrastive loss (simplified)
            # Positive pairs: embeddings[i] with embeddings[i]
            # Negative pairs: embeddings[i] with embeddings[j≠i]
            similarity = torch.matmul(embeddings, embeddings.t())
            labels = torch.arange(dp_config.batch_size)
            loss = F.cross_entropy(similarity, labels)
            
            # Backward pass (compute gradients)
            loss.backward()
            
            # DP-SGD step
            dp_optimizer.step(loss, batch_ids)
            
            epoch_loss += loss.item()
        
        # Check privacy spent
        epsilon_spent, delta_spent = dp_optimizer.get_privacy_spent()
        
        avg_loss = epoch_loss / num_batches
        print(f"Epoch {epoch+1}/{dp_config.num_epochs}")
        print(f"  Loss: {avg_loss:.4f}")
        print(f"  Privacy spent: ε={epsilon_spent:.4f}, δ={delta_spent:.2e}")
        
        # Stop if privacy budget exhausted
        if epsilon_spent > dp_config.target_epsilon:
            print(f"  Privacy budget exhausted! Stopping training.")
            break
    
    print()
    print("Training complete!")
    final_eps, final_delta = dp_optimizer.get_privacy_spent()
    print(f"Final privacy guarantee: (ε={final_eps:.2f}, δ={final_delta:.2e})")

if __name__ == "__main__":
    dp_embedding_training_example()
```

:::{.callout-warning}
## Differential Privacy Trade-offs

**Privacy-Utility Frontier**:
- **ε = 0.1**: Very strong privacy, 40-60% utility loss
  - Use: Extremely sensitive data (genetic, health)
- **ε = 1.0**: Strong privacy, 10-20% utility loss
  - Use: Personal data (recommended for GDPR)
- **ε = 10.0**: Weak privacy, <5% utility loss
  - Use: Aggregate statistics, exploratory analysis

**DP-SGD Challenges**:
- **Training time**: 2-5× longer due to per-example gradients
- **Memory**: 1.5-2× higher for gradient storage
- **Hyperparameters**: Requires careful tuning (clipping, noise)
- **Convergence**: May require more epochs

**Production Recommendations**:
- Start with ε=3.0, tune down based on requirements
- Use adaptive clipping (percentile-based)
- Implement privacy accounting with Opacus or TF Privacy
- Monitor utility metrics throughout training
- Consider PATE for better utility when applicable
:::

:::{.callout-tip}
## Practical DP Implementation

**Use Established Libraries**:
- **Opacus** (PyTorch): Production-ready DP-SGD
  - `pip install opacus`
  - Handles per-example gradients automatically
  - Advanced privacy accounting (RDP, GDP)

- **TensorFlow Privacy**: TF ecosystem DP
  - `pip install tensorflow-privacy`
  - DP optimizers, privacy analysis
  - Supports Keras models

**Privacy Accounting**:
- Use Rényi DP (RDP) for tighter bounds
- Track privacy loss per epoch
- Set privacy budget alarm (warn at 80%)
- Report final (ε,δ) with model release

**Hyperparameter Tuning**:
- Grid search over clipping threshold (0.1-5.0)
- Adjust noise multiplier based on target ε
- Use learning rate warm-up
- Increase batch size (helps privacy)

**Validation**:
- Measure utility on holdout set
- Compare with non-private baseline
- Check for privacy leakage via membership inference
- Document privacy parameters in model card
:::
## Access Control and Audit Trails

Embedding systems serve multiple users with varying permissions—data scientists need read access for analysis, application servers need query access for recommendations, administrators need full access for management, and auditors need query logs for compliance. **Access control and audit trails** implement fine-grained permissions (who can query which embeddings with what filters), comprehensive logging (all queries, results, and access attempts), immutable audit trails for compliance, and real-time monitoring for anomaly detection—enabling secure multi-tenant deployments, regulatory compliance (SOC2, HIPAA, PCI-DSS), and forensic investigation of security incidents.

### The Access Control Challenge

Production embedding systems face access requirements:

- **Multi-tenancy**: Isolate tenant data, prevent cross-tenant leakage
- **Role-based access**: Different permissions for roles (admin, analyst, service)
- **Attribute-based access**: Filter queries based on data attributes (region, classification)
- **Query constraints**: Limit query rate, result size, complexity
- **Data sovereignty**: Enforce geographic restrictions
- **Temporal access**: Time-limited credentials, temporary shares
- **Audit compliance**: Immutable logs for regulatory requirements
- **Real-time monitoring**: Detect suspicious access patterns

**Access control approach**: Implement role-based access control (RBAC) with attribute-based extensions (ABAC), use signed tokens (JWT) with embedded permissions, enforce row-level security filtering based on user attributes, implement rate limiting and quota management, maintain comprehensive audit logs with query details and results, use append-only storage for tamper-proof auditing, and monitor access patterns for anomaly detection.

```python
"""
Access Control and Audit Trails for Embeddings

Architecture:
1. Authentication: Verify user identity (OAuth, API keys, mTLS)
2. Authorization: Check permissions for requested operation
3. Query filtering: Apply row-level security based on attributes
4. Rate limiting: Enforce quotas per user/tenant
5. Audit logging: Record all access attempts and results
6. Monitoring: Real-time anomaly detection

Components:
- Identity provider: OAuth2, SAML, API keys
- Policy engine: Evaluate access policies
- Query rewriter: Inject security filters
- Audit log: Immutable access record
- Monitoring: Real-time alerting

Security properties:
- Least privilege: Users get minimum necessary access
- Defense in depth: Multiple layers of security
- Separation of duties: Different roles for different functions
- Audit trail: Complete record of all access
- Non-repudiation: Users cannot deny actions

Performance targets:
- Authorization: <5ms per query
- Audit logging: Async, no query latency impact
- Rate limiting: <1ms check
- Query filtering: <10% overhead
"""

import numpy as np
import torch
from typing import List, Dict, Optional, Tuple, Set, Any, Callable
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from enum import Enum
import hashlib
import hmac
import json
import time
from collections import defaultdict

class Permission(Enum):
    """Permission types"""
    READ = "read"
    WRITE = "write"
    DELETE = "delete"
    ADMIN = "admin"
    QUERY = "query"
    EXPORT = "export"

class ResourceType(Enum):
    """Resource types"""
    EMBEDDING = "embedding"
    INDEX = "index"
    COLLECTION = "collection"
    SYSTEM = "system"

@dataclass
class User:
    """
    User identity and attributes
    
    Attributes:
        user_id: Unique identifier
        username: Human-readable name
        email: Email address
        roles: Assigned roles
        attributes: User attributes for ABAC
        tenant_id: Tenant identifier for multi-tenancy
        created_at: Account creation time
        last_login: Last login time
    """
    user_id: str
    username: str
    email: str
    roles: List[str] = field(default_factory=list)
    attributes: Dict[str, Any] = field(default_factory=dict)
    tenant_id: Optional[str] = None
    created_at: datetime = field(default_factory=datetime.now)
    last_login: Optional[datetime] = None

@dataclass
class Role:
    """
    Role-based access control role
    
    Attributes:
        role_id: Role identifier
        name: Role name
        permissions: List of granted permissions
        resource_patterns: Resource patterns this role can access
        constraints: Additional constraints (time, rate limits)
    """
    role_id: str
    name: str
    permissions: List[Permission] = field(default_factory=list)
    resource_patterns: List[str] = field(default_factory=list)
    constraints: Dict[str, Any] = field(default_factory=dict)

@dataclass
class AccessPolicy:
    """
    Attribute-based access control policy
    
    Attributes:
        policy_id: Policy identifier
        name: Policy name
        effect: Allow or deny
        principals: Who this applies to (users, roles)
        actions: What actions are allowed
        resources: What resources can be accessed
        conditions: When policy applies
    """
    policy_id: str
    name: str
    effect: str  # "allow" or "deny"
    principals: List[str] = field(default_factory=list)
    actions: List[str] = field(default_factory=list)
    resources: List[str] = field(default_factory=list)
    conditions: Dict[str, Any] = field(default_factory=dict)

@dataclass
class AuditLogEntry:
    """
    Audit log entry for access tracking
    
    Attributes:
        log_id: Unique log entry ID
        timestamp: When action occurred
        user_id: Who performed action
        action: What action was performed
        resource_type: Type of resource accessed
        resource_id: Which resource was accessed
        result: Success or failure
        metadata: Additional context
        query_details: For query actions, the query details
        ip_address: Source IP address
        user_agent: Client user agent
    """
    log_id: str
    timestamp: datetime
    user_id: str
    action: str
    resource_type: ResourceType
    resource_id: str
    result: str  # "success", "failure", "denied"
    metadata: Dict[str, Any] = field(default_factory=dict)
    query_details: Optional[Dict[str, Any]] = None
    ip_address: Optional[str] = None
    user_agent: Optional[str] = None

@dataclass
class QueryQuota:
    """
    Query quota for rate limiting
    
    Attributes:
        user_id: User identifier
        queries_per_hour: Hourly query limit
        queries_per_day: Daily query limit
        max_result_size: Maximum results per query
        max_concurrent: Maximum concurrent queries
        current_hour_count: Queries this hour
        current_day_count: Queries today
        reset_time: When quotas reset
    """
    user_id: str
    queries_per_hour: int = 1000
    queries_per_day: int = 10000
    max_result_size: int = 100
    max_concurrent: int = 10
    current_hour_count: int = 0
    current_day_count: int = 0
    reset_time: datetime = field(default_factory=datetime.now)

class AccessControlEngine:
    """
    Access control engine for embedding systems
    
    Implements:
    - Authentication: Verify user identity
    - Authorization: Check permissions
    - Row-level security: Filter queries by attributes
    - Rate limiting: Enforce quotas
    - Audit logging: Record all access
    """
    
    def __init__(self):
        # User database
        self.users: Dict[str, User] = {}
        
        # Role definitions
        self.roles: Dict[str, Role] = {}
        
        # Access policies
        self.policies: List[AccessPolicy] = []
        
        # Query quotas
        self.quotas: Dict[str, QueryQuota] = {}
        
        # Audit log (in production: use database or log aggregation)
        self.audit_log: List[AuditLogEntry] = []
        
        # Active sessions
        self.sessions: Dict[str, Dict[str, Any]] = {}
        
        # Initialize default roles
        self._initialize_default_roles()
        
        print("Access Control Engine initialized")
    
    def _initialize_default_roles(self):
        """Create default roles"""
        
        # Admin role: Full access
        self.roles["admin"] = Role(
            role_id="admin",
            name="Administrator",
            permissions=[p for p in Permission],
            resource_patterns=["*"],
            constraints={}
        )
        
        # Analyst role: Read and query access
        self.roles["analyst"] = Role(
            role_id="analyst",
            name="Data Analyst",
            permissions=[Permission.READ, Permission.QUERY],
            resource_patterns=["embedding:*", "collection:*"],
            constraints={
                "max_result_size": 1000,
                "queries_per_hour": 100
            }
        )
        
        # Service role: Query-only access
        self.roles["service"] = Role(
            role_id="service",
            name="Application Service",
            permissions=[Permission.QUERY],
            resource_patterns=["embedding:*"],
            constraints={
                "max_result_size": 100,
                "queries_per_hour": 10000
            }
        )
        
        # Auditor role: Read audit logs only
        self.roles["auditor"] = Role(
            role_id="auditor",
            name="Security Auditor",
            permissions=[Permission.READ],
            resource_patterns=["audit:*"],
            constraints={}
        )
    
    def create_user(
        self,
        username: str,
        email: str,
        roles: List[str],
        tenant_id: Optional[str] = None,
        attributes: Optional[Dict[str, Any]] = None
    ) -> User:
        """
        Create new user
        
        Args:
            username: Username
            email: Email address
            roles: List of role IDs
            tenant_id: Tenant identifier
            attributes: User attributes
            
        Returns:
            Created user
        """
        user_id = hashlib.sha256(
            f"{username}:{email}".encode()
        ).hexdigest()[:16]
        
        user = User(
            user_id=user_id,
            username=username,
            email=email,
            roles=roles,
            attributes=attributes or {},
            tenant_id=tenant_id
        )
        
        self.users[user_id] = user
        
        # Initialize quota
        self.quotas[user_id] = QueryQuota(user_id=user_id)
        
        # Audit log
        self._log_access(
            user_id=user_id,
            action="user_created",
            resource_type=ResourceType.SYSTEM,
            resource_id=user_id,
            result="success"
        )
        
        return user
    
    def authenticate(
        self,
        api_key: str,
        ip_address: Optional[str] = None
    ) -> Optional[User]:
        """
        Authenticate user via API key
        
        In production:
        - Use OAuth2 tokens (JWT)
        - Verify token signature
        - Check expiration
        - Support multiple auth methods
        
        Args:
            api_key: API key or token
            ip_address: Client IP address
            
        Returns:
            User if authenticated, None otherwise
        """
        # Simplified: API key is just user_id
        # In production: verify JWT, check revocation, etc.
        
        user_id = api_key
        user = self.users.get(user_id)
        
        if user:
            user.last_login = datetime.now()
            
            self._log_access(
                user_id=user_id,
                action="authenticate",
                resource_type=ResourceType.SYSTEM,
                resource_id="auth",
                result="success",
                metadata={"ip_address": ip_address}
            )
        else:
            self._log_access(
                user_id=api_key,
                action="authenticate",
                resource_type=ResourceType.SYSTEM,
                resource_id="auth",
                result="failure",
                metadata={"ip_address": ip_address}
            )
        
        return user
    
    def authorize(
        self,
        user: User,
        action: Permission,
        resource_type: ResourceType,
        resource_id: str
    ) -> bool:
        """
        Check if user is authorized for action on resource
        
        Steps:
        1. Check user roles
        2. Check role permissions
        3. Check resource patterns
        4. Evaluate ABAC policies
        5. Apply deny policies
        
        Args:
            user: User requesting access
            action: Requested permission
            resource_type: Type of resource
            resource_id: Specific resource
            
        Returns:
            True if authorized, False otherwise
        """
        # Check each role
        for role_id in user.roles:
            role = self.roles.get(role_id)
            if not role:
                continue
            
            # Check if action is permitted
            if action not in role.permissions and Permission.ADMIN not in role.permissions:
                continue
            
            # Check resource patterns
            resource_str = f"{resource_type.value}:{resource_id}"
            if not self._match_resource_pattern(resource_str, role.resource_patterns):
                continue
            
            # Role grants access
            self._log_access(
                user_id=user.user_id,
                action=f"authorize_{action.value}",
                resource_type=resource_type,
                resource_id=resource_id,
                result="success"
            )
            return True
        
        # Check ABAC policies
        if self._evaluate_policies(user, action, resource_type, resource_id):
            return True
        
        # Access denied
        self._log_access(
            user_id=user.user_id,
            action=f"authorize_{action.value}",
            resource_type=resource_type,
            resource_id=resource_id,
            result="denied"
        )
        return False
    
    def _match_resource_pattern(
        self,
        resource: str,
        patterns: List[str]
    ) -> bool:
        """
        Check if resource matches any pattern
        
        Supports wildcards:
        - "*" matches anything
        - "embedding:*" matches all embeddings
        - "collection:customer_*" matches customer collections
        
        Args:
            resource: Resource to check
            patterns: Allowed patterns
            
        Returns:
            True if matches
        """
        for pattern in patterns:
            if pattern == "*":
                return True
            
            if pattern.endswith("*"):
                prefix = pattern[:-1]
                if resource.startswith(prefix):
                    return True
            
            if pattern == resource:
                return True
        
        return False
    
    def _evaluate_policies(
        self,
        user: User,
        action: Permission,
        resource_type: ResourceType,
        resource_id: str
    ) -> bool:
        """
        Evaluate ABAC policies
        
        Args:
            user: User
            action: Action
            resource_type: Resource type
            resource_id: Resource ID
            
        Returns:
            True if any allow policy matches
        """
        for policy in self.policies:
            # Check if policy applies to this user
            if user.user_id not in policy.principals and "*" not in policy.principals:
                continue
            
            # Check if action is covered
            if action.value not in policy.actions and "*" not in policy.actions:
                continue
            
            # Check if resource matches
            resource_str = f"{resource_type.value}:{resource_id}"
            if not self._match_resource_pattern(resource_str, policy.resources):
                continue
            
            # Evaluate conditions
            if policy.conditions:
                if not self._evaluate_conditions(user, policy.conditions):
                    continue
            
            # Policy applies
            if policy.effect == "allow":
                return True
            elif policy.effect == "deny":
                return False
        
        return False
    
    def _evaluate_conditions(
        self,
        user: User,
        conditions: Dict[str, Any]
    ) -> bool:
        """
        Evaluate policy conditions
        
        Examples:
        - "time": {"after": "09:00", "before": "17:00"}
        - "attribute": {"region": "US"}
        - "tenant": {"equals": "tenant_123"}
        
        Args:
            user: User
            conditions: Condition dictionary
            
        Returns:
            True if all conditions met
        """
        for key, value in conditions.items():
            if key == "tenant":
                if user.tenant_id != value.get("equals"):
                    return False
            
            elif key == "attribute":
                for attr_key, attr_value in value.items():
                    if user.attributes.get(attr_key) != attr_value:
                        return False
            
            elif key == "time":
                now = datetime.now().time()
                if "after" in value:
                    after = datetime.strptime(value["after"], "%H:%M").time()
                    if now < after:
                        return False
                if "before" in value:
                    before = datetime.strptime(value["before"], "%H:%M").time()
                    if now > before:
                        return False
        
        return True
    
    def check_quota(
        self,
        user: User,
        result_size: int = 10
    ) -> Tuple[bool, str]:
        """
        Check if user has remaining quota
        
        Args:
            user: User
            result_size: Number of results requested
            
        Returns:
            (allowed, reason) tuple
        """
        quota = self.quotas.get(user.user_id)
        if not quota:
            return False, "No quota found"
        
        # Reset counters if needed
        now = datetime.now()
        if now >= quota.reset_time:
            quota.current_hour_count = 0
            quota.reset_time = now + timedelta(hours=1)
        
        if now.date() != quota.reset_time.date():
            quota.current_day_count = 0
        
        # Check hourly limit
        if quota.current_hour_count >= quota.queries_per_hour:
            return False, "Hourly quota exceeded"
        
        # Check daily limit
        if quota.current_day_count >= quota.queries_per_day:
            return False, "Daily quota exceeded"
        
        # Check result size
        if result_size > quota.max_result_size:
            return False, f"Result size exceeds limit ({quota.max_result_size})"
        
        return True, "OK"
    
    def increment_quota(self, user: User):
        """Increment user's query count"""
        quota = self.quotas.get(user.user_id)
        if quota:
            quota.current_hour_count += 1
            quota.current_day_count += 1
    
    def apply_row_level_security(
        self,
        user: User,
        query_filter: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Apply row-level security filters based on user attributes
        
        Injects additional filters based on:
        - Tenant ID (multi-tenancy isolation)
        - User attributes (region, department, etc.)
        - Data classification level
        
        Args:
            user: User
            query_filter: Original query filter
            
        Returns:
            Modified filter with security constraints
        """
        secure_filter = query_filter.copy()
        
        # Enforce tenant isolation
        if user.tenant_id:
            secure_filter["tenant_id"] = user.tenant_id
        
        # Enforce attribute-based filters
        if "region" in user.attributes:
            secure_filter["region"] = user.attributes["region"]
        
        if "department" in user.attributes:
            secure_filter["department"] = user.attributes["department"]
        
        # Enforce classification level
        if "clearance_level" in user.attributes:
            clearance = user.attributes["clearance_level"]
            secure_filter["classification"] = {"$lte": clearance}
        
        return secure_filter
    
    def _log_access(
        self,
        user_id: str,
        action: str,
        resource_type: ResourceType,
        resource_id: str,
        result: str,
        metadata: Optional[Dict[str, Any]] = None,
        query_details: Optional[Dict[str, Any]] = None
    ):
        """
        Log access attempt to audit trail
        
        Args:
            user_id: User ID
            action: Action performed
            resource_type: Resource type
            resource_id: Resource ID
            result: Result (success/failure/denied)
            metadata: Additional metadata
            query_details: Query details if applicable
        """
        log_entry = AuditLogEntry(
            log_id=hashlib.sha256(
                f"{user_id}:{action}:{time.time()}".encode()
            ).hexdigest()[:16],
            timestamp=datetime.now(),
            user_id=user_id,
            action=action,
            resource_type=resource_type,
            resource_id=resource_id,
            result=result,
            metadata=metadata or {},
            query_details=query_details
        )
        
        self.audit_log.append(log_entry)
        
        # In production: write to immutable storage
        # - Append-only database
        # - Write to SIEM (Splunk, ELK)
        # - Store in S3 with versioning
        # - Use blockchain for tamper-proof audit
    
    def query_audit_log(
        self,
        user_id: Optional[str] = None,
        action: Optional[str] = None,
        start_time: Optional[datetime] = None,
        end_time: Optional[datetime] = None,
        result: Optional[str] = None
    ) -> List[AuditLogEntry]:
        """
        Query audit log
        
        Args:
            user_id: Filter by user
            action: Filter by action
            start_time: Start time
            end_time: End time
            result: Filter by result
            
        Returns:
            Matching audit entries
        """
        results = self.audit_log
        
        if user_id:
            results = [e for e in results if e.user_id == user_id]
        
        if action:
            results = [e for e in results if e.action == action]
        
        if start_time:
            results = [e for e in results if e.timestamp >= start_time]
        
        if end_time:
            results = [e for e in results if e.timestamp <= end_time]
        
        if result:
            results = [e for e in results if e.result == result]
        
        return results

# Example usage
def access_control_example():
    """
    Demonstrate access control and audit trails
    """
    print("=== Access Control and Audit Trails ===")
    print()
    
    # Initialize access control engine
    ac = AccessControlEngine()
    print()
    
    # Create users with different roles
    admin_user = ac.create_user(
        username="alice_admin",
        email="alice@example.com",
        roles=["admin"],
        tenant_id="tenant_acme"
    )
    
    analyst_user = ac.create_user(
        username="bob_analyst",
        email="bob@example.com",
        roles=["analyst"],
        tenant_id="tenant_acme",
        attributes={"region": "US", "department": "marketing"}
    )
    
    service_user = ac.create_user(
        username="api_service",
        email="service@example.com",
        roles=["service"],
        tenant_id="tenant_acme"
    )
    
    print("Created users:")
    print(f"  Admin: {admin_user.username} (roles: {admin_user.roles})")
    print(f"  Analyst: {analyst_user.username} (roles: {analyst_user.roles})")
    print(f"  Service: {service_user.username} (roles: {service_user.roles})")
    print()
    
    # Test authorization
    print("Authorization tests:")
    
    # Admin can do everything
    can_delete = ac.authorize(
        admin_user,
        Permission.DELETE,
        ResourceType.EMBEDDING,
        "emb_123"
    )
    print(f"  Admin delete embedding: {can_delete}")
    
    # Analyst can query but not delete
    can_query = ac.authorize(
        analyst_user,
        Permission.QUERY,
        ResourceType.EMBEDDING,
        "emb_123"
    )
    can_delete = ac.authorize(
        analyst_user,
        Permission.DELETE,
        ResourceType.EMBEDDING,
        "emb_123"
    )
    print(f"  Analyst query embedding: {can_query}")
    print(f"  Analyst delete embedding: {can_delete}")
    
    # Service can only query
    can_query = ac.authorize(
        service_user,
        Permission.QUERY,
        ResourceType.EMBEDDING,
        "emb_123"
    )
    can_export = ac.authorize(
        service_user,
        Permission.EXPORT,
        ResourceType.EMBEDDING,
        "emb_123"
    )
    print(f"  Service query embedding: {can_query}")
    print(f"  Service export embedding: {can_export}")
    print()
    
    # Test quota enforcement
    print("Quota checks:")
    allowed, reason = ac.check_quota(analyst_user, result_size=50)
    print(f"  Analyst query (k=50): {allowed} ({reason})")
    
    allowed, reason = ac.check_quota(analyst_user, result_size=5000)
    print(f"  Analyst query (k=5000): {allowed} ({reason})")
    print()
    
    # Test row-level security
    print("Row-level security:")
    query_filter = {"category": "products"}
    secure_filter = ac.apply_row_level_security(analyst_user, query_filter)
    print(f"  Original filter: {query_filter}")
    print(f"  Secure filter: {secure_filter}")
    print()
    
    # Query audit log
    print("Audit log (last 5 entries):")
    recent_logs = ac.query_audit_log()[-5:]
    for log in recent_logs:
        print(f"  [{log.timestamp.strftime('%H:%M:%S')}] "
              f"{log.user_id[:8]}... {log.action} "
              f"{log.resource_type.value}:{log.resource_id} -> {log.result}")

if __name__ == "__main__":
    access_control_example()
```

:::{.callout-warning}
## Production Access Control

**Authentication Methods**:
- **API Keys**: Simple, suitable for service-to-service
  - Generate cryptographically random keys (32+ bytes)
  - Store hashed, never plaintext
  - Support rotation without downtime

- **OAuth 2.0 / JWT**: Standard for user authentication
  - Verify token signature (RS256, ES256)
  - Check expiration (exp claim)
  - Validate issuer and audience
  - Use short-lived tokens (15-60 minutes)

- **Mutual TLS**: Strongest for service authentication
  - Client certificate verification
  - Certificate pinning
  - Automatic rotation

**Authorization Best Practices**:
- Start with least privilege
- Use role hierarchy (inherit permissions)
- Implement deny policies (override allows)
- Cache authorization decisions (with TTL)
- Audit failed authorization attempts

**Audit Log Requirements**:
- Immutable storage (append-only)
- Tamper-proof (cryptographic hashes, blockchain)
- Long retention (7 years for HIPAA)
- Searchable and exportable
- Automated alerting on suspicious patterns
:::

:::{.callout-tip}
## Compliance Considerations

**SOC 2 Requirements**:
- Logical access controls
- Authentication and authorization
- Audit logging and monitoring
- Incident response procedures
- Annual penetration testing

**HIPAA Requirements**:
- Unique user identification
- Automatic logoff (session timeout)
- Encryption of ePHI
- Audit controls (access logs)
- Integrity controls (tamper detection)

**PCI-DSS Requirements**:
- Two-factor authentication for admin
- Unique ID per user
- Audit trail for all access to cardholder data
- Log retention (1 year online, 3 years archived)
- Quarterly log review

**GDPR Considerations**:
- Log personal data access
- Support data subject access requests
- Implement right to be forgotten
- Document data processing activities
- Report breaches within 72 hours
:::
## GDPR and Data Sovereignty Compliance

Embedding systems processing personal data must comply with data protection regulations—GDPR requires data minimization, purpose limitation, user consent, right to access, right to deletion, and data portability. **GDPR and data sovereignty compliance** implements technical measures for regulatory compliance: data residency controls ensuring embeddings stay in required jurisdictions, consent management tracking lawful basis for processing, right-to-deletion workflows removing user data from embeddings and training sets, data portability enabling export in machine-readable formats, privacy impact assessments documenting risks and mitigations, and breach notification procedures detecting and reporting incidents—enabling legal deployment across EU, California (CCPA), Brazil (LGPD), and other jurisdictions with comprehensive data protection laws.

### The Regulatory Compliance Challenge

Embedding systems face regulatory requirements:

- **Data residency**: Keep EU citizens' data in EU datacenters
- **Lawful basis**: Document consent, contract, or legitimate interest
- **Purpose limitation**: Use data only for stated purposes
- **Data minimization**: Collect and retain minimum necessary data
- **Right to access**: Provide copy of user's data on request
- **Right to deletion**: Remove user data from all systems
- **Right to portability**: Export data in machine-readable format
- **Breach notification**: Detect and report incidents within 72 hours
- **Data protection by design**: Build privacy into system architecture
- **Privacy impact assessment**: Document risks for high-risk processing

**Compliance approach**: Implement geographic data partitioning for residency, maintain consent records and privacy policies, build deletion workflows that remove data from embeddings and indexes, provide data export APIs for portability, conduct privacy impact assessments before deployment, implement breach detection and notification procedures, and document all data processing activities.

```python
"""
GDPR and Data Sovereignty Compliance

Architecture:
1. Data residency: Geographic partitioning of embeddings
2. Consent management: Track lawful basis for processing
3. Deletion workflows: Remove user data from system
4. Export APIs: Provide data portability
5. Breach detection: Monitor for security incidents
6. Privacy documentation: Maintain compliance records

Components:
- Geographic partitions: EU, US, APAC data silos
- Consent database: Track user permissions
- Deletion queue: Asynchronous data removal
- Export service: Generate portable data packages
- Breach monitor: Detect anomalous access
- Compliance dashboard: Track regulatory status

Legal requirements:
- GDPR: EU data protection regulation
- CCPA: California Consumer Privacy Act
- LGPD: Brazilian data protection law
- PIPEDA: Canadian privacy law
- PDPA: Singapore/Thailand data protection

Performance targets:
- Data residency: 100% compliance with geo-fencing
- Right to access: <30 days response time (legal maximum)
- Right to deletion: <24 hours for removal request
- Breach notification: <72 hours detection and reporting
"""

import numpy as np
from typing import List, Dict, Optional, Tuple, Set, Any
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from enum import Enum
import json
import hashlib

class DataRegion(Enum):
    """Data residency regions"""
    EU = "eu"  # European Union
    US = "us"  # United States
    APAC = "apac"  # Asia-Pacific
    LATAM = "latam"  # Latin America
    MEA = "mea"  # Middle East & Africa

class LegalBasis(Enum):
    """GDPR lawful basis for processing"""
    CONSENT = "consent"
    CONTRACT = "contract"
    LEGAL_OBLIGATION = "legal_obligation"
    VITAL_INTERESTS = "vital_interests"
    PUBLIC_TASK = "public_task"
    LEGITIMATE_INTEREST = "legitimate_interest"

class DataCategory(Enum):
    """Types of personal data"""
    BASIC_IDENTITY = "basic_identity"  # Name, email
    SENSITIVE = "sensitive"  # Health, race, religion
    FINANCIAL = "financial"  # Payment info, credit score
    BEHAVIORAL = "behavioral"  # Browsing, usage patterns
    BIOMETRIC = "biometric"  # Fingerprints, face scans
    LOCATION = "location"  # GPS coordinates, IP address

@dataclass
class ConsentRecord:
    """
    User consent for data processing
    
    Attributes:
        user_id: User identifier
        purpose: Purpose of processing
        legal_basis: Lawful basis for processing
        data_categories: Types of data covered
        granted_at: When consent was given
        expires_at: When consent expires (if applicable)
        withdrawn_at: When consent was withdrawn
        consent_text: Text shown to user
        consent_version: Version of privacy policy
    """
    user_id: str
    purpose: str
    legal_basis: LegalBasis
    data_categories: List[DataCategory]
    granted_at: datetime
    expires_at: Optional[datetime] = None
    withdrawn_at: Optional[datetime] = None
    consent_text: str = ""
    consent_version: str = "1.0"

@dataclass
class DeletionRequest:
    """
    User request for data deletion
    
    Attributes:
        request_id: Unique request identifier
        user_id: User requesting deletion
        requested_at: When request was made
        scope: What to delete (all, specific categories)
        status: Current status
        completed_at: When deletion completed
        verification: User identity verification details
        retention_exceptions: Data retained for legal reasons
    """
    request_id: str
    user_id: str
    requested_at: datetime
    scope: str = "all"  # "all", "embeddings", "training_data"
    status: str = "pending"  # "pending", "in_progress", "completed", "failed"
    completed_at: Optional[datetime] = None
    verification: Dict[str, Any] = field(default_factory=dict)
    retention_exceptions: List[str] = field(default_factory=list)

@dataclass
class ExportRequest:
    """
    User request for data export
    
    Attributes:
        request_id: Unique request identifier
        user_id: User requesting export
        requested_at: When request was made
        format: Export format (json, csv, xml)
        status: Current status
        download_url: URL to download export
        expires_at: When download link expires
    """
    request_id: str
    user_id: str
    requested_at: datetime
    format: str = "json"
    status: str = "pending"
    download_url: Optional[str] = None
    expires_at: Optional[datetime] = None

@dataclass
class BreachIncident:
    """
    Data breach incident record
    
    Attributes:
        incident_id: Unique incident identifier
        detected_at: When breach was detected
        breach_type: Type of breach
        affected_users: Number of affected users
        data_categories: Types of data exposed
        severity: Incident severity
        reported_at: When authorities were notified
        notification_sent: Whether users were notified
        mitigation: Steps taken to mitigate
    """
    incident_id: str
    detected_at: datetime
    breach_type: str
    affected_users: int
    data_categories: List[DataCategory]
    severity: str  # "low", "medium", "high", "critical"
    reported_at: Optional[datetime] = None
    notification_sent: bool = False
    mitigation: List[str] = field(default_factory=list)

class GDPRComplianceEngine:
    """
    GDPR and data sovereignty compliance engine
    
    Implements:
    - Data residency: Geographic partitioning
    - Consent management: Track legal basis
    - Right to access: Export user data
    - Right to deletion: Remove user data
    - Breach notification: Detect and report incidents
    """
    
    def __init__(self):
        # Data partitions by region
        self.data_partitions: Dict[DataRegion, Dict[str, Any]] = {
            region: {"embeddings": {}, "metadata": {}}
            for region in DataRegion
        }
        
        # Consent records
        self.consent_records: Dict[str, List[ConsentRecord]] = {}
        
        # Deletion requests
        self.deletion_requests: List[DeletionRequest] = []
        
        # Export requests
        self.export_requests: List[ExportRequest] = []
        
        # Breach incidents
        self.breach_incidents: List[BreachIncident] = []
        
        # Data processing activities (Article 30 records)
        self.processing_activities: List[Dict[str, Any]] = []
        
        print("GDPR Compliance Engine initialized")
    
    def determine_region(self, user_location: str) -> DataRegion:
        """
        Determine data residency region based on user location
        
        Args:
            user_location: User's country code (ISO 3166-1 alpha-2)
            
        Returns:
            Data residency region
        """
        # EU countries
        eu_countries = {
            "AT", "BE", "BG", "CY", "CZ", "DE", "DK", "EE", "ES", "FI",
            "FR", "GR", "HR", "HU", "IE", "IT", "LT", "LU", "LV", "MT",
            "NL", "PL", "PT", "RO", "SE", "SI", "SK"
        }
        
        # APAC countries
        apac_countries = {
            "AU", "CN", "HK", "ID", "IN", "JP", "KR", "MY", "NZ", "PH",
            "SG", "TH", "TW", "VN"
        }
        
        # LATAM countries
        latam_countries = {
            "AR", "BR", "CL", "CO", "MX", "PE"
        }
        
        if user_location in eu_countries:
            return DataRegion.EU
        elif user_location in apac_countries:
            return DataRegion.APAC
        elif user_location in latam_countries:
            return DataRegion.LATAM
        elif user_location == "US":
            return DataRegion.US
        else:
            return DataRegion.MEA
    
    def store_embedding(
        self,
        user_id: str,
        embedding: np.ndarray,
        user_location: str,
        metadata: Optional[Dict[str, Any]] = None
    ):
        """
        Store embedding in appropriate regional partition
        
        Args:
            user_id: User identifier
            embedding: Embedding vector
            user_location: User's location (country code)
            metadata: Additional metadata
        """
        # Determine region
        region = self.determine_region(user_location)
        
        # Check consent
        if not self.check_consent(user_id, "embedding_storage"):
            raise ValueError(
                f"User {user_id} has not consented to embedding storage"
            )
        
        # Store in regional partition
        self.data_partitions[region]["embeddings"][user_id] = embedding
        self.data_partitions[region]["metadata"][user_id] = {
            "stored_at": datetime.now(),
            "location": user_location,
            "metadata": metadata or {}
        }
        
        print(f"Stored embedding for {user_id} in {region.value} region")
    
    def record_consent(
        self,
        user_id: str,
        purpose: str,
        legal_basis: LegalBasis,
        data_categories: List[DataCategory],
        consent_text: str,
        expires_in_days: Optional[int] = None
    ) -> ConsentRecord:
        """
        Record user consent for data processing
        
        Args:
            user_id: User identifier
            purpose: Purpose of processing
            legal_basis: Legal basis for processing
            data_categories: Types of data
            consent_text: Text shown to user
            expires_in_days: Consent expiration (days)
            
        Returns:
            Consent record
        """
        expires_at = None
        if expires_in_days:
            expires_at = datetime.now() + timedelta(days=expires_in_days)
        
        consent = ConsentRecord(
            user_id=user_id,
            purpose=purpose,
            legal_basis=legal_basis,
            data_categories=data_categories,
            granted_at=datetime.now(),
            expires_at=expires_at,
            consent_text=consent_text
        )
        
        if user_id not in self.consent_records:
            self.consent_records[user_id] = []
        
        self.consent_records[user_id].append(consent)
        
        print(f"Recorded consent for {user_id}: {purpose}")
        return consent
    
    def check_consent(
        self,
        user_id: str,
        purpose: str
    ) -> bool:
        """
        Check if user has valid consent for purpose
        
        Args:
            user_id: User identifier
            purpose: Purpose to check
            
        Returns:
            True if consent exists and is valid
        """
        consents = self.consent_records.get(user_id, [])
        
        for consent in consents:
            # Check if consent covers purpose
            if consent.purpose != purpose:
                continue
            
            # Check if withdrawn
            if consent.withdrawn_at:
                continue
            
            # Check if expired
            if consent.expires_at and datetime.now() > consent.expires_at:
                continue
            
            return True
        
        return False
    
    def withdraw_consent(
        self,
        user_id: str,
        purpose: str
    ):
        """
        Withdraw user consent for purpose
        
        Args:
            user_id: User identifier
            purpose: Purpose to withdraw
        """
        consents = self.consent_records.get(user_id, [])
        
        for consent in consents:
            if consent.purpose == purpose and not consent.withdrawn_at:
                consent.withdrawn_at = datetime.now()
                print(f"Withdrew consent for {user_id}: {purpose}")
    
    def request_deletion(
        self,
        user_id: str,
        scope: str = "all",
        verification: Optional[Dict[str, Any]] = None
    ) -> DeletionRequest:
        """
        Submit deletion request (GDPR Article 17)
        
        Steps:
        1. Verify user identity
        2. Create deletion request
        3. Process asynchronously
        4. Confirm completion to user
        
        Args:
            user_id: User identifier
            scope: What to delete
            verification: Identity verification details
            
        Returns:
            Deletion request
        """
        request_id = hashlib.sha256(
            f"{user_id}:{datetime.now().isoformat()}".encode()
        ).hexdigest()[:16]
        
        request = DeletionRequest(
            request_id=request_id,
            user_id=user_id,
            requested_at=datetime.now(),
            scope=scope,
            verification=verification or {}
        )
        
        self.deletion_requests.append(request)
        
        # Process deletion (simplified - would be async in production)
        self._process_deletion(request)
        
        print(f"Deletion request submitted: {request_id}")
        return request
    
    def _process_deletion(self, request: DeletionRequest):
        """
        Process deletion request
        
        Steps:
        1. Identify all data for user
        2. Check retention requirements
        3. Remove from embeddings and indexes
        4. Remove from training data
        5. Remove from backups (gradual)
        6. Update request status
        
        Args:
            request: Deletion request
        """
        request.status = "in_progress"
        user_id = request.user_id
        
        # Find user data across regions
        for region in DataRegion:
            if user_id in self.data_partitions[region]["embeddings"]:
                del self.data_partitions[region]["embeddings"][user_id]
                print(f"  Removed embedding from {region.value}")
            
            if user_id in self.data_partitions[region]["metadata"]:
                del self.data_partitions[region]["metadata"][user_id]
                print(f"  Removed metadata from {region.value}")
        
        # Remove consent records
        if user_id in self.consent_records:
            del self.consent_records[user_id]
            print(f"  Removed consent records")
        
        # Note: In production, also:
        # - Remove from vector indexes
        # - Remove from training datasets
        # - Schedule backup deletion
        # - Update analytics (aggregate only)
        
        request.status = "completed"
        request.completed_at = datetime.now()
        
        print(f"Deletion completed for {user_id}")
    
    def request_export(
        self,
        user_id: str,
        format: str = "json"
    ) -> ExportRequest:
        """
        Submit data export request (GDPR Article 20)
        
        Generate machine-readable export of user's data
        
        Args:
            user_id: User identifier
            format: Export format
            
        Returns:
            Export request
        """
        request_id = hashlib.sha256(
            f"{user_id}:export:{datetime.now().isoformat()}".encode()
        ).hexdigest()[:16]
        
        request = ExportRequest(
            request_id=request_id,
            user_id=user_id,
            requested_at=datetime.now(),
            format=format
        )
        
        self.export_requests.append(request)
        
        # Generate export (simplified - would be async in production)
        self._generate_export(request)
        
        print(f"Export request submitted: {request_id}")
        return request
    
    def _generate_export(self, request: ExportRequest):
        """
        Generate data export for user
        
        Include:
        - Personal information
        - Embeddings
        - Processing history
        - Consent records
        
        Args:
            request: Export request
        """
        request.status = "in_progress"
        user_id = request.user_id
        
        export_data = {
            "user_id": user_id,
            "export_date": datetime.now().isoformat(),
            "embeddings": {},
            "consents": [],
            "processing_history": []
        }
        
        # Collect embeddings from all regions
        for region in DataRegion:
            if user_id in self.data_partitions[region]["embeddings"]:
                embedding = self.data_partitions[region]["embeddings"][user_id]
                metadata = self.data_partitions[region]["metadata"][user_id]
                
                export_data["embeddings"][region.value] = {
                    "vector": embedding.tolist(),
                    "stored_at": metadata["stored_at"].isoformat(),
                    "metadata": metadata.get("metadata", {})
                }
        
        # Include consent records
        consents = self.consent_records.get(user_id, [])
        for consent in consents:
            export_data["consents"].append({
                "purpose": consent.purpose,
                "legal_basis": consent.legal_basis.value,
                "granted_at": consent.granted_at.isoformat(),
                "withdrawn_at": consent.withdrawn_at.isoformat() if consent.withdrawn_at else None
            })
        
        # Generate download URL (simplified)
        request.download_url = f"https://exports.example.com/{request.request_id}"
        request.expires_at = datetime.now() + timedelta(days=7)
        request.status = "completed"
        
        print(f"Export generated for {user_id}: {request.download_url}")
    
    def report_breach(
        self,
        breach_type: str,
        affected_users: int,
        data_categories: List[DataCategory],
        severity: str,
        description: str
    ) -> BreachIncident:
        """
        Report data breach incident
        
        GDPR Article 33: Report to supervisory authority within 72 hours
        GDPR Article 34: Notify affected individuals if high risk
        
        Args:
            breach_type: Type of breach
            affected_users: Number affected
            data_categories: Types of data exposed
            severity: Severity level
            description: Incident description
            
        Returns:
            Breach incident record
        """
        incident_id = hashlib.sha256(
            f"breach:{datetime.now().isoformat()}".encode()
        ).hexdigest()[:16]
        
        incident = BreachIncident(
            incident_id=incident_id,
            detected_at=datetime.now(),
            breach_type=breach_type,
            affected_users=affected_users,
            data_categories=data_categories,
            severity=severity
        )
        
        self.breach_incidents.append(incident)
        
        print(f"Breach incident reported: {incident_id}")
        print(f"  Type: {breach_type}")
        print(f"  Affected users: {affected_users}")
        print(f"  Severity: {severity}")
        
        # Check if notification required
        if severity in ["high", "critical"]:
            print(f"  WARNING: High-risk breach, notify supervisory authority within 72 hours")
            print(f"  WARNING: Notify affected users without undue delay")
        
        return incident
    
    def generate_compliance_report(self) -> Dict[str, Any]:
        """
        Generate compliance status report
        
        Returns:
            Compliance metrics and status
        """
        report = {
            "generated_at": datetime.now().isoformat(),
            "data_residency": {},
            "consent_status": {},
            "deletion_requests": {},
            "export_requests": {},
            "breach_incidents": len(self.breach_incidents)
        }
        
        # Data residency breakdown
        for region in DataRegion:
            report["data_residency"][region.value] = {
                "embeddings": len(self.data_partitions[region]["embeddings"]),
                "metadata": len(self.data_partitions[region]["metadata"])
            }
        
        # Consent statistics
        total_consents = sum(len(c) for c in self.consent_records.values())
        active_consents = sum(
            1 for consents in self.consent_records.values()
            for c in consents
            if not c.withdrawn_at and (not c.expires_at or c.expires_at > datetime.now())
        )
        report["consent_status"] = {
            "total": total_consents,
            "active": active_consents,
            "expired_or_withdrawn": total_consents - active_consents
        }
        
        # Deletion requests
        report["deletion_requests"] = {
            "total": len(self.deletion_requests),
            "completed": sum(1 for r in self.deletion_requests if r.status == "completed"),
            "pending": sum(1 for r in self.deletion_requests if r.status == "pending")
        }
        
        # Export requests
        report["export_requests"] = {
            "total": len(self.export_requests),
            "completed": sum(1 for r in self.export_requests if r.status == "completed"),
            "pending": sum(1 for r in self.export_requests if r.status == "pending")
        }
        
        return report

# Example usage
def gdpr_compliance_example():
    """
    Demonstrate GDPR compliance features
    """
    print("=== GDPR and Data Sovereignty Compliance ===")
    print()
    
    # Initialize compliance engine
    gdpr = GDPRComplianceEngine()
    print()
    
    # Record consent for EU user
    print("1. Recording consent:")
    consent = gdpr.record_consent(
        user_id="user_eu_123",
        purpose="embedding_storage",
        legal_basis=LegalBasis.CONSENT,
        data_categories=[DataCategory.BEHAVIORAL, DataCategory.BASIC_IDENTITY],
        consent_text="I agree to the processing of my data for recommendation purposes",
        expires_in_days=365
    )
    print()
    
    # Store embedding with data residency
    print("2. Storing embedding with data residency:")
    embedding = np.random.randn(768).astype(np.float32)
    gdpr.store_embedding(
        user_id="user_eu_123",
        embedding=embedding,
        user_location="DE",  # Germany
        metadata={"source": "web_app"}
    )
    print()
    
    # Request data export (Article 20)
    print("3. Data portability - user requests export:")
    export_request = gdpr.request_export(user_id="user_eu_123", format="json")
    print()
    
    # Withdraw consent
    print("4. User withdraws consent:")
    gdpr.withdraw_consent(user_id="user_eu_123", purpose="embedding_storage")
    print()
    
    # Request deletion (Article 17)
    print("5. Right to deletion - user requests removal:")
    deletion_request = gdpr.request_deletion(
        user_id="user_eu_123",
        scope="all",
        verification={"method": "email", "verified": True}
    )
    print()
    
    # Report data breach
    print("6. Breach notification:")
    breach = gdpr.report_breach(
        breach_type="unauthorized_access",
        affected_users=1,
        data_categories=[DataCategory.BEHAVIORAL],
        severity="medium",
        description="Unauthorized API access detected"
    )
    print()
    
    # Generate compliance report
    print("7. Compliance status report:")
    report = gdpr.generate_compliance_report()
    print(json.dumps(report, indent=2))

if __name__ == "__main__":
    gdpr_compliance_example()
```

:::{.callout-warning}
## GDPR Compliance Requirements

**Must-Have Technical Measures**:
- **Data residency**: EU data must stay in EU
- **Consent management**: Track legal basis, allow withdrawal
- **Deletion workflows**: Complete removal within reasonable time
- **Export capability**: Machine-readable format (JSON, CSV, XML)
- **Breach detection**: Identify incidents within hours
- **Audit trails**: Complete processing history

**Common Compliance Pitfalls**:
- Forgetting backups: Deletion must include backups
- Training data: Removing from training sets is hard
- Third parties: Ensure processors are GDPR-compliant
- Consent fatigue: Don't ask for consent too frequently
- Dark patterns: Don't make withdrawal harder than consent
- Incomplete deletion: Check logs, analytics, caches

**Penalties**:
- Up to €20M or 4% of global annual revenue
- Fines for: No legal basis, inadequate security, no breach notification
- Reputation damage, loss of customer trust
:::

:::{.callout-tip}
## Practical GDPR Implementation

**Data Residency**:
- Use cloud providers with regional guarantees
  - AWS: Specific regions (eu-west-1, eu-central-1)
  - GCP: Regional resources
  - Azure: Geography-specific data residency
- Implement geo-fencing at application level
- Regular audits of data location

**Deletion Implementation**:
- Asynchronous processing (don't block user)
- Multi-stage: Active data → Archives → Backups
- Track deletion status, notify user on completion
- Consider "soft delete" with delayed hard delete

**Consent Management**:
- Granular consent (separate purposes)
- Easy withdrawal (one-click)
- Consent refresh (annual reminder)
- Log all consent changes with timestamps

**Breach Response Plan**:
1. Detection: Automated anomaly detection
2. Assessment: Severity, scope, affected users (1 hour)
3. Containment: Stop the breach, secure systems (2 hours)
4. Notification: Supervisory authority (72 hours)
5. User notification: High-risk breaches (no undue delay)
6. Documentation: Complete incident report

**Documentation**:
- Privacy policy (user-facing)
- Data processing activities (Article 30)
- Privacy impact assessment (high-risk processing)
- Data protection by design documentation
- Vendor data processing agreements
:::

## Key Takeaways

- **Embedding encryption enables computation on sensitive data with practical overhead**: Homomorphic encryption (CKKS) provides cryptographic security for similarity search with 10-100× performance overhead suitable for high-security scenarios, Intel SGX secure enclaves offer 2-5× overhead enabling production deployment on confidential data, hybrid approaches combine techniques adapting to deployment constraints, and key management infrastructure ensures secure key distribution and rotation—enabling healthcare, financial, and government deployments that were previously impossible

- **Privacy-preserving similarity search prevents information leakage while maintaining utility**: Differentially private LSH adds calibrated noise to hash functions achieving ε≤1.0 privacy with 10-20% recall loss, secure multi-party computation distributes queries across data silos preventing single-party data exposure with 5-50× overhead, private information retrieval enables queries without revealing query content using homomorphic encryption, and access pattern hiding through oblivious RAM prevents correlation attacks—enabling public-facing APIs and cross-organizational collaboration

- **Differential privacy for embeddings provides formal guarantees for training and serving**: DP-SGD adds Gaussian noise during training achieving (ε,δ)-differential privacy with 10-20% utility loss at ε=1.0, gradient clipping bounds per-example influence preventing training data memorization, privacy accounting tracks cumulative privacy loss across queries and model releases, PATE enables student model training without direct privacy cost when public data available, and privacy-utility trade-offs require careful hyperparameter tuning balancing regulatory compliance with model performance

- **Access control and audit trails ensure secure multi-tenant deployment and compliance**: Role-based access control (RBAC) with attribute-based extensions (ABAC) enables fine-grained permissions, row-level security filtering prevents cross-tenant data leakage, rate limiting and quota management prevent abuse and ensure fair resource allocation, comprehensive audit logging with immutable storage satisfies regulatory requirements, and real-time anomaly detection identifies suspicious access patterns before damage occurs—achieving SOC2, HIPAA, and PCI-DSS compliance

- **GDPR and data sovereignty compliance enables legal deployment across jurisdictions**: Geographic data partitioning ensures EU data stays in EU datacenters satisfying residency requirements, consent management tracks lawful basis for processing with granular purpose-specific consent and easy withdrawal, right-to-deletion workflows remove user data from embeddings and training sets within required timeframes, data portability exports provide machine-readable data packages, breach notification procedures detect and report incidents within 72 hours, and comprehensive documentation satisfies privacy impact assessment and Article 30 requirements

- **Security and privacy are system-wide requirements not afterthoughts**: No single technique provides complete protection—production systems layer encryption (at rest and in transit), secure computation (SGX/CKKS), differential privacy (formal guarantees), access control (authentication and authorization), and compliance workflows (GDPR/CCPA)—each addressing different threat models and regulatory requirements while maintaining 80-95% of unencrypted system performance

- **Regulatory landscape evolves requiring adaptable compliance architecture**: GDPR (EU), CCPA (California), LGPD (Brazil), PIPEDA (Canada), and PDPA (Singapore/Thailand) have overlapping but distinct requirements, new regulations emerge regularly (e.g., AI Act, state privacy laws), enforcement increases with multi-million dollar fines, and technical measures must adapt without complete system redesign—necessitating modular compliance architecture with configurable policies, regular legal review, and proactive monitoring of regulatory developments

## Looking Ahead

Chapter 25 establishes comprehensive monitoring and observability practices: embedding quality metrics that detect model degradation and concept drift, performance monitoring dashboards tracking latency and throughput across deployment, alerting on embedding drift when semantic space shifts require model retraining, cost tracking and optimization ensuring efficient resource utilization, and user experience analytics measuring how embedding quality impacts business metrics.

## Further Reading

### Homomorphic Encryption and Secure Computation
- Cheon, Jung Hee, et al. (2017). "Homomorphic Encryption for Arithmetic of Approximate Numbers." Advances in Cryptology – ASIACRYPT.
- Smart, Nigel P., and Frederik Vercauteren (2014). "Fully Homomorphic SIMD Operations." Designs, Codes and Cryptography.
- Costan, Victor, and Srinivas Devadas (2016). "Intel SGX Explained." IACR Cryptology ePrint Archive.
- Hunt, Tyler, et al. (2018). "Ryoan: A Distributed Sandbox for Untrusted Computation on Secret Data." ACM Transactions on Computer Systems.

### Privacy-Preserving Machine Learning
- Dwork, Cynthia, and Aaron Roth (2014). "The Algorithmic Foundations of Differential Privacy." Foundations and Trends in Theoretical Computer Science.
- Abadi, Martin, et al. (2016). "Deep Learning with Differential Privacy." ACM Conference on Computer and Communications Security.
- Papernot, Nicolas, et al. (2017). "Scalable Private Learning with PATE." International Conference on Learning Representations.
- McMahan, Brendan, et al. (2017). "Communication-Efficient Learning of Deep Networks from Decentralized Data." Artificial Intelligence and Statistics.

### Differential Privacy
- Dwork, Cynthia, Frank McSherry, Kobbi Nissim, and Adam Smith (2006). "Calibrating Noise to Sensitivity in Private Data Analysis." Theory of Cryptography Conference.
- Mironov, Ilya (2017). "Rényi Differential Privacy." IEEE Computer Security Foundations Symposium.
- Bun, Mark, and Thomas Steinke (2016). "Concentrated Differential Privacy: Simplifications, Extensions, and Lower Bounds." Theory of Cryptography Conference.
- Kairouz, Peter, Sewoong Oh, and Pramod Viswanath (2015). "The Composition Theorem for Differential Privacy." International Conference on Machine Learning.

### Access Control and Auditing
- Sandhu, Ravi S., et al. (1996). "Role-Based Access Control Models." Computer.
- Hu, Vincent C., et al. (2014). "Guide to Attribute Based Access Control (ABAC) Definition and Considerations." NIST Special Publication 800-162.
- Schneier, Bruce (2015). "Data and Goliath: The Hidden Battles to Collect Your Data and Control Your World." W. W. Norton & Company.
- Kelley, Patrick Gage, et al. (2009). "A Conundrum of Permissions: Installing Applications on an Android Smartphone." International Conference on Financial Cryptography and Data Security.

### GDPR and Data Protection
- Voigt, Paul, and Axel von dem Bussche (2017). "The EU General Data Protection Regulation (GDPR): A Practical Guide." Springer.
- European Union Agency for Cybersecurity (2020). "Guidelines on Data Protection Impact Assessment (DPIA)." Article 29 Working Party.
- Information Commissioner's Office (2018). "Guide to the General Data Protection Regulation (GDPR)." ICO.
- Solove, Daniel J., and Paul M. Schwartz (2021). "Information Privacy Law." Wolters Kluwer.

### Privacy in Practice
- Nissim, Kobbi, et al. (2017). "Bridging the Gap Between Computer Science and Legal Approaches to Privacy." Harvard Journal of Law & Technology.
- Veale, Michael, Reuben Binns, and Lilian Edwards (2018). "Algorithms that Remember: Model Inversion Attacks and Data Protection Law." Philosophical Transactions of the Royal Society A.
- Wachter, Sandra, Brent Mittelstadt, and Chris Russell (2021). "Why Fairness Cannot Be Automated: Bridging the Gap Between EU Non-Discrimination Law and AI." Computer Law & Security Review.
- Narayanan, Arvind, and Vitaly Shmatikov (2008). "Robust De-anonymization of Large Sparse Datasets." IEEE Symposium on Security and Privacy.

### Security Best Practices
- OWASP (2021). "OWASP Top Ten Project." Open Web Application Security Project.
- Cloud Security Alliance (2020). "Security Guidance for Critical Areas of Focus in Cloud Computing." CSA.
- NIST (2018). "Framework for Improving Critical Infrastructure Cybersecurity." National Institute of Standards and Technology.
- ISO/IEC (2013). "ISO/IEC 27001:2013 Information Security Management." International Organization for Standardization.
