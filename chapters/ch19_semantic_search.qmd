# Semantic Search Beyond Text {#sec-semantic-search}

:::{.callout-note}
## Chapter Overview
Semantic search transcends traditional keyword matching, enabling organizations to find meaning across modalities: images, code, scientific papers, media assets, and interconnected knowledge. This chapter explores multi-modal semantic search architectures that unify text, vision, and audio embeddings for cross-modal retrieval, code search systems that understand program semantics beyond syntax for software intelligence, scientific literature and patent search at research scale with citation networks and entity resolution, media and content discovery engines that match visual style and creative intent, and enterprise knowledge graphs that connect entities through learned embeddings. These capabilities transform search from keyword matching to semantic understanding, unlocking insights hidden in unstructured data across modalities.
:::

After mastering RAG for text (@sec-rag-at-scale), the next frontier is **semantic search beyond text**. Traditional search operates on keywords: match query terms to document terms, rank by term frequency. This works for text but fails for images (no keywords), code (syntax vs semantics), scientific literature (citation networks matter), media (style and composition), and knowledge graphs (relationships matter more than attributes). **Embedding-based semantic search** solves these challenges by representing all modalities—text, images, code, papers, media, entities—in a unified vector space where similarity reflects semantic meaning, not surface features.

## Multi-Modal Semantic Search

Multi-modal search finds content across different modalities: search images with text queries ("sunset over mountains"), search text with image queries (upload photo, find similar articles), search videos with audio queries (hum a melody, find the song). **Multi-modal embeddings** map different modalities into a shared vector space where cross-modal similarity is meaningful.

### The Multi-Modal Challenge

Each modality has unique characteristics:

- **Text**: Sequential, compositional, high-dimensional vocabulary
- **Images**: Spatial, hierarchical features (pixels → edges → objects)
- **Audio**: Temporal, frequency-based, variable length
- **Video**: Spatial-temporal, combines images + audio + text (captions)

**Challenge**: Map these heterogeneous modalities into a unified space where "cat" (text) is near cat images (vision) and "meow" sounds (audio).

```python
{{< include /code_examples/ch19_semantic_search/from.py >}}
```

:::{.callout-tip}
## Multi-Modal Search Best Practices

**Architecture:**

- **Separate encoders**: Train modality-specific encoders (don't share weights)
- **Shared embedding space**: Project to common space (512-1024 dim)
- **Contrastive training**: Align modalities via paired data (image-caption pairs) (see @sec-contrastive-learning)
- **Late fusion**: Combine scores at query time (more flexible than early fusion)

**Training data:**

- **Paired examples**: Need (text, image) or (audio, video) pairs
- **Web-scale data**: LAION-5B (5 billion image-text pairs)
- **Data quality**: Filter low-quality pairs (CLIP score, aesthetic score)
- **Augmentation**: Augment images/audio, not text (preserve semantics)

**Performance:**

- **Pre-encode documents**: Encode offline, store embeddings
- **Per-modality indices**: Separate HNSW index per modality
- **GPU inference**: Batch encode queries on GPU
- **Caching**: Cache popular queries (50% of queries are repeats)
:::

:::{.callout-warning}
## Cross-Modal Alignment Challenges

Multi-modal embeddings require **aligned training data**:

- Image-text pairs must be semantically related
- Noisy pairs degrade alignment (web data often mismatched)
- Rare concepts harder to align (less training data)

**Mitigation strategies:**

- Filter pairs by CLIP score (cosine similarity threshold)
- Use curated datasets for critical domains
- Hard negative mining (find hard mismatches to learn from) (see @sec-contrastive-learning)
- Domain-specific fine-tuning (medical, legal, etc.)
:::

## Code Search and Software Intelligence

Code search finds functions, classes, and patterns in massive codebases—but traditional search fails because code semantics differ from syntax. **Semantic code search** uses embeddings to find code by intent ("sort a list"), not keywords, enabling software intelligence for code completion, bug detection, and API discovery.

### The Code Search Challenge

Code has unique properties:

- **Syntax vs semantics**: `list.sort()` and `sorted(list)` are syntactically different but semantically similar
- **Multiple representations**: Code, comments, docstrings, test cases all describe intent
- **Compositional**: Functions compose; understanding requires context
- **Polyglot**: Multiple languages (Python, Java, C++, JavaScript)

**Challenge**: Find code that **does X** (semantic intent), not code that **contains X** (keyword match).

```python
{{< include /code_examples/ch19_semantic_search/import.py >}}
```

:::{.callout-tip}
## Code Search Best Practices

**Training:**

- **Pre-training**: Use CodeBERT or GraphCodeBERT (pre-trained on GitHub)
- **Fine-tuning**: Fine-tune on domain-specific code (internal codebase) (see @sec-custom-embedding-strategies for guidance on when to fine-tune vs. train from scratch)
- **Data augmentation**: Rename variables, reformat code (preserve semantics)
- **Hard negatives**: Mine hard negatives (similar code, different semantics) (see @sec-contrastive-learning)

**Indexing:**

- **Function-level**: Index individual functions, not entire files
- **Deduplication**: Remove duplicate functions (common in forks)
- **Metadata**: Include docstrings, comments, test cases
- **Incremental**: Update index as new code is added (CI/CD integration)

**Search quality:**

- **Reranking**: Use cross-encoder to rerank top-100 results
- **Diversity**: Ensure diverse results (not all bubble sort variants)
- **Filtering**: Filter by language, library, recency
- **Personalization**: Rank by user's coding style and preferences
:::

## Scientific Literature and Patent Search

Scientific research produces millions of papers annually—PubMed has 35M+ articles, arXiv adds 200K/year, and patent offices hold 100M+ patents. **Semantic literature search** finds relevant research by understanding concepts, methods, and relationships, enabling discovery across citation networks and entity resolution for authors, institutions, and compounds.

### The Scientific Search Challenge

Scientific literature has unique characteristics:

- **Domain terminology**: Specialized vocabulary (medical, chemistry, physics)
- **Citation networks**: Papers cite related work (graph structure matters)
- **Multi-entity**: Authors, institutions, chemicals, genes (entity linking)
- **Temporal evolution**: Concepts evolve over time
- **Multimodal**: Text + figures + tables + equations

**Challenge**: Find **relevant research** (concept match), not **keyword match** (term frequency).

```python
{{< include /code_examples/ch19_semantic_search/from_1.py >}}
```

:::{.callout-tip}
## Scientific Search Best Practices

**Domain-specific embeddings:**

- **Pre-training**: Use SPECTER (citation-based), SciBERT (scientific text)
- **Fine-tuning**: Fine-tune on domain-specific corpora (biomedical, physics)
- **Multi-field**: Encode title + abstract + full text (weight by importance)
- **Citation context**: Include sentences that cite the paper

**Citation graph:**

- **Co-citation**: Papers cited together are related
- **Bibliographic coupling**: Papers citing the same work are related
- **PageRank**: Rank by citation graph centrality
- **Temporal weighting**: Recent citations matter more

**Entity linking:**

- **Named entity recognition**: Extract entities (chemicals, genes, diseases)
- **Entity disambiguation**: Link to knowledge base (PubChem, UniProt)
- **Relation extraction**: Extract relationships between entities
- **Entity embeddings**: Embed entities in same space as papers
:::

## Media and Content Discovery

Media assets—images, videos, audio—represent trillions of files across organizations. **Semantic media search** finds content by visual style, composition, audio characteristics, and creative intent, enabling discovery beyond metadata tagging and filename matching.

### The Media Discovery Challenge

Media has unique properties:

- **Visual style**: Color palette, composition, lighting
- **Creative intent**: Mood, emotion, message
- **Temporal dynamics**: Video and audio evolve over time
- **Quality variation**: Resolution, noise, compression artifacts
- **Massive scale**: Petabytes of media files

**Challenge**: Find **visually similar** or **stylistically related** media, not **keyword matches** on filenames.

```python
{{< include /code_examples/ch19_semantic_search/from_2.py >}}
```

:::{.callout-tip}
## Media Search Best Practices

**Visual features:**

- **Content embeddings**: Use CLIP, ResNet, or ViT for semantic content
- **Style embeddings**: Use Gram matrices or style-specific encoders
- **Multi-scale**: Extract features at multiple resolutions
- **Color histograms**: Supplement embeddings with color features

**Duplicate detection:**

- **Perceptual hashing**: pHash, dHash for near-duplicate detection
- **Hamming distance**: Fast comparison (XOR + popcount)
- **Clustering**: Group near-duplicates for review
- **Threshold tuning**: Balance false positives vs false negatives

**Performance:**

- **Pre-compute embeddings**: Encode assets offline during ingestion
- **GPU batching**: Batch encode 100-1000 images per GPU
- **Caching**: Cache embeddings in vector database
- **Progressive loading**: Show low-res previews while searching
:::

## Enterprise Knowledge Graphs

Enterprise knowledge graphs connect entities—customers, products, employees, documents—through relationships. **Embedding-based knowledge graphs** use learned embeddings to represent entities and relations, enabling link prediction, entity resolution, and graph-aware search that understands how entities relate.

### The Knowledge Graph Challenge

Traditional knowledge graphs use discrete representations (triples: subject-predicate-object). **Embedding-based graphs** represent entities and relations as vectors, enabling:

- **Link prediction**: Predict missing relationships
- **Entity resolution**: Merge duplicate entities
- **Multi-hop reasoning**: Answer complex queries across relationships
- **Similarity search**: Find similar entities by embeddings

**Challenge**: Learn embeddings that preserve graph structure and semantics.

```python
{{< include /code_examples/ch19_semantic_search/import_1.py >}}
```

:::{.callout-tip}
## Knowledge Graph Embedding Best Practices

**Model selection:**

- **TransE**: Simple, works well for 1-to-1 relations
- **DistMult**: Better for symmetric relations
- **ComplEx**: Handles asymmetric and inverse relations
- **RotatE**: State-of-the-art for complex relations

**Training:**

- **Negative sampling**: Sample false triples for contrastive learning
- **Hard negatives**: Mine hard negatives (plausible but false)
- **Regularization**: L2 regularization on embeddings
- **Batch training**: Use large batches (1000-10000 triples)

**Applications:**

- **Link prediction**: Predict missing relationships
- **Entity resolution**: Merge duplicate entities by embedding similarity
- **Graph completion**: Fill in missing edges
- **Multi-hop reasoning**: Answer complex queries (e.g., "customers who bought products similar to X")
:::

:::{.callout-warning}
## Graph Embedding Challenges

**Data quality:**

- Incomplete graphs (missing edges) degrade embeddings
- Noisy relations (incorrect edges) poison training
- Entity disambiguation (same name, different entities)

**Scalability:**

- Billion-entity graphs require distributed training
- Full graph materialization doesn't fit in memory
- Subgraph sampling required for large graphs

**Interpretability:**

- Embeddings are black boxes (hard to debug)
- Relation semantics may not align with vector operations
- Need attribution methods to explain predictions
:::

## Key Takeaways

- **Multi-modal search unifies text, images, audio, and video in shared embedding spaces**: Cross-modal retrieval (query text, retrieve images) requires contrastive training on paired data and separate per-modality encoders that project to a common vector space

- **Code search transcends syntax to find code by semantic intent**: Semantic code embeddings trained on code-docstring pairs enable natural language queries like "sort a list" to find relevant implementations across languages and coding styles

- **Scientific literature search leverages citation networks and domain embeddings**: SPECTER and SciBERT embeddings combined with citation graph analysis (co-citation, bibliographic coupling) enable discovery of related research beyond keyword matching

- **Media discovery finds visual similarity and creative style**: Separate embeddings for content (semantic meaning) and style (color, composition, texture) enable both "find similar images" and "find images with similar aesthetic" use cases

- **Knowledge graph embeddings enable link prediction and entity resolution**: TransE and related models represent entities and relations as vectors, enabling prediction of missing relationships, merging of duplicate entities, and graph-aware similarity search

- **Semantic search beyond text requires domain-specific encoders**: General-purpose embeddings (CLIP, BERT) provide baseline capabilities, but production systems need fine-tuning on domain-specific data (code repositories, scientific papers, media assets)—see @sec-custom-embedding-strategies for a decision framework on choosing the right level of customization

- **Search quality depends on training data quality**: Multi-modal alignment requires clean paired data, code search needs accurate code-docstring pairs, and knowledge graphs need high-quality relationship annotations

## Looking Ahead

Part IV (Advanced Applications) continues with Chapter 15, which revolutionizes recommendation systems with embeddings: embedding-based collaborative filtering that scales to billions of users and items, cold start solutions using content embeddings and meta-learning, real-time personalization with streaming embeddings, diversity and fairness constraints that prevent filter bubbles, and cross-domain recommendation transfer that leverages embeddings across product categories and platforms.

## Further Reading

### Multi-Modal Learning
- Radford, Alec, et al. (2021). "Learning Transferable Visual Models From Natural Language Supervision (CLIP)." ICML.
- Jia, Chao, et al. (2021). "Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision (ALIGN)." ICML.
- Girdhar, Rohit, et al. (2023). "ImageBind: One Embedding Space To Bind Them All." CVPR.
- Baltrusaitis, Tadas, et al. (2019). "Multimodal Machine Learning: A Survey and Taxonomy." IEEE TPAMI.

### Code Search and Software Intelligence
- Feng, Zhangyin, et al. (2020). "CodeBERT: A Pre-Trained Model for Programming and Natural Languages." EMNLP.
- Guo, Daya, et al. (2021). "GraphCodeBERT: Pre-training Code Representations with Data Flow." ICLR.
- Husain, Hamel, et al. (2019). "CodeSearchNet Challenge: Evaluating the State of Semantic Code Search." arXiv.
- Chen, Mark, et al. (2021). "Evaluating Large Language Models Trained on Code (Codex)." arXiv.

### Scientific Literature Search
- Cohan, Arman, et al. (2020). "SPECTER: Document-level Representation Learning using Citation-informed Transformers." ACL.
- Beltagy, Iz, et al. (2019). "SciBERT: A Pretrained Language Model for Scientific Text." EMNLP.
- Lo, Kyle, et al. (2020). "S2ORC: The Semantic Scholar Open Research Corpus." ACL.
- Priem, Jason, et al. (2022). "OpenAlex: A Fully-Open Index of Scholarly Works, Authors, Venues, and Concepts." arXiv.

### Media and Content Discovery
- Gatys, Leon A., et al. (2016). "Image Style Transfer Using Convolutional Neural Networks." CVPR.
- Johnson, Justin, et al. (2016). "Perceptual Losses for Real-Time Style Transfer and Super-Resolution." ECCV.
- Simonyan, Karen, and Andrew Zisserman (2014). "Very Deep Convolutional Networks for Large-Scale Image Recognition." ICLR.
- Dosovitskiy, Alexey, et al. (2021). "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (ViT)." ICLR.

### Knowledge Graph Embeddings
- Bordes, Antoine, et al. (2013). "Translating Embeddings for Modeling Multi-relational Data (TransE)." NeurIPS.
- Yang, Bishan, et al. (2015). "Embedding Entities and Relations for Learning and Inference in Knowledge Bases (DistMult)." ICLR.
- Trouillon, Théo, et al. (2016). "Complex Embeddings for Simple Link Prediction (ComplEx)." ICML.
- Sun, Zhiqing, et al. (2019). "RotatE: Knowledge Graph Embedding by Relational Rotation in Complex Space." ICLR.
- Wang, Quan, et al. (2017). "Knowledge Graph Embedding: A Survey of Approaches and Applications." IEEE TKDE.
