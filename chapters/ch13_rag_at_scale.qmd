# Retrieval-Augmented Generation (RAG) at Scale {#sec-rag-at-scale}

:::{.callout-note}
## Chapter Overview
Retrieval-Augmented Generation combines the power of embedding-based retrieval with large language model generation, enabling LLMs to answer questions grounded in enterprise knowledge rather than relying solely on parametric memory. This chapter explores production RAG systems at scale: enterprise architecture patterns that handle billion-document corpora, context window optimization strategies that maximize information density while respecting token limits, multi-stage retrieval pipelines that balance recall and precision across filtering and reranking stages, evaluation frameworks that measure end-to-end quality beyond simple metrics, and techniques for handling contradictory information when sources disagree. These patterns enable RAG systems that serve millions of users with accurate, attributable, up-to-date responses.
:::

With robust data engineering in place (@sec-data-engineering), the foundation exists to build advanced applications that leverage embeddings at scale. **Retrieval-Augmented Generation (RAG)** has emerged as the dominant pattern for grounding large language models in enterprise knowledge. Rather than fine-tuning models on proprietary data (expensive, slow to update, risk of hallucination), RAG retrieves relevant context from vector databases and includes it in the LLM prompt. This approach enables accurate answers over billion-document corpora, maintains attribution to sources, updates knowledge in real-time, and scales to trillion-row datasets—all critical requirements for enterprise deployment.

## Enterprise RAG Architecture Patterns

Production RAG systems serve thousands of concurrent users querying billion-document knowledge bases with sub-second latency and high accuracy. **Enterprise RAG architectures** decompose this challenge into specialized components: query understanding, retrieval, reranking, context assembly, generation, and response validation. Each component must scale independently while maintaining end-to-end quality.

### The RAG Pipeline

A complete RAG system comprises six stages:

1. **Query Understanding**: Parse user intent, extract entities, expand with synonyms
2. **Retrieval**: Vector search for top-k relevant documents (k=100-1000)
3. **Reranking**: Reorder results by relevance using cross-encoder (reduce to k=5-20)
4. **Context Assembly**: Fit selected documents into context window
5. **Generation**: LLM generates response given query + context
6. **Validation**: Verify response accuracy, check for hallucinations

```python
"""
Enterprise RAG System Architecture

Components:
1. Query Processor: Intent classification, entity extraction, query expansion
2. Retrieval Engine: Vector search across billion-document corpus
3. Reranker: Cross-encoder model for precise relevance scoring
4. Context Manager: Optimize context window utilization
5. Generator: LLM inference with structured prompting
6. Validator: Fact-checking and hallucination detection

Performance targets:
- Latency: p95 < 2 seconds end-to-end
- Accuracy: 95%+ on domain-specific questions
- Throughput: 10K queries/second
- Availability: 99.9%
"""

import numpy as np
from typing import List, Dict, Optional, Tuple, Any
from dataclasses import dataclass
from datetime import datetime
import time
from collections import defaultdict

@dataclass
class Document:
    """
    Document in knowledge base

    Attributes:
        doc_id: Unique identifier
        content: Document text
        metadata: Title, source, author, date, etc.
        embedding: Vector representation
        score: Relevance score (set during retrieval/reranking)
    """
    doc_id: str
    content: str
    metadata: Dict[str, Any]
    embedding: Optional[np.ndarray] = None
    score: float = 0.0

@dataclass
class Query:
    """
    User query with processing metadata

    Attributes:
        query_id: Unique identifier
        text: Original query text
        intent: Classified intent (factual, how-to, comparison, etc.)
        entities: Extracted named entities
        expanded_queries: Query variations for retrieval
        filters: Metadata filters (date range, source, etc.)
    """
    query_id: str
    text: str
    intent: Optional[str] = None
    entities: List[str] = None
    expanded_queries: List[str] = None
    filters: Dict[str, Any] = None

@dataclass
class RAGResponse:
    """
    Complete RAG system response

    Attributes:
        query_id: Links to original query
        answer: Generated answer text
        sources: Documents used as sources
        confidence: Model confidence score
        latency_ms: End-to-end latency
        metadata: Retrieval/generation metadata
    """
    query_id: str
    answer: str
    sources: List[Document]
    confidence: float
    latency_ms: float
    metadata: Dict[str, Any]

class QueryProcessor:
    """
    Query understanding and expansion

    Responsibilities:
    - Intent classification: Identify query type
    - Entity extraction: Extract key entities
    - Query expansion: Generate variations (synonyms, reformulations)
    - Filter extraction: Parse metadata filters

    Techniques:
    - NER models for entity extraction
    - Classification models for intent
    - Query expansion via embeddings (find similar queries)
    - Rule-based parsing for filters
    """

    def __init__(self):
        """Initialize query processor"""
        # In production: Load NER model, intent classifier, etc.
        self.supported_intents = [
            'factual',      # "What is X?"
            'how-to',       # "How do I do X?"
            'comparison',   # "What's the difference between X and Y?"
            'explanation',  # "Why does X happen?"
            'list'          # "What are the types of X?"
        ]

        print("Initialized Query Processor")

    def process(self, query_text: str) -> Query:
        """
        Process raw query into structured representation

        Args:
            query_text: Raw query string

        Returns:
            Query object with intent, entities, expansions
        """
        query_id = f"query_{int(time.time() * 1000)}"

        # Intent classification
        intent = self._classify_intent(query_text)

        # Entity extraction
        entities = self._extract_entities(query_text)

        # Query expansion
        expanded = self._expand_query(query_text, intent)

        # Filter extraction
        filters = self._extract_filters(query_text)

        return Query(
            query_id=query_id,
            text=query_text,
            intent=intent,
            entities=entities,
            expanded_queries=expanded,
            filters=filters
        )

    def _classify_intent(self, query_text: str) -> str:
        """
        Classify query intent

        Simple rule-based classification (production: use trained model)

        Args:
            query_text: Query text

        Returns:
            Intent label
        """
        text_lower = query_text.lower()

        if any(word in text_lower for word in ['what is', 'define', 'meaning of']):
            return 'factual'
        elif any(word in text_lower for word in ['how to', 'how do i', 'how can i']):
            return 'how-to'
        elif any(word in text_lower for word in ['difference between', 'compare', 'vs']):
            return 'comparison'
        elif any(word in text_lower for word in ['why', 'reason', 'cause']):
            return 'explanation'
        elif any(word in text_lower for word in ['list', 'what are', 'types of']):
            return 'list'
        else:
            return 'factual'

    def _extract_entities(self, query_text: str) -> List[str]:
        """
        Extract named entities

        Production: Use spaCy, Flair, or custom NER model

        Args:
            query_text: Query text

        Returns:
            List of entities
        """
        # Placeholder: Simple word extraction
        # In production: Use proper NER
        entities = []

        # Extract capitalized words as potential entities
        words = query_text.split()
        for word in words:
            if word and word[0].isupper() and len(word) > 1:
                entities.append(word)

        return entities

    def _expand_query(self, query_text: str, intent: str) -> List[str]:
        """
        Expand query with variations

        Strategies:
        - Synonym replacement
        - Intent-specific reformulations
        - Related questions

        Args:
            query_text: Original query
            intent: Query intent

        Returns:
            List of expanded queries
        """
        expansions = [query_text]  # Include original

        # Intent-specific expansions
        if intent == 'how-to':
            # Add "steps" variation
            expansions.append(query_text.replace('how to', 'steps to'))
            expansions.append(query_text.replace('how do I', 'how can I'))
        elif intent == 'factual':
            # Add "explain" variation
            expansions.append(query_text.replace('what is', 'explain'))
            expansions.append(query_text.replace('what is', 'definition of'))

        return expansions

    def _extract_filters(self, query_text: str) -> Dict[str, Any]:
        """
        Extract metadata filters from query

        Examples:
        - "papers from 2023" → date_year: 2023
        - "articles by John Smith" → author: "John Smith"

        Args:
            query_text: Query text

        Returns:
            Dictionary of filters
        """
        filters = {}

        # Year extraction
        for year in range(2000, 2030):
            if str(year) in query_text:
                filters['date_year'] = year
                break

        # Author extraction (simple pattern)
        if 'by' in query_text.lower():
            parts = query_text.lower().split('by')
            if len(parts) > 1:
                author = parts[1].strip().split()[0:2]  # Get up to 2 words
                if author:
                    filters['author'] = ' '.join(author)

        return filters

class RetrievalEngine:
    """
    Vector-based retrieval from document corpus

    Architecture:
    - Embedding model for query encoding
    - Vector index (HNSW, IVF) for billion-doc search
    - Metadata filtering
    - Top-k retrieval (k=100-1000)

    Performance:
    - Latency: p95 < 100ms for 1B documents
    - Recall@1000: 95%+
    - Throughput: 10K queries/second
    """

    def __init__(
        self,
        vector_index,
        embedding_model,
        default_k: int = 100
    ):
        """
        Args:
            vector_index: Vector search index (HNSW, Faiss, etc.)
            embedding_model: Model for encoding queries
            default_k: Default number of results to retrieve
        """
        self.vector_index = vector_index
        self.embedding_model = embedding_model
        self.default_k = default_k

        print(f"Initialized Retrieval Engine")
        print(f"  Default k: {default_k}")

    def retrieve(
        self,
        query: Query,
        k: Optional[int] = None
    ) -> List[Document]:
        """
        Retrieve top-k relevant documents

        Strategy:
        1. Encode query (and expansions) to vectors
        2. Search vector index
        3. Apply metadata filters
        4. Merge results across query expansions
        5. Return top-k by score

        Args:
            query: Processed query
            k: Number of documents to retrieve

        Returns:
            List of documents with relevance scores
        """
        if k is None:
            k = self.default_k

        start_time = time.time()

        # Encode query
        query_embedding = self._encode_query(query.text)

        # Search vector index
        results = self.vector_index.search(query_embedding, k=k)

        # Apply filters
        if query.filters:
            results = self._apply_filters(results, query.filters)

        # If using query expansions, merge results
        if query.expanded_queries:
            expansion_results = []
            for expanded_query in query.expanded_queries[1:]:  # Skip first (original)
                expanded_embedding = self._encode_query(expanded_query)
                expanded_results = self.vector_index.search(expanded_embedding, k=k)
                expansion_results.extend(expanded_results)

            # Merge and deduplicate
            results = self._merge_results([results, expansion_results], k=k)

        latency_ms = (time.time() - start_time) * 1000

        print(f"Retrieved {len(results)} documents in {latency_ms:.1f}ms")

        return results

    def _encode_query(self, query_text: str) -> np.ndarray:
        """
        Encode query to embedding vector

        Args:
            query_text: Query string

        Returns:
            Embedding vector
        """
        # In production: Use actual embedding model
        # For now: Return random embedding
        embedding = np.random.randn(768).astype(np.float32)
        embedding = embedding / np.linalg.norm(embedding)
        return embedding

    def _apply_filters(
        self,
        documents: List[Document],
        filters: Dict[str, Any]
    ) -> List[Document]:
        """
        Filter documents by metadata

        Args:
            documents: Documents to filter
            filters: Metadata filters

        Returns:
            Filtered documents
        """
        filtered = []

        for doc in documents:
            # Check all filters
            matches = True
            for key, value in filters.items():
                if key not in doc.metadata or doc.metadata[key] != value:
                    matches = False
                    break

            if matches:
                filtered.append(doc)

        return filtered

    def _merge_results(
        self,
        result_lists: List[List[Document]],
        k: int
    ) -> List[Document]:
        """
        Merge results from multiple searches

        Strategy: Reciprocal Rank Fusion (RRF)
        - Score = sum(1 / (rank + 60)) across all result lists
        - Robust to score scale differences

        Args:
            result_lists: Multiple result lists
            k: Number of results to return

        Returns:
            Merged and deduplicated results
        """
        # Compute RRF scores
        rrf_scores = defaultdict(float)
        doc_map = {}

        for results in result_lists:
            for rank, doc in enumerate(results):
                rrf_scores[doc.doc_id] += 1.0 / (rank + 60)
                doc_map[doc.doc_id] = doc

        # Sort by RRF score
        sorted_ids = sorted(rrf_scores.keys(), key=lambda x: rrf_scores[x], reverse=True)

        # Return top-k
        merged = []
        for doc_id in sorted_ids[:k]:
            doc = doc_map[doc_id]
            doc.score = rrf_scores[doc_id]
            merged.append(doc)

        return merged

class Reranker:
    """
    Precise reranking of retrieved documents

    Approach:
    - Cross-encoder model (BERT-based)
    - Processes query + document pairs
    - More accurate than vector similarity (but slower)
    - Reduces top-1000 to top-10-20 for context window

    Performance:
    - Latency: 50-200ms for 100 documents
    - Accuracy: 10-20% improvement over retrieval alone
    """

    def __init__(self, model_name: str = "cross-encoder"):
        """
        Args:
            model_name: Cross-encoder model identifier
        """
        self.model_name = model_name
        # In production: Load actual cross-encoder model
        print(f"Initialized Reranker with model: {model_name}")

    def rerank(
        self,
        query: Query,
        documents: List[Document],
        top_k: int = 10
    ) -> List[Document]:
        """
        Rerank documents by relevance

        Args:
            query: User query
            documents: Retrieved documents to rerank
            top_k: Number of top documents to return

        Returns:
            Reranked documents
        """
        start_time = time.time()

        # Compute relevance scores
        for doc in documents:
            doc.score = self._compute_relevance(query.text, doc.content)

        # Sort by score
        documents.sort(key=lambda d: d.score, reverse=True)

        # Return top-k
        reranked = documents[:top_k]

        latency_ms = (time.time() - start_time) * 1000
        print(f"Reranked {len(documents)} → {len(reranked)} documents in {latency_ms:.1f}ms")

        return reranked

    def _compute_relevance(self, query: str, document: str) -> float:
        """
        Compute query-document relevance score

        In production: Use cross-encoder model

        Args:
            query: Query text
            document: Document text

        Returns:
            Relevance score (0-1)
        """
        # Placeholder: Simple lexical overlap
        # In production: Use trained cross-encoder
        query_words = set(query.lower().split())
        doc_words = set(document.lower().split())

        overlap = len(query_words & doc_words)
        union = len(query_words | doc_words)

        return overlap / union if union > 0 else 0.0

class ContextManager:
    """
    Optimize context window utilization

    Challenges:
    - LLM context limits (4K-128K tokens)
    - Multiple documents may exceed limit
    - Need to maximize information density

    Strategies:
    - Truncation: Keep first N tokens per document
    - Extraction: Extract most relevant sentences
    - Summarization: Summarize long documents
    - Compression: Remove redundancy across documents
    """

    def __init__(
        self,
        max_context_tokens: int = 4096,
        max_tokens_per_doc: int = 500
    ):
        """
        Args:
            max_context_tokens: Maximum context window size
            max_tokens_per_doc: Maximum tokens per document
        """
        self.max_context_tokens = max_context_tokens
        self.max_tokens_per_doc = max_tokens_per_doc

        print(f"Initialized Context Manager")
        print(f"  Max context tokens: {max_context_tokens:,}")
        print(f"  Max tokens per doc: {max_tokens_per_doc}")

    def assemble_context(
        self,
        query: Query,
        documents: List[Document]
    ) -> str:
        """
        Assemble context from documents

        Strategy:
        1. Truncate each document to max_tokens_per_doc
        2. Concatenate documents in rank order
        3. Truncate total to max_context_tokens
        4. Add document citations

        Args:
            query: User query
            documents: Reranked documents

        Returns:
            Assembled context string
        """
        context_parts = []
        total_tokens = 0

        for i, doc in enumerate(documents):
            # Estimate tokens (rough: 1 token ≈ 4 characters)
            doc_tokens = len(doc.content) // 4

            # Truncate document if too long
            if doc_tokens > self.max_tokens_per_doc:
                # Keep first N characters
                char_limit = self.max_tokens_per_doc * 4
                content = doc.content[:char_limit] + "..."
                doc_tokens = self.max_tokens_per_doc
            else:
                content = doc.content

            # Check if adding this document would exceed limit
            if total_tokens + doc_tokens > self.max_context_tokens:
                # Stop adding documents
                break

            # Add document with citation
            doc_text = f"[Document {i+1}]\n{content}\n"
            context_parts.append(doc_text)
            total_tokens += doc_tokens

        context = "\n".join(context_parts)

        print(f"Assembled context: {total_tokens:,} tokens from {len(context_parts)} documents")

        return context

class RAGSystem:
    """
    Complete RAG system orchestration

    Architecture:
    - Query Processor → Retrieval → Reranking → Context Assembly → Generation

    Performance:
    - End-to-end latency: p95 < 2s
    - Accuracy: 95%+ on domain questions
    - Throughput: 10K queries/sec (with parallelization)
    """

    def __init__(
        self,
        vector_index,
        embedding_model,
        llm,
        retrieval_k: int = 100,
        rerank_k: int = 10
    ):
        """
        Args:
            vector_index: Vector search index
            embedding_model: Embedding model for queries
            llm: Large language model for generation
            retrieval_k: Number of documents to retrieve
            rerank_k: Number of documents to rerank to
        """
        self.query_processor = QueryProcessor()
        self.retrieval_engine = RetrievalEngine(vector_index, embedding_model, default_k=retrieval_k)
        self.reranker = Reranker()
        self.context_manager = ContextManager()
        self.llm = llm
        self.rerank_k = rerank_k

        print(f"Initialized RAG System")
        print(f"  Retrieval k: {retrieval_k}")
        print(f"  Rerank k: {rerank_k}")

    def answer(self, query_text: str) -> RAGResponse:
        """
        Answer user query using RAG

        Pipeline:
        1. Process query (intent, entities, expansion)
        2. Retrieve top-k documents (k=100)
        3. Rerank to top-n (n=10)
        4. Assemble context
        5. Generate answer
        6. Validate response

        Args:
            query_text: User query

        Returns:
            RAGResponse with answer and sources
        """
        start_time = time.time()

        # 1. Process query
        query = self.query_processor.process(query_text)
        print(f"\nProcessing query: {query_text}")
        print(f"  Intent: {query.intent}")
        print(f"  Entities: {query.entities}")

        # 2. Retrieve documents
        retrieved_docs = self.retrieval_engine.retrieve(query)

        # 3. Rerank documents
        reranked_docs = self.reranker.rerank(query, retrieved_docs, top_k=self.rerank_k)

        # 4. Assemble context
        context = self.context_manager.assemble_context(query, reranked_docs)

        # 5. Generate answer
        answer, confidence = self._generate_answer(query, context)

        # 6. Validate (placeholder)
        # In production: Check for hallucinations, verify citations

        latency_ms = (time.time() - start_time) * 1000

        return RAGResponse(
            query_id=query.query_id,
            answer=answer,
            sources=reranked_docs,
            confidence=confidence,
            latency_ms=latency_ms,
            metadata={
                'intent': query.intent,
                'num_retrieved': len(retrieved_docs),
                'num_reranked': len(reranked_docs),
                'context_tokens': len(context) // 4
            }
        )

    def _generate_answer(
        self,
        query: Query,
        context: str
    ) -> Tuple[str, float]:
        """
        Generate answer using LLM

        Args:
            query: User query
            context: Assembled context

        Returns:
            (answer, confidence)
        """
        # Construct prompt
        prompt = f"""Answer the following question based on the provided context.
Cite sources using [Document N] notation.

Question: {query.text}

Context:
{context}

Answer:"""

        # Generate (placeholder)
        # In production: Call actual LLM
        answer = f"Based on the provided documents, {query.text.lower().replace('?', '').replace('what is', 'refers to')}. [Document 1]"
        confidence = 0.85

        return answer, confidence

# Example: Enterprise RAG system
def rag_system_example():
    """
    Demonstrate enterprise RAG system

    Scenario: Technical documentation Q&A
    - 1M documents (product docs, API refs, tutorials)
    - User asks technical questions
    - System retrieves relevant docs and generates answers
    """

    # Mock components
    class MockVectorIndex:
        def search(self, query_embedding, k=100):
            # Return mock documents
            docs = []
            for i in range(k):
                doc = Document(
                    doc_id=f"doc_{i}",
                    content=f"This is document {i} containing relevant technical information about the topic.",
                    metadata={
                        'title': f'Document {i}',
                        'source': 'documentation',
                        'date_year': 2024
                    },
                    score=1.0 - (i * 0.01)  # Decreasing scores
                )
                docs.append(doc)
            return docs

    class MockEmbeddingModel:
        def encode(self, text):
            return np.random.randn(768).astype(np.float32)

    class MockLLM:
        def generate(self, prompt):
            return "Generated answer based on context..."

    # Initialize RAG system
    vector_index = MockVectorIndex()
    embedding_model = MockEmbeddingModel()
    llm = MockLLM()

    rag_system = RAGSystem(
        vector_index=vector_index,
        embedding_model=embedding_model,
        llm=llm,
        retrieval_k=100,
        rerank_k=10
    )

    # Query
    query_text = "How do I configure authentication for the API?"

    # Get answer
    response = rag_system.answer(query_text)

    # Display results
    print(f"\n{'='*60}")
    print(f"Query: {query_text}")
    print(f"{'='*60}")
    print(f"\nAnswer: {response.answer}")
    print(f"\nConfidence: {response.confidence:.2f}")
    print(f"Latency: {response.latency_ms:.1f}ms")
    print(f"\nSources:")
    for i, doc in enumerate(response.sources[:3]):
        print(f"  [{i+1}] {doc.metadata.get('title', doc.doc_id)} (score: {doc.score:.3f})")
    print(f"\nMetadata:")
    for key, value in response.metadata.items():
        print(f"  {key}: {value}")

# Uncomment to run:
# rag_system_example()
```

:::{.callout-tip}
## Enterprise RAG Best Practices

**Architecture:**
- Decouple components (retrieval, reranking, generation)
- Use async/parallel processing where possible
- Implement circuit breakers for each component
- Cache frequent queries and intermediate results

**Query Processing:**
- Always classify intent (different strategies per type)
- Extract and normalize entities
- Use query expansion for better recall
- Parse metadata filters from natural language

**Retrieval:**
- Start with high k (100-1000) for recall
- Use multiple retrieval strategies (vector + keyword)
- Apply metadata filters early (before reranking)
- Log retrieval metrics for continuous improvement

**Reranking:**
- Essential for production accuracy (10-20% improvement)
- Use cross-encoder models (more accurate than bi-encoders)
- Batch reranking requests for efficiency
- Consider two-stage reranking (coarse then fine)
:::

## Context Window Optimization

LLMs have fixed context windows (4K-128K tokens), but enterprise knowledge bases contain millions of documents. **Context window optimization** maximizes information density: selecting the most relevant passages, removing redundancy, compressing verbose content, and structuring information for LLM comprehension.

### The Context Window Challenge

**Problem**: Retrieved documents often exceed context limits
- 10 documents × 1000 tokens each = 10K tokens
- Typical LLM limit: 4K-8K tokens
- Need to reduce 10K → 4K while preserving key information

**Naive approach**: Truncate each document
- **Problem**: May cut off critical information, often removes conclusions

**Better approach**: Extract relevant passages, deduplicate, compress

```python
"""
Context Window Optimization Strategies

Techniques:
1. Passage extraction: Extract most relevant sentences
2. Deduplication: Remove redundant information
3. Summarization: Compress long documents
4. Hierarchical assembly: Progressively add detail
5. Token counting: Precise tracking to maximize usage
"""

import re
from typing import List, Tuple
import numpy as np

class PassageExtractor:
    """
    Extract most relevant passages from documents

    Approach:
    - Sentence-level relevance scoring
    - Extract top sentences per document
    - Maintain narrative flow (keep consecutive sentences)

    Benefits:
    - Preserves key information
    - Removes boilerplate/fluff
    - Reduces tokens 50-70%
    """

    def __init__(self, max_sentences_per_doc: int = 5):
        """
        Args:
            max_sentences_per_doc: Maximum sentences to extract per document
        """
        self.max_sentences_per_doc = max_sentences_per_doc
        print(f"Initialized Passage Extractor (max {max_sentences_per_doc} sentences/doc)")

    def extract(
        self,
        query: str,
        document: str
    ) -> str:
        """
        Extract most relevant passages

        Args:
            query: User query
            document: Full document text

        Returns:
            Extracted passages
        """
        # Split into sentences
        sentences = self._split_sentences(document)

        if len(sentences) <= self.max_sentences_per_doc:
            return document

        # Score sentences by relevance
        scores = []
        for sent in sentences:
            score = self._score_sentence(query, sent)
            scores.append(score)

        # Get top sentences
        top_indices = np.argsort(scores)[-self.max_sentences_per_doc:]
        top_indices = sorted(top_indices)  # Maintain order

        # Extract top sentences
        extracted = [sentences[i] for i in top_indices]

        return " ".join(extracted)

    def _split_sentences(self, text: str) -> List[str]:
        """Split text into sentences"""
        # Simple sentence splitting (production: use nltk or spacy)
        sentences = re.split(r'[.!?]+', text)
        sentences = [s.strip() for s in sentences if s.strip()]
        return sentences

    def _score_sentence(self, query: str, sentence: str) -> float:
        """
        Score sentence relevance to query

        Simple approach: Lexical overlap
        Production: Use sentence embeddings

        Args:
            query: Query text
            sentence: Sentence text

        Returns:
            Relevance score
        """
        query_words = set(query.lower().split())
        sent_words = set(sentence.lower().split())

        overlap = len(query_words & sent_words)
        return overlap

class ContextDeduplicator:
    """
    Remove redundant information across documents

    Problem: Multiple documents often contain overlapping information
    - Wastes tokens
    - Confuses LLM with repetition

    Solution: Detect and remove near-duplicate passages

    Approach:
    - Sentence-level similarity (embeddings or MinHash)
    - Remove sentences similar to previously included ones
    - Keep first occurrence
    """

    def __init__(self, similarity_threshold: float = 0.85):
        """
        Args:
            similarity_threshold: Threshold for considering sentences duplicate
        """
        self.similarity_threshold = similarity_threshold
        print(f"Initialized Context Deduplicator (threshold: {similarity_threshold})")

    def deduplicate(self, documents: List[str]) -> List[str]:
        """
        Remove redundant content across documents

        Args:
            documents: List of document texts

        Returns:
            Deduplicated documents
        """
        seen_sentences = set()
        deduplicated_docs = []

        for doc in documents:
            sentences = self._split_sentences(doc)
            unique_sentences = []

            for sent in sentences:
                # Normalize sentence for comparison
                normalized = sent.lower().strip()

                # Check if similar sentence already seen
                if not self._is_duplicate(normalized, seen_sentences):
                    unique_sentences.append(sent)
                    seen_sentences.add(normalized)

            if unique_sentences:
                deduplicated_docs.append(" ".join(unique_sentences))

        return deduplicated_docs

    def _split_sentences(self, text: str) -> List[str]:
        """Split text into sentences"""
        sentences = re.split(r'[.!?]+', text)
        sentences = [s.strip() for s in sentences if s.strip()]
        return sentences

    def _is_duplicate(self, sentence: str, seen: set) -> bool:
        """
        Check if sentence is duplicate of any seen sentence

        Simple approach: Exact match
        Production: Use embedding similarity or MinHash

        Args:
            sentence: Sentence to check
            seen: Set of seen sentences

        Returns:
            True if duplicate
        """
        return sentence in seen

class HierarchicalContextAssembler:
    """
    Hierarchical context assembly

    Strategy: Progressive detail
    1. Start with document titles/summaries (high-level)
    2. Add key passages (medium detail)
    3. Add supporting details (low detail)
    4. Stop when context window full

    Benefits:
    - Ensures high-level information included
    - Gracefully handles variable context sizes
    - LLM sees overview before details
    """

    def __init__(self, max_tokens: int = 4096):
        """
        Args:
            max_tokens: Maximum context tokens
        """
        self.max_tokens = max_tokens
        self.passage_extractor = PassageExtractor(max_sentences_per_doc=3)
        self.deduplicator = ContextDeduplicator()

        print(f"Initialized Hierarchical Context Assembler")
        print(f"  Max tokens: {max_tokens:,}")

    def assemble(
        self,
        query: str,
        documents: List[Document]
    ) -> str:
        """
        Assemble context hierarchically

        Levels:
        1. Titles and metadata
        2. Key passages
        3. Full content (if space available)

        Args:
            query: User query
            documents: Ranked documents

        Returns:
            Assembled context string
        """
        context_parts = []
        current_tokens = 0

        # Level 1: Titles and metadata
        for i, doc in enumerate(documents):
            title = doc.metadata.get('title', f'Document {i+1}')
            source = doc.metadata.get('source', 'unknown')
            header = f"[{i+1}] {title} (Source: {source})"

            header_tokens = len(header) // 4
            if current_tokens + header_tokens < self.max_tokens:
                context_parts.append(header)
                current_tokens += header_tokens

        # Level 2: Key passages
        for i, doc in enumerate(documents):
            # Extract relevant passages
            passages = self.passage_extractor.extract(query, doc.content)
            passages_tokens = len(passages) // 4

            if current_tokens + passages_tokens < self.max_tokens * 0.8:  # Leave 20% buffer
                context_parts.append(f"\nKey points from Document {i+1}:")
                context_parts.append(passages)
                current_tokens += passages_tokens
            else:
                break  # Context window nearly full

        # Level 3: Additional details (if space)
        # Skipped in this basic implementation

        # Deduplicate
        deduplicated = self.deduplicator.deduplicate(context_parts)
        context = "\n".join(deduplicated)

        print(f"Assembled hierarchical context: {current_tokens:,} tokens from {len(documents)} docs")

        return context

# Example: Context window optimization
def context_optimization_example():
    """
    Demonstrate context window optimization

    Scenario:
    - Retrieved 10 documents, each 1000 tokens
    - Total: 10K tokens
    - Context limit: 4K tokens
    - Need to reduce by 60% while preserving key info
    """

    # Mock documents
    documents = []
    for i in range(10):
        doc = Document(
            doc_id=f"doc_{i}",
            content=f"""This is document {i}. It contains important information about the topic.
            Key point 1: The system uses advanced algorithms.
            Key point 2: Performance is optimized for scale.
            Key point 3: Security is built-in at every layer.
            Additional detail: The architecture follows best practices.
            More detail: Integration is seamless with existing systems.
            Even more detail: The team has extensive experience.
            Background: Development started in 2023.
            Context: The project aims to solve real-world problems.
            Summary: This represents a significant advancement.""",
            metadata={
                'title': f'Technical Report {i}',
                'source': 'documentation'
            }
        )
        documents.append(doc)

    query = "What are the key features of the system?"

    # Standard assembly (truncation)
    print("=== Standard Assembly (Truncation) ===")
    context_manager = ContextManager(max_context_tokens=4096, max_tokens_per_doc=400)
    standard_context = context_manager.assemble_context(
        Query(query_id="q1", text=query),
        documents
    )
    print(f"Context length: {len(standard_context)} chars (~{len(standard_context)//4} tokens)")

    # Optimized assembly (hierarchical + extraction)
    print("\n=== Optimized Assembly (Hierarchical) ===")
    hierarchical_assembler = HierarchicalContextAssembler(max_tokens=4096)
    optimized_context = hierarchical_assembler.assemble(query, documents)
    print(f"Context length: {len(optimized_context)} chars (~{len(optimized_context)//4} tokens)")

    print(f"\nSample optimized context:")
    print(optimized_context[:500] + "...")

# Uncomment to run:
# context_optimization_example()
```

:::{.callout-tip}
## Context Window Optimization Best Practices

**Passage extraction:**
- Use sentence embeddings for relevance scoring
- Keep consecutive sentences for narrative flow
- Extract different amounts per query type (factual: less, explanation: more)

**Deduplication:**
- Use MinHash or embeddings for semantic similarity
- Set threshold based on acceptable information loss (0.8-0.9)
- Keep first occurrence (usually most complete)

**Token counting:**
- Use tokenizer from target LLM (different tokenizers vary)
- Count precisely, don't estimate (estimation errors compound)
- Reserve tokens for query, instructions, output (typically 20-30%)

**Hierarchical assembly:**
- Always include document titles/metadata
- Prioritize key passages over full text
- Add detail progressively until limit reached
:::

:::{.callout-warning}
## Context Window Pitfalls

Common mistakes that degrade RAG quality:

**Over-truncation**: Cutting documents mid-sentence or mid-paragraph loses context
- **Solution**: Truncate at sentence/paragraph boundaries

**Lost citations**: After extraction/summarization, can't attribute claims
- **Solution**: Maintain document IDs throughout processing

**Query not in context**: Forgot to include original query in prompt
- **Solution**: Always include query, even if redundant

**Exceeding limit**: Token estimation off, actual usage exceeds limit
- **Solution**: Use actual tokenizer, add 10% safety buffer
:::

## Multi-Stage Retrieval Systems

Single-stage retrieval (retrieve top-k, done) sacrifices either recall or latency. **Multi-stage retrieval** separates concerns: early stages optimize for recall (don't miss relevant documents), later stages optimize for precision (rank best documents highest). This enables billion-document search with high accuracy and low latency.

### The Multi-Stage Architecture

**Stage 1: Coarse Retrieval (Recall-focused)**
- Goal: Don't miss relevant documents
- Method: Fast vector search (ANN)
- Scale: Search full corpus (1B+ documents)
- Output: Top-1000 candidates
- Latency: 50-100ms

**Stage 2: Reranking (Precision-focused)**
- Goal: Rank best documents highest
- Method: Cross-encoder model
- Scale: Rerank 1000 candidates
- Output: Top-20 documents
- Latency: 50-200ms

**Stage 3: Final Selection (Context-focused)**
- Goal: Maximize context window utilization
- Method: Passage extraction, deduplication
- Scale: Process 20 documents
- Output: Optimized context
- Latency: 10-50ms

```python
"""
Multi-Stage Retrieval Pipeline

Stages:
1. Coarse retrieval: Vector search (recall-focused)
2. Keyword filtering: Ensure key terms present
3. Reranking: Cross-encoder (precision-focused)
4. Diversity: Ensure variety in results
5. Final selection: Context optimization

Benefits:
- High recall (don't miss relevant docs)
- High precision (best docs ranked highest)
- Low latency (heavy computation on small set)
- Diversity (avoid redundant results)
"""

from typing import List, Set
import numpy as np

class MultiStageRetriever:
    """
    Multi-stage retrieval pipeline

    Stages:
    1. Vector retrieval (k=1000)
    2. Keyword filter (k=500)
    3. Reranking (k=20)
    4. Diversity filter (k=10)

    Performance:
    - Recall@1000: 98%+
    - Precision@10: 90%+
    - Latency: 200-400ms
    """

    def __init__(
        self,
        vector_index,
        embedding_model,
        reranker,
        stage1_k: int = 1000,
        stage2_k: int = 500,
        stage3_k: int = 20,
        stage4_k: int = 10
    ):
        """
        Args:
            vector_index: Vector search index
            embedding_model: Embedding model
            reranker: Cross-encoder reranker
            stage1_k: Candidates after stage 1
            stage2_k: Candidates after stage 2
            stage3_k: Candidates after stage 3
            stage4_k: Final results
        """
        self.vector_index = vector_index
        self.embedding_model = embedding_model
        self.reranker = reranker
        self.stage1_k = stage1_k
        self.stage2_k = stage2_k
        self.stage3_k = stage3_k
        self.stage4_k = stage4_k

        print(f"Initialized Multi-Stage Retriever")
        print(f"  Stage 1 (vector): {stage1_k}")
        print(f"  Stage 2 (keyword): {stage2_k}")
        print(f"  Stage 3 (rerank): {stage3_k}")
        print(f"  Stage 4 (diversity): {stage4_k}")

    def retrieve(self, query: Query) -> List[Document]:
        """
        Multi-stage retrieval

        Args:
            query: User query

        Returns:
            Final ranked documents
        """
        # Stage 1: Vector retrieval (recall)
        print(f"\nStage 1: Vector retrieval (k={self.stage1_k})")
        stage1_start = time.time()

        query_embedding = self._encode_query(query.text)
        stage1_docs = self.vector_index.search(query_embedding, k=self.stage1_k)

        stage1_latency = (time.time() - stage1_start) * 1000
        print(f"  Retrieved {len(stage1_docs)} documents in {stage1_latency:.1f}ms")

        # Stage 2: Keyword filtering (precision)
        print(f"\nStage 2: Keyword filtering (k={self.stage2_k})")
        stage2_start = time.time()

        stage2_docs = self._keyword_filter(query, stage1_docs, k=self.stage2_k)

        stage2_latency = (time.time() - stage2_start) * 1000
        print(f"  Filtered to {len(stage2_docs)} documents in {stage2_latency:.1f}ms")

        # Stage 3: Reranking (precision)
        print(f"\nStage 3: Reranking (k={self.stage3_k})")
        stage3_start = time.time()

        stage3_docs = self.reranker.rerank(query, stage2_docs, top_k=self.stage3_k)

        stage3_latency = (time.time() - stage3_start) * 1000
        print(f"  Reranked to {len(stage3_docs)} documents in {stage3_latency:.1f}ms")

        # Stage 4: Diversity filtering (quality)
        print(f"\nStage 4: Diversity filtering (k={self.stage4_k})")
        stage4_start = time.time()

        stage4_docs = self._diversity_filter(stage3_docs, k=self.stage4_k)

        stage4_latency = (time.time() - stage4_start) * 1000
        print(f"  Selected {len(stage4_docs)} diverse documents in {stage4_latency:.1f}ms")

        total_latency = (time.time() - stage1_start) * 1000
        print(f"\nTotal pipeline latency: {total_latency:.1f}ms")

        return stage4_docs

    def _encode_query(self, query_text: str) -> np.ndarray:
        """Encode query to embedding"""
        # Placeholder
        return np.random.randn(768).astype(np.float32)

    def _keyword_filter(
        self,
        query: Query,
        documents: List[Document],
        k: int
    ) -> List[Document]:
        """
        Filter documents by keyword presence

        Strategy:
        - Extract key terms from query (entities, important words)
        - Score documents by presence of key terms
        - Keep top-k by combined score (vector + keyword)

        Args:
            query: User query
            documents: Retrieved documents
            k: Number to keep

        Returns:
            Filtered documents
        """
        # Extract query keywords
        query_terms = self._extract_keywords(query.text)

        # Score documents
        for doc in documents:
            keyword_score = self._keyword_overlap(query_terms, doc.content)
            # Combine with existing vector score
            doc.score = 0.7 * doc.score + 0.3 * keyword_score

        # Sort and return top-k
        documents.sort(key=lambda d: d.score, reverse=True)
        return documents[:k]

    def _extract_keywords(self, text: str) -> Set[str]:
        """
        Extract important keywords from text

        Production: Use TF-IDF, TextRank, or NER

        Args:
            text: Text to extract from

        Returns:
            Set of keywords
        """
        # Simple: Lowercase words, remove stopwords
        stopwords = {'the', 'a', 'an', 'is', 'are', 'was', 'were', 'be', 'been',
                     'how', 'what', 'where', 'when', 'why', 'do', 'does', 'did'}

        words = text.lower().split()
        keywords = {w for w in words if w not in stopwords and len(w) > 3}

        return keywords

    def _keyword_overlap(self, query_terms: Set[str], document: str) -> float:
        """
        Compute keyword overlap score

        Args:
            query_terms: Query keywords
            document: Document text

        Returns:
            Overlap score (0-1)
        """
        doc_terms = set(document.lower().split())
        overlap = len(query_terms & doc_terms)
        return overlap / len(query_terms) if query_terms else 0.0

    def _diversity_filter(
        self,
        documents: List[Document],
        k: int
    ) -> List[Document]:
        """
        Filter for diversity in results

        Problem: Top results often very similar (redundant)
        Solution: Maximal Marginal Relevance (MMR)
        - Balance relevance and diversity
        - Iteratively select: most relevant among those dissimilar to selected

        Args:
            documents: Reranked documents
            k: Number to select

        Returns:
            Diverse set of documents
        """
        if len(documents) <= k:
            return documents

        selected = [documents[0]]  # Start with most relevant
        remaining = documents[1:]

        while len(selected) < k and remaining:
            # Compute MMR for each remaining document
            best_mmr = -np.inf
            best_idx = 0

            for i, doc in enumerate(remaining):
                # Relevance
                relevance = doc.score

                # Diversity (dissimilarity to selected)
                max_similarity = max(
                    self._similarity(doc, selected_doc)
                    for selected_doc in selected
                )
                diversity = 1 - max_similarity

                # MMR (lambda=0.5 balances relevance and diversity)
                mmr = 0.5 * relevance + 0.5 * diversity

                if mmr > best_mmr:
                    best_mmr = mmr
                    best_idx = i

            # Select best MMR document
            selected.append(remaining[best_idx])
            remaining.pop(best_idx)

        return selected

    def _similarity(self, doc1: Document, doc2: Document) -> float:
        """
        Compute similarity between documents

        Production: Use document embeddings

        Args:
            doc1: First document
            doc2: Second document

        Returns:
            Similarity score (0-1)
        """
        # Placeholder: Lexical overlap
        words1 = set(doc1.content.lower().split())
        words2 = set(doc2.content.lower().split())

        overlap = len(words1 & words2)
        union = len(words1 | words2)

        return overlap / union if union > 0 else 0.0

# Example: Multi-stage retrieval
def multi_stage_retrieval_example():
    """
    Demonstrate multi-stage retrieval

    Scenario: Technical Q&A over 10M documents
    """

    # Mock components
    class MockVectorIndex:
        def search(self, query_embedding, k=1000):
            docs = []
            for i in range(k):
                doc = Document(
                    doc_id=f"doc_{i}",
                    content=f"Document {i} about technical topic with relevant information and keywords.",
                    metadata={'title': f'Doc {i}'},
                    score=1.0 - (i * 0.001)
                )
                docs.append(doc)
            return docs

    class MockEmbeddingModel:
        pass

    class MockReranker:
        def rerank(self, query, documents, top_k=20):
            # Simulate reranking by slightly shuffling
            for doc in documents:
                doc.score += np.random.uniform(-0.1, 0.1)
            documents.sort(key=lambda d: d.score, reverse=True)
            return documents[:top_k]

    # Initialize multi-stage retriever
    retriever = MultiStageRetriever(
        vector_index=MockVectorIndex(),
        embedding_model=MockEmbeddingModel(),
        reranker=MockReranker(),
        stage1_k=1000,
        stage2_k=500,
        stage3_k=20,
        stage4_k=10
    )

    # Query
    query = Query(
        query_id="q1",
        text="How do I configure authentication for the API using OAuth2?"
    )

    # Retrieve
    results = retriever.retrieve(query)

    print(f"\n{'='*60}")
    print(f"Final Results ({len(results)} documents):")
    print(f"{'='*60}")
    for i, doc in enumerate(results):
        print(f"{i+1}. {doc.doc_id} (score: {doc.score:.4f})")

# Uncomment to run:
# multi_stage_retrieval_example()
```

:::{.callout-tip}
## Multi-Stage Retrieval Best Practices

**Stage separation:**
- Early stages: Fast, high recall (don't miss relevant docs)
- Later stages: Slow, high precision (rank best docs highest)
- Each stage should reduce candidates 50-90%

**Stage selection:**
- Always include: Vector retrieval (stage 1) + Reranking (stage 2)
- Optional: Keyword filter, diversity filter, metadata filter
- Add stages based on failure analysis (what's missing? what's wrong?)

**Performance optimization:**
- Cache vector search results (query embeddings stable)
- Batch reranking requests (100 docs × 1ms each = 100ms, batched = 20ms)
- Run filters in parallel when possible (keyword + metadata)
- Monitor stage latencies separately (find bottlenecks)

**Quality monitoring:**
- Track recall @ each stage (is stage 1 missing relevant docs?)
- Track precision @ each stage (is stage 2 improving ranking?)
- A/B test stage variations (does keyword filter help?)
:::

## RAG Evaluation Frameworks

RAG systems combine retrieval and generation, requiring evaluation beyond standard IR or NLG metrics. **RAG evaluation frameworks** measure end-to-end quality: retrieval relevance, context utilization, answer accuracy, factual consistency, attribution quality, and user satisfaction.

### The RAG Evaluation Challenge

**Traditional IR metrics** (Recall@k, MRR, NDCG):
- Measure retrieval quality only
- Don't capture if LLM used retrieved context
- Don't measure answer accuracy

**Traditional NLG metrics** (BLEU, ROUGE, BERTScore):
- Measure generation quality only
- Don't capture if answer grounded in context
- Don't detect hallucinations

**RAG needs both + more**: Did system retrieve relevant docs AND generate accurate answer grounded in those docs?

```python
"""
RAG Evaluation Framework

Metrics:
1. Retrieval metrics: Recall@k, MRR, NDCG
2. Context utilization: Did LLM use retrieved context?
3. Answer accuracy: Is answer correct?
4. Factual consistency: Is answer consistent with context?
5. Attribution quality: Are citations accurate?
6. User satisfaction: Would user accept this answer?

Evaluation data:
- Query
- Ground truth answer
- Ground truth relevant documents
- System retrieved documents
- System generated answer
- System citations
"""

from dataclasses import dataclass
from typing import List, Set, Optional
import numpy as np

@dataclass
class EvaluationSample:
    """
    Single evaluation sample

    Attributes:
        query: User query
        ground_truth_answer: Gold standard answer
        ground_truth_doc_ids: Relevant document IDs
        retrieved_doc_ids: System retrieved document IDs
        generated_answer: System generated answer
        citations: Document IDs cited in answer
    """
    query: str
    ground_truth_answer: str
    ground_truth_doc_ids: Set[str]
    retrieved_doc_ids: List[str]
    generated_answer: str
    citations: List[str]

class RAGEvaluator:
    """
    Comprehensive RAG evaluation

    Metrics:
    1. Retrieval quality
       - Recall@k: % of relevant docs in top-k
       - Precision@k: % of top-k that are relevant
       - MRR: Mean reciprocal rank of first relevant doc

    2. Answer quality
       - Accuracy: % of answers judged correct
       - Factual consistency: Answer consistent with context
       - Faithfulness: Answer only uses context (no hallucination)

    3. Attribution quality
       - Citation recall: % of facts cited
       - Citation precision: % of citations accurate

    4. End-to-end
       - User satisfaction: Would user accept answer?
    """

    def __init__(self):
        """Initialize evaluator"""
        print("Initialized RAG Evaluator")

    def evaluate(
        self,
        samples: List[EvaluationSample]
    ) -> Dict[str, float]:
        """
        Evaluate RAG system on sample set

        Args:
            samples: Evaluation samples

        Returns:
            Dictionary of metrics
        """
        print(f"\nEvaluating {len(samples)} samples...")

        # Retrieval metrics
        retrieval_metrics = self._evaluate_retrieval(samples)

        # Answer quality metrics
        answer_metrics = self._evaluate_answers(samples)

        # Attribution metrics
        attribution_metrics = self._evaluate_attribution(samples)

        # Combine all metrics
        metrics = {
            **retrieval_metrics,
            **answer_metrics,
            **attribution_metrics
        }

        # Print results
        print(f"\n{'='*60}")
        print("Evaluation Results")
        print(f"{'='*60}")
        for metric, value in metrics.items():
            print(f"{metric:30s}: {value:.3f}")

        return metrics

    def _evaluate_retrieval(
        self,
        samples: List[EvaluationSample]
    ) -> Dict[str, float]:
        """
        Evaluate retrieval quality

        Metrics:
        - Recall@k: Did we retrieve relevant documents?
        - Precision@k: Are retrieved documents relevant?
        - MRR: Rank of first relevant document

        Args:
            samples: Evaluation samples

        Returns:
            Retrieval metrics
        """
        recalls = []
        precisions = []
        reciprocal_ranks = []

        for sample in samples:
            # Recall@k: % of relevant docs retrieved
            retrieved_set = set(sample.retrieved_doc_ids)
            relevant_retrieved = retrieved_set & sample.ground_truth_doc_ids

            recall = len(relevant_retrieved) / len(sample.ground_truth_doc_ids) if sample.ground_truth_doc_ids else 0
            recalls.append(recall)

            # Precision@k: % of retrieved docs that are relevant
            precision = len(relevant_retrieved) / len(retrieved_set) if retrieved_set else 0
            precisions.append(precision)

            # MRR: Rank of first relevant document
            rr = 0.0
            for rank, doc_id in enumerate(sample.retrieved_doc_ids, 1):
                if doc_id in sample.ground_truth_doc_ids:
                    rr = 1.0 / rank
                    break
            reciprocal_ranks.append(rr)

        return {
            'recall@k': np.mean(recalls),
            'precision@k': np.mean(precisions),
            'mrr': np.mean(reciprocal_ranks)
        }

    def _evaluate_answers(
        self,
        samples: List[EvaluationSample]
    ) -> Dict[str, float]:
        """
        Evaluate answer quality

        Metrics:
        - Accuracy: Is answer correct?
        - Faithfulness: Does answer only use context (no hallucination)?
        - Completeness: Does answer cover all aspects?

        Args:
            samples: Evaluation samples

        Returns:
            Answer metrics
        """
        accuracies = []
        faithfulness_scores = []
        completeness_scores = []

        for sample in samples:
            # Accuracy: Compare to ground truth
            # Production: Use LLM-as-judge or human evaluation
            accuracy = self._semantic_similarity(
                sample.generated_answer,
                sample.ground_truth_answer
            )
            accuracies.append(accuracy)

            # Faithfulness: Answer grounded in context?
            # Check if answer makes claims not in context
            faithfulness = self._check_faithfulness(sample)
            faithfulness_scores.append(faithfulness)

            # Completeness: All aspects covered?
            completeness = self._check_completeness(sample)
            completeness_scores.append(completeness)

        return {
            'accuracy': np.mean(accuracies),
            'faithfulness': np.mean(faithfulness_scores),
            'completeness': np.mean(completeness_scores)
        }

    def _evaluate_attribution(
        self,
        samples: List[EvaluationSample]
    ) -> Dict[str, float]:
        """
        Evaluate citation quality

        Metrics:
        - Citation recall: % of facts with citations
        - Citation precision: % of citations that are accurate

        Args:
            samples: Evaluation samples

        Returns:
            Attribution metrics
        """
        citation_recalls = []
        citation_precisions = []

        for sample in samples:
            # Citation recall: Are facts cited?
            # Production: Extract claims, check if cited
            # Placeholder: Check if any citations present
            has_citations = len(sample.citations) > 0
            citation_recalls.append(1.0 if has_citations else 0.0)

            # Citation precision: Are citations accurate?
            # Check if cited docs actually support claims
            if sample.citations:
                accurate_citations = len(set(sample.citations) & set(sample.retrieved_doc_ids))
                precision = accurate_citations / len(sample.citations)
            else:
                precision = 0.0

            citation_precisions.append(precision)

        return {
            'citation_recall': np.mean(citation_recalls),
            'citation_precision': np.mean(citation_precisions)
        }

    def _semantic_similarity(self, text1: str, text2: str) -> float:
        """
        Compute semantic similarity between texts

        Production: Use sentence embeddings (SentenceTransformers)

        Args:
            text1: First text
            text2: Second text

        Returns:
            Similarity score (0-1)
        """
        # Placeholder: Lexical overlap
        words1 = set(text1.lower().split())
        words2 = set(text2.lower().split())

        overlap = len(words1 & words2)
        union = len(words1 | words2)

        return overlap / union if union > 0 else 0.0

    def _check_faithfulness(self, sample: EvaluationSample) -> float:
        """
        Check if answer is faithful to context

        Faithfulness = answer only makes claims supported by context

        Production: Use NLI model or LLM-as-judge

        Args:
            sample: Evaluation sample

        Returns:
            Faithfulness score (0-1)
        """
        # Placeholder: Check if answer mentions retrieved docs
        # In production: Use entailment checking
        mentioned_docs = sum(
            1 for doc_id in sample.retrieved_doc_ids
            if doc_id in sample.generated_answer
        )

        return min(mentioned_docs / 3, 1.0)  # Expect ~3 doc mentions

    def _check_completeness(self, sample: EvaluationSample) -> float:
        """
        Check if answer is complete

        Completeness = answer covers all aspects of ground truth

        Args:
            sample: Evaluation sample

        Returns:
            Completeness score (0-1)
        """
        # Placeholder: Length ratio
        # In production: Check coverage of ground truth aspects
        length_ratio = len(sample.generated_answer) / max(len(sample.ground_truth_answer), 1)

        # Penalize too short or too long
        if length_ratio < 0.5 or length_ratio > 2.0:
            return 0.5
        return 0.8

# Example: RAG evaluation
def rag_evaluation_example():
    """
    Demonstrate RAG evaluation

    Scenario: Technical Q&A system
    """

    # Create evaluation samples
    samples = [
        EvaluationSample(
            query="What is RAG?",
            ground_truth_answer="RAG (Retrieval-Augmented Generation) combines retrieval and generation.",
            ground_truth_doc_ids={'doc_1', 'doc_5'},
            retrieved_doc_ids=['doc_1', 'doc_2', 'doc_5', 'doc_8'],
            generated_answer="RAG combines retrieval and generation using retrieved context. [doc_1]",
            citations=['doc_1']
        ),
        EvaluationSample(
            query="How does vector search work?",
            ground_truth_answer="Vector search finds similar items using embedding similarity.",
            ground_truth_doc_ids={'doc_3', 'doc_7'},
            retrieved_doc_ids=['doc_3', 'doc_4', 'doc_7', 'doc_9'],
            generated_answer="Vector search uses embeddings to find similar items efficiently. [doc_3] [doc_7]",
            citations=['doc_3', 'doc_7']
        ),
        EvaluationSample(
            query="What are the benefits of embeddings?",
            ground_truth_answer="Embeddings capture semantic meaning and enable similarity search.",
            ground_truth_doc_ids={'doc_2', 'doc_6'},
            retrieved_doc_ids=['doc_2', 'doc_3', 'doc_6', 'doc_10'],
            generated_answer="Embeddings provide semantic representations for ML tasks. [doc_2]",
            citations=['doc_2']
        )
    ]

    # Evaluate
    evaluator = RAGEvaluator()
    metrics = evaluator.evaluate(samples)

    # Analyze results
    print(f"\n{'='*60}")
    print("Analysis")
    print(f"{'='*60}")

    if metrics['recall@k'] < 0.8:
        print("⚠️  Low recall: Retrieval missing relevant documents")
    if metrics['faithfulness'] < 0.8:
        print("⚠️  Low faithfulness: Answers may contain hallucinations")
    if metrics['citation_recall'] < 0.7:
        print("⚠️  Low citation recall: Facts not properly attributed")

    if metrics['recall@k'] >= 0.8 and metrics['accuracy'] >= 0.8 and metrics['faithfulness'] >= 0.8:
        print("✓ System performing well across all metrics")

# Uncomment to run:
# rag_evaluation_example()
```

:::{.callout-tip}
## RAG Evaluation Best Practices

**Evaluation data:**
- Start with 100-500 query-answer pairs
- Cover diversity of query types (factual, how-to, comparison, etc.)
- Include hard cases (contradictory docs, missing info, ambiguous queries)
- Get human annotations for ground truth (expensive but essential)

**Automated metrics:**
- Retrieval: Recall@10, Recall@100, MRR
- Generation: Semantic similarity to ground truth (SentenceTransformers)
- Faithfulness: NLI models (check entailment between context and answer)
- Attribution: Check if citations support claims

**Human evaluation:**
- Sample 10-20% for human review
- Ask: Is answer accurate? Is answer complete? Are citations correct?
- Use majority vote from 3+ annotators
- Expensive but ground truth for calibrating automated metrics

**Continuous evaluation:**
- Evaluate on every model/prompt change
- Track metrics over time (detect regressions)
- A/B test in production (measure user satisfaction)
:::

## Handling Contradictory Information

Real-world knowledge bases contain contradictions: different sources disagree, information becomes outdated, perspectives conflict. **Contradiction handling** strategies enable RAG systems to navigate disagreements: detecting conflicts, weighing source credibility, presenting multiple perspectives, and updating knowledge as information evolves.

### The Contradiction Challenge

**Types of contradictions:**

1. **Temporal**: Information changes over time
   - "Product price is $99" (2023) vs "$149" (2024)
   - Solution: Prioritize recent information

2. **Source disagreement**: Different sources conflict
   - Source A: "API supports OAuth2" vs Source B: "API uses API keys"
   - Solution: Weigh by source authority/credibility

3. **Perspective differences**: Subjective judgments vary
   - Review 1: "Excellent product" vs Review 2: "Poor quality"
   - Solution: Present multiple perspectives

4. **Partial vs complete**: One source has partial information
   - Doc 1: "Supports Python" vs Doc 2: "Supports Python, Java, Go"
   - Solution: Prefer more complete information

```python
"""
Contradiction Detection and Resolution

Strategies:
1. Detect contradictions: NLI models, entity conflict detection
2. Resolve temporal conflicts: Prioritize recent information
3. Resolve source conflicts: Weigh by credibility
4. Present multiple perspectives: Show disagreement to user

Production approach:
- Detect: Use NLI model to find contradicting statements
- Resolve: Apply resolution strategy based on conflict type
- Present: Show confidence, multiple views, or ask user
"""

from dataclasses import dataclass
from typing import List, Tuple, Optional, Dict
from datetime import datetime
import numpy as np

@dataclass
class Claim:
    """
    Claim extracted from document

    Attributes:
        text: Claim text
        doc_id: Source document
        entity: Entity claim is about
        attribute: Attribute being claimed
        value: Value of attribute
        date: When claim was made
        confidence: Model confidence in extraction
    """
    text: str
    doc_id: str
    entity: str
    attribute: str
    value: str
    date: Optional[datetime] = None
    confidence: float = 1.0

@dataclass
class Contradiction:
    """
    Detected contradiction between claims

    Attributes:
        claim1: First claim
        claim2: Contradicting claim
        contradiction_type: Type of contradiction
        confidence: Confidence in contradiction detection
    """
    claim1: Claim
    claim2: Claim
    contradiction_type: str  # 'temporal', 'source', 'perspective'
    confidence: float

class ContradictionDetector:
    """
    Detect contradictions in retrieved documents

    Approach:
    1. Extract claims from documents
    2. Group claims by entity + attribute
    3. Check if values conflict
    4. Classify contradiction type

    Models:
    - Claim extraction: IE model or LLM
    - Contradiction detection: NLI model
    """

    def __init__(self):
        """Initialize contradiction detector"""
        print("Initialized Contradiction Detector")

    def detect(
        self,
        documents: List[Document]
    ) -> List[Contradiction]:
        """
        Detect contradictions across documents

        Args:
            documents: Retrieved documents

        Returns:
            List of detected contradictions
        """
        # Extract claims from each document
        all_claims = []
        for doc in documents:
            claims = self._extract_claims(doc)
            all_claims.extend(claims)

        print(f"Extracted {len(all_claims)} claims from {len(documents)} documents")

        # Group claims by entity + attribute
        claim_groups = self._group_claims(all_claims)

        # Detect contradictions within each group
        contradictions = []
        for (entity, attribute), claims in claim_groups.items():
            if len(claims) > 1:
                conflicts = self._find_conflicts(claims)
                contradictions.extend(conflicts)

        print(f"Detected {len(contradictions)} contradictions")

        return contradictions

    def _extract_claims(self, document: Document) -> List[Claim]:
        """
        Extract factual claims from document

        Production: Use IE model or LLM with structured output

        Args:
            document: Source document

        Returns:
            List of claims
        """
        # Placeholder: Simple pattern-based extraction
        # In production: Use proper claim extraction model

        claims = []

        # Example: Extract "X is Y" patterns
        # Real implementation would use NER + relation extraction

        # Mock claim for demo
        if 'price' in document.content.lower():
            claims.append(Claim(
                text="Product price is $99",
                doc_id=document.doc_id,
                entity="Product",
                attribute="price",
                value="$99",
                date=document.metadata.get('date'),
                confidence=0.9
            ))

        return claims

    def _group_claims(
        self,
        claims: List[Claim]
    ) -> Dict[Tuple[str, str], List[Claim]]:
        """
        Group claims by entity and attribute

        Args:
            claims: All extracted claims

        Returns:
            Dictionary: (entity, attribute) → [claims]
        """
        groups = {}

        for claim in claims:
            key = (claim.entity, claim.attribute)
            if key not in groups:
                groups[key] = []
            groups[key].append(claim)

        return groups

    def _find_conflicts(
        self,
        claims: List[Claim]
    ) -> List[Contradiction]:
        """
        Find contradictions among claims

        Args:
            claims: Claims about same entity + attribute

        Returns:
            List of contradictions
        """
        contradictions = []

        # Compare all pairs
        for i, claim1 in enumerate(claims):
            for claim2 in claims[i+1:]:
                if self._are_contradictory(claim1, claim2):
                    # Determine contradiction type
                    contra_type = self._classify_contradiction(claim1, claim2)

                    contradictions.append(Contradiction(
                        claim1=claim1,
                        claim2=claim2,
                        contradiction_type=contra_type,
                        confidence=0.8
                    ))

        return contradictions

    def _are_contradictory(self, claim1: Claim, claim2: Claim) -> bool:
        """
        Check if two claims contradict

        Production: Use NLI model

        Args:
            claim1: First claim
            claim2: Second claim

        Returns:
            True if contradictory
        """
        # Simple: Different values for same entity + attribute
        return claim1.value != claim2.value

    def _classify_contradiction(
        self,
        claim1: Claim,
        claim2: Claim
    ) -> str:
        """
        Classify type of contradiction

        Args:
            claim1: First claim
            claim2: Second claim

        Returns:
            Contradiction type
        """
        # Temporal: Different dates
        if claim1.date and claim2.date and claim1.date != claim2.date:
            return 'temporal'

        # Source: Different documents
        if claim1.doc_id != claim2.doc_id:
            return 'source'

        # Perspective (default)
        return 'perspective'

class ContradictionResolver:
    """
    Resolve contradictions using various strategies

    Strategies:
    1. Temporal: Use most recent claim
    2. Source authority: Use most credible source
    3. Confidence: Use highest confidence claim
    4. Present multiple: Show disagreement to user
    """

    def __init__(
        self,
        source_credibility: Optional[Dict[str, float]] = None
    ):
        """
        Args:
            source_credibility: Map of source → credibility score
        """
        self.source_credibility = source_credibility or {}
        print("Initialized Contradiction Resolver")

    def resolve(
        self,
        contradiction: Contradiction
    ) -> Claim:
        """
        Resolve contradiction by selecting best claim

        Args:
            contradiction: Detected contradiction

        Returns:
            Resolved claim
        """
        if contradiction.contradiction_type == 'temporal':
            return self._resolve_temporal(contradiction)
        elif contradiction.contradiction_type == 'source':
            return self._resolve_by_authority(contradiction)
        else:
            return self._resolve_by_confidence(contradiction)

    def _resolve_temporal(self, contradiction: Contradiction) -> Claim:
        """
        Resolve temporal contradiction: use most recent

        Args:
            contradiction: Temporal contradiction

        Returns:
            Most recent claim
        """
        claim1 = contradiction.claim1
        claim2 = contradiction.claim2

        if claim1.date and claim2.date:
            if claim1.date > claim2.date:
                return claim1
            else:
                return claim2

        # If no dates, fall back to confidence
        return self._resolve_by_confidence(contradiction)

    def _resolve_by_authority(self, contradiction: Contradiction) -> Claim:
        """
        Resolve by source authority: use more credible source

        Args:
            contradiction: Source contradiction

        Returns:
            Claim from more credible source
        """
        claim1 = contradiction.claim1
        claim2 = contradiction.claim2

        credibility1 = self.source_credibility.get(claim1.doc_id, 0.5)
        credibility2 = self.source_credibility.get(claim2.doc_id, 0.5)

        if credibility1 > credibility2:
            return claim1
        else:
            return claim2

    def _resolve_by_confidence(self, contradiction: Contradiction) -> Claim:
        """
        Resolve by confidence: use higher confidence claim

        Args:
            contradiction: Contradiction

        Returns:
            Higher confidence claim
        """
        if contradiction.claim1.confidence > contradiction.claim2.confidence:
            return contradiction.claim1
        else:
            return contradiction.claim2

    def format_multiple_perspectives(
        self,
        contradictions: List[Contradiction]
    ) -> str:
        """
        Format contradictions for user presentation

        When unable to resolve automatically, present multiple views

        Args:
            contradictions: List of contradictions

        Returns:
            Formatted text presenting multiple perspectives
        """
        if not contradictions:
            return ""

        output = "Note: Sources provide different information:\n\n"

        for contra in contradictions:
            output += f"• According to [{contra.claim1.doc_id}]: {contra.claim1.text}\n"
            output += f"• According to [{contra.claim2.doc_id}]: {contra.claim2.text}\n"
            output += "\n"

        return output

# Example: Contradiction handling
def contradiction_handling_example():
    """
    Demonstrate contradiction detection and resolution

    Scenario: Product information from multiple sources
    """

    # Create documents with contradictory information
    doc1 = Document(
        doc_id="catalog_2023",
        content="The Premium Laptop is priced at $999 and includes 16GB RAM.",
        metadata={'date': datetime(2023, 6, 1), 'source': 'catalog'}
    )

    doc2 = Document(
        doc_id="website_2024",
        content="The Premium Laptop now costs $1299 with upgraded 32GB RAM.",
        metadata={'date': datetime(2024, 1, 15), 'source': 'website'}
    )

    doc3 = Document(
        doc_id="review_2024",
        content="The Premium Laptop at $1299 offers excellent performance.",
        metadata={'date': datetime(2024, 2, 1), 'source': 'review'}
    )

    documents = [doc1, doc2, doc3]

    # Detect contradictions
    detector = ContradictionDetector()
    contradictions = detector.detect(documents)

    # Resolve contradictions
    resolver = ContradictionResolver(
        source_credibility={
            'catalog_2023': 0.9,
            'website_2024': 0.95,  # Most authoritative
            'review_2024': 0.7
        }
    )

    print(f"\n{'='*60}")
    print("Contradiction Analysis")
    print(f"{'='*60}")

    for i, contra in enumerate(contradictions, 1):
        print(f"\nContradiction {i}: {contra.contradiction_type}")
        print(f"  Claim 1 [{contra.claim1.doc_id}]: {contra.claim1.text}")
        print(f"  Claim 2 [{contra.claim2.doc_id}]: {contra.claim2.text}")

        # Resolve
        resolved = resolver.resolve(contra)
        print(f"  → Resolved: {resolved.text} (from {resolved.doc_id})")
        print(f"  → Reasoning: {contra.contradiction_type} → {'most recent' if contra.contradiction_type == 'temporal' else 'most credible'}")

    # Alternative: Present multiple perspectives
    print(f"\n{'='*60}")
    print("Alternative: Multiple Perspectives")
    print(f"{'='*60}")
    print(resolver.format_multiple_perspectives(contradictions))

# Uncomment to run:
# contradiction_handling_example()
```

:::{.callout-tip}
## Contradiction Handling Best Practices

**Detection:**
- Use NLI models for semantic contradiction detection
- Extract claims with high precision (false contradictions confuse users)
- Focus on factual contradictions (prices, dates, specifications)
- Ignore stylistic differences (different phrasings of same fact)

**Resolution strategies:**
- **Temporal**: Always prefer recent information (but show date)
- **Source authority**: Build credibility scores per source type
- **Confidence**: Use when other signals unavailable
- **Present multiple**: When confident both are valid (perspectives)

**User experience:**
- Always show sources when contradictions exist
- Indicate confidence level ("likely", "possibly", "conflicting sources")
- Provide dates when information might change
- Allow users to see all perspectives (expandable sections)

**Continuous improvement:**
- Log user selections when presented with contradictions
- Update source credibility based on user preferences
- Retrain contradiction detection on corrected examples
:::

:::{.callout-warning}
## Contradiction Pitfalls

**Over-resolving**: Automatically picking one answer when both are valid
- Example: "Best database for X" has multiple valid answers
- Solution: Recognize when question has multiple valid answers

**Temporal confusion**: Using old information because it's higher quality
- Example: Detailed 2022 guide vs brief 2024 update
- Solution: Always prioritize recency for rapidly changing topics

**Authority bias**: Always trusting "authoritative" source
- Example: Official docs outdated, community docs current
- Solution: Consider recency + authority together

**Hidden contradictions**: Not detecting subtle conflicts
- Example: "Supports OAuth2" vs "Requires API keys" (implicit contradiction)
- Solution: Use semantic contradiction detection, not just exact mismatches
:::

## Key Takeaways

- **RAG combines retrieval and generation for grounded LLM responses**: Retrieving relevant context from vector databases enables accurate answers over billion-document corpora while maintaining attribution and enabling real-time knowledge updates

- **Enterprise RAG requires multi-component architecture**: Query understanding, retrieval, reranking, context assembly, generation, and validation each play critical roles, and each must scale independently

- **Context window optimization maximizes information density**: Passage extraction, deduplication, and hierarchical assembly enable fitting relevant information within LLM token limits while preserving key facts

- **Multi-stage retrieval balances recall and precision**: Early stages (vector search) optimize for recall across billion-doc corpora, later stages (reranking, diversity) optimize for precision with expensive models on small candidate sets

- **RAG evaluation requires measuring beyond retrieval and generation**: End-to-end metrics must capture retrieval relevance, context utilization, answer accuracy, factual consistency, attribution quality, and user satisfaction

- **Contradiction handling enables navigating disagreements in knowledge bases**: Temporal resolution (prefer recent), source authority weighting (prefer credible), and multi-perspective presentation handle conflicts when sources disagree

- **Production RAG demands comprehensive engineering**: Caching, batching, circuit breakers, monitoring, A/B testing, and continuous evaluation separate research prototypes from production systems serving millions of users

## Looking Ahead

Chapter 13 demonstrated how RAG leverages embeddings for grounded generation at enterprise scale. Chapter 14 expands semantic search beyond text: multi-modal search across text, images, audio, and video; code search for software intelligence; scientific literature and patent search with domain-specific understanding; media and content discovery across creative assets; and knowledge graph integration for structured reasoning. These applications demonstrate embeddings' versatility across diverse modalities and domains.

## Further Reading

### RAG Foundations
- Lewis, Patrick, et al. (2020). "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks." NeurIPS.
- Guu, Kelvin, et al. (2020). "REALM: Retrieval-Augmented Language Model Pre-Training." ICML.
- Izacard, Gautier, et al. (2021). "Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering." EACL.

### Retrieval Systems
- Karpukhin, Vladimir, et al. (2020). "Dense Passage Retrieval for Open-Domain Question Answering." EMNLP.
- Xiong, Lee, et al. (2021). "Approximate Nearest Neighbor Negative Contrastive Learning for Dense Text Retrieval." ICLR.
- Santhanam, Keshav, et al. (2022). "ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction." NAACL.

### Context Optimization
- Jiang, Zhengbao, et al. (2023). "Long-Form Factuality in Large Language Models." arxiv.
- Liu, Nelson F., et al. (2023). "Lost in the Middle: How Language Models Use Long Contexts." arxiv.
- Press, Ofir, et al. (2022). "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation." ICLR.

### RAG Evaluation
- Chen, Daixuan, et al. (2023). "CRUD-RAG: Benchmarking Retrieval-Augmented Generation for Time-Sensitive Knowledge." arxiv.
- Es, Shahul, et al. (2023). "RAGAS: Automated Evaluation of Retrieval Augmented Generation." arxiv.
- Liu, Yang, et al. (2023). "Evaluating the Factuality of Large Language Models." ACL.

### Production Systems
- Anthropic (2023). "Claude 2 System Card."
- OpenAI (2023). "GPT-4 Technical Report."
- Thoppilan, Romal, et al. (2022). "LaMDA: Language Models for Dialog Applications." arxiv.

### Contradiction Detection
- Welleck, Sean, et al. (2019). "Dialogue Natural Language Inference." ACL.
- Honovich, Or, et al. (2022). "TRUE: Re-evaluating Factual Consistency Evaluation." NAACL.
- Wang, Cunxiang, et al. (2020). "CARE: Commonsense-Aware Reasoning for Conversational AI." ACL.

### Multi-Stage Retrieval
- Nogueira, Rodrigo, et al. (2019). "Passage Re-ranking with BERT." arxiv.
- Gao, Luyu, et al. (2021). "Rethink Training of BERT Rerankers in Multi-Stage Retrieval Pipeline." ECIR.
- Carbonell, Jaime, and Jade Goldstein (1998). "The Use of MMR, Diversity-Based Reranking for Reordering Documents and Producing Summaries." SIGIR.
