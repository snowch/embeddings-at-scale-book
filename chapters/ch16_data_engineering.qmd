# Data Engineering for Embeddings {#sec-data-engineering}

:::{.callout-note}
## Chapter Overview
High-quality embeddings demand high-quality data engineering. This chapter explores the data infrastructure that enables trillion-row embedding systems: ETL pipelines that transform raw data into training-ready formats while preserving semantic relationships, streaming architectures that update embeddings in near-real-time as data evolves, data quality frameworks that detect and remediate issues before they corrupt embeddings, schema evolution strategies that maintain backwards compatibility across model versions, and multi-source data fusion techniques that combine embeddings from heterogeneous datasets. These data engineering practices ensure embedding models have the clean, consistent, well-structured data needed to achieve their potential in production.
:::

After optimizing vector operations for sub-millisecond search (@sec-high-performance-vector-ops), the remaining production challenge is **data engineering**. Embeddings are only as good as the data they're trained on. A model trained on corrupted data produces corrupted embeddings. A pipeline that can't handle schema changes breaks during routine database migrations. A system that can't fuse data from multiple sources misses critical context. This chapter addresses the data engineering practices that separate prototype embedding systems from production-ready platforms serving billions of users.

## ETL Pipelines for Embedding Generation

Embedding generation requires transforming raw data—database records, documents, images, logs—into vector representations while preserving semantic meaning. **ETL (Extract, Transform, Load) pipelines** orchestrate this transformation at scale, handling data extraction from diverse sources, feature engineering that captures relevant signals, quality validation that ensures training stability, and efficient loading into training systems.

### The Embedding ETL Challenge

Traditional ETL optimizes for data warehousing: schema normalization, aggregation, and SQL-friendly formats. **Embedding ETL** has unique requirements:

- **Semantic preservation**: Transformations must preserve meaning (normalization can destroy signal)
- **Feature engineering**: Extract features that capture relationships (not just facts)
- **Scale**: Process billions of records efficiently (trillion-row datasets)
- **Freshness**: Keep training data current (embedding drift occurs within weeks)
- **Multimodal**: Handle text, images, structured data, time series simultaneously

```python
{{< include /code_examples/ch16_data_engineering/from.py >}}
```

:::{.callout-tip}
## ETL Best Practices for Embeddings

**Data quality:**

- Validate at every stage (extract, transform, load)
- Implement deduplication (exact and near-duplicate)
- Handle missing values explicitly (don't drop silently)
- Monitor data drift (distribution shifts over time)

**Performance:**

- Partition data for parallel processing (100-1000 partitions)
- Use columnar formats (Parquet) for analytics
- Implement checkpointing for fault tolerance
- Optimize for I/O (sequential reads, batching)

**Maintainability:**

- Keep transformations simple and composable
- Document feature engineering logic
- Version control pipeline code
- Test with representative samples before production runs
:::

## Streaming Embedding Updates

Batch ETL processes data hourly or daily, but many applications need **real-time embeddings**. A news recommender must embed articles seconds after publication. A fraud detector must embed transactions milliseconds after they occur. **Streaming architectures** enable continuous embedding updates with end-to-end latency measured in seconds, not hours.

### Streaming vs. Batch: The Trade-off

**Batch processing** (hourly/daily):

- **Advantages**: Simple, efficient, easy to debug, supports complex aggregations
- **Disadvantages**: Stale embeddings (hours old), high latency for new items
- **Use when**: Daily updates sufficient, complex transformations required

**Stream processing** (seconds):

- **Advantages**: Fresh embeddings (seconds old), low latency for new items, event-driven
- **Disadvantages**: Complex architecture, harder to debug, limited aggregation window
- **Use when**: Real-time updates critical, simple transformations, event-driven workflows

```python
{{< include /code_examples/ch16_data_engineering/class.py >}}
```

:::{.callout-tip}
## Streaming Architecture Best Practices

**Micro-batching:**

- Batch window: 100-1000ms (balance latency vs throughput)
- Batch size: 10-100 items (optimize for GPU)
- Adaptive batching: Adjust based on load

**Fault tolerance:**

- Checkpointing: Save progress every N events
- Exactly-once semantics: Idempotent operations
- Dead letter queue: Handle failed events separately
- Retry logic: Exponential backoff for transient failures

**Monitoring:**

- End-to-end latency (p50, p95, p99)
- Throughput (events/second)
- Error rate (failures / total events)
- Queue depth (backpressure indicator)
:::

:::{.callout-warning}
## Streaming Complexity

Streaming pipelines are significantly more complex than batch:

- **Debugging**: Harder to reproduce issues (event order matters)
- **Testing**: Need to simulate real-time event streams
- **Operations**: 24/7 monitoring required
- **Cost**: Higher infrastructure costs (always running)

Start with batch, migrate to streaming only when business value justifies complexity.
:::

## Data Quality for Embedding Training

Poor data quality causes poor embeddings. **Data quality frameworks** detect and remediate issues before they corrupt training: duplicate detection prevents training on repeated examples, outlier detection identifies corrupted or adversarial data, consistency validation ensures relationships hold across updates, and drift detection alerts when distributions shift unexpectedly.

### The Data Quality Challenge for Embeddings

Traditional data quality focuses on completeness and correctness. **Embedding quality** has additional requirements:

- **Semantic consistency**: Similar items must have similar features
- **Label quality**: Incorrect labels poison contrastive learning
- **Distribution stability**: Embedding space shifts when data distribution changes
- **Relationship preservation**: Entity relationships must remain consistent

```python
{{< include /code_examples/ch16_data_engineering/from_1.py >}}
```

:::{.callout-tip}
## Data Quality Best Practices

**Prevention:**

- Validate at ingestion (catch issues early)
- Implement schema contracts (enforce structure)
- Use type systems (prevent type errors)
- Automate quality checks (continuous validation)

**Detection:**

- Statistical profiling (baseline distributions)
- Anomaly detection (outliers, drift)
- Relationship validation (foreign keys, consistency)
- Duplicate detection (exact and near-duplicate)

**Remediation:**

- Automated fixes (fill missing values, clip outliers)
- Human review queue (ambiguous cases)
- Dead letter queue (unfixable records)
- Feedback loops (fix upstream sources)

**Monitoring:**

- Quality dashboards (real-time metrics)
- Alerts on degradation (threshold breaches)
- Trend analysis (quality over time)
- Root cause analysis (trace issues to source)
:::

## Schema Evolution and Backwards Compatibility

Production embedding systems evolve: new features are added, old features deprecated, data types change. **Schema evolution** enables safe changes while maintaining backwards compatibility with existing embeddings, models, and downstream consumers.

### The Schema Evolution Challenge

Embedding systems have complex dependencies:

- **Trained models**: Expect specific feature schema
- **Vector indices**: Store embeddings from specific model versions
- **Downstream consumers**: Query embeddings with specific schemas
- **Historical data**: May use old schemas

**Change one component**, and the entire system can break.

```python
{{< include /code_examples/ch16_data_engineering/from_2.py >}}
```

:::{.callout-tip}
## Schema Evolution Best Practices

**Safe evolution strategies:**

- **Additive changes only**: Add fields, don't remove (backwards compatible)
- **Deprecation before removal**: Mark fields deprecated for 1-2 versions
- **Default values**: Provide defaults for new required fields
- **Version tagging**: Tag data with schema version explicitly

**Migration strategies:**

- **Online migration**: Transform data on-read (lazy)
- **Offline migration**: Reprocess entire dataset (eager)
- **Hybrid**: Migrate hot data online, cold data offline

**Compatibility levels:**

- **Forward compatible**: New consumers can read old data
- **Backward compatible**: Old consumers can read new data
- **Full compatibility**: Both directions work
:::

:::{.callout-warning}
## Breaking Changes

Some changes cannot be made backwards-compatible:

- Removing required fields
- Changing field types incompatibly
- Removing entire entities

For breaking changes:
1. **Version bump**: Increment major version (v1 → v2)
2. **Parallel operation**: Run both versions simultaneously
3. **Gradual migration**: Migrate consumers incrementally
4. **Deprecation timeline**: Announce timeline (3-6 months)
5. **Sunset old version**: Remove after migration complete
:::

## Multi-Source Data Fusion

Production embedding systems integrate data from multiple sources: user profiles from CRM, product data from inventory, behavioral logs from analytics, external data from partners. **Multi-source data fusion** combines these heterogeneous datasets into unified embeddings while handling schema mismatches, different update frequencies, and varying data quality.

### The Data Fusion Challenge

Each data source has unique characteristics:

- **Schema**: Different field names, types, structures
- **Frequency**: Some update real-time, others daily/weekly
- **Quality**: Varying completeness, correctness, timeliness
- **Scale**: Some have millions of records, others billions
- **Access**: APIs, databases, files, streams

**Challenge**: Combine these sources into training data that preserves relationships across sources.

```python
{{< include /code_examples/ch16_data_engineering/from_3.py >}}
```

:::{.callout-tip}
## Multi-Source Fusion Best Practices

**Schema management:**

- **Canonical schema**: Define single target schema
- **Schema registry**: Centralize source schema definitions
- **Schema evolution**: Version schemas and migrate incrementally
- **Type safety**: Validate types during alignment

**Conflict resolution:**

- **Priority-based**: Assign priority to sources (authority)
- **Recency-based**: Prefer most recently updated value
- **Quality-based**: Weight by source quality score
- **Context-aware**: Consider semantic meaning

**Performance:**

- **Incremental fusion**: Only fuse changed entities
- **Partitioning**: Partition by entity_id for parallel fusion
- **Caching**: Cache fused results (invalidate on update)
- **Lazy loading**: Fuse on-demand for rarely accessed entities
:::

## Key Takeaways

- **ETL pipelines must preserve semantic relationships**: Unlike traditional ETL that optimizes for SQL analytics, embedding ETL requires feature engineering that captures similarity and meaning, not just facts

- **Streaming enables real-time embeddings with sub-second latency**: Micro-batching architectures (100-1000ms windows) balance throughput and latency, enabling fresh embeddings for dynamic content like news and social media

- **Data quality directly determines embedding quality**: Comprehensive validation (schema, values, semantics, duplicates, drift) prevents training on corrupted data that would poison embeddings for months

- **Schema evolution requires careful coordination across components**: Backwards-compatible changes (add fields, provide defaults) enable safe evolution while breaking changes (remove fields, change types) require parallel operation and gradual migration

- **Multi-source fusion combines heterogeneous datasets into unified embeddings**: Schema alignment, entity resolution, conflict resolution, and temporal alignment enable leveraging data from multiple systems with different schemas and update frequencies

- **Data engineering is the foundation of embedding systems**: High-quality embeddings require high-quality data engineering; invest in pipelines, quality frameworks, and fusion strategies before scaling models

- **The data engineering hierarchy**: Quality (1000× impact) > Schema design (100× impact) > Performance (10× impact). Focus on correctness before optimizing throughput

## Looking Ahead

Part III (Production Engineering) concludes with robust data engineering practices that ensure embedding systems have the clean, consistent, high-quality data needed to achieve their potential. Part IV (Advanced Applications) begins with Chapter 13, which explores Retrieval-Augmented Generation at enterprise scale: RAG architecture patterns that combine embedding retrieval with language models, context window optimization for billion-document corpora, multi-stage retrieval systems that balance recall and precision, evaluation frameworks that measure end-to-end quality, and strategies for handling contradictory information across sources.

## Further Reading

### Data Engineering
- Kleppmann, Martin (2017). "Designing Data-Intensive Applications." O'Reilly Media.
- Reis, Cathy, and Rupal Mahajan (2019). "Data Engineering with Apache Spark, Delta Lake, and Lakehouse." O'Reilly Media.
- Kalidindi, Santhosh (2021). "Data Engineering with Python." Packt Publishing.

### ETL and Pipelines
- Kimball, Ralph, and Margy Ross (2013). "The Data Warehouse Toolkit." Wiley.
- Apache Airflow Documentation. "Best Practices."
- dbt Documentation. "Best Practices for Data Transformation."

### Streaming Systems
- Kleppmann, Martin (2016). "Making Sense of Stream Processing." O'Reilly Media.
- Narkhede, Neha, et al. (2017). "Kafka: The Definitive Guide." O'Reilly Media.
- Apache Flink Documentation. "Streaming Concepts."

### Data Quality
- Redman, Thomas (2016). "Getting in Front on Data Quality." Harvard Business Review.
- Batini, Carlo, and Monica Scannapieco (2016). "Data and Information Quality." Springer.
- Talend Data Quality Documentation. "Data Quality Best Practices."

### Schema Evolution
- Kleppmann, Martin (2015). "Schema Evolution in Avro, Protocol Buffers and Thrift." Blog post.
- Confluent Documentation. "Schema Evolution and Compatibility."
- Fowler, Martin (2016). "Evolutionary Database Design." martinfowler.com.

### Data Integration
- Doan, AnHai, et al. (2012). "Principles of Data Integration." Morgan Kaufmann.
- Haas, Laura, et al. (2005). "Clio Grows Up: From Research Prototype to Industrial Tool." SIGMOD.
- Madhavan, Jayant, et al. (2001). "Generic Schema Matching with Cupid." VLDB.
