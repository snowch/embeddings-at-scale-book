# Data Engineering for Embeddings {#sec-data-engineering}

:::{.callout-note}
## Chapter Overview
High-quality embeddings demand high-quality data engineering. This chapter explores the data infrastructure that enables trillion-row embedding systems: ETL pipelines that transform raw data into training-ready formats while preserving semantic relationships, streaming architectures that update embeddings in near-real-time as data evolves, data quality frameworks that detect and remediate issues before they corrupt embeddings, schema evolution strategies that maintain backwards compatibility across model versions, and multi-source data fusion techniques that combine embeddings from heterogeneous datasets. These data engineering practices ensure embedding models have the clean, consistent, well-structured data needed to achieve their potential in production.
:::

After optimizing vector operations for sub-millisecond search (@sec-high-performance-vector-ops), the remaining production challenge is **data engineering**. Embeddings are only as good as the data they're trained on. A model trained on corrupted data produces corrupted embeddings. A pipeline that can't handle schema changes breaks during routine database migrations. A system that can't fuse data from multiple sources misses critical context. This chapter addresses the data engineering practices that separate prototype embedding systems from production-ready platforms serving billions of users.

## ETL Pipelines for Embedding Generation

Embedding generation requires transforming raw data—database records, documents, images, logs—into vector representations while preserving semantic meaning. **ETL (Extract, Transform, Load) pipelines** orchestrate this transformation at scale, handling data extraction from diverse sources, feature engineering that captures relevant signals, quality validation that ensures training stability, and efficient loading into training systems.

### The Embedding ETL Challenge

Traditional ETL optimizes for data warehousing: schema normalization, aggregation, and SQL-friendly formats. **Embedding ETL** has unique requirements:

- **Semantic preservation**: Transformations must preserve meaning (normalization can destroy signal)
- **Feature engineering**: Extract features that capture relationships (not just facts)
- **Scale**: Process billions of records efficiently (trillion-row datasets)
- **Freshness**: Keep training data current (embedding drift occurs within weeks)
- **Multimodal**: Handle text, images, structured data, time series simultaneously

```python
"""
Embedding-Aware ETL Pipeline

Key differences from traditional ETL:
1. Preserve semantic relationships during transformation
2. Generate features that capture similarity, not just attributes
3. Handle multimodal data (text, images, structured)
4. Maintain data lineage for debugging embedding quality
5. Support incremental updates for continuous training
"""

import pandas as pd
import numpy as np
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass
from datetime import datetime, timedelta
import json
from pathlib import Path
import hashlib

@dataclass
class DataRecord:
    """
    Raw data record from source system

    Attributes:
        record_id: Unique identifier
        data: Raw data payload
        source: Source system identifier
        timestamp: When record was created
        metadata: Additional context
    """
    record_id: str
    data: Dict[str, Any]
    source: str
    timestamp: datetime
    metadata: Dict[str, Any]

@dataclass
class EmbeddingFeatures:
    """
    Transformed features ready for embedding generation

    Attributes:
        record_id: Links back to original record
        text_features: Text content for embedding
        structured_features: Numerical/categorical features
        context_features: Additional context (user, session, etc.)
        labels: Labels for supervised learning (optional)
        data_hash: Hash for duplicate detection
    """
    record_id: str
    text_features: Optional[str] = None
    structured_features: Optional[Dict[str, float]] = None
    context_features: Optional[Dict[str, Any]] = None
    labels: Optional[List[str]] = None
    data_hash: Optional[str] = None

class EmbeddingETLPipeline:
    """
    ETL pipeline for embedding generation at scale

    Architecture:
    1. Extract: Pull data from multiple sources (databases, APIs, files)
    2. Transform: Feature engineering + quality validation
    3. Load: Write to training system (cloud storage, feature store)

    Design principles:
    - Idempotent: Re-running produces same results
    - Incremental: Process only new/changed records
    - Traceable: Full lineage from raw data to embeddings
    - Scalable: Handles billions of records via partitioning

    Production considerations:
    - Checkpointing for fault tolerance
    - Monitoring for data drift
    - Schema validation at each stage
    - Resource optimization (memory, compute)
    """

    def __init__(
        self,
        output_path: str,
        checkpoint_path: Optional[str] = None,
        batch_size: int = 10000
    ):
        """
        Args:
            output_path: Where to write transformed features
            checkpoint_path: Where to save progress checkpoints
            batch_size: Records per batch
        """
        self.output_path = Path(output_path)
        self.checkpoint_path = Path(checkpoint_path) if checkpoint_path else None
        self.batch_size = batch_size

        # Statistics
        self.records_processed = 0
        self.records_skipped = 0
        self.records_failed = 0

        # State
        self.last_checkpoint_id: Optional[str] = None
        self._load_checkpoint()

        print(f"Initialized Embedding ETL Pipeline")
        print(f"  Output: {output_path}")
        print(f"  Batch size: {batch_size:,}")
        if self.last_checkpoint_id:
            print(f"  Resuming from checkpoint: {self.last_checkpoint_id}")

    def extract(
        self,
        source_iterator,
        start_time: Optional[datetime] = None
    ) -> List[DataRecord]:
        """
        Extract data from source system

        Supports incremental extraction:
        - Read only records after last checkpoint
        - Handle pagination for large datasets
        - Implement retry logic for transient failures

        Args:
            source_iterator: Iterator over source records
            start_time: Only extract records after this time (incremental)

        Returns:
            List of DataRecords
        """
        records = []

        for raw_record in source_iterator:
            # Skip records before checkpoint (incremental processing)
            if start_time and raw_record.get('timestamp') < start_time:
                continue

            # Parse into DataRecord
            try:
                record = DataRecord(
                    record_id=raw_record['id'],
                    data=raw_record.get('data', {}),
                    source=raw_record.get('source', 'unknown'),
                    timestamp=raw_record.get('timestamp', datetime.now()),
                    metadata=raw_record.get('metadata', {})
                )
                records.append(record)

            except Exception as e:
                print(f"⚠️  Failed to parse record: {e}")
                self.records_failed += 1
                continue

        print(f"Extracted {len(records):,} records from source")
        return records

    def transform(
        self,
        records: List[DataRecord]
    ) -> List[EmbeddingFeatures]:
        """
        Transform raw records into embedding features

        Key transformations:
        1. Text normalization (preserve semantic meaning)
        2. Feature extraction (capture relationships)
        3. Context enrichment (add user/session context)
        4. Deduplication (hash-based)
        5. Quality validation

        Args:
            records: Raw data records

        Returns:
            List of EmbeddingFeatures
        """
        features_list = []
        seen_hashes = set()

        for record in records:
            try:
                # Extract features
                features = self._extract_features(record)

                # Generate hash for deduplication
                features.data_hash = self._compute_hash(features)

                # Skip duplicates
                if features.data_hash in seen_hashes:
                    self.records_skipped += 1
                    continue
                seen_hashes.add(features.data_hash)

                # Validate quality
                if not self._validate_quality(features):
                    self.records_skipped += 1
                    continue

                features_list.append(features)
                self.records_processed += 1

            except Exception as e:
                print(f"⚠️  Transform failed for record {record.record_id}: {e}")
                self.records_failed += 1
                continue

        print(f"Transformed {len(features_list):,} records")
        print(f"  Skipped: {self.records_skipped:,} (duplicates + quality)")
        print(f"  Failed: {self.records_failed:,}")

        return features_list

    def _extract_features(
        self,
        record: DataRecord
    ) -> EmbeddingFeatures:
        """
        Extract features from raw record

        Feature engineering strategies:
        - Text: Combine title, description, tags into single string
        - Structured: Normalize numerical features, encode categoricals
        - Context: Add temporal, user, session information

        Args:
            record: Raw data record

        Returns:
            EmbeddingFeatures
        """
        # Text features: Combine multiple text fields
        text_parts = []
        for field in ['title', 'description', 'content', 'tags']:
            if field in record.data and record.data[field]:
                text_parts.append(str(record.data[field]))

        text_features = " ".join(text_parts) if text_parts else None

        # Structured features: Extract numerical/categorical
        structured_features = {}
        for key, value in record.data.items():
            if isinstance(value, (int, float)):
                structured_features[key] = float(value)
            elif isinstance(value, bool):
                structured_features[key] = float(value)

        # Context features: Metadata that provides additional signal
        context_features = {
            'source': record.source,
            'timestamp': record.timestamp.isoformat(),
            **record.metadata
        }

        # Labels: Extract from metadata if available
        labels = record.metadata.get('labels', None)

        return EmbeddingFeatures(
            record_id=record.record_id,
            text_features=text_features,
            structured_features=structured_features if structured_features else None,
            context_features=context_features,
            labels=labels
        )

    def _compute_hash(self, features: EmbeddingFeatures) -> str:
        """
        Compute hash for deduplication

        Hash content (not metadata):
        - Text features
        - Structured features

        Exclude:
        - record_id (same content, different IDs should dedupe)
        - timestamps (same content, different times should dedupe)

        Args:
            features: Features to hash

        Returns:
            Hash string
        """
        hash_input = {
            'text': features.text_features,
            'structured': features.structured_features
        }

        hash_str = json.dumps(hash_input, sort_keys=True)
        return hashlib.md5(hash_str.encode()).hexdigest()

    def _validate_quality(self, features: EmbeddingFeatures) -> bool:
        """
        Validate feature quality

        Quality checks:
        - Has at least one feature type (text or structured)
        - Text is not empty or too short
        - Structured features are valid numbers
        - No extreme outliers

        Args:
            features: Features to validate

        Returns:
            True if valid, False otherwise
        """
        # Must have at least text or structured features
        if not features.text_features and not features.structured_features:
            return False

        # Text validation
        if features.text_features:
            # Must have minimum length
            if len(features.text_features.strip()) < 10:
                return False

            # Must not be too long (likely corrupted)
            if len(features.text_features) > 100000:
                return False

        # Structured features validation
        if features.structured_features:
            for key, value in features.structured_features.items():
                # Must be valid number
                if not np.isfinite(value):
                    return False

                # Check for extreme outliers (likely errors)
                if abs(value) > 1e10:
                    return False

        return True

    def load(
        self,
        features_list: List[EmbeddingFeatures],
        output_format: str = 'jsonl'
    ):
        """
        Load features to output destination

        Output formats:
        - jsonl: JSON Lines (one record per line)
        - parquet: Columnar format (efficient for large datasets)
        - tfrecord: TensorFlow format (for TF-based training)

        Partitioning strategy:
        - Partition by date for time-based incremental processing
        - Partition by hash for parallel processing

        Args:
            features_list: Features to write
            output_format: Output format ('jsonl', 'parquet', 'tfrecord')
        """
        print(f"Loading {len(features_list):,} records to {self.output_path}")

        # Create output directory
        self.output_path.mkdir(parents=True, exist_ok=True)

        if output_format == 'jsonl':
            self._load_jsonl(features_list)
        elif output_format == 'parquet':
            self._load_parquet(features_list)
        else:
            raise ValueError(f"Unsupported output format: {output_format}")

        print(f"✓ Loaded {len(features_list):,} records")

    def _load_jsonl(self, features_list: List[EmbeddingFeatures]):
        """Write features as JSON Lines"""
        output_file = self.output_path / f"features_{datetime.now().strftime('%Y%m%d_%H%M%S')}.jsonl"

        with open(output_file, 'w') as f:
            for features in features_list:
                record = {
                    'record_id': features.record_id,
                    'text_features': features.text_features,
                    'structured_features': features.structured_features,
                    'context_features': features.context_features,
                    'labels': features.labels,
                    'data_hash': features.data_hash
                }
                f.write(json.dumps(record) + '\n')

        print(f"  Wrote {output_file}")

    def _load_parquet(self, features_list: List[EmbeddingFeatures]):
        """Write features as Parquet"""
        # Convert to DataFrame
        records = []
        for features in features_list:
            records.append({
                'record_id': features.record_id,
                'text_features': features.text_features,
                'structured_features': json.dumps(features.structured_features),
                'context_features': json.dumps(features.context_features),
                'labels': json.dumps(features.labels),
                'data_hash': features.data_hash
            })

        df = pd.DataFrame(records)

        output_file = self.output_path / f"features_{datetime.now().strftime('%Y%m%d_%H%M%S')}.parquet"
        df.to_parquet(output_file, index=False)

        print(f"  Wrote {output_file}")

    def run(
        self,
        source_iterator,
        incremental: bool = True,
        output_format: str = 'jsonl'
    ):
        """
        Run complete ETL pipeline

        Args:
            source_iterator: Iterator over source records
            incremental: Only process new records since last checkpoint
            output_format: Output format
        """
        print("Starting ETL pipeline...")
        start_time = datetime.now()

        # Extract
        extract_start = None
        if incremental and self.last_checkpoint_id:
            # In production: Query checkpoint to get timestamp
            extract_start = datetime.now() - timedelta(days=1)  # Placeholder

        records = self.extract(source_iterator, start_time=extract_start)

        if not records:
            print("No new records to process")
            return

        # Transform
        features_list = self.transform(records)

        if not features_list:
            print("No valid features after transformation")
            return

        # Load
        self.load(features_list, output_format=output_format)

        # Checkpoint
        if self.checkpoint_path:
            self._save_checkpoint(records[-1].record_id if records else None)

        elapsed = (datetime.now() - start_time).total_seconds()

        print(f"\n✓ ETL pipeline complete")
        print(f"  Duration: {elapsed:.1f}s")
        print(f"  Throughput: {self.records_processed / elapsed:.0f} records/sec")
        print(f"  Success rate: {self.records_processed / (self.records_processed + self.records_failed):.2%}")

    def _save_checkpoint(self, last_record_id: Optional[str]):
        """Save checkpoint for incremental processing"""
        if not self.checkpoint_path or not last_record_id:
            return

        self.checkpoint_path.mkdir(parents=True, exist_ok=True)

        checkpoint = {
            'last_record_id': last_record_id,
            'timestamp': datetime.now().isoformat(),
            'records_processed': self.records_processed,
            'records_skipped': self.records_skipped,
            'records_failed': self.records_failed
        }

        checkpoint_file = self.checkpoint_path / 'checkpoint.json'
        with open(checkpoint_file, 'w') as f:
            json.dump(checkpoint, f, indent=2)

        print(f"✓ Saved checkpoint: {last_record_id}")

    def _load_checkpoint(self):
        """Load checkpoint for resuming"""
        if not self.checkpoint_path:
            return

        checkpoint_file = self.checkpoint_path / 'checkpoint.json'
        if not checkpoint_file.exists():
            return

        with open(checkpoint_file, 'r') as f:
            checkpoint = json.load(f)

        self.last_checkpoint_id = checkpoint.get('last_record_id')

class DistributedETLPipeline:
    """
    Distributed ETL for trillion-row scale

    Architecture:
    - Partition data by key (user_id, date, hash)
    - Process partitions in parallel (100-1000 workers)
    - Shuffle and merge for global operations (deduplication)
    - Write to distributed storage (S3, GCS, HDFS)

    Technologies:
    - Spark for distributed processing
    - Delta Lake for ACID transactions
    - Airflow for orchestration

    Performance:
    - Single-node: 100K records/sec
    - 100-node cluster: 10M records/sec
    - 1000-node cluster: 100M records/sec
    """

    def __init__(
        self,
        num_partitions: int = 100,
        output_path: str = "s3://embeddings/features/"
    ):
        """
        Args:
            num_partitions: Number of partitions for parallel processing
            output_path: Distributed storage path
        """
        self.num_partitions = num_partitions
        self.output_path = output_path

        print(f"Initialized Distributed ETL Pipeline")
        print(f"  Partitions: {num_partitions}")
        print(f"  Output: {output_path}")

    def partition_data(
        self,
        records: List[DataRecord],
        partition_key: str = 'hash'
    ) -> Dict[int, List[DataRecord]]:
        """
        Partition data for parallel processing

        Partition strategies:
        - hash: Hash record_id for even distribution
        - date: Partition by timestamp for temporal locality
        - key: Partition by specific field (user_id, category)

        Args:
            records: Records to partition
            partition_key: Partitioning strategy

        Returns:
            Dict mapping partition_id to records
        """
        partitions = {i: [] for i in range(self.num_partitions)}

        for record in records:
            if partition_key == 'hash':
                partition_id = hash(record.record_id) % self.num_partitions
            elif partition_key == 'date':
                partition_id = record.timestamp.day % self.num_partitions
            else:
                partition_id = hash(str(record.data.get(partition_key, ''))) % self.num_partitions

            partitions[partition_id].append(record)

        print(f"Partitioned {len(records):,} records into {self.num_partitions} partitions")

        # Print partition sizes
        sizes = [len(p) for p in partitions.values()]
        print(f"  Partition sizes: min={min(sizes)}, max={max(sizes)}, avg={sum(sizes)/len(sizes):.0f}")

        return partitions

    def process_partition(
        self,
        partition_id: int,
        records: List[DataRecord]
    ) -> List[EmbeddingFeatures]:
        """
        Process single partition (runs on worker node)

        Args:
            partition_id: Partition identifier
            records: Records in this partition

        Returns:
            Transformed features
        """
        print(f"Processing partition {partition_id} with {len(records):,} records")

        # Create single-node pipeline for this partition
        pipeline = EmbeddingETLPipeline(
            output_path=f"{self.output_path}/partition_{partition_id}",
            batch_size=10000
        )

        # Transform records
        features_list = pipeline.transform(records)

        return features_list

# Example: E-commerce product ETL
def ecommerce_etl_example():
    """
    ETL pipeline for e-commerce product embeddings

    Source: Product catalog (database)
    Transform: Combine title, description, category, attributes
    Load: Training-ready features

    Scale: 100M products, updated daily
    """

    # Simulate source data
    def generate_source_records(count=1000):
        """Simulate product catalog records"""
        for i in range(count):
            yield {
                'id': f'product_{i}',
                'data': {
                    'title': f'Product {i}',
                    'description': f'This is a great product for {i % 10} use cases',
                    'category': ['Electronics', 'Clothing', 'Books'][i % 3],
                    'price': 10.0 + (i % 100),
                    'rating': 3.0 + (i % 5) * 0.5,
                    'tags': ['tag1', 'tag2', 'tag3']
                },
                'source': 'product_db',
                'timestamp': datetime.now() - timedelta(hours=i % 24),
                'metadata': {
                    'labels': [['Electronics', 'Clothing', 'Books'][i % 3]]
                }
            }

    # Initialize pipeline
    pipeline = EmbeddingETLPipeline(
        output_path='/tmp/embeddings/features',
        checkpoint_path='/tmp/embeddings/checkpoints',
        batch_size=100
    )

    # Run ETL
    source_iterator = generate_source_records(1000)
    pipeline.run(
        source_iterator,
        incremental=True,
        output_format='jsonl'
    )

    print(f"\n✓ E-commerce ETL complete")
    print(f"  Output: {pipeline.output_path}")

# Uncomment to run:
# ecommerce_etl_example()
```

:::{.callout-tip}
## ETL Best Practices for Embeddings

**Data quality:**
- Validate at every stage (extract, transform, load)
- Implement deduplication (exact and near-duplicate)
- Handle missing values explicitly (don't drop silently)
- Monitor data drift (distribution shifts over time)

**Performance:**
- Partition data for parallel processing (100-1000 partitions)
- Use columnar formats (Parquet) for analytics
- Implement checkpointing for fault tolerance
- Optimize for I/O (sequential reads, batching)

**Maintainability:**
- Keep transformations simple and composable
- Document feature engineering logic
- Version control pipeline code
- Test with representative samples before production runs
:::

## Streaming Embedding Updates

Batch ETL processes data hourly or daily, but many applications need **real-time embeddings**. A news recommender must embed articles seconds after publication. A fraud detector must embed transactions milliseconds after they occur. **Streaming architectures** enable continuous embedding updates with end-to-end latency measured in seconds, not hours.

### Streaming vs. Batch: The Trade-off

**Batch processing** (hourly/daily):
- **Advantages**: Simple, efficient, easy to debug, supports complex aggregations
- **Disadvantages**: Stale embeddings (hours old), high latency for new items
- **Use when**: Daily updates sufficient, complex transformations required

**Stream processing** (seconds):
- **Advantages**: Fresh embeddings (seconds old), low latency for new items, event-driven
- **Disadvantages**: Complex architecture, harder to debug, limited aggregation window
- **Use when**: Real-time updates critical, simple transformations, event-driven workflows

```python
"""
Streaming Embedding Pipeline

Architecture:
1. Event Stream: Kafka, Kinesis, Pub/Sub
2. Stream Processor: Flink, Spark Streaming, custom consumers
3. Embedding Generator: Real-time model inference
4. Vector Index: Incremental updates (HNSW, Faiss)

Latency budget:
- Event ingestion: 10-100ms
- Feature extraction: 10-50ms
- Embedding generation: 50-200ms
- Index update: 10-50ms
- Total: 100-400ms (< 1 second)
"""

import asyncio
import time
from typing import Dict, List, Optional, Callable
from dataclasses import dataclass, field
from datetime import datetime
from collections import deque
import numpy as np
import threading
import queue

@dataclass
class StreamEvent:
    """
    Event in embedding stream

    Attributes:
        event_id: Unique identifier
        event_type: Type of event (create, update, delete)
        entity_id: ID of entity to embed
        data: Entity data
        timestamp: Event timestamp
    """
    event_id: str
    event_type: str  # 'create', 'update', 'delete'
    entity_id: str
    data: Dict
    timestamp: datetime = field(default_factory=datetime.now)

class StreamingEmbeddingPipeline:
    """
    Real-time embedding pipeline with micro-batching

    Architecture:
    - Consume events from stream (Kafka topic, Kinesis stream)
    - Micro-batch events (10-100 items, 100-1000ms window)
    - Generate embeddings (batched inference on GPU)
    - Update vector index (incremental HNSW update)
    - Emit updated embeddings downstream

    Guarantees:
    - At-least-once processing (events may be reprocessed)
    - Eventual consistency (index eventually reflects all events)
    - Low latency (p99 < 1 second)

    Fault tolerance:
    - Checkpointing to recover from failures
    - Exactly-once semantics via idempotent updates
    - Dead letter queue for failed events
    """

    def __init__(
        self,
        embedding_model,
        vector_index,
        batch_window_ms: int = 500,
        max_batch_size: int = 100,
        enable_checkpointing: bool = True
    ):
        """
        Args:
            embedding_model: Model for generating embeddings
            vector_index: Vector index for storing embeddings (HNSW, Faiss)
            batch_window_ms: Time window for micro-batching (milliseconds)
            max_batch_size: Maximum events per micro-batch
            enable_checkpointing: Enable fault-tolerant checkpointing
        """
        self.embedding_model = embedding_model
        self.vector_index = vector_index
        self.batch_window_ms = batch_window_ms
        self.max_batch_size = max_batch_size
        self.enable_checkpointing = enable_checkpointing

        # Event queue for micro-batching
        self.event_queue = queue.Queue()

        # Processing thread
        self.processing_thread = None
        self.running = False

        # Metrics
        self.events_processed = 0
        self.batches_processed = 0
        self.total_latency_ms = 0
        self.errors = 0

        # Checkpointing
        self.last_checkpoint_offset = 0

        print(f"Initialized Streaming Embedding Pipeline")
        print(f"  Batch window: {batch_window_ms}ms")
        print(f"  Max batch size: {max_batch_size}")

    def start(self):
        """Start background processing thread"""
        if self.running:
            print("Pipeline already running")
            return

        self.running = True
        self.processing_thread = threading.Thread(target=self._processing_loop)
        self.processing_thread.daemon = True
        self.processing_thread.start()

        print("✓ Started streaming pipeline")

    def stop(self):
        """Stop background processing thread"""
        self.running = False
        if self.processing_thread:
            self.processing_thread.join(timeout=5.0)

        print("✓ Stopped streaming pipeline")

    def ingest_event(self, event: StreamEvent):
        """
        Ingest event into stream

        Args:
            event: Stream event to process
        """
        self.event_queue.put(event)

    def _processing_loop(self):
        """
        Background thread: Consume events and process in micro-batches

        Loop:
        1. Accumulate events for batch_window_ms
        2. Process batch (extract features, generate embeddings, update index)
        3. Checkpoint offset
        4. Repeat
        """
        while self.running:
            batch = self._accumulate_batch()

            if batch:
                self._process_batch(batch)

    def _accumulate_batch(self) -> List[StreamEvent]:
        """
        Accumulate events into micro-batch

        Strategy:
        - Wait up to batch_window_ms for events
        - Return batch when either:
          - batch_window_ms elapsed
          - max_batch_size reached

        Returns:
            List of events in batch
        """
        batch = []
        batch_start = time.time()

        while len(batch) < self.max_batch_size:
            # Calculate remaining wait time
            elapsed_ms = (time.time() - batch_start) * 1000
            remaining_ms = max(0, self.batch_window_ms - elapsed_ms)

            # If window elapsed and we have events, return batch
            if remaining_ms == 0 and len(batch) > 0:
                break

            # Wait for next event
            try:
                timeout = remaining_ms / 1000
                event = self.event_queue.get(timeout=max(0.001, timeout))
                batch.append(event)
            except queue.Empty:
                # Timeout - return current batch if non-empty
                if len(batch) > 0:
                    break
                continue

        return batch

    def _process_batch(self, batch: List[StreamEvent]):
        """
        Process micro-batch of events

        Steps:
        1. Extract features from events
        2. Generate embeddings (batched inference)
        3. Update vector index
        4. Checkpoint progress

        Args:
            batch: Events to process
        """
        batch_start = time.time()

        try:
            # 1. Extract features
            features_list = []
            entity_ids = []

            for event in batch:
                features = self._extract_features(event)
                if features is not None:
                    features_list.append(features)
                    entity_ids.append(event.entity_id)

            if not features_list:
                return

            # 2. Generate embeddings (batched)
            embeddings = self._generate_embeddings_batch(features_list)

            # 3. Update vector index
            self._update_index(entity_ids, embeddings, batch)

            # 4. Checkpoint
            if self.enable_checkpointing:
                self._checkpoint(batch[-1].event_id)

            # Metrics
            batch_latency_ms = (time.time() - batch_start) * 1000
            self.events_processed += len(batch)
            self.batches_processed += 1
            self.total_latency_ms += batch_latency_ms

            # Log progress
            avg_latency = self.total_latency_ms / self.batches_processed
            throughput = self.events_processed / (self.total_latency_ms / 1000)

            if self.batches_processed % 10 == 0:
                print(f"Processed {self.events_processed:,} events in {self.batches_processed} batches")
                print(f"  Avg latency: {avg_latency:.1f}ms")
                print(f"  Throughput: {throughput:.0f} events/sec")

        except Exception as e:
            print(f"⚠️  Batch processing failed: {e}")
            self.errors += 1

            # Send failed events to dead letter queue
            self._send_to_dlq(batch, error=str(e))

    def _extract_features(self, event: StreamEvent) -> Optional[np.ndarray]:
        """
        Extract features from event

        Args:
            event: Stream event

        Returns:
            Feature vector or None if extraction fails
        """
        try:
            # Extract text features
            text_parts = []
            for field in ['title', 'description', 'content']:
                if field in event.data:
                    text_parts.append(str(event.data[field]))

            if not text_parts:
                return None

            text = " ".join(text_parts)

            # In production: Use proper feature extraction
            # For now: Return dummy features
            return np.random.randn(512).astype(np.float32)

        except Exception as e:
            print(f"⚠️  Feature extraction failed for event {event.event_id}: {e}")
            return None

    def _generate_embeddings_batch(
        self,
        features_list: List[np.ndarray]
    ) -> np.ndarray:
        """
        Generate embeddings for batch (GPU-accelerated)

        Args:
            features_list: List of feature vectors

        Returns:
            Batch of embeddings (N, embedding_dim)
        """
        # Stack features into batch
        features_batch = np.stack(features_list)

        # Generate embeddings
        # In production: Use actual embedding model
        # For now: Return random embeddings
        embeddings = np.random.randn(len(features_list), 256).astype(np.float32)

        # Normalize
        embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)

        return embeddings

    def _update_index(
        self,
        entity_ids: List[str],
        embeddings: np.ndarray,
        events: List[StreamEvent]
    ):
        """
        Update vector index with new embeddings

        Operations:
        - CREATE: Add new vector to index
        - UPDATE: Replace existing vector
        - DELETE: Remove vector from index

        Args:
            entity_ids: Entity identifiers
            embeddings: Embedding vectors
            events: Original events (for event_type)
        """
        for entity_id, embedding, event in zip(entity_ids, embeddings, events):
            if event.event_type == 'create':
                # Add to index
                self.vector_index.add(entity_id, embedding)

            elif event.event_type == 'update':
                # Replace in index (delete + add)
                self.vector_index.delete(entity_id)
                self.vector_index.add(entity_id, embedding)

            elif event.event_type == 'delete':
                # Remove from index
                self.vector_index.delete(entity_id)

    def _checkpoint(self, last_event_id: str):
        """
        Checkpoint progress for fault tolerance

        In production: Write to persistent storage (database, S3)

        Args:
            last_event_id: Last processed event ID
        """
        self.last_checkpoint_offset = last_event_id
        # In production: Persist to database

    def _send_to_dlq(self, batch: List[StreamEvent], error: str):
        """
        Send failed events to dead letter queue

        Args:
            batch: Failed events
            error: Error message
        """
        print(f"⚠️  Sending {len(batch)} events to DLQ: {error}")
        # In production: Write to Kafka DLQ topic, SQS DLQ, etc.

    def get_metrics(self) -> Dict:
        """Get pipeline metrics"""
        return {
            'events_processed': self.events_processed,
            'batches_processed': self.batches_processed,
            'avg_latency_ms': self.total_latency_ms / max(1, self.batches_processed),
            'throughput_eps': self.events_processed / max(1, self.total_latency_ms / 1000),
            'errors': self.errors,
            'error_rate': self.errors / max(1, self.batches_processed)
        }

class MockVectorIndex:
    """Mock vector index for demonstration"""
    def __init__(self):
        self.vectors = {}

    def add(self, entity_id: str, embedding: np.ndarray):
        self.vectors[entity_id] = embedding

    def delete(self, entity_id: str):
        if entity_id in self.vectors:
            del self.vectors[entity_id]

    def search(self, query: np.ndarray, k: int = 10):
        # Mock search
        return list(self.vectors.keys())[:k]

# Example: Real-time news article embeddings
def streaming_news_example():
    """
    Streaming pipeline for news article embeddings

    Scenario:
    - News articles published throughout the day
    - Need embeddings within 1 second for recommendations
    - 1000 articles/hour = 0.3 articles/second

    Architecture:
    - Kafka topic: news_articles
    - Streaming pipeline: Consume, embed, index
    - Vector index: HNSW for fast search
    """

    # Mock embedding model
    class MockEmbeddingModel:
        def embed(self, texts):
            return np.random.randn(len(texts), 256).astype(np.float32)

    # Initialize components
    embedding_model = MockEmbeddingModel()
    vector_index = MockVectorIndex()

    # Initialize streaming pipeline
    pipeline = StreamingEmbeddingPipeline(
        embedding_model=embedding_model,
        vector_index=vector_index,
        batch_window_ms=500,
        max_batch_size=50
    )

    # Start pipeline
    pipeline.start()

    # Simulate incoming events
    print("Simulating news article stream...")
    for i in range(100):
        event = StreamEvent(
            event_id=f"event_{i}",
            event_type='create',
            entity_id=f"article_{i}",
            data={
                'title': f'Breaking News {i}',
                'content': f'This is article content for story {i}',
                'category': ['Politics', 'Sports', 'Tech'][i % 3]
            }
        )

        pipeline.ingest_event(event)

        # Simulate event arrival rate
        time.sleep(0.01)  # 100 events/sec

    # Wait for processing to complete
    time.sleep(2)

    # Stop pipeline
    pipeline.stop()

    # Print metrics
    metrics = pipeline.get_metrics()
    print(f"\n✓ Streaming pipeline metrics:")
    print(f"  Events processed: {metrics['events_processed']}")
    print(f"  Batches processed: {metrics['batches_processed']}")
    print(f"  Avg latency: {metrics['avg_latency_ms']:.1f}ms")
    print(f"  Throughput: {metrics['throughput_eps']:.0f} events/sec")
    print(f"  Errors: {metrics['errors']}")

# Uncomment to run:
# streaming_news_example()
```

:::{.callout-tip}
## Streaming Architecture Best Practices

**Micro-batching:**
- Batch window: 100-1000ms (balance latency vs throughput)
- Batch size: 10-100 items (optimize for GPU)
- Adaptive batching: Adjust based on load

**Fault tolerance:**
- Checkpointing: Save progress every N events
- Exactly-once semantics: Idempotent operations
- Dead letter queue: Handle failed events separately
- Retry logic: Exponential backoff for transient failures

**Monitoring:**
- End-to-end latency (p50, p95, p99)
- Throughput (events/second)
- Error rate (failures / total events)
- Queue depth (backpressure indicator)
:::

:::{.callout-warning}
## Streaming Complexity

Streaming pipelines are significantly more complex than batch:
- **Debugging**: Harder to reproduce issues (event order matters)
- **Testing**: Need to simulate real-time event streams
- **Operations**: 24/7 monitoring required
- **Cost**: Higher infrastructure costs (always running)

Start with batch, migrate to streaming only when business value justifies complexity.
:::

## Data Quality for Embedding Training

Poor data quality causes poor embeddings. **Data quality frameworks** detect and remediate issues before they corrupt training: duplicate detection prevents training on repeated examples, outlier detection identifies corrupted or adversarial data, consistency validation ensures relationships hold across updates, and drift detection alerts when distributions shift unexpectedly.

### The Data Quality Challenge for Embeddings

Traditional data quality focuses on completeness and correctness. **Embedding quality** has additional requirements:

- **Semantic consistency**: Similar items must have similar features
- **Label quality**: Incorrect labels poison contrastive learning
- **Distribution stability**: Embedding space shifts when data distribution changes
- **Relationship preservation**: Entity relationships must remain consistent

```python
"""
Data Quality Framework for Embeddings

Quality dimensions:
1. Completeness: No missing required fields
2. Correctness: Values within expected ranges
3. Consistency: Relationships preserved across updates
4. Freshness: Data reflects current state
5. Uniqueness: No exact duplicates
6. Semantic validity: Features preserve meaning
"""

import numpy as np
import pandas as pd
from typing import List, Dict, Optional, Tuple, Set
from dataclasses import dataclass
from datetime import datetime
from collections import defaultdict
import hashlib

@dataclass
class QualityIssue:
    """
    Data quality issue detected during validation

    Attributes:
        issue_type: Type of issue (duplicate, outlier, missing, etc.)
        severity: Severity level (critical, warning, info)
        record_id: Affected record
        field: Affected field (if applicable)
        description: Human-readable description
        metadata: Additional context
    """
    issue_type: str
    severity: str  # 'critical', 'warning', 'info'
    record_id: str
    field: Optional[str] = None
    description: str = ""
    metadata: Dict = None

class EmbeddingDataQualityValidator:
    """
    Comprehensive data quality validation for embedding training

    Validations:
    1. Schema validation: Required fields present and correct types
    2. Duplicate detection: Exact and near-duplicate records
    3. Outlier detection: Statistical anomalies in features
    4. Consistency validation: Relationship integrity
    5. Drift detection: Distribution shifts over time

    Usage:
    - Run before training (filter/fix bad data)
    - Run during serving (detect production drift)
    - Run after updates (validate quality maintained)
    """

    def __init__(
        self,
        required_fields: List[str],
        numeric_fields: List[str],
        categorical_fields: List[str]
    ):
        """
        Args:
            required_fields: Fields that must be present
            numeric_fields: Fields expected to be numeric
            categorical_fields: Fields expected to be categorical
        """
        self.required_fields = required_fields
        self.numeric_fields = numeric_fields
        self.categorical_fields = categorical_fields

        # Quality metrics
        self.issues: List[QualityIssue] = []
        self.records_validated = 0

        # Baseline statistics (for drift detection)
        self.baseline_stats: Optional[Dict] = None

        print(f"Initialized Data Quality Validator")
        print(f"  Required fields: {len(required_fields)}")
        print(f"  Numeric fields: {len(numeric_fields)}")
        print(f"  Categorical fields: {len(categorical_fields)}")

    def validate(
        self,
        records: List[EmbeddingFeatures]
    ) -> Tuple[List[EmbeddingFeatures], List[QualityIssue]]:
        """
        Validate data quality and return clean records

        Args:
            records: Records to validate

        Returns:
            (clean_records, issues): Valid records and detected issues
        """
        print(f"Validating {len(records):,} records...")

        self.issues = []
        clean_records = []

        for record in records:
            self.records_validated += 1

            # Run all validations
            if self._validate_schema(record) and \
               self._validate_values(record) and \
               self._validate_semantic(record):
                clean_records.append(record)

        # Global validations (across all records)
        self._detect_duplicates(records)
        self._detect_drift(records)

        # Summary
        critical = sum(1 for i in self.issues if i.severity == 'critical')
        warnings = sum(1 for i in self.issues if i.severity == 'warning')

        print(f"✓ Validation complete")
        print(f"  Clean records: {len(clean_records):,} ({len(clean_records)/len(records):.1%})")
        print(f"  Issues found: {len(self.issues)}")
        print(f"    Critical: {critical}")
        print(f"    Warnings: {warnings}")

        return clean_records, self.issues

    def _validate_schema(self, record: EmbeddingFeatures) -> bool:
        """
        Validate schema: Required fields present and correct types

        Args:
            record: Record to validate

        Returns:
            True if valid, False otherwise
        """
        # Check required fields
        if not record.text_features and not record.structured_features:
            self.issues.append(QualityIssue(
                issue_type='missing_features',
                severity='critical',
                record_id=record.record_id,
                description="Record has no text or structured features"
            ))
            return False

        # Validate structured features types
        if record.structured_features:
            for field, value in record.structured_features.items():
                if field in self.numeric_fields:
                    if not isinstance(value, (int, float)):
                        self.issues.append(QualityIssue(
                            issue_type='type_mismatch',
                            severity='critical',
                            record_id=record.record_id,
                            field=field,
                            description=f"Expected numeric, got {type(value).__name__}"
                        ))
                        return False

        return True

    def _validate_values(self, record: EmbeddingFeatures) -> bool:
        """
        Validate values: Range checks, outlier detection

        Args:
            record: Record to validate

        Returns:
            True if valid, False otherwise
        """
        if not record.structured_features:
            return True

        for field, value in record.structured_features.items():
            # Check for invalid numbers
            if not np.isfinite(value):
                self.issues.append(QualityIssue(
                    issue_type='invalid_value',
                    severity='critical',
                    record_id=record.record_id,
                    field=field,
                    description=f"Invalid numeric value: {value}"
                ))
                return False

            # Check for extreme outliers (>6 sigma from mean)
            # In production: Use learned statistics from training set
            if abs(value) > 1e6:
                self.issues.append(QualityIssue(
                    issue_type='outlier',
                    severity='warning',
                    record_id=record.record_id,
                    field=field,
                    description=f"Extreme value: {value}",
                    metadata={'value': value}
                ))

        return True

    def _validate_semantic(self, record: EmbeddingFeatures) -> bool:
        """
        Validate semantic quality: Text quality, meaningful content

        Args:
            record: Record to validate

        Returns:
            True if valid, False otherwise
        """
        if record.text_features:
            text = record.text_features.strip()

            # Must have minimum length
            if len(text) < 10:
                self.issues.append(QualityIssue(
                    issue_type='text_too_short',
                    severity='warning',
                    record_id=record.record_id,
                    field='text_features',
                    description=f"Text too short: {len(text)} chars"
                ))
                return False

            # Check for gibberish (high ratio of non-alphanumeric)
            alphanumeric = sum(c.isalnum() for c in text)
            if alphanumeric / len(text) < 0.5:
                self.issues.append(QualityIssue(
                    issue_type='gibberish',
                    severity='warning',
                    record_id=record.record_id,
                    field='text_features',
                    description=f"Low alphanumeric ratio: {alphanumeric/len(text):.2f}"
                ))
                return False

        return True

    def _detect_duplicates(self, records: List[EmbeddingFeatures]):
        """
        Detect exact and near-duplicate records

        Duplicates poison contrastive learning:
        - Same example appears as positive and negative
        - Model learns spurious correlations

        Args:
            records: All records to check
        """
        # Hash-based exact duplicate detection
        seen_hashes: Dict[str, str] = {}  # hash -> record_id

        for record in records:
            if record.data_hash:
                if record.data_hash in seen_hashes:
                    self.issues.append(QualityIssue(
                        issue_type='exact_duplicate',
                        severity='critical',
                        record_id=record.record_id,
                        description=f"Duplicate of {seen_hashes[record.data_hash]}",
                        metadata={'duplicate_of': seen_hashes[record.data_hash]}
                    ))
                else:
                    seen_hashes[record.data_hash] = record.record_id

    def _detect_drift(self, records: List[EmbeddingFeatures]):
        """
        Detect distribution drift compared to baseline

        Drift detection:
        - Compare current batch statistics to baseline
        - Flag significant changes (>3 sigma)
        - Alert if mean, std, or quantiles shift

        Args:
            records: Current batch of records
        """
        # Compute current statistics
        current_stats = self._compute_statistics(records)

        # If no baseline, set current as baseline
        if self.baseline_stats is None:
            self.baseline_stats = current_stats
            print("  Set baseline statistics for drift detection")
            return

        # Compare to baseline
        for field in self.numeric_fields:
            if field not in current_stats or field not in self.baseline_stats:
                continue

            baseline_mean = self.baseline_stats[field]['mean']
            baseline_std = self.baseline_stats[field]['std']
            current_mean = current_stats[field]['mean']

            # Detect significant mean shift
            if baseline_std > 0:
                z_score = abs(current_mean - baseline_mean) / baseline_std

                if z_score > 3:
                    self.issues.append(QualityIssue(
                        issue_type='distribution_drift',
                        severity='warning',
                        record_id='<global>',
                        field=field,
                        description=f"Mean shifted by {z_score:.1f} sigma",
                        metadata={
                            'baseline_mean': baseline_mean,
                            'current_mean': current_mean,
                            'z_score': z_score
                        }
                    ))

    def _compute_statistics(
        self,
        records: List[EmbeddingFeatures]
    ) -> Dict[str, Dict]:
        """
        Compute statistics for numeric fields

        Args:
            records: Records to analyze

        Returns:
            Dict mapping field -> {mean, std, min, max, quantiles}
        """
        stats = {}

        # Collect values per field
        field_values = defaultdict(list)
        for record in records:
            if record.structured_features:
                for field, value in record.structured_features.items():
                    if field in self.numeric_fields:
                        field_values[field].append(value)

        # Compute statistics
        for field, values in field_values.items():
            if values:
                stats[field] = {
                    'mean': np.mean(values),
                    'std': np.std(values),
                    'min': np.min(values),
                    'max': np.max(values),
                    'p25': np.percentile(values, 25),
                    'p50': np.percentile(values, 50),
                    'p75': np.percentile(values, 75)
                }

        return stats

    def get_quality_report(self) -> Dict:
        """
        Generate quality report

        Returns:
            Quality metrics and issue summary
        """
        issue_counts = defaultdict(int)
        for issue in self.issues:
            issue_counts[issue.issue_type] += 1

        return {
            'records_validated': self.records_validated,
            'total_issues': len(self.issues),
            'critical_issues': sum(1 for i in self.issues if i.severity == 'critical'),
            'warnings': sum(1 for i in self.issues if i.severity == 'warning'),
            'issue_breakdown': dict(issue_counts),
            'quality_score': 1 - (len(self.issues) / max(1, self.records_validated))
        }

# Example: Validate product data quality
def data_quality_example():
    """
    Validate data quality for product embeddings

    Scenario: E-commerce product catalog
    - 100K products
    - Detect duplicates, outliers, drift
    """

    # Generate sample data with quality issues
    def generate_test_data(count=1000):
        records = []

        for i in range(count):
            # 10% duplicates
            if i > 0 and np.random.random() < 0.1:
                # Duplicate previous record
                dup_record = records[-1]
                records.append(EmbeddingFeatures(
                    record_id=f"product_{i}",
                    text_features=dup_record.text_features,
                    structured_features=dup_record.structured_features,
                    data_hash=dup_record.data_hash
                ))
                continue

            # 5% missing features
            if np.random.random() < 0.05:
                records.append(EmbeddingFeatures(
                    record_id=f"product_{i}",
                    text_features=None,
                    structured_features=None
                ))
                continue

            # 2% outliers
            if np.random.random() < 0.02:
                price = 1e8  # Extreme outlier
            else:
                price = 10.0 + i % 100

            # Normal record
            text = f"Product {i}" if i % 20 != 0 else "X"  # 5% too short

            record = EmbeddingFeatures(
                record_id=f"product_{i}",
                text_features=text,
                structured_features={
                    'price': price,
                    'rating': 3.0 + (i % 5) * 0.5
                },
                data_hash=hashlib.md5(text.encode()).hexdigest()
            )
            records.append(record)

        return records

    # Generate test data
    test_records = generate_test_data(1000)

    # Initialize validator
    validator = EmbeddingDataQualityValidator(
        required_fields=['text_features'],
        numeric_fields=['price', 'rating'],
        categorical_fields=[]
    )

    # Validate
    clean_records, issues = validator.validate(test_records)

    # Print quality report
    report = validator.get_quality_report()
    print(f"\n=== Quality Report ===")
    print(f"Records validated: {report['records_validated']:,}")
    print(f"Quality score: {report['quality_score']:.2%}")
    print(f"Total issues: {report['total_issues']}")
    print(f"  Critical: {report['critical_issues']}")
    print(f"  Warnings: {report['warnings']}")
    print(f"\nIssue breakdown:")
    for issue_type, count in report['issue_breakdown'].items():
        print(f"  {issue_type}: {count}")

    # Print sample issues
    print(f"\nSample issues:")
    for issue in issues[:5]:
        print(f"  [{issue.severity.upper()}] {issue.issue_type}: {issue.description}")

# Uncomment to run:
# data_quality_example()
```

:::{.callout-tip}
## Data Quality Best Practices

**Prevention:**
- Validate at ingestion (catch issues early)
- Implement schema contracts (enforce structure)
- Use type systems (prevent type errors)
- Automate quality checks (continuous validation)

**Detection:**
- Statistical profiling (baseline distributions)
- Anomaly detection (outliers, drift)
- Relationship validation (foreign keys, consistency)
- Duplicate detection (exact and near-duplicate)

**Remediation:**
- Automated fixes (fill missing values, clip outliers)
- Human review queue (ambiguous cases)
- Dead letter queue (unfixable records)
- Feedback loops (fix upstream sources)

**Monitoring:**
- Quality dashboards (real-time metrics)
- Alerts on degradation (threshold breaches)
- Trend analysis (quality over time)
- Root cause analysis (trace issues to source)
:::

## Schema Evolution and Backwards Compatibility

Production embedding systems evolve: new features are added, old features deprecated, data types change. **Schema evolution** enables safe changes while maintaining backwards compatibility with existing embeddings, models, and downstream consumers.

### The Schema Evolution Challenge

Embedding systems have complex dependencies:
- **Trained models**: Expect specific feature schema
- **Vector indices**: Store embeddings from specific model versions
- **Downstream consumers**: Query embeddings with specific schemas
- **Historical data**: May use old schemas

**Change one component**, and the entire system can break.

```python
"""
Schema Evolution for Embedding Systems

Strategies:
1. Additive changes: Add new fields (backwards compatible)
2. Deprecation: Mark fields as deprecated before removal
3. Versioning: Version schemas and coordinate migrations
4. Defaults: Provide defaults for missing fields
5. Transformers: Convert between schema versions
"""

from typing import Dict, List, Optional, Any, Callable
from dataclasses import dataclass
from enum import Enum
import json
from datetime import datetime

class ChangeType(Enum):
    """Type of schema change"""
    ADD_FIELD = "add_field"
    REMOVE_FIELD = "remove_field"
    RENAME_FIELD = "rename_field"
    CHANGE_TYPE = "change_type"
    ADD_CONSTRAINT = "add_constraint"

@dataclass
class SchemaChange:
    """
    Schema change event

    Attributes:
        change_type: Type of change
        field_name: Affected field
        old_value: Previous value/type (if applicable)
        new_value: New value/type (if applicable)
        version: Schema version after change
        timestamp: When change was made
        backwards_compatible: Whether change is backwards compatible
    """
    change_type: ChangeType
    field_name: str
    old_value: Optional[Any] = None
    new_value: Optional[Any] = None
    version: str = ""
    timestamp: datetime = None
    backwards_compatible: bool = True

class SchemaVersion:
    """
    Schema version with validation and transformation

    Attributes:
        version: Version identifier (e.g., "1.0.0")
        fields: Field definitions {field_name -> type}
        required_fields: Fields that must be present
        deprecated_fields: Fields marked for future removal
        transformers: Functions to transform from previous versions
    """

    def __init__(
        self,
        version: str,
        fields: Dict[str, type],
        required_fields: List[str],
        deprecated_fields: Optional[List[str]] = None
    ):
        """
        Args:
            version: Version identifier
            fields: Field definitions
            required_fields: Required fields
            deprecated_fields: Deprecated fields (optional)
        """
        self.version = version
        self.fields = fields
        self.required_fields = required_fields
        self.deprecated_fields = deprecated_fields or []
        self.transformers: Dict[str, Callable] = {}  # from_version -> transformer

        print(f"Created schema version {version}")
        print(f"  Fields: {len(fields)}")
        print(f"  Required: {len(required_fields)}")
        print(f"  Deprecated: {len(self.deprecated_fields)}")

    def validate(self, data: Dict) -> Tuple[bool, List[str]]:
        """
        Validate data against this schema version

        Args:
            data: Data to validate

        Returns:
            (is_valid, errors): Validation result and error messages
        """
        errors = []

        # Check required fields
        for field in self.required_fields:
            if field not in data:
                errors.append(f"Missing required field: {field}")

        # Check field types
        for field, value in data.items():
            if field in self.fields:
                expected_type = self.fields[field]
                if not isinstance(value, expected_type):
                    errors.append(
                        f"Field {field}: expected {expected_type.__name__}, "
                        f"got {type(value).__name__}"
                    )

        # Warn about deprecated fields
        for field in self.deprecated_fields:
            if field in data:
                print(f"⚠️  Warning: Using deprecated field '{field}'")

        is_valid = len(errors) == 0
        return is_valid, errors

    def add_transformer(
        self,
        from_version: str,
        transformer: Callable[[Dict], Dict]
    ):
        """
        Add transformer from previous version

        Args:
            from_version: Source version
            transformer: Function to transform data
        """
        self.transformers[from_version] = transformer
        print(f"Added transformer: {from_version} -> {self.version}")

class SchemaRegistry:
    """
    Registry of all schema versions with migration support

    Responsibilities:
    - Track all schema versions
    - Validate data against appropriate version
    - Migrate data between versions
    - Detect breaking changes

    Usage:
    1. Register all schema versions
    2. Validate data (auto-detect version or specify)
    3. Migrate data to target version
    """

    def __init__(self):
        """Initialize empty registry"""
        self.versions: Dict[str, SchemaVersion] = {}
        self.version_history: List[SchemaChange] = []
        self.current_version: Optional[str] = None

        print("Initialized Schema Registry")

    def register_version(
        self,
        schema_version: SchemaVersion,
        set_current: bool = True
    ):
        """
        Register new schema version

        Args:
            schema_version: Schema version to register
            set_current: Set as current version
        """
        self.versions[schema_version.version] = schema_version

        if set_current:
            self.current_version = schema_version.version

        print(f"Registered schema version {schema_version.version}")
        if set_current:
            print(f"  Set as current version")

    def detect_version(self, data: Dict) -> Optional[str]:
        """
        Auto-detect schema version from data

        Strategy:
        - Try validating against each version
        - Return first version that validates successfully
        - Prefer newer versions over older

        Args:
            data: Data to detect version for

        Returns:
            Detected version or None
        """
        # Try versions in reverse order (newest first)
        for version in reversed(list(self.versions.keys())):
            schema = self.versions[version]
            is_valid, _ = schema.validate(data)
            if is_valid:
                return version

        return None

    def migrate(
        self,
        data: Dict,
        from_version: str,
        to_version: str
    ) -> Dict:
        """
        Migrate data from one version to another

        Args:
            data: Data to migrate
            from_version: Source version
            to_version: Target version

        Returns:
            Migrated data
        """
        if from_version == to_version:
            return data

        # Get migration path
        path = self._get_migration_path(from_version, to_version)

        if not path:
            raise ValueError(
                f"No migration path from {from_version} to {to_version}"
            )

        # Apply transformers along path
        current_data = data
        for i in range(len(path) - 1):
            current_version = path[i]
            next_version = path[i + 1]

            # Get transformer
            next_schema = self.versions[next_version]
            if current_version in next_schema.transformers:
                transformer = next_schema.transformers[current_version]
                current_data = transformer(current_data)
                print(f"  Migrated: {current_version} -> {next_version}")
            else:
                raise ValueError(
                    f"No transformer from {current_version} to {next_version}"
                )

        return current_data

    def _get_migration_path(
        self,
        from_version: str,
        to_version: str
    ) -> Optional[List[str]]:
        """
        Find migration path between versions

        For now: Assume linear version history
        In production: Use graph search for complex version trees

        Args:
            from_version: Source version
            to_version: Target version

        Returns:
            List of versions along migration path or None
        """
        versions = list(self.versions.keys())

        try:
            from_idx = versions.index(from_version)
            to_idx = versions.index(to_version)

            if from_idx < to_idx:
                # Forward migration
                return versions[from_idx:to_idx + 1]
            else:
                # Backward migration (not typically supported)
                return None
        except ValueError:
            return None

    def record_change(self, change: SchemaChange):
        """
        Record schema change in history

        Args:
            change: Schema change to record
        """
        self.version_history.append(change)

        # Alert if breaking change
        if not change.backwards_compatible:
            print(f"⚠️  BREAKING CHANGE: {change.change_type.value} on {change.field_name}")

# Example: Schema evolution for product embeddings
def schema_evolution_example():
    """
    Demonstrate schema evolution for product embeddings

    Versions:
    - v1.0: Initial schema (title, description, price)
    - v2.0: Add category field
    - v3.0: Add embedding_vector field, deprecate raw features
    """

    registry = SchemaRegistry()

    # Version 1.0: Initial schema
    v1 = SchemaVersion(
        version="1.0",
        fields={
            'title': str,
            'description': str,
            'price': float
        },
        required_fields=['title', 'price']
    )
    registry.register_version(v1, set_current=False)

    # Version 2.0: Add category (backwards compatible)
    v2 = SchemaVersion(
        version="2.0",
        fields={
            'title': str,
            'description': str,
            'price': float,
            'category': str  # New field
        },
        required_fields=['title', 'price']
    )

    # Add transformer from v1 to v2
    def v1_to_v2(data: Dict) -> Dict:
        """Add default category for v1 data"""
        data_v2 = data.copy()
        if 'category' not in data_v2:
            data_v2['category'] = 'Unknown'  # Default value
        return data_v2

    v2.add_transformer("1.0", v1_to_v2)
    registry.register_version(v2, set_current=False)

    # Version 3.0: Add embedding_vector, deprecate raw features
    v3 = SchemaVersion(
        version="3.0",
        fields={
            'title': str,
            'description': str,
            'price': float,
            'category': str,
            'embedding_vector': list  # New field
        },
        required_fields=['title', 'price', 'embedding_vector'],
        deprecated_fields=['description']  # Deprecate
    )

    # Add transformer from v2 to v3
    def v2_to_v3(data: Dict) -> Dict:
        """Add embedding vector for v2 data"""
        data_v3 = data.copy()
        if 'embedding_vector' not in data_v3:
            # Generate embedding from title + description
            # In production: Use actual embedding model
            data_v3['embedding_vector'] = [0.1, 0.2, 0.3]
        return data_v3

    v3.add_transformer("2.0", v2_to_v3)
    registry.register_version(v3, set_current=True)

    # Test: Migrate v1 data to v3
    print("\n=== Migration Test ===")

    v1_data = {
        'title': 'Laptop',
        'description': 'High-performance laptop',
        'price': 999.99
    }

    print(f"Original data (v1.0): {v1_data}")

    # Migrate v1 -> v2
    v2_data = registry.migrate(v1_data, from_version="1.0", to_version="2.0")
    print(f"Migrated to v2.0: {v2_data}")

    # Migrate v2 -> v3
    v3_data = registry.migrate(v2_data, from_version="2.0", to_version="3.0")
    print(f"Migrated to v3.0: {v3_data}")

    # Validate against v3
    is_valid, errors = v3.validate(v3_data)
    print(f"\nValidation: {'✓ Valid' if is_valid else '✗ Invalid'}")
    if errors:
        for error in errors:
            print(f"  - {error}")

# Uncomment to run:
# schema_evolution_example()
```

:::{.callout-tip}
## Schema Evolution Best Practices

**Safe evolution strategies:**
- **Additive changes only**: Add fields, don't remove (backwards compatible)
- **Deprecation before removal**: Mark fields deprecated for 1-2 versions
- **Default values**: Provide defaults for new required fields
- **Version tagging**: Tag data with schema version explicitly

**Migration strategies:**
- **Online migration**: Transform data on-read (lazy)
- **Offline migration**: Reprocess entire dataset (eager)
- **Hybrid**: Migrate hot data online, cold data offline

**Compatibility levels:**
- **Forward compatible**: New consumers can read old data
- **Backward compatible**: Old consumers can read new data
- **Full compatibility**: Both directions work
:::

:::{.callout-warning}
## Breaking Changes

Some changes cannot be made backwards-compatible:
- Removing required fields
- Changing field types incompatibly
- Removing entire entities

For breaking changes:
1. **Version bump**: Increment major version (v1 → v2)
2. **Parallel operation**: Run both versions simultaneously
3. **Gradual migration**: Migrate consumers incrementally
4. **Deprecation timeline**: Announce timeline (3-6 months)
5. **Sunset old version**: Remove after migration complete
:::

## Multi-Source Data Fusion

Production embedding systems integrate data from multiple sources: user profiles from CRM, product data from inventory, behavioral logs from analytics, external data from partners. **Multi-source data fusion** combines these heterogeneous datasets into unified embeddings while handling schema mismatches, different update frequencies, and varying data quality.

### The Data Fusion Challenge

Each data source has unique characteristics:
- **Schema**: Different field names, types, structures
- **Frequency**: Some update real-time, others daily/weekly
- **Quality**: Varying completeness, correctness, timeliness
- **Scale**: Some have millions of records, others billions
- **Access**: APIs, databases, files, streams

**Challenge**: Combine these sources into training data that preserves relationships across sources.

```python
"""
Multi-Source Data Fusion for Embeddings

Fusion strategies:
1. Early fusion: Combine raw data before feature extraction
2. Late fusion: Generate embeddings separately, combine at query time
3. Hybrid fusion: Join on keys, enrich with context from other sources

Challenges:
- Schema alignment: Map fields across sources
- Entity resolution: Link same entities across sources
- Temporal alignment: Handle different update frequencies
- Quality weighting: Prioritize high-quality sources
"""

from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass
from datetime import datetime
import pandas as pd
import numpy as np

@dataclass
class DataSource:
    """
    Configuration for a data source

    Attributes:
        source_id: Unique identifier
        source_type: Type of source (database, API, file, stream)
        update_frequency: How often data updates (realtime, hourly, daily)
        schema_mapping: Map source fields to canonical fields
        quality_score: Data quality score (0-1)
        priority: Priority when conflicts occur (higher = preferred)
    """
    source_id: str
    source_type: str
    update_frequency: str
    schema_mapping: Dict[str, str]  # source_field -> canonical_field
    quality_score: float = 1.0
    priority: int = 0

class MultiSourceDataFusion:
    """
    Fuse data from multiple sources for embedding generation

    Architecture:
    1. Extract: Pull data from each source
    2. Align: Map to canonical schema
    3. Resolve: Handle conflicts (same entity, different values)
    4. Enrich: Combine features from multiple sources
    5. Transform: Generate unified feature vectors

    Strategies:
    - Entity resolution: Link same entity across sources (fuzzy matching)
    - Conflict resolution: Prioritize high-quality/recent sources
    - Temporal alignment: Snapshot at consistent timestamp
    - Schema mapping: Translate source schemas to canonical
    """

    def __init__(
        self,
        canonical_schema: Dict[str, type],
        primary_key: str = 'entity_id'
    ):
        """
        Args:
            canonical_schema: Target schema for fused data
            primary_key: Field used to join across sources
        """
        self.canonical_schema = canonical_schema
        self.primary_key = primary_key
        self.sources: Dict[str, DataSource] = {}

        print(f"Initialized Multi-Source Data Fusion")
        print(f"  Canonical schema: {len(canonical_schema)} fields")
        print(f"  Primary key: {primary_key}")

    def register_source(self, source: DataSource):
        """
        Register a data source

        Args:
            source: Data source configuration
        """
        self.sources[source.source_id] = source
        print(f"Registered source: {source.source_id}")
        print(f"  Type: {source.source_type}")
        print(f"  Update frequency: {source.update_frequency}")
        print(f"  Quality score: {source.quality_score}")

    def extract(
        self,
        source_id: str,
        query: Optional[str] = None
    ) -> pd.DataFrame:
        """
        Extract data from source

        Args:
            source_id: Source to extract from
            query: Optional query/filter

        Returns:
            DataFrame with source data
        """
        if source_id not in self.sources:
            raise ValueError(f"Unknown source: {source_id}")

        source = self.sources[source_id]

        # In production: Actually extract from source
        # For now: Return mock data
        mock_data = pd.DataFrame({
            'id': [f'{source_id}_1', f'{source_id}_2'],
            'source_field_1': ['value_1', 'value_2'],
            'source_field_2': [1.0, 2.0]
        })

        print(f"Extracted {len(mock_data)} records from {source_id}")
        return mock_data

    def align_schema(
        self,
        source_id: str,
        data: pd.DataFrame
    ) -> pd.DataFrame:
        """
        Align source data to canonical schema

        Steps:
        1. Map source fields to canonical fields
        2. Add missing canonical fields (with defaults)
        3. Drop unmapped source fields
        4. Convert types to match canonical schema

        Args:
            source_id: Source identifier
            data: Source data

        Returns:
            DataFrame with canonical schema
        """
        source = self.sources[source_id]

        # Apply schema mapping
        aligned = pd.DataFrame()

        for source_field, canonical_field in source.schema_mapping.items():
            if source_field in data.columns:
                aligned[canonical_field] = data[source_field]

        # Add missing canonical fields with defaults
        for field, field_type in self.canonical_schema.items():
            if field not in aligned.columns:
                # Default values by type
                if field_type == str:
                    aligned[field] = ''
                elif field_type in (int, float):
                    aligned[field] = 0
                else:
                    aligned[field] = None

        # Convert types
        for field, field_type in self.canonical_schema.items():
            if field in aligned.columns:
                try:
                    if field_type == str:
                        aligned[field] = aligned[field].astype(str)
                    elif field_type == float:
                        aligned[field] = pd.to_numeric(aligned[field], errors='coerce')
                except Exception as e:
                    print(f"⚠️  Type conversion failed for {field}: {e}")

        print(f"Aligned {len(aligned)} records to canonical schema")
        return aligned

    def resolve_conflicts(
        self,
        datasets: Dict[str, pd.DataFrame]
    ) -> pd.DataFrame:
        """
        Resolve conflicts when same entity appears in multiple sources

        Conflict resolution strategies:
        1. Priority-based: Use value from highest-priority source
        2. Recency-based: Use most recently updated value
        3. Quality-based: Use value from highest-quality source
        4. Voting: Use most common value across sources

        Args:
            datasets: Map of source_id -> DataFrame

        Returns:
            Fused DataFrame with conflicts resolved
        """
        # Merge all datasets on primary key
        merged = None

        for source_id, df in datasets.items():
            source = self.sources[source_id]

            # Add source metadata
            df = df.copy()
            df['_source'] = source_id
            df['_priority'] = source.priority
            df['_quality'] = source.quality_score

            if merged is None:
                merged = df
            else:
                # Outer join to include all entities
                merged = pd.merge(
                    merged,
                    df,
                    on=self.primary_key,
                    how='outer',
                    suffixes=('', f'_{source_id}')
                )

        # Resolve conflicts for each field
        resolved = pd.DataFrame()
        resolved[self.primary_key] = merged[self.primary_key]

        for field in self.canonical_schema.keys():
            if field == self.primary_key:
                continue

            # Find all columns for this field across sources
            field_columns = [col for col in merged.columns if col.startswith(field)]

            if len(field_columns) == 1:
                # No conflict
                resolved[field] = merged[field_columns[0]]
            else:
                # Resolve conflict using priority
                resolved[field] = self._resolve_field_conflict(
                    merged,
                    field,
                    field_columns
                )

        print(f"Resolved conflicts for {len(resolved)} entities")
        return resolved

    def _resolve_field_conflict(
        self,
        merged: pd.DataFrame,
        field: str,
        field_columns: List[str]
    ) -> pd.Series:
        """
        Resolve conflict for single field

        Strategy: Use value from highest-priority source

        Args:
            merged: Merged DataFrame with all sources
            field: Field to resolve
            field_columns: Columns containing values for this field

        Returns:
            Series with resolved values
        """
        # Priority-based resolution
        # (In production: Also consider recency, quality)

        resolved_values = []

        for idx, row in merged.iterrows():
            # Get values and their priorities
            candidates = []
            for col in field_columns:
                if pd.notna(row[col]):
                    # Extract source from column name
                    source_id = col.split('_')[-1] if '_' in col else None
                    priority = row.get(f'_priority_{source_id}', 0) if source_id else 0
                    candidates.append((priority, row[col]))

            if candidates:
                # Choose highest priority
                candidates.sort(reverse=True)
                resolved_values.append(candidates[0][1])
            else:
                # No value available
                resolved_values.append(None)

        return pd.Series(resolved_values)

    def fuse(
        self,
        source_ids: List[str],
        timestamp: Optional[datetime] = None
    ) -> pd.DataFrame:
        """
        Fuse data from multiple sources

        Steps:
        1. Extract from each source
        2. Align to canonical schema
        3. Resolve conflicts
        4. Return unified dataset

        Args:
            source_ids: Sources to fuse
            timestamp: Snapshot timestamp (for temporal alignment)

        Returns:
            Fused DataFrame
        """
        print(f"\nFusing data from {len(source_ids)} sources...")

        # Extract and align from each source
        aligned_datasets = {}

        for source_id in source_ids:
            # Extract
            raw_data = self.extract(source_id)

            # Align schema
            aligned_data = self.align_schema(source_id, raw_data)

            aligned_datasets[source_id] = aligned_data

        # Resolve conflicts
        fused_data = self.resolve_conflicts(aligned_datasets)

        print(f"✓ Fusion complete: {len(fused_data)} entities")

        return fused_data

# Example: Fuse product data from multiple sources
def multi_source_fusion_example():
    """
    Fuse product data from catalog, inventory, and analytics

    Sources:
    - Product Catalog: Title, description, category (daily updates)
    - Inventory System: Price, stock_count (real-time updates)
    - Analytics Platform: View_count, rating (hourly updates)

    Goal: Unified product features for embedding generation
    """

    # Define canonical schema
    canonical_schema = {
        'product_id': str,
        'title': str,
        'description': str,
        'category': str,
        'price': float,
        'stock_count': int,
        'view_count': int,
        'rating': float
    }

    # Initialize fusion engine
    fusion = MultiSourceDataFusion(
        canonical_schema=canonical_schema,
        primary_key='product_id'
    )

    # Register sources
    catalog_source = DataSource(
        source_id='product_catalog',
        source_type='database',
        update_frequency='daily',
        schema_mapping={
            'id': 'product_id',
            'name': 'title',
            'desc': 'description',
            'cat': 'category'
        },
        quality_score=0.95,
        priority=2
    )
    fusion.register_source(catalog_source)

    inventory_source = DataSource(
        source_id='inventory_system',
        source_type='api',
        update_frequency='realtime',
        schema_mapping={
            'product_id': 'product_id',
            'current_price': 'price',
            'available_quantity': 'stock_count'
        },
        quality_score=0.98,
        priority=3  # Highest priority (most authoritative)
    )
    fusion.register_source(inventory_source)

    analytics_source = DataSource(
        source_id='analytics_platform',
        source_type='stream',
        update_frequency='hourly',
        schema_mapping={
            'product_id': 'product_id',
            'views_24h': 'view_count',
            'avg_rating': 'rating'
        },
        quality_score=0.90,
        priority=1
    )
    fusion.register_source(analytics_source)

    # Fuse data
    fused_data = fusion.fuse(
        source_ids=['product_catalog', 'inventory_system', 'analytics_platform'],
        timestamp=datetime.now()
    )

    print(f"\n=== Fused Data ===")
    print(fused_data.head())
    print(f"\nColumns: {list(fused_data.columns)}")
    print(f"Records: {len(fused_data)}")

# Uncomment to run:
# multi_source_fusion_example()
```

:::{.callout-tip}
## Multi-Source Fusion Best Practices

**Schema management:**
- **Canonical schema**: Define single target schema
- **Schema registry**: Centralize source schema definitions
- **Schema evolution**: Version schemas and migrate incrementally
- **Type safety**: Validate types during alignment

**Conflict resolution:**
- **Priority-based**: Assign priority to sources (authority)
- **Recency-based**: Prefer most recently updated value
- **Quality-based**: Weight by source quality score
- **Context-aware**: Consider semantic meaning

**Performance:**
- **Incremental fusion**: Only fuse changed entities
- **Partitioning**: Partition by entity_id for parallel fusion
- **Caching**: Cache fused results (invalidate on update)
- **Lazy loading**: Fuse on-demand for rarely accessed entities
:::

## Key Takeaways

- **ETL pipelines must preserve semantic relationships**: Unlike traditional ETL that optimizes for SQL analytics, embedding ETL requires feature engineering that captures similarity and meaning, not just facts

- **Streaming enables real-time embeddings with sub-second latency**: Micro-batching architectures (100-1000ms windows) balance throughput and latency, enabling fresh embeddings for dynamic content like news and social media

- **Data quality directly determines embedding quality**: Comprehensive validation (schema, values, semantics, duplicates, drift) prevents training on corrupted data that would poison embeddings for months

- **Schema evolution requires careful coordination across components**: Backwards-compatible changes (add fields, provide defaults) enable safe evolution while breaking changes (remove fields, change types) require parallel operation and gradual migration

- **Multi-source fusion combines heterogeneous datasets into unified embeddings**: Schema alignment, entity resolution, conflict resolution, and temporal alignment enable leveraging data from multiple systems with different schemas and update frequencies

- **Data engineering is the foundation of embedding systems**: High-quality embeddings require high-quality data engineering; invest in pipelines, quality frameworks, and fusion strategies before scaling models

- **The data engineering hierarchy**: Quality (1000× impact) > Schema design (100× impact) > Performance (10× impact). Focus on correctness before optimizing throughput

## Looking Ahead

Part III (Production Engineering) concludes with robust data engineering practices that ensure embedding systems have the clean, consistent, high-quality data needed to achieve their potential. Part IV (Advanced Applications) begins with Chapter 13, which explores Retrieval-Augmented Generation at enterprise scale: RAG architecture patterns that combine embedding retrieval with language models, context window optimization for billion-document corpora, multi-stage retrieval systems that balance recall and precision, evaluation frameworks that measure end-to-end quality, and strategies for handling contradictory information across sources.

## Further Reading

### Data Engineering
- Kleppmann, Martin (2017). "Designing Data-Intensive Applications." O'Reilly Media.
- Reis, Cathy, and Rupal Mahajan (2019). "Data Engineering with Apache Spark, Delta Lake, and Lakehouse." O'Reilly Media.
- Kalidindi, Santhosh (2021). "Data Engineering with Python." Packt Publishing.

### ETL and Pipelines
- Kimball, Ralph, and Margy Ross (2013). "The Data Warehouse Toolkit." Wiley.
- Apache Airflow Documentation. "Best Practices."
- dbt Documentation. "Best Practices for Data Transformation."

### Streaming Systems
- Kleppmann, Martin (2016). "Making Sense of Stream Processing." O'Reilly Media.
- Narkhede, Neha, et al. (2017). "Kafka: The Definitive Guide." O'Reilly Media.
- Apache Flink Documentation. "Streaming Concepts."

### Data Quality
- Redman, Thomas (2016). "Getting in Front on Data Quality." Harvard Business Review.
- Batini, Carlo, and Monica Scannapieco (2016). "Data and Information Quality." Springer.
- Talend Data Quality Documentation. "Data Quality Best Practices."

### Schema Evolution
- Kleppmann, Martin (2015). "Schema Evolution in Avro, Protocol Buffers and Thrift." Blog post.
- Confluent Documentation. "Schema Evolution and Compatibility."
- Fowler, Martin (2016). "Evolutionary Database Design." martinfowler.com.

### Data Integration
- Doan, AnHai, et al. (2012). "Principles of Data Integration." Morgan Kaufmann.
- Haas, Laura, et al. (2005). "Clio Grows Up: From Research Prototype to Industrial Tool." SIGMOD.
- Madhavan, Jayant, et al. (2001). "Generic Schema Matching with Cupid." VLDB.
