# Contrastive Learning for Enterprise Embeddings {#sec-contrastive-learning}

:::{.callout-note}
## Chapter Overview
Contrastive learning has emerged as the dominant paradigm for training state-of-the-art embeddings without labeled data. This chapter explores how to leverage contrastive learning at enterprise scale—from fundamental principles through production architectures that handle trillion-row training. We cover SimCLR, MoCo, hard negative mining strategies, batch optimization techniques, and distributed training patterns that power modern embedding systems.
:::

## Contrastive Learning Fundamentals

Contrastive learning transforms the embedding problem from "predict labels" to "distinguish similar from dissimilar." This shift unlocks massive unlabeled datasets and produces embeddings that capture nuanced semantic relationships beyond what supervised learning achieves.

### The Core Principle

The fundamental insight: **embeddings should place similar items close together and dissimilar items far apart**. Simple in concept, revolutionary in practice.

Traditional supervised learning requires:
- Expensive labeled data (millions of examples)
- Fixed label space (categories defined upfront)
- Limited to explicit labels (can't capture unlabeled nuances)

Contrastive learning requires only:
- Pairs or triplets indicating similarity
- Any method to generate positive pairs (augmentation, co-occurrence, etc.)
- Scales to billions of unlabeled examples

### The Contrastive Loss Landscape

**InfoNCE Loss: The Foundation**

InfoNCE (Noise Contrastive Estimation with Information theory) is the most widely used contrastive loss:

```python
import torch
import torch.nn.functional as F

class InfoNCELoss:
    """
    InfoNCE loss for contrastive learning

    Core idea: Given an anchor and one positive example, distinguish the
    positive from N-1 negative examples drawn from the distribution.
    """

    def __init__(self, temperature=0.07):
        """
        Args:
            temperature: Controls the concentration of the distribution.
                        Lower = harder negatives, higher = softer.
                        Typical range: 0.01 - 0.5
        """
        self.temperature = temperature

    def compute_loss(self, anchor_embeddings, positive_embeddings,
                     negative_embeddings=None, all_embeddings=None):
        """
        Compute InfoNCE loss

        Args:
            anchor_embeddings: (batch_size, embed_dim)
            positive_embeddings: (batch_size, embed_dim)
            negative_embeddings: (batch_size, num_negatives, embed_dim)
                                or None if using all_embeddings
            all_embeddings: (total_size, embed_dim) - use all as negatives

        Returns:
            loss: scalar tensor
            metrics: dict with accuracy, positive/negative similarities
        """
        batch_size = anchor_embeddings.shape[0]

        # Normalize embeddings (critical for stable training)
        anchor_norm = F.normalize(anchor_embeddings, p=2, dim=1)
        positive_norm = F.normalize(positive_embeddings, p=2, dim=1)

        # Positive similarities: anchor · positive
        # Shape: (batch_size,)
        positive_sim = torch.sum(anchor_norm * positive_norm, dim=1) / self.temperature

        if all_embeddings is not None:
            # In-batch negatives: compare anchor to all embeddings
            # This is the efficient approach for large batches

            all_norm = F.normalize(all_embeddings, p=2, dim=1)

            # Similarity matrix: anchor × all
            # Shape: (batch_size, total_size)
            similarity_matrix = torch.matmul(
                anchor_norm,
                all_norm.T
            ) / self.temperature

            # Mask out the positive (assume positives at same index)
            # Create labels: positive is at index i for anchor i
            labels = torch.arange(batch_size, device=anchor_embeddings.device)

            # Cross-entropy: -log(exp(pos) / sum(exp(all)))
            loss = F.cross_entropy(similarity_matrix, labels)

            # Metrics
            with torch.no_grad():
                # Accuracy: how often is positive the highest similarity?
                predictions = similarity_matrix.argmax(dim=1)
                accuracy = (predictions == labels).float().mean()

                # Average positive/negative similarities
                positive_sim_mean = positive_sim.mean()

                # Negative similarities (excluding positive)
                mask = torch.ones_like(similarity_matrix, dtype=torch.bool)
                mask[torch.arange(batch_size), labels] = False
                negative_sim_mean = similarity_matrix[mask].mean()

        elif negative_embeddings is not None:
            # Explicit negatives provided
            negative_norm = F.normalize(negative_embeddings, p=2, dim=1)

            # Negative similarities: anchor · negatives
            # Shape: (batch_size, num_negatives)
            negative_sim = torch.matmul(
                anchor_norm.unsqueeze(1),  # (batch, 1, dim)
                negative_norm.transpose(1, 2)  # (batch, dim, num_neg)
            ).squeeze(1) / self.temperature

            # Concatenate positive and negative similarities
            # Shape: (batch_size, 1 + num_negatives)
            logits = torch.cat([
                positive_sim.unsqueeze(1),  # Positive is first
                negative_sim
            ], dim=1)

            # Labels: positive is always at index 0
            labels = torch.zeros(batch_size, dtype=torch.long,
                               device=anchor_embeddings.device)

            loss = F.cross_entropy(logits, labels)

            # Metrics
            with torch.no_grad():
                predictions = logits.argmax(dim=1)
                accuracy = (predictions == labels).float().mean()
                positive_sim_mean = positive_sim.mean()
                negative_sim_mean = negative_sim.mean()

        else:
            raise ValueError("Must provide either negative_embeddings or all_embeddings")

        metrics = {
            'accuracy': accuracy.item(),
            'positive_similarity': positive_sim_mean.item(),
            'negative_similarity': negative_sim_mean.item(),
            'similarity_gap': (positive_sim_mean - negative_sim_mean).item()
        }

        return loss, metrics


# Example usage
encoder = torch.nn.Sequential(
    torch.nn.Linear(512, 256),
    torch.nn.ReLU(),
    torch.nn.Linear(256, 128)
)

# Batch of data
anchors = torch.randn(64, 512)  # 64 examples
positives = torch.randn(64, 512)  # Corresponding positives
all_batch = torch.cat([anchors, positives], dim=0)  # Use full batch as negatives

# Encode
anchor_emb = encoder(anchors)
positive_emb = encoder(positives)
all_emb = encoder(all_batch)

# Compute loss
loss_fn = InfoNCELoss(temperature=0.07)
loss, metrics = loss_fn.compute_loss(
    anchor_emb,
    positive_emb,
    all_embeddings=all_emb
)

print(f"Loss: {loss.item():.4f}")
print(f"Accuracy: {metrics['accuracy']:.2%}")
print(f"Positive similarity: {metrics['positive_similarity']:.4f}")
print(f"Negative similarity: {metrics['negative_similarity']:.4f}")
```

**The Temperature Parameter: Critical but Often Misunderstood**

Temperature τ controls the "softness" of the distribution:

- **Low temperature (0.01-0.05)**: Sharp distribution, focuses on hardest negatives
  - Pro: Faster convergence, better final performance
  - Con: Numerical instability, requires careful tuning
  - Use when: Large batches (1024+), well-curated negatives

- **Medium temperature (0.07-0.1)**: Balanced (most common)
  - Pro: Stable training, good performance
  - Con: May not fully utilize hard negatives
  - Use when: Standard training, batch size 256-1024

- **High temperature (0.2-0.5)**: Soft distribution, considers all negatives
  - Pro: Very stable, handles noisy negatives well
  - Con: Slower convergence, potentially lower final performance
  - Use when: Small batches, noisy data, initial training phase

```python
import numpy as np
import matplotlib.pyplot as plt

class TemperatureAnalysis:
    """
    Analyze impact of temperature on contrastive learning
    """

    def demonstrate_temperature_effect(self):
        """
        Show how temperature affects the probability distribution
        """
        # Simulate similarities: 1 positive, 9 negatives
        # Positive is much more similar (0.8) than negatives (0.1-0.3)
        similarities = np.array([0.8, 0.3, 0.25, 0.2, 0.15, 0.15, 0.12, 0.1, 0.1, 0.1])

        temperatures = [0.01, 0.05, 0.07, 0.1, 0.3, 0.5]

        fig, axes = plt.subplots(2, 3, figsize=(15, 8))
        axes = axes.flatten()

        for idx, tau in enumerate(temperatures):
            # Compute probabilities with temperature
            logits = similarities / tau
            probs = np.exp(logits) / np.exp(logits).sum()

            # Plot
            ax = axes[idx]
            colors = ['green'] + ['red'] * 9  # Positive green, negatives red
            ax.bar(range(10), probs, color=colors, alpha=0.6)
            ax.set_title(f'Temperature τ={tau}\nP(positive)={probs[0]:.3f}')
            ax.set_xlabel('Example index')
            ax.set_ylabel('Probability')
            ax.set_ylim(0, 1)

        plt.tight_layout()
        return fig

    def recommend_temperature(self, batch_size, data_quality='high'):
        """
        Recommend temperature based on batch size and data quality

        Args:
            batch_size: Training batch size
            data_quality: 'high', 'medium', or 'low' (refers to negative quality)

        Returns:
            recommended temperature and rationale
        """
        if batch_size >= 4096:
            # Very large batches: many high-quality negatives available
            if data_quality == 'high':
                return 0.03, "Large batch + high quality → very low temperature for hard negatives"
            else:
                return 0.05, "Large batch but lower quality → slightly higher temperature"

        elif batch_size >= 1024:
            # Large batches: good number of negatives
            if data_quality == 'high':
                return 0.05, "Large batch + high quality → low temperature"
            else:
                return 0.07, "Standard setting for large batches"

        elif batch_size >= 256:
            # Medium batches: standard setting
            return 0.07, "Standard temperature for medium batches"

        elif batch_size >= 64:
            # Small batches: need softer distribution
            if data_quality == 'low':
                return 0.15, "Small batch + noisy data → higher temperature for stability"
            else:
                return 0.1, "Small batch → moderately high temperature"

        else:
            # Very small batches: high temperature required
            return 0.2, "Very small batch → high temperature to utilize all negatives"


# Example usage
analyzer = TemperatureAnalysis()

# Get recommendation for your setup
batch_size = 512
data_quality = 'high'

temp, reasoning = analyzer.recommend_temperature(batch_size, data_quality)
print(f"Recommended temperature: {temp}")
print(f"Reasoning: {reasoning}")
```

### Alternative Contrastive Losses

**Triplet Loss: The Classic Approach**

```python
class TripletLoss:
    """
    Triplet loss: minimize distance(anchor, positive) while maximizing
    distance(anchor, negative) with a margin
    """

    def __init__(self, margin=1.0, distance='euclidean'):
        """
        Args:
            margin: Minimum distance between positive and negative
            distance: 'euclidean' or 'cosine'
        """
        self.margin = margin
        self.distance = distance

    def compute_loss(self, anchor, positive, negative):
        """
        Args:
            anchor: (batch_size, embed_dim)
            positive: (batch_size, embed_dim)
            negative: (batch_size, embed_dim) or (batch_size, num_neg, embed_dim)
        """
        if self.distance == 'euclidean':
            # L2 distance
            pos_dist = torch.norm(anchor - positive, p=2, dim=-1)

            if negative.dim() == 3:
                # Multiple negatives per anchor
                # Compute distance to each negative
                neg_dist = torch.norm(
                    anchor.unsqueeze(1) - negative,  # (batch, 1, dim) - (batch, num_neg, dim)
                    p=2,
                    dim=-1
                )  # (batch, num_neg)

                # Use hardest negative (smallest distance)
                neg_dist = neg_dist.min(dim=1)[0]
            else:
                neg_dist = torch.norm(anchor - negative, p=2, dim=-1)

        elif self.distance == 'cosine':
            # Cosine distance = 1 - cosine similarity
            pos_sim = F.cosine_similarity(anchor, positive, dim=-1)
            pos_dist = 1 - pos_sim

            if negative.dim() == 3:
                # Multiple negatives
                neg_sim = F.cosine_similarity(
                    anchor.unsqueeze(1),
                    negative,
                    dim=-1
                )
                neg_dist = 1 - neg_sim.max(dim=1)[0]  # Hardest negative
            else:
                neg_sim = F.cosine_similarity(anchor, negative, dim=-1)
                neg_dist = 1 - neg_sim

        # Triplet loss: max(0, pos_dist - neg_dist + margin)
        loss = F.relu(pos_dist - neg_dist + self.margin)

        return loss.mean()
```

**NTXentLoss (Normalized Temperature-scaled Cross Entropy)**

The loss used in SimCLR, a normalized variant of InfoNCE:

```python
class NTXentLoss:
    """
    NT-Xent loss from SimCLR paper

    Key differences from InfoNCE:
    - Symmetric: both (i, j) and (j, i) are positives
    - Uses cosine similarity
    - Specific temperature scaling
    """

    def __init__(self, temperature=0.5):
        self.temperature = temperature

    def compute_loss(self, embeddings, labels=None):
        """
        Args:
            embeddings: (2*batch_size, embed_dim) - contains augmented pairs
            labels: Optional labels indicating which pairs are positive
                   If None, assumes pairs are (2i, 2i+1)

        Returns:
            loss: scalar
        """
        batch_size = embeddings.shape[0] // 2

        # Normalize embeddings
        embeddings = F.normalize(embeddings, p=2, dim=1)

        # Compute similarity matrix for all pairs
        # Shape: (2*batch_size, 2*batch_size)
        similarity_matrix = torch.matmul(embeddings, embeddings.T) / self.temperature

        # Create mask for positive pairs
        # For each i, positive is the other augmentation of the same instance
        mask = torch.zeros_like(similarity_matrix, dtype=torch.bool)

        for i in range(batch_size):
            # (2i, 2i+1) are a positive pair
            mask[2*i, 2*i+1] = True
            mask[2*i+1, 2*i] = True

        # Remove self-similarities (diagonal)
        mask.fill_diagonal_(False)

        # For each sample, compute loss against its positive and all negatives
        losses = []

        for i in range(2 * batch_size):
            # Positive: the one entry in mask that's True for row i
            positive_indices = mask[i]

            # Get similarity to positive
            positive_sim = similarity_matrix[i, positive_indices]

            # Get similarities to all except self
            all_similarities = similarity_matrix[i]
            all_similarities = all_similarities[~torch.eye(2*batch_size, dtype=torch.bool, device=embeddings.device)[i]]

            # Numerator: exp(positive similarity)
            numerator = torch.exp(positive_sim)

            # Denominator: sum of exp(all similarities except self)
            denominator = torch.exp(all_similarities).sum()

            # Loss: -log(numerator / denominator)
            loss_i = -torch.log(numerator / denominator)
            losses.append(loss_i)

        return torch.stack(losses).mean()
```

### Why Contrastive Learning Works: The Theoretical Foundation

**Mutual Information Maximization**

Contrastive learning maximizes the mutual information between different views of the same data:

```
I(x; x̃) = H(x) - H(x|x̃)
```

Where:
- x is the original data
- x̃ is a transformed view (augmentation, different modality, etc.)
- I(x; x̃) is mutual information
- H is entropy

InfoNCE provides a lower bound on mutual information:

```
I(x; x̃) ≥ log(K) - L_InfoNCE
```

Where K is the number of negatives. **Larger batches (more negatives) provide a tighter bound, explaining why contrastive learning benefits dramatically from large batch sizes.**

**Alignment and Uniformity**

Recent work decomposes contrastive learning success into two properties:

1. **Alignment**: Positive pairs should be close
   ```
   L_align = E[||f(x) - f(x̃)||²]
   ```

2. **Uniformity**: Embeddings should be uniformly distributed on unit hypersphere
   ```
   L_uniform = log E[e^(-||f(x) - f(y)||²)]
   ```

Optimal embeddings balance both: tight alignment without collapse (all embeddings identical).

```python
class AlignmentUniformityAnalysis:
    """
    Analyze embedding quality via alignment and uniformity metrics
    """

    def compute_alignment(self, embeddings1, embeddings2):
        """
        Compute alignment between positive pairs

        Args:
            embeddings1, embeddings2: (N, dim) paired embeddings

        Returns:
            alignment: lower is better (closer pairs)
        """
        # Normalize
        emb1 = F.normalize(embeddings1, p=2, dim=1)
        emb2 = F.normalize(embeddings2, p=2, dim=1)

        # Squared L2 distance
        alignment = torch.norm(emb1 - emb2, p=2, dim=1).pow(2).mean()

        return alignment.item()

    def compute_uniformity(self, embeddings, t=2):
        """
        Compute uniformity of embedding distribution

        Args:
            embeddings: (N, dim)
            t: temperature parameter (default 2 from paper)

        Returns:
            uniformity: lower is better (more uniform)
        """
        # Normalize
        emb = F.normalize(embeddings, p=2, dim=1)

        # Pairwise similarities
        # (N, N) matrix of similarities
        sim_matrix = torch.matmul(emb, emb.T)

        # Exclude diagonal (self-similarity)
        mask = ~torch.eye(len(emb), dtype=torch.bool, device=emb.device)
        similarities = sim_matrix[mask]

        # Uniformity: log of average exp(-t * squared_distance)
        # Since ||x - y||² = 2(1 - x·y) for normalized vectors:
        squared_distances = 2 * (1 - similarities)

        uniformity = torch.log(torch.exp(-t * squared_distances).mean())

        return uniformity.item()

    def analyze_training_progress(self, embeddings_history):
        """
        Track alignment and uniformity throughout training

        Args:
            embeddings_history: List of (epoch, positive_pairs) tuples

        Returns:
            DataFrame with alignment and uniformity over time
        """
        results = []

        for epoch, (emb1, emb2) in embeddings_history:
            # Combine for uniformity calculation
            all_emb = torch.cat([emb1, emb2], dim=0)

            alignment = self.compute_alignment(emb1, emb2)
            uniformity = self.compute_uniformity(all_emb)

            results.append({
                'epoch': epoch,
                'alignment': alignment,
                'uniformity': uniformity
            })

        return pd.DataFrame(results)


# Healthy training should show:
# - Alignment decreasing (positives getting closer)
# - Uniformity stable or slightly decreasing (not collapsing)
# - If uniformity increases significantly → embeddings collapsing
```

## SimCLR, MoCo, and Enterprise Adaptations

The contrastive learning revolution began with SimCLR and MoCo in 2020. These frameworks provide battle-tested architectures for production embedding systems.

### SimCLR: Simple Framework, Powerful Results

SimCLR (Simple Framework for Contrastive Learning of Visual Representations) achieves remarkable results with a straightforward recipe:

1. **Data augmentation pipeline**: Generate two views of each example
2. **Encoder network**: Extract embeddings (typically ResNet or Transformer)
3. **Projection head**: Non-linear MLP that maps embeddings to contrastive space
4. **NT-Xent loss**: Normalized temperature-scaled cross entropy
5. **Large batch training**: 4096+ examples per batch

**SimCLR Implementation for Text Embeddings**

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from transformers import AutoModel, AutoTokenizer

class SimCLRTextEmbedding(nn.Module):
    """
    SimCLR adapted for text embeddings

    Key adaptations from visual SimCLR:
    - Text augmentation instead of image augmentation
    - BERT/RoBERTa encoder instead of ResNet
    - Larger projection head for text (768 → 128 works well)
    """

    def __init__(self,
                 base_model='bert-base-uncased',
                 projection_dim=128,
                 hidden_dim=512,
                 temperature=0.07):
        super().__init__()

        # Base encoder (frozen or fine-tuned)
        self.encoder = AutoModel.from_pretrained(base_model)
        self.tokenizer = AutoTokenizer.from_pretrained(base_model)

        encoder_dim = self.encoder.config.hidden_size  # 768 for BERT-base

        # Projection head: critical component
        # SimCLR paper shows projection head is essential
        # Without it, performance drops 10-20%
        self.projection_head = nn.Sequential(
            nn.Linear(encoder_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, projection_dim)
        )

        self.temperature = temperature

    def forward(self, input_ids, attention_mask):
        """
        Forward pass through encoder and projection head

        Args:
            input_ids: (batch_size, seq_len)
            attention_mask: (batch_size, seq_len)

        Returns:
            embeddings: (batch_size, projection_dim)
            representations: (batch_size, encoder_dim) - before projection
        """
        # Encode text
        outputs = self.encoder(
            input_ids=input_ids,
            attention_mask=attention_mask
        )

        # Use [CLS] token representation or mean pooling
        # [CLS] token: outputs.last_hidden_state[:, 0]
        # Mean pooling: (outputs.last_hidden_state * mask).sum(1) / mask.sum(1)

        # Mean pooling (often better for sentence embeddings)
        mask_expanded = attention_mask.unsqueeze(-1).expand(outputs.last_hidden_state.size())
        sum_embeddings = torch.sum(outputs.last_hidden_state * mask_expanded, dim=1)
        sum_mask = torch.clamp(mask_expanded.sum(dim=1), min=1e-9)
        representations = sum_embeddings / sum_mask

        # Project to contrastive space
        embeddings = self.projection_head(representations)

        return embeddings, representations

    def compute_simclr_loss(self, embeddings):
        """
        Compute SimCLR NT-Xent loss

        Args:
            embeddings: (2*batch_size, projection_dim)
                       First half are view 1, second half are view 2

        Returns:
            loss: scalar
            metrics: dict with accuracy and similarities
        """
        batch_size = embeddings.shape[0] // 2

        # Normalize embeddings
        embeddings = F.normalize(embeddings, p=2, dim=1)

        # Similarity matrix: (2*batch, 2*batch)
        similarity_matrix = torch.matmul(embeddings, embeddings.T) / self.temperature

        # Create labels: for each 2i, positive is 2i+1 and vice versa
        labels = torch.cat([
            torch.arange(batch_size, 2*batch_size),  # For first half
            torch.arange(0, batch_size)  # For second half
        ]).to(embeddings.device)

        # Mask out self-similarities
        mask = torch.eye(2*batch_size, dtype=torch.bool, device=embeddings.device)
        similarity_matrix.masked_fill_(mask, -9e15)

        # Cross-entropy loss
        loss = F.cross_entropy(similarity_matrix, labels)

        # Compute metrics
        with torch.no_grad():
            predictions = similarity_matrix.argmax(dim=1)
            accuracy = (predictions == labels).float().mean()

            # Positive similarities
            positive_mask = torch.zeros_like(similarity_matrix, dtype=torch.bool)
            positive_mask[torch.arange(2*batch_size), labels] = True
            positive_sim = similarity_matrix[positive_mask].mean()

            # Negative similarities
            negative_sim = similarity_matrix[~positive_mask & ~mask].mean()

        metrics = {
            'accuracy': accuracy.item(),
            'positive_similarity': positive_sim.item(),
            'negative_similarity': negative_sim.item()
        }

        return loss, metrics


class TextAugmentation:
    """
    Text augmentation strategies for contrastive learning

    Unlike images (crop, flip, color jitter), text augmentation is trickier.
    Must preserve semantic meaning while changing surface form.
    """

    def __init__(self):
        self.augmentation_methods = [
            'synonym_replacement',
            'back_translation',
            'random_deletion',
            'random_swap',
            'paraphrase_generation'
        ]

    def augment_simple(self, text, method='random_deletion', p=0.1):
        """
        Simple augmentation: deletion, swapping, synonym replacement

        Args:
            text: Input text
            method: Augmentation method
            p: Probability of applying to each word

        Returns:
            augmented text
        """
        import random
        from nltk.corpus import wordnet

        words = text.split()

        if method == 'random_deletion':
            # Randomly delete words
            if len(words) == 1:
                return text

            new_words = [w for w in words if random.random() > p]

            # Ensure at least one word remains
            if len(new_words) == 0:
                return random.choice(words)

            return ' '.join(new_words)

        elif method == 'random_swap':
            # Randomly swap word positions
            new_words = words.copy()
            for i in range(len(new_words)):
                if random.random() < p:
                    swap_idx = random.randint(0, len(new_words) - 1)
                    new_words[i], new_words[swap_idx] = new_words[swap_idx], new_words[i]

            return ' '.join(new_words)

        elif method == 'synonym_replacement':
            # Replace words with synonyms
            new_words = []
            for word in words:
                if random.random() < p:
                    synonyms = self.get_synonyms(word)
                    if synonyms:
                        new_words.append(random.choice(synonyms))
                    else:
                        new_words.append(word)
                else:
                    new_words.append(word)

            return ' '.join(new_words)

        return text

    def augment_with_llm(self, text, model='gpt-3.5-turbo'):
        """
        Use LLM to generate paraphrases (higher quality, more expensive)

        For production at scale:
        - Pre-generate augmentations offline
        - Cache augmentations
        - Use smaller, faster models (T5, BART)
        """
        from openai import OpenAI

        client = OpenAI()

        prompt = f"""Paraphrase the following text while preserving its meaning:

Text: {text}

Paraphrase:"""

        response = client.chat.completions.create(
            model=model,
            messages=[{"role": "user", "content": prompt}],
            max_tokens=len(text.split()) * 2,
            temperature=0.7
        )

        paraphrase = response.choices[0].message.content.strip()
        return paraphrase

    def get_synonyms(self, word):
        """Get synonyms using WordNet"""
        from nltk.corpus import wordnet

        synonyms = set()
        for syn in wordnet.synsets(word):
            for lemma in syn.lemmas():
                synonym = lemma.name().replace('_', ' ')
                if synonym.lower() != word.lower():
                    synonyms.add(synonym)

        return list(synonyms)


# Training loop
def train_simclr_epoch(model, dataloader, optimizer, device):
    """
    Training epoch for SimCLR
    """
    model.train()
    total_loss = 0
    total_accuracy = 0
    augmenter = TextAugmentation()

    for batch in dataloader:
        texts = batch['text']

        # Generate two augmented views for each text
        augmented_texts = []
        for text in texts:
            aug1 = augmenter.augment_simple(text, method='random_deletion')
            aug2 = augmenter.augment_simple(text, method='synonym_replacement')
            augmented_texts.extend([aug1, aug2])

        # Tokenize both views
        encoded = model.tokenizer(
            augmented_texts,
            padding=True,
            truncation=True,
            max_length=128,
            return_tensors='pt'
        ).to(device)

        # Forward pass
        embeddings, _ = model(encoded['input_ids'], encoded['attention_mask'])

        # Compute loss
        loss, metrics = model.compute_simclr_loss(embeddings)

        # Backward pass
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        total_loss += loss.item()
        total_accuracy += metrics['accuracy']

    avg_loss = total_loss / len(dataloader)
    avg_accuracy = total_accuracy / len(dataloader)

    return avg_loss, avg_accuracy


# Example usage
model = SimCLRTextEmbedding(
    base_model='bert-base-uncased',
    projection_dim=128,
    temperature=0.07
).to('cuda')

optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)

# Train
# for epoch in range(num_epochs):
#     loss, acc = train_simclr_epoch(model, train_loader, optimizer, 'cuda')
#     print(f"Epoch {epoch}: Loss={loss:.4f}, Accuracy={acc:.2%}")
```

### MoCo: Memory-Efficient Contrastive Learning

MoCo (Momentum Contrast) solves a critical problem: **SimCLR requires enormous batch sizes (4096+) for good negatives, which demands massive GPU memory.**

MoCo's solution: **maintain a queue of negative examples across batches**.

**Key Components:**

1. **Query encoder**: Actively trained network
2. **Key encoder**: Momentum-updated copy of query encoder (slowly follows query encoder)
3. **Queue**: FIFO queue of encoded keys (negatives) from previous batches
4. **Dictionary lookup**: Treat contrastive learning as dictionary lookup problem

```python
class MoCoTextEmbedding(nn.Module):
    """
    MoCo (Momentum Contrast) for text embeddings

    Advantages over SimCLR:
    - Small batch sizes work well (256 vs. 4096+)
    - Queue provides large, consistent set of negatives (65K typical)
    - Momentum encoder provides stable keys

    Trade-offs:
    - More complex implementation
    - Queue introduces staleness (keys encoded by older model)
    - Requires careful momentum tuning
    """

    def __init__(self,
                 base_model='bert-base-uncased',
                 projection_dim=128,
                 hidden_dim=512,
                 queue_size=65536,
                 momentum=0.999,
                 temperature=0.07):
        super().__init__()

        self.queue_size = queue_size
        self.momentum = momentum
        self.temperature = temperature

        # Query encoder (actively trained)
        self.encoder_q = AutoModel.from_pretrained(base_model)

        # Key encoder (momentum updated)
        self.encoder_k = AutoModel.from_pretrained(base_model)

        # Initialize key encoder with same weights as query encoder
        for param_q, param_k in zip(self.encoder_q.parameters(),
                                    self.encoder_k.parameters()):
            param_k.data.copy_(param_q.data)
            param_k.requires_grad = False  # Key encoder not trained by backprop

        encoder_dim = self.encoder_q.config.hidden_size

        # Projection heads
        self.projection_q = nn.Sequential(
            nn.Linear(encoder_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, projection_dim)
        )

        self.projection_k = nn.Sequential(
            nn.Linear(encoder_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, projection_dim)
        )

        # Initialize projection_k with same weights
        for param_q, param_k in zip(self.projection_q.parameters(),
                                    self.projection_k.parameters()):
            param_k.data.copy_(param_q.data)
            param_k.requires_grad = False

        # Queue: stores negative keys
        # Shape: (projection_dim, queue_size)
        self.register_buffer('queue', torch.randn(projection_dim, queue_size))
        self.queue = F.normalize(self.queue, dim=0)
        self.register_buffer('queue_ptr', torch.zeros(1, dtype=torch.long))

        self.tokenizer = AutoTokenizer.from_pretrained(base_model)

    @torch.no_grad()
    def _momentum_update_key_encoder(self):
        """
        Momentum update: key_encoder = m * key_encoder + (1 - m) * query_encoder

        This creates a slowly evolving key encoder, providing stable keys
        """
        for param_q, param_k in zip(self.encoder_q.parameters(),
                                    self.encoder_k.parameters()):
            param_k.data = param_k.data * self.momentum + param_q.data * (1 - self.momentum)

        for param_q, param_k in zip(self.projection_q.parameters(),
                                    self.projection_k.parameters()):
            param_k.data = param_k.data * self.momentum + param_q.data * (1 - self.momentum)

    @torch.no_grad()
    def _dequeue_and_enqueue(self, keys):
        """
        Update queue: remove oldest keys, add new keys

        Args:
            keys: (batch_size, projection_dim)
        """
        batch_size = keys.shape[0]

        ptr = int(self.queue_ptr)

        # Replace oldest keys with new keys
        # Queue is circular buffer
        if ptr + batch_size <= self.queue_size:
            self.queue[:, ptr:ptr + batch_size] = keys.T
        else:
            # Wrap around
            remaining = self.queue_size - ptr
            self.queue[:, ptr:] = keys[:remaining].T
            self.queue[:, :batch_size - remaining] = keys[remaining:].T

        ptr = (ptr + batch_size) % self.queue_size
        self.queue_ptr[0] = ptr

    def encode_text(self, input_ids, attention_mask, encoder, projection):
        """
        Encode text through encoder and projection
        """
        outputs = encoder(input_ids=input_ids, attention_mask=attention_mask)

        # Mean pooling
        mask_expanded = attention_mask.unsqueeze(-1).expand(outputs.last_hidden_state.size())
        sum_embeddings = torch.sum(outputs.last_hidden_state * mask_expanded, dim=1)
        sum_mask = torch.clamp(mask_expanded.sum(dim=1), min=1e-9)
        representations = sum_embeddings / sum_mask

        # Project
        embeddings = projection(representations)
        embeddings = F.normalize(embeddings, dim=1)

        return embeddings

    def forward(self, query_input_ids, query_attention_mask,
                key_input_ids, key_attention_mask):
        """
        Forward pass for MoCo

        Args:
            query_input_ids, query_attention_mask: Query batch
            key_input_ids, key_attention_mask: Key batch (augmented version)

        Returns:
            loss, metrics
        """
        # Encode queries (through actively trained encoder)
        q = self.encode_text(query_input_ids, query_attention_mask,
                           self.encoder_q, self.projection_q)

        # Encode keys (through momentum encoder)
        with torch.no_grad():
            # Update key encoder
            self._momentum_update_key_encoder()

            # Encode keys
            k = self.encode_text(key_input_ids, key_attention_mask,
                               self.encoder_k, self.projection_k)

        # Positive logits: (batch_size, 1)
        l_pos = torch.einsum('nc,nc->n', [q, k]).unsqueeze(-1)

        # Negative logits: (batch_size, queue_size)
        l_neg = torch.einsum('nc,ck->nk', [q, self.queue.clone().detach()])

        # Concatenate positive and negative logits
        # Shape: (batch_size, 1 + queue_size)
        logits = torch.cat([l_pos, l_neg], dim=1) / self.temperature

        # Labels: positive is always first (index 0)
        labels = torch.zeros(logits.shape[0], dtype=torch.long, device=q.device)

        # Compute loss
        loss = F.cross_entropy(logits, labels)

        # Update queue
        self._dequeue_and_enqueue(k)

        # Metrics
        with torch.no_grad():
            predictions = logits.argmax(dim=1)
            accuracy = (predictions == labels).float().mean()

            positive_sim = l_pos.mean()
            negative_sim = l_neg.mean()

        metrics = {
            'accuracy': accuracy.item(),
            'positive_similarity': positive_sim.item(),
            'negative_similarity': negative_sim.item(),
            'queue_ptr': int(self.queue_ptr[0])
        }

        return loss, metrics


# Training function for MoCo
def train_moco_epoch(model, dataloader, optimizer, device):
    """
    Train MoCo for one epoch

    Key difference from SimCLR: small batch sizes work!
    """
    model.train()
    total_loss = 0
    total_accuracy = 0
    augmenter = TextAugmentation()

    for batch in dataloader:
        texts = batch['text']

        # Generate query and key augmentations
        queries = [augmenter.augment_simple(text, method='random_deletion')
                  for text in texts]
        keys = [augmenter.augment_simple(text, method='synonym_replacement')
               for text in texts]

        # Tokenize
        query_encoded = model.tokenizer(
            queries, padding=True, truncation=True,
            max_length=128, return_tensors='pt'
        ).to(device)

        key_encoded = model.tokenizer(
            keys, padding=True, truncation=True,
            max_length=128, return_tensors='pt'
        ).to(device)

        # Forward pass
        loss, metrics = model(
            query_encoded['input_ids'], query_encoded['attention_mask'],
            key_encoded['input_ids'], key_encoded['attention_mask']
        )

        # Backward pass
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        total_loss += loss.item()
        total_accuracy += metrics['accuracy']

    return total_loss / len(dataloader), total_accuracy / len(dataloader)


# Example: MoCo works well with smaller batches
model_moco = MoCoTextEmbedding(
    base_model='bert-base-uncased',
    projection_dim=128,
    queue_size=65536,  # Large queue of negatives
    momentum=0.999,  # Slow momentum update
    temperature=0.07
).to('cuda')

optimizer = torch.optim.AdamW(
    [p for p in model_moco.parameters() if p.requires_grad],
    lr=2e-5
)

# Can use batch size 256 instead of 4096!
# Queue provides large set of high-quality negatives
```

### Enterprise Adaptations: From Research to Production

Research implementations (SimCLR, MoCo) require significant adaptation for enterprise scale:

**Adaptation 1: Multi-Modality Support**

```python
class MultiModalContrastive(nn.Module):
    """
    Contrastive learning for multi-modal data (text + image, text + metadata, etc.)

    Use case: E-commerce (product text + images),
              Healthcare (clinical notes + scans),
              Media (articles + photos)
    """

    def __init__(self, text_encoder, image_encoder, projection_dim=256):
        super().__init__()

        self.text_encoder = text_encoder  # BERT, etc.
        self.image_encoder = image_encoder  # ResNet, ViT, etc.

        # Separate projections for each modality
        self.text_projection = nn.Linear(768, projection_dim)
        self.image_projection = nn.Linear(2048, projection_dim)

        self.temperature = 0.07

    def forward(self, text_inputs, images):
        """
        Compute cross-modal contrastive loss

        Args:
            text_inputs: Dictionary with input_ids, attention_mask
            images: (batch_size, 3, H, W)

        Returns:
            loss: bidirectional contrastive loss
        """
        # Encode text and images
        text_features = self.text_encoder(**text_inputs).last_hidden_state[:, 0]
        image_features = self.image_encoder(images)

        # Project to shared space
        text_emb = F.normalize(self.text_projection(text_features), dim=1)
        image_emb = F.normalize(self.image_projection(image_features), dim=1)

        # Compute similarities: (batch, batch)
        logits_text_to_image = torch.matmul(text_emb, image_emb.T) / self.temperature
        logits_image_to_text = logits_text_to_image.T

        # Labels: diagonal elements are positives
        batch_size = text_emb.shape[0]
        labels = torch.arange(batch_size, device=text_emb.device)

        # Bidirectional loss
        loss_text_to_image = F.cross_entropy(logits_text_to_image, labels)
        loss_image_to_text = F.cross_entropy(logits_image_to_text, labels)

        loss = (loss_text_to_image + loss_image_to_text) / 2

        return loss
```

**Adaptation 2: Domain-Specific Contrastive Objectives**

```python
class DomainAdaptedContrastive:
    """
    Domain-specific adaptations for enterprise use cases
    """

    def hierarchical_contrastive_loss(self, embeddings, hierarchical_labels):
        """
        Hierarchical contrastive learning for taxonomies

        Use case: E-commerce categories, medical ontologies, document classification

        Args:
            embeddings: (batch_size, dim)
            hierarchical_labels: List of (parent_id, child_id, item_id)

        Returns:
            loss that respects hierarchy
        """
        # Items with same parent should be somewhat similar
        # Items with same grandparent should be slightly similar
        # Items in different hierarchies should be dissimilar

        # Implement soft contrastive loss with hierarchy weights
        batch_size = len(embeddings)

        # Compute similarity matrix
        sim_matrix = torch.matmul(embeddings, embeddings.T)

        # Create weight matrix based on hierarchical distance
        weights = torch.zeros((batch_size, batch_size))

        for i in range(batch_size):
            for j in range(batch_size):
                if i == j:
                    weights[i, j] = 0  # Self
                elif hierarchical_labels[i]['child_id'] == hierarchical_labels[j]['child_id']:
                    weights[i, j] = 1.0  # Same leaf node
                elif hierarchical_labels[i]['parent_id'] == hierarchical_labels[j]['parent_id']:
                    weights[i, j] = 0.5  # Same parent, different child
                else:
                    weights[i, j] = -1.0  # Different hierarchy

        # Weighted contrastive loss
        # Positive examples: weight > 0
        # Negative examples: weight < 0
        positive_mask = weights > 0
        negative_mask = weights < 0

        loss = 0
        for i in range(batch_size):
            if positive_mask[i].sum() == 0:
                continue

            # Positive similarities (maximize)
            pos_sim = sim_matrix[i, positive_mask[i]] * weights[i, positive_mask[i]]

            # Negative similarities (minimize)
            neg_sim = sim_matrix[i, negative_mask[i]]

            # Contrastive loss for this example
            numerator = torch.exp(pos_sim).sum()
            denominator = numerator + torch.exp(neg_sim).sum()

            loss -= torch.log(numerator / denominator)

        return loss / batch_size

    def temporal_contrastive_loss(self, embeddings, timestamps, decay_halflife=30):
        """
        Temporal contrastive learning: recent items more similar

        Use case: News, social media, time-series data

        Args:
            embeddings: (batch_size, dim)
            timestamps: (batch_size,) - Unix timestamps
            decay_halflife: Days for similarity decay

        Returns:
            loss with temporal weighting
        """
        batch_size = len(embeddings)

        # Compute temporal distances (in days)
        time_diff_matrix = torch.abs(
            timestamps.unsqueeze(1) - timestamps.unsqueeze(0)
        ) / (60 * 60 * 24)  # Convert to days

        # Temporal similarity: exponential decay
        temporal_similarity = torch.exp(-time_diff_matrix / decay_halflife)

        # Compute embedding similarities
        emb_sim = torch.matmul(
            F.normalize(embeddings, dim=1),
            F.normalize(embeddings, dim=1).T
        )

        # Loss: embedding similarity should match temporal similarity
        # Recent items should have similar embeddings
        loss = F.mse_loss(emb_sim, temporal_similarity)

        return loss
```

**Adaptation 3: Production-Scale Data Loading**

```python
import torch.utils.data as data

class ContrastiveDataset(data.Dataset):
    """
    Efficient dataset for contrastive learning at scale
    """

    def __init__(self, data_source, augmenter, cache_size=10000):
        """
        Args:
            data_source: Iterator or dataset
            augmenter: TextAugmentation instance
            cache_size: Number of items to cache in memory
        """
        self.data_source = data_source
        self.augmenter = augmenter
        self.cache = {}
        self.cache_size = cache_size

    def __len__(self):
        return len(self.data_source)

    def __getitem__(self, idx):
        """
        Return two augmented views of the same item
        """
        # Check cache first
        if idx in self.cache:
            text = self.cache[idx]
        else:
            text = self.data_source[idx]

            # Update cache (LRU-style)
            if len(self.cache) >= self.cache_size:
                # Remove random item
                self.cache.pop(next(iter(self.cache)))
            self.cache[idx] = text

        # Generate two augmented views
        view1 = self.augmenter.augment_simple(text, method='random_deletion')
        view2 = self.augmenter.augment_simple(text, method='synonym_replacement')

        return {
            'view1': view1,
            'view2': view2,
            'original': text,
            'idx': idx
        }


class DistributedContrastiveDataLoader:
    """
    Data loader for distributed contrastive training

    Ensures each GPU sees different augmentations of the same base data
    """

    def __init__(self, dataset, batch_size, world_size, rank):
        """
        Args:
            dataset: ContrastiveDataset
            batch_size: Per-GPU batch size
            world_size: Number of GPUs
            rank: Current GPU rank
        """
        self.dataset = dataset
        self.batch_size = batch_size
        self.world_size = world_size
        self.rank = rank

        # Sampler ensures each GPU gets different data
        self.sampler = torch.utils.data.distributed.DistributedSampler(
            dataset,
            num_replicas=world_size,
            rank=rank,
            shuffle=True
        )

        self.dataloader = torch.utils.data.DataLoader(
            dataset,
            batch_size=batch_size,
            sampler=self.sampler,
            num_workers=4,
            pin_memory=True,
            drop_last=True  # Important for contrastive learning
        )

    def __iter__(self):
        return iter(self.dataloader)

    def __len__(self):
        return len(self.dataloader)
```

## Hard Negative Mining at Scale

The quality of negative examples determines contrastive learning success. Random negatives are often too easy—the model learns to distinguish obvious differences rather than subtle semantic distinctions. **Hard negative mining finds challenging negatives that push the model to learn fine-grained representations.**

### The Hard Negative Spectrum

Not all negatives are created equal:

**Easy Negatives**: Completely dissimilar to anchor (different domain, topic, language)
- Pro: Abundant, cheap to find
- Con: Too easy; model doesn't learn much
- Example: Comparing "quantum physics" to "banana recipes"

**Medium Negatives**: Somewhat similar but clearly different
- Pro: Provide useful training signal
- Con: May not push model to full potential
- Example: Comparing "machine learning" to "data engineering"

**Hard Negatives**: Very similar to anchor but not truly positive
- Pro: Force model to learn fine-grained distinctions
- Con: Expensive to find, risk of false negatives
- Example: Comparing "supervised learning" to "semi-supervised learning"

**False Negatives**: Labeled negative but actually positive
- Pro: None
- Con: Actively hurt training, confuse model
- Example: Comparing two paraphrases of the same content

### Hard Negative Mining Strategies

**Strategy 1: In-Batch Hard Negative Mining**

Simplest approach: within each batch, use hardest negatives as training signal.

```python
class InBatchHardNegativeMining:
    """
    Mine hard negatives from within batch

    Advantages:
    - Zero overhead (no additional computation)
    - Works with any batch size
    - Simple to implement

    Disadvantages:
    - Limited by batch diversity
    - May miss global hard negatives
    """

    def __init__(self, temperature=0.07, hardness_threshold=0.5):
        """
        Args:
            temperature: Contrastive loss temperature
            hardness_threshold: Minimum similarity to consider "hard"
        """
        self.temperature = temperature
        self.hardness_threshold = hardness_threshold

    def compute_loss_with_hard_negatives(self, anchor_emb, positive_emb,
                                         all_emb, num_hard_negatives=10):
        """
        Compute contrastive loss using hard negatives from batch

        Args:
            anchor_emb: (batch_size, dim)
            positive_emb: (batch_size, dim)
            all_emb: (2*batch_size, dim) - all embeddings in batch
            num_hard_negatives: How many hard negatives to use per anchor

        Returns:
            loss, metrics
        """
        batch_size = anchor_emb.shape[0]

        # Normalize
        anchor_norm = F.normalize(anchor_emb, p=2, dim=1)
        positive_norm = F.normalize(positive_emb, p=2, dim=1)
        all_norm = F.normalize(all_emb, p=2, dim=1)

        # Compute all similarities: (batch_size, 2*batch_size)
        all_similarities = torch.matmul(anchor_norm, all_norm.T)

        # For each anchor, find hard negatives
        # Hard negative: high similarity but not the positive
        losses = []
        hard_negative_sims = []

        for i in range(batch_size):
            # Positive similarity
            pos_sim = F.cosine_similarity(
                anchor_norm[i:i+1],
                positive_norm[i:i+1]
            )

            # All negative similarities (excluding positive)
            neg_sims = torch.cat([
                all_similarities[i, :i],
                all_similarities[i, i+1:]
            ])

            # Find hard negatives: highest similarities that aren't positive
            # Filter to similarities above threshold (hard enough to be useful)
            hard_mask = neg_sims > self.hardness_threshold

            if hard_mask.sum() > 0:
                hard_neg_sims = neg_sims[hard_mask]

                # Take top-k hardest
                if len(hard_neg_sims) > num_hard_negatives:
                    hard_neg_sims = hard_neg_sims.topk(num_hard_negatives)[0]
            else:
                # Fallback: use hardest negatives even if below threshold
                hard_neg_sims = neg_sims.topk(
                    min(num_hard_negatives, len(neg_sims))
                )[0]

            # Contrastive loss with hard negatives
            pos_exp = torch.exp(pos_sim / self.temperature)
            neg_exp = torch.exp(hard_neg_sims / self.temperature).sum()

            loss_i = -torch.log(pos_exp / (pos_exp + neg_exp))
            losses.append(loss_i)

            hard_negative_sims.append(hard_neg_sims.mean())

        loss = torch.stack(losses).mean()

        metrics = {
            'hard_negative_similarity': torch.stack(hard_negative_sims).mean().item(),
            'num_hard_negatives_used': num_hard_negatives
        }

        return loss, metrics
```

**Strategy 2: Cross-Batch Hard Negative Mining with Queue**

Extend search space beyond current batch using a queue of recent embeddings.

```python
class QueueBasedHardNegativeMining:
    """
    Maintain queue of recent embeddings for hard negative mining

    Advantages:
    - Larger pool of candidates (10K-100K typical)
    - Can find harder negatives than single batch
    - Smoother training (less batch-dependent)

    Disadvantages:
    - Staleness: queue contains embeddings from old model
    - Memory overhead
    """

    def __init__(self, embedding_dim, queue_size=65536,
                 temperature=0.07, momentum=0.999):
        """
        Args:
            embedding_dim: Dimension of embeddings
            queue_size: Number of embeddings to store
            temperature: Contrastive temperature
            momentum: Momentum for queue encoder updates
        """
        self.embedding_dim = embedding_dim
        self.queue_size = queue_size
        self.temperature = temperature
        self.momentum = momentum

        # Queue of embeddings
        self.queue = torch.randn(queue_size, embedding_dim)
        self.queue = F.normalize(self.queue, dim=1)

        # Metadata for each embedding in queue (optional)
        self.queue_metadata = [None] * queue_size

        self.queue_ptr = 0

    def update_queue(self, new_embeddings, metadata=None):
        """
        Add new embeddings to queue, removing oldest

        Args:
            new_embeddings: (batch_size, embedding_dim)
            metadata: Optional metadata for each embedding
        """
        batch_size = new_embeddings.shape[0]

        # Add to queue
        end_ptr = (self.queue_ptr + batch_size) % self.queue_size

        if end_ptr > self.queue_ptr:
            self.queue[self.queue_ptr:end_ptr] = new_embeddings.detach()
            if metadata:
                self.queue_metadata[self.queue_ptr:end_ptr] = metadata
        else:
            # Wrap around
            first_part = self.queue_size - self.queue_ptr
            self.queue[self.queue_ptr:] = new_embeddings[:first_part].detach()
            self.queue[:end_ptr] = new_embeddings[first_part:].detach()

            if metadata:
                self.queue_metadata[self.queue_ptr:] = metadata[:first_part]
                self.queue_metadata[:end_ptr] = metadata[first_part:]

        self.queue_ptr = end_ptr

    def mine_hard_negatives(self, anchor_emb, k=100):
        """
        Find k hardest negatives from queue for each anchor

        Args:
            anchor_emb: (batch_size, embedding_dim)
            k: Number of hard negatives to return

        Returns:
            hard_negatives: (batch_size, k, embedding_dim)
            hard_negative_scores: (batch_size, k)
        """
        # Normalize
        anchor_norm = F.normalize(anchor_emb, dim=1)

        # Compute similarities to all queue entries
        # (batch_size, queue_size)
        similarities = torch.matmul(anchor_norm, self.queue.T)

        # For each anchor, get top-k hardest (highest similarity) negatives
        # (batch_size, k)
        hard_negative_scores, hard_negative_indices = similarities.topk(k, dim=1)

        # Gather hard negative embeddings
        # (batch_size, k, embedding_dim)
        hard_negatives = self.queue[hard_negative_indices]

        return hard_negatives, hard_negative_scores

    def compute_loss(self, anchor_emb, positive_emb, num_hard_negatives=100):
        """
        Compute contrastive loss using hard negatives from queue
        """
        batch_size = anchor_emb.shape[0]

        # Get hard negatives
        hard_negs, hard_neg_sims = self.mine_hard_negatives(
            anchor_emb,
            k=num_hard_negatives
        )

        # Normalize
        anchor_norm = F.normalize(anchor_emb, dim=1)
        positive_norm = F.normalize(positive_emb, dim=1)

        # Positive similarities
        pos_sim = F.cosine_similarity(anchor_norm, positive_norm, dim=1)

        # Hard negative similarities (already computed in mining)
        # Shape: (batch_size, num_hard_negatives)

        # Compute loss
        pos_exp = torch.exp(pos_sim / self.temperature)
        neg_exp = torch.exp(hard_neg_sims / self.temperature).sum(dim=1)

        loss = -torch.log(pos_exp / (pos_exp + neg_exp)).mean()

        # Update queue with current batch
        self.update_queue(anchor_emb)

        metrics = {
            'positive_similarity': pos_sim.mean().item(),
            'hard_negative_similarity': hard_neg_sims.mean().item(),
            'queue_utilization': min(self.queue_ptr, self.queue_size) / self.queue_size
        }

        return loss, metrics
```

**Strategy 3: Offline Hard Negative Mining**

Most powerful but most expensive: periodically mine hard negatives from entire dataset.

```python
import faiss
import numpy as np

class OfflineHardNegativeMining:
    """
    Offline hard negative mining using approximate nearest neighbors

    Process:
    1. Encode entire dataset with current model
    2. Build ANN index
    3. For each training example, find K nearest neighbors (hard negatives)
    4. Store hard negatives for next epoch
    5. Repeat periodically (e.g., every epoch or every N steps)

    Advantages:
    - Access to global hard negatives (best quality)
    - Can mine very hard negatives
    - Control over negative distribution

    Disadvantages:
    - Expensive: requires full dataset encoding
    - Storage: need to store mined negatives
    - Staleness: negatives from old model checkpoints
    """

    def __init__(self, embedding_dim, num_hard_negatives=10):
        """
        Args:
            embedding_dim: Dimension of embeddings
            num_hard_negatives: How many hard negatives to mine per example
        """
        self.embedding_dim = embedding_dim
        self.num_hard_negatives = num_hard_negatives

        # FAISS index for approximate nearest neighbor search
        self.index = None
        self.dataset_embeddings = None
        self.dataset_ids = None

    def encode_dataset(self, model, dataloader, device):
        """
        Encode entire dataset with current model

        Args:
            model: Current embedding model
            dataloader: DataLoader for full dataset
            device: torch device

        Returns:
            embeddings: (num_examples, embedding_dim)
            ids: (num_examples,) - example IDs
        """
        model.eval()

        all_embeddings = []
        all_ids = []

        with torch.no_grad():
            for batch in dataloader:
                # Encode batch
                embeddings = model(
                    batch['input_ids'].to(device),
                    batch['attention_mask'].to(device)
                )

                all_embeddings.append(embeddings.cpu())
                all_ids.extend(batch['id'])

        # Concatenate all
        embeddings = torch.cat(all_embeddings, dim=0).numpy()

        # Normalize for cosine similarity
        embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)

        return embeddings, all_ids

    def build_index(self, embeddings):
        """
        Build FAISS index for efficient nearest neighbor search

        Args:
            embeddings: (num_examples, embedding_dim) numpy array
        """
        num_examples = embeddings.shape[0]

        # For large datasets (>1M), use IVF index for speed
        # For smaller datasets, use flat index for accuracy
        if num_examples > 1_000_000:
            # IVF index: partitions space into cells
            nlist = int(np.sqrt(num_examples))  # Number of cells
            quantizer = faiss.IndexFlatIP(self.embedding_dim)  # Inner product
            self.index = faiss.IndexIVFFlat(
                quantizer,
                self.embedding_dim,
                nlist,
                faiss.METRIC_INNER_PRODUCT
            )

            # Train index (required for IVF)
            print("Training FAISS index...")
            self.index.train(embeddings)
        else:
            # Flat index: exact search
            self.index = faiss.IndexFlatIP(self.embedding_dim)

        # Add embeddings to index
        print(f"Adding {num_examples} embeddings to index...")
        self.index.add(embeddings)

        self.dataset_embeddings = embeddings

        print(f"Index built with {self.index.ntotal} vectors")

    def mine_hard_negatives(self, query_ids, positive_ids=None, k=None):
        """
        Mine hard negatives for given queries

        Args:
            query_ids: List of query example IDs
            positive_ids: List of lists of positive IDs (to exclude)
            k: Number of hard negatives to mine (default: self.num_hard_negatives)

        Returns:
            hard_negatives: Dict mapping query_id to list of hard negative IDs
        """
        if k is None:
            k = self.num_hard_negatives

        hard_negatives = {}

        for idx, query_id in enumerate(query_ids):
            # Get query embedding
            query_embedding = self.dataset_embeddings[query_id:query_id+1]

            # Search for k+N nearest neighbors (some may be positives to exclude)
            # Get extra to account for self and positives
            search_k = k + 10

            scores, indices = self.index.search(query_embedding, search_k)

            # Filter out self and known positives
            hard_neg_ids = []

            for score, idx_result in zip(scores[0], indices[0]):
                # Skip self
                if idx_result == query_id:
                    continue

                # Skip known positives
                if positive_ids and idx_result in positive_ids[idx]:
                    continue

                hard_neg_ids.append(idx_result)

                if len(hard_neg_ids) >= k:
                    break

            hard_negatives[query_id] = hard_neg_ids

        return hard_negatives

    def refresh_hard_negatives(self, model, dataloader, device,
                               positive_pairs=None):
        """
        Full refresh: encode dataset, build index, mine negatives

        Args:
            model: Current embedding model
            dataloader: DataLoader for full dataset
            device: torch device
            positive_pairs: Dict mapping query_id to list of positive_ids

        Returns:
            hard_negatives: Dict mapping each example to hard negative IDs
        """
        print("Encoding dataset with current model...")
        embeddings, ids = self.encode_dataset(model, dataloader, device)

        print("Building FAISS index...")
        self.build_index(embeddings)

        print("Mining hard negatives...")
        query_ids = list(range(len(ids)))
        positive_ids_list = [positive_pairs.get(i, []) for i in query_ids] if positive_pairs else None

        hard_negatives = self.mine_hard_negatives(
            query_ids,
            positive_ids=positive_ids_list
        )

        print(f"Mined {self.num_hard_negatives} hard negatives for {len(hard_negatives)} examples")

        return hard_negatives


# Training with offline hard negative mining
class HardNegativeDataset(data.Dataset):
    """
    Dataset that uses pre-mined hard negatives
    """

    def __init__(self, base_dataset, hard_negatives_map):
        """
        Args:
            base_dataset: Original dataset
            hard_negatives_map: Dict mapping example_id to list of hard negative IDs
        """
        self.base_dataset = base_dataset
        self.hard_negatives_map = hard_negatives_map

    def __len__(self):
        return len(self.base_dataset)

    def __getitem__(self, idx):
        # Get anchor and positive
        item = self.base_dataset[idx]

        # Get hard negatives
        hard_neg_ids = self.hard_negatives_map.get(idx, [])

        if hard_neg_ids:
            # Sample one hard negative
            neg_id = np.random.choice(hard_neg_ids)
            negative = self.base_dataset[neg_id]
        else:
            # Fallback: random negative
            neg_id = np.random.randint(0, len(self.base_dataset))
            while neg_id == idx:
                neg_id = np.random.randint(0, len(self.base_dataset))
            negative = self.base_dataset[neg_id]

        return {
            'anchor': item['text'],
            'positive': item['positive'],  # Assume dataset provides positives
            'negative': negative['text']
        }


# Usage in training loop
def train_with_offline_hard_negatives(model, base_dataset, device, num_epochs=10):
    """
    Training loop with periodic hard negative mining
    """
    hard_negative_miner = OfflineHardNegativeMining(
        embedding_dim=model.embedding_dim,
        num_hard_negatives=10
    )

    # Mine hard negatives every epoch
    for epoch in range(num_epochs):
        print(f"\nEpoch {epoch}")

        # Refresh hard negatives
        dataloader = torch.utils.data.DataLoader(
            base_dataset,
            batch_size=256,
            shuffle=False
        )

        hard_negatives = hard_negative_miner.refresh_hard_negatives(
            model, dataloader, device
        )

        # Create dataset with hard negatives
        train_dataset = HardNegativeDataset(base_dataset, hard_negatives)
        train_loader = torch.utils.data.DataLoader(
            train_dataset,
            batch_size=64,
            shuffle=True
        )

        # Train epoch
        model.train()
        for batch in train_loader:
            # Training step with anchor, positive, hard negative
            # ... (standard training loop)
            pass
```

**Strategy 4: Debiased Hard Negative Mining**

Critical issue: hard negatives can be **false negatives**—actually positive but labeled negative. This is catastrophic for training.

```python
class DebiasedHardNegativeMining:
    """
    Filter out false negatives from hard negative candidates

    Techniques:
    1. Cross-encoder filtering
    2. Clustering-based filtering
    3. Human-in-the-loop verification
    4. Confidence thresholding
    """

    def __init__(self, embedding_dim, cross_encoder=None):
        """
        Args:
            embedding_dim: Dimension of embeddings
            cross_encoder: Optional cross-encoder model for verification
        """
        self.embedding_dim = embedding_dim
        self.cross_encoder = cross_encoder

    def filter_with_cross_encoder(self, anchor_texts, negative_candidates,
                                  threshold=0.3):
        """
        Use cross-encoder to filter out false negatives

        Cross-encoder: directly compares pair of texts (more accurate than embeddings)

        Args:
            anchor_texts: List of anchor texts
            negative_candidates: List of lists of candidate negatives
            threshold: Similarity threshold for false negative (0-1)

        Returns:
            filtered_negatives: List of lists with false negatives removed
        """
        if self.cross_encoder is None:
            raise ValueError("Cross-encoder model required")

        filtered_negatives = []

        for anchor, candidates in zip(anchor_texts, negative_candidates):
            # Score each candidate with cross-encoder
            pairs = [[anchor, cand] for cand in candidates]
            scores = self.cross_encoder.predict(pairs)

            # Filter out candidates with high similarity (likely false negatives)
            filtered = [
                cand for cand, score in zip(candidates, scores)
                if score < threshold
            ]

            filtered_negatives.append(filtered)

        return filtered_negatives

    def filter_with_clustering(self, embeddings, hard_negative_indices,
                              cluster_threshold=0.8):
        """
        Filter negatives that cluster with positives

        Logic: If hard negative is in same tight cluster as anchor,
               likely false negative

        Args:
            embeddings: (num_examples, embedding_dim)
            hard_negative_indices: (num_queries, num_candidates)
            cluster_threshold: Similarity threshold for same cluster

        Returns:
            filtered_indices: Hard negatives outside anchor's cluster
        """
        from sklearn.cluster import AgglomerativeClustering

        # Cluster embeddings
        clustering = AgglomerativeClustering(
            n_clusters=None,
            distance_threshold=1 - cluster_threshold,
            linkage='average'
        )

        labels = clustering.fit_predict(embeddings)

        # For each query, filter negatives in same cluster
        filtered_indices = []

        for query_idx, neg_indices in enumerate(hard_negative_indices):
            query_cluster = labels[query_idx]

            # Keep negatives from different clusters
            filtered = [
                neg_idx for neg_idx in neg_indices
                if labels[neg_idx] != query_cluster
            ]

            filtered_indices.append(filtered)

        return filtered_indices

    def confidence_based_filtering(self, anchor_emb, positive_emb,
                                   negative_candidates, min_margin=0.1):
        """
        Filter negatives too similar to positive

        Ensures: sim(anchor, negative) < sim(anchor, positive) - margin

        Args:
            anchor_emb: (batch_size, dim)
            positive_emb: (batch_size, dim)
            negative_candidates: (batch_size, num_candidates, dim)
            min_margin: Minimum margin between positive and negative

        Returns:
            filtered_negatives: Negatives with sufficient margin
        """
        # Normalize
        anchor_norm = F.normalize(anchor_emb, dim=1)
        positive_norm = F.normalize(positive_emb, dim=1)

        # Positive similarities
        pos_sim = F.cosine_similarity(anchor_norm, positive_norm, dim=1)

        filtered_negatives = []

        for i in range(len(anchor_emb)):
            # Negative similarities
            neg_sims = F.cosine_similarity(
                anchor_norm[i:i+1],
                negative_candidates[i],
                dim=1
            )

            # Filter: keep negatives with sim < pos_sim - margin
            threshold = pos_sim[i] - min_margin
            valid_mask = neg_sims < threshold

            filtered = negative_candidates[i][valid_mask]
            filtered_negatives.append(filtered)

        return filtered_negatives
```

## Batch Optimization for Trillion-Row Training

Contrastive learning thrives on large batches: more negatives = better signal. But trillion-row datasets demand sophisticated batch optimization to balance statistical efficiency with computational constraints.

### The Batch Size-Performance Relationship

**Why Larger Batches Help Contrastive Learning:**

1. **More negatives**: Batch size 4096 provides 4095 negatives per positive
2. **Better gradients**: Larger batches reduce gradient noise
3. **Tighter mutual information bound**: InfoNCE bound improves with K negatives
4. **Harder negatives**: Larger batches more likely to contain challenging examples

**Empirical Scaling:**

| Batch Size | Relative Performance | Memory (A100) | Training Time |
|------------|---------------------|---------------|---------------|
| 256        | 0.85                | 12 GB         | 1x            |
| 512        | 0.90                | 24 GB         | 0.9x          |
| 1024       | 0.94                | 45 GB         | 0.8x          |
| 2048       | 0.97                | 80 GB         | 0.7x          |
| 4096       | 1.00                | OOM           | 0.65x         |
| 8192       | 1.01                | OOM           | 0.6x          |

**The Challenge**: Modern GPUs have limited memory. NVIDIA A100 (80GB) caps at ~2048 batch size for BERT-base contrastive learning. Trillion-row datasets need larger batches for optimal performance.

### Gradient Accumulation: Simulate Large Batches

```python
class GradientAccumulationContrastive:
    """
    Simulate large batch sizes through gradient accumulation

    Idea: Accumulate gradients over multiple small batches,
          then update weights once.

    Effective batch size = micro_batch_size × accumulation_steps
    """

    def __init__(self, model, optimizer, micro_batch_size=256,
                 effective_batch_size=4096):
        """
        Args:
            model: Embedding model
            optimizer: Optimizer
            micro_batch_size: Batch size that fits in memory
            effective_batch_size: Target effective batch size
        """
        self.model = model
        self.optimizer = optimizer
        self.micro_batch_size = micro_batch_size
        self.effective_batch_size = effective_batch_size

        self.accumulation_steps = effective_batch_size // micro_batch_size

        print(f"Gradient accumulation: {self.accumulation_steps} steps")
        print(f"Micro batch size: {micro_batch_size}")
        print(f"Effective batch size: {effective_batch_size}")

    def train_step(self, dataloader, device):
        """
        Training step with gradient accumulation
        """
        self.model.train()
        self.optimizer.zero_grad()

        accumulated_loss = 0
        accumulated_metrics = {'accuracy': 0, 'positive_sim': 0, 'negative_sim': 0}

        # Accumulate gradients over multiple micro-batches
        for step, batch in enumerate(dataloader):
            # Forward pass
            embeddings = self.model(
                batch['input_ids'].to(device),
                batch['attention_mask'].to(device)
            )

            loss, metrics = self.model.compute_loss(embeddings)

            # Scale loss by accumulation steps
            # This ensures gradient magnitude is correct
            loss = loss / self.accumulation_steps

            # Backward pass (accumulates gradients)
            loss.backward()

            accumulated_loss += loss.item()
            accumulated_metrics['accuracy'] += metrics['accuracy']
            accumulated_metrics['positive_sim'] += metrics['positive_sim']
            accumulated_metrics['negative_sim'] += metrics['negative_sim']

            # Update weights after accumulation_steps
            if (step + 1) % self.accumulation_steps == 0:
                # Clip gradients
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)

                # Optimizer step
                self.optimizer.step()
                self.optimizer.zero_grad()

                # Average metrics
                for key in accumulated_metrics:
                    accumulated_metrics[key] /= self.accumulation_steps

                return accumulated_loss, accumulated_metrics
```

**Limitation of Gradient Accumulation for Contrastive Learning:**

Gradient accumulation works well for supervised learning but has a critical flaw for contrastive learning: **each micro-batch only sees its own negatives, not the full effective batch**. This defeats the purpose of large batches.

Solution: Use distributed training to actually process large batches.

### Distributed Contrastive Learning

Distribute computation across multiple GPUs/nodes to enable truly large batches.

```python
import torch.distributed as dist
import torch.nn.functional as F

class DistributedContrastiveLearning:
    """
    Distributed contrastive learning across multiple GPUs

    Key challenge: Gathering embeddings from all GPUs for negative mining
    """

    def __init__(self, model, world_size, rank, backend='nccl'):
        """
        Args:
            model: Embedding model
            world_size: Number of GPUs/processes
            rank: Current process rank
            backend: 'nccl' for GPU, 'gloo' for CPU
        """
        self.model = model
        self.world_size = world_size
        self.rank = rank

        # Initialize process group
        if not dist.is_initialized():
            dist.init_process_group(
                backend=backend,
                rank=rank,
                world_size=world_size
            )

        # Wrap model in DistributedDataParallel
        self.model = torch.nn.parallel.DistributedDataParallel(
            model,
            device_ids=[rank],
            output_device=rank
        )

    def gather_embeddings(self, local_embeddings):
        """
        Gather embeddings from all GPUs

        Args:
            local_embeddings: (local_batch_size, dim) on this GPU

        Returns:
            all_embeddings: (world_size × local_batch_size, dim)
            all_gathered on all GPUs
        """
        # Prepare tensors for gathering
        world_size = dist.get_world_size()
        local_batch_size = local_embeddings.shape[0]
        embedding_dim = local_embeddings.shape[1]

        # Create list of tensors to receive gathered embeddings
        gathered_embeddings = [
            torch.zeros_like(local_embeddings)
            for _ in range(world_size)
        ]

        # All-gather: each GPU gets embeddings from all GPUs
        dist.all_gather(gathered_embeddings, local_embeddings)

        # Concatenate all embeddings
        # Shape: (world_size × local_batch_size, embedding_dim)
        all_embeddings = torch.cat(gathered_embeddings, dim=0)

        return all_embeddings

    def compute_distributed_contrastive_loss(self, anchor_emb, positive_emb,
                                            temperature=0.07):
        """
        Compute contrastive loss using embeddings from all GPUs

        Each GPU computes loss for its local batch using negatives from all GPUs

        Args:
            anchor_emb: (local_batch, dim) anchors on this GPU
            positive_emb: (local_batch, dim) positives on this GPU
            temperature: Contrastive temperature

        Returns:
            loss: scalar loss for this GPU
            metrics: dict with metrics
        """
        local_batch_size = anchor_emb.shape[0]

        # Gather embeddings from all GPUs
        # all_anchors: (world_size × local_batch, dim)
        # all_positives: (world_size × local_batch, dim)
        all_anchors = self.gather_embeddings(anchor_emb)
        all_positives = self.gather_embeddings(positive_emb)

        # Total batch size across all GPUs
        global_batch_size = all_anchors.shape[0]

        # Normalize
        all_anchors = F.normalize(all_anchors, dim=1)
        all_positives = F.normalize(all_positives, dim=1)

        # Compute similarity matrix for LOCAL anchors vs ALL embeddings
        # This is memory-efficient: only compute for local batch
        local_anchors_norm = F.normalize(anchor_emb, dim=1)

        # Concatenate all positives and anchors as potential negatives
        all_embeddings = torch.cat([all_anchors, all_positives], dim=0)

        # Similarity: (local_batch, 2 × global_batch)
        similarity_matrix = torch.matmul(
            local_anchors_norm,
            all_embeddings.T
        ) / temperature

        # Create labels: positive for each anchor is at specific global index
        # Local anchor i corresponds to global anchor (rank × local_batch + i)
        # Its positive is at the same global index in all_positives
        global_indices = torch.arange(
            self.rank * local_batch_size,
            (self.rank + 1) * local_batch_size,
            device=anchor_emb.device
        )

        # Positive is in second half of all_embeddings (all_positives)
        labels = global_indices + global_batch_size

        # Cross-entropy loss
        loss = F.cross_entropy(similarity_matrix, labels)

        # Metrics
        with torch.no_grad():
            predictions = similarity_matrix.argmax(dim=1)
            accuracy = (predictions == labels).float().mean()

            # Positive similarities
            positive_sim = similarity_matrix[
                torch.arange(local_batch_size),
                labels
            ].mean()

            # Negative similarities (excluding positive)
            mask = torch.ones_like(similarity_matrix, dtype=torch.bool)
            mask[torch.arange(local_batch_size), labels] = False
            negative_sim = similarity_matrix[mask].mean()

        metrics = {
            'accuracy': accuracy.item(),
            'positive_similarity': positive_sim.item(),
            'negative_similarity': negative_sim.item(),
            'effective_batch_size': global_batch_size
        }

        return loss, metrics


def launch_distributed_training(world_size, backend='nccl'):
    """
    Launch distributed training across multiple GPUs

    Args:
        world_size: Number of GPUs to use
        backend: 'nccl' for GPU, 'gloo' for CPU
    """
    import torch.multiprocessing as mp

    def train_worker(rank, world_size):
        """
        Training function for each GPU
        """
        # Set device
        torch.cuda.set_device(rank)
        device = torch.device(f'cuda:{rank}')

        # Initialize distributed backend
        dist.init_process_group(
            backend=backend,
            init_method='tcp://localhost:23456',
            world_size=world_size,
            rank=rank
        )

        # Create model
        model = SimCLRTextEmbedding(
            base_model='bert-base-uncased',
            projection_dim=128
        ).to(device)

        # Wrap in distributed trainer
        distributed_trainer = DistributedContrastiveLearning(
            model, world_size, rank
        )

        # Create distributed dataset and loader
        dataset = ContrastiveDataset(...)  # Your dataset
        sampler = torch.utils.data.distributed.DistributedSampler(
            dataset,
            num_replicas=world_size,
            rank=rank
        )

        dataloader = torch.utils.data.DataLoader(
            dataset,
            batch_size=512,  # Per-GPU batch size
            sampler=sampler,
            num_workers=4
        )

        optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)

        # Training loop
        for epoch in range(num_epochs):
            sampler.set_epoch(epoch)  # Important: shuffle differently each epoch

            for batch in dataloader:
                # Encode
                anchor_emb, _ = model(
                    batch['anchor_ids'].to(device),
                    batch['anchor_mask'].to(device)
                )
                positive_emb, _ = model(
                    batch['positive_ids'].to(device),
                    batch['positive_mask'].to(device)
                )

                # Compute distributed loss
                loss, metrics = distributed_trainer.compute_distributed_contrastive_loss(
                    anchor_emb, positive_emb
                )

                # Backward pass
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()

                if rank == 0:  # Only print from master process
                    print(f"Epoch {epoch}, Loss: {loss.item():.4f}, "
                          f"Effective batch: {metrics['effective_batch_size']}")

        # Cleanup
        dist.destroy_process_group()

    # Spawn processes for each GPU
    mp.spawn(
        train_worker,
        args=(world_size,),
        nprocs=world_size,
        join=True
    )


# Launch training on 8 GPUs
# launch_distributed_training(world_size=8)
# Effective batch size: 8 GPUs × 512 per-GPU = 4096
```

### Mixed Precision Training for Larger Batches

Use FP16/BF16 to reduce memory consumption, enabling larger batches.

```python
from torch.cuda.amp import autocast, GradScaler

class MixedPrecisionContrastive:
    """
    Mixed precision training for contrastive learning

    Benefits:
    - 2x memory savings (FP16 vs FP32)
    - 2-3x faster training on modern GPUs (Tensor Cores)
    - Enables larger batch sizes

    Challenges:
    - Numerical stability for contrastive loss (exp operations)
    - Gradient scaling required
    """

    def __init__(self, model, optimizer):
        self.model = model
        self.optimizer = optimizer

        # Gradient scaler for FP16 training
        self.scaler = GradScaler()

    def train_step(self, batch, device):
        """
        Training step with mixed precision
        """
        # Move batch to device
        anchor_ids = batch['anchor_ids'].to(device)
        anchor_mask = batch['anchor_mask'].to(device)
        positive_ids = batch['positive_ids'].to(device)
        positive_mask = batch['positive_mask'].to(device)

        # Forward pass in FP16
        with autocast():
            # Encode
            anchor_emb, _ = self.model(anchor_ids, anchor_mask)
            positive_emb, _ = self.model(positive_ids, positive_mask)

            # Compute loss
            # Note: loss computation must be numerically stable in FP16
            loss, metrics = self.model.compute_loss(anchor_emb, positive_emb)

        # Backward pass with gradient scaling
        self.optimizer.zero_grad()
        self.scaler.scale(loss).backward()

        # Unscale gradients and clip
        self.scaler.unscale_(self.optimizer)
        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)

        # Optimizer step with scaler
        self.scaler.step(self.optimizer)
        self.scaler.update()

        return loss.item(), metrics


# Numerical stability considerations for FP16 contrastive learning
class StableInfoNCELoss:
    """
    Numerically stable InfoNCE loss for mixed precision training
    """

    def __init__(self, temperature=0.07):
        self.temperature = temperature

    def compute_loss(self, anchor_emb, positive_emb, all_emb):
        """
        Compute InfoNCE with numerical stability tricks
        """
        # Normalize in FP32 for stability
        anchor_norm = F.normalize(anchor_emb.float(), dim=1)
        positive_norm = F.normalize(positive_emb.float(), dim=1)
        all_norm = F.normalize(all_emb.float(), dim=1)

        # Similarities in FP32
        similarity_matrix = torch.matmul(anchor_norm, all_norm.T) / self.temperature

        # Log-sum-exp trick for numerical stability
        # Instead of: -log(exp(x) / sum(exp(x_i)))
        # Use: -x + log(sum(exp(x_i)))
        # Which equals: log(sum(exp(x_i))) - x
        # This is: torch.logsumexp(logits) - logits[positive]

        labels = torch.arange(len(anchor_emb), device=anchor_emb.device)

        # Log-sum-exp of all logits (denominator)
        log_denominator = torch.logsumexp(similarity_matrix, dim=1)

        # Positive logits (numerator)
        positive_logits = similarity_matrix[torch.arange(len(anchor_emb)), labels]

        # Loss: -log(exp(pos) / sum(exp(all))) = log(sum) - pos
        loss = (log_denominator - positive_logits).mean()

        return loss
```

### Batch Composition Strategies

Not all examples are equally valuable in a batch. Smart batch composition improves training efficiency.

```python
class SmartBatchSampler:
    """
    Intelligent batch composition for contrastive learning

    Strategies:
    1. Diversity maximization: ensure batches contain diverse examples
    2. Difficulty balancing: mix easy and hard examples
    3. Domain balancing: mix examples from different domains/topics
    """

    def __init__(self, dataset, batch_size=512):
        self.dataset = dataset
        self.batch_size = batch_size

        # Pre-compute embeddings or clusters for sampling
        self.example_embeddings = None
        self.example_clusters = None

    def diversity_maximizing_batch(self, embeddings, batch_size):
        """
        Sample batch that maximizes embedding diversity

        Ensures batch contains diverse examples → better negatives

        Args:
            embeddings: (num_examples, dim) pre-computed embeddings
            batch_size: Desired batch size

        Returns:
            indices: Selected example indices
        """
        import numpy as np
        from sklearn.metrics.pairwise import cosine_similarity

        num_examples = len(embeddings)

        # Greedy diversity sampling
        # 1. Start with random example
        selected_indices = [np.random.randint(0, num_examples)]
        remaining_indices = list(range(num_examples))
        remaining_indices.remove(selected_indices[0])

        # 2. Iteratively add example most dissimilar to selected set
        for _ in range(batch_size - 1):
            if not remaining_indices:
                break

            # Compute minimum similarity to any selected example
            min_similarities = []

            for idx in remaining_indices:
                # Similarities to all selected examples
                sims = cosine_similarity(
                    embeddings[idx:idx+1],
                    embeddings[selected_indices]
                )[0]

                # Minimum similarity (most dissimilar from closest)
                min_sim = sims.min()
                min_similarities.append(min_sim)

            # Choose example with minimum similarity (most diverse)
            most_diverse_idx = np.argmin(min_similarities)
            selected_idx = remaining_indices[most_diverse_idx]

            selected_indices.append(selected_idx)
            remaining_indices.remove(selected_idx)

        return selected_indices

    def difficulty_balanced_batch(self, difficulty_scores, batch_size,
                                  easy_ratio=0.3, medium_ratio=0.5):
        """
        Sample batch with balanced difficulty

        Mix of easy, medium, and hard examples ensures stable training

        Args:
            difficulty_scores: (num_examples,) difficulty score per example
            batch_size: Desired batch size
            easy_ratio: Proportion of easy examples
            medium_ratio: Proportion of medium examples

        Returns:
            indices: Selected example indices
        """
        import numpy as np

        # Sort by difficulty
        sorted_indices = np.argsort(difficulty_scores)

        # Split into easy, medium, hard
        num_examples = len(difficulty_scores)
        split1 = num_examples // 3
        split2 = 2 * num_examples // 3

        easy_indices = sorted_indices[:split1]
        medium_indices = sorted_indices[split1:split2]
        hard_indices = sorted_indices[split2:]

        # Sample proportionally
        num_easy = int(batch_size * easy_ratio)
        num_medium = int(batch_size * medium_ratio)
        num_hard = batch_size - num_easy - num_medium

        selected = []
        selected.extend(np.random.choice(easy_indices, num_easy, replace=False))
        selected.extend(np.random.choice(medium_indices, num_medium, replace=False))
        selected.extend(np.random.choice(hard_indices, num_hard, replace=False))

        # Shuffle
        np.random.shuffle(selected)

        return selected

    def domain_balanced_batch(self, domain_labels, batch_size):
        """
        Sample batch with balanced domain representation

        Ensures model learns from all domains, not just dominant ones

        Args:
            domain_labels: (num_examples,) domain ID for each example
            batch_size: Desired batch size

        Returns:
            indices: Selected example indices
        """
        import numpy as np
        from collections import Counter

        # Count examples per domain
        domain_counts = Counter(domain_labels)
        num_domains = len(domain_counts)

        # Samples per domain (balanced)
        samples_per_domain = batch_size // num_domains

        # Sample from each domain
        selected = []

        for domain_id in domain_counts:
            # Indices for this domain
            domain_indices = np.where(domain_labels == domain_id)[0]

            # Sample
            num_samples = min(samples_per_domain, len(domain_indices))
            sampled = np.random.choice(domain_indices, num_samples, replace=False)

            selected.extend(sampled)

        # Fill remaining slots randomly
        if len(selected) < batch_size:
            remaining = batch_size - len(selected)
            all_indices = set(range(len(domain_labels)))
            available = list(all_indices - set(selected))

            additional = np.random.choice(available, remaining, replace=False)
            selected.extend(additional)

        # Shuffle
        np.random.shuffle(selected)

        return selected[:batch_size]
```

## Distributed Contrastive Learning Architectures

Scaling contrastive learning to trillion rows requires distributed architectures that span hundreds of GPUs across multiple machines.

### Multi-Node Distributed Training

```python
import torch.distributed as dist

class MultiNodeContrastiveLearning:
    """
    Multi-node distributed contrastive learning

    Architecture:
    - N nodes (machines)
    - M GPUs per node
    - Total: N × M GPUs

    Communication:
    - All-reduce for gradients (handled by DDP)
    - All-gather for embeddings (manual)
    """

    def __init__(self, model, rank, world_size, local_rank,
                 master_addr='localhost', master_port='12355'):
        """
        Args:
            model: Embedding model
            rank: Global rank (0 to world_size-1)
            world_size: Total number of GPUs across all nodes
            local_rank: GPU rank within this node
            master_addr: IP of master node
            master_port: Port for communication
        """
        self.model = model
        self.rank = rank
        self.world_size = world_size
        self.local_rank = local_rank

        # Initialize process group
        self._init_distributed(master_addr, master_port)

        # Wrap model
        torch.cuda.set_device(local_rank)
        self.model = self.model.to(local_rank)
        self.model = torch.nn.parallel.DistributedDataParallel(
            self.model,
            device_ids=[local_rank],
            output_device=local_rank
        )

    def _init_distributed(self, master_addr, master_port):
        """Initialize distributed backend"""
        import os

        os.environ['MASTER_ADDR'] = master_addr
        os.environ['MASTER_PORT'] = master_port

        dist.init_process_group(
            backend='nccl',  # NCCL for multi-node GPU communication
            rank=self.rank,
            world_size=self.world_size
        )

        print(f"Initialized rank {self.rank}/{self.world_size}")

    def gather_embeddings_multi_node(self, local_embeddings):
        """
        Gather embeddings from all GPUs across all nodes

        More complex than single-node: network communication between nodes
        """
        # Create buffer for gathered embeddings
        gathered_embeddings = [
            torch.zeros_like(local_embeddings)
            for _ in range(self.world_size)
        ]

        # All-gather across all GPUs
        dist.all_gather(gathered_embeddings, local_embeddings)

        # Concatenate
        all_embeddings = torch.cat(gathered_embeddings, dim=0)

        return all_embeddings

    def train_step_multi_node(self, batch):
        """
        Training step with multi-node distribution
        """
        # Forward pass
        anchor_emb = self.model(
            batch['anchor_ids'].cuda(self.local_rank),
            batch['anchor_mask'].cuda(self.local_rank)
        )
        positive_emb = self.model(
            batch['positive_ids'].cuda(self.local_rank),
            batch['positive_mask'].cuda(self.local_rank)
        )

        # Gather embeddings from all GPUs (across nodes)
        all_anchors = self.gather_embeddings_multi_node(anchor_emb)
        all_positives = self.gather_embeddings_multi_node(positive_emb)

        # Compute loss using all embeddings
        loss = self.compute_contrastive_loss(
            anchor_emb,  # Local anchors
            all_anchors,  # Global negatives
            all_positives,  # Global positives
            rank=self.rank
        )

        return loss


def launch_multi_node_training(num_nodes, gpus_per_node):
    """
    Launch multi-node distributed training

    Typically launched via SLURM or similar cluster manager:

    ```bash
    # SLURM example
    srun --nodes=16 --gpus-per-node=8 --ntasks-per-node=8 \
         python train_multinode.py
    ```

    Args:
        num_nodes: Number of machines
        gpus_per_node: GPUs per machine
    """
    import os

    # Get environment variables set by SLURM/cluster manager
    rank = int(os.environ['SLURM_PROCID'])  # Global rank
    local_rank = int(os.environ['SLURM_LOCALID'])  # Local rank within node
    world_size = int(os.environ['SLURM_NTASKS'])  # Total processes

    master_addr = os.environ['SLURM_LAUNCH_NODE_IPADDR']

    # Create model
    model = SimCLRTextEmbedding(...)

    # Initialize multi-node trainer
    trainer = MultiNodeContrastiveLearning(
        model,
        rank=rank,
        world_size=world_size,
        local_rank=local_rank,
        master_addr=master_addr
    )

    # Training loop
    # ...
```

### Gradient Checkpointing for Memory Efficiency

```python
from torch.utils.checkpoint import checkpoint

class MemoryEfficientContrastive(nn.Module):
    """
    Use gradient checkpointing to reduce memory consumption

    Trades computation for memory:
    - Forward pass: only store inputs and outputs of checkpointed layers
    - Backward pass: recompute forward pass for checkpointed layers

    Enables:
    - Larger models
    - Larger batch sizes
    - Deeper architectures

    Cost:
    - ~20-30% slower (recomputation overhead)
    """

    def __init__(self, base_model, projection_dim=128):
        super().__init__()

        self.encoder = base_model

        # Enable gradient checkpointing for encoder
        if hasattr(self.encoder, 'gradient_checkpointing_enable'):
            self.encoder.gradient_checkpointing_enable()

        self.projection = nn.Sequential(
            nn.Linear(768, 512),
            nn.ReLU(),
            nn.Linear(512, projection_dim)
        )

    def forward(self, input_ids, attention_mask):
        """
        Forward with gradient checkpointing
        """
        # Encoder with checkpointing (handled internally if enabled)
        outputs = self.encoder(input_ids, attention_mask)
        embeddings = outputs.last_hidden_state[:, 0]

        # Projection with manual checkpointing
        projected = checkpoint(self.projection, embeddings)

        return projected
```

### Communication-Efficient Distributed Training

```python
class CommunicationEfficientDistributed:
    """
    Reduce communication overhead in distributed training

    Techniques:
    1. Gradient compression
    2. Local SGD (sync less frequently)
    3. Overlap communication with computation
    """

    def __init__(self, model, world_size, rank):
        self.model = model
        self.world_size = world_size
        self.rank = rank

        # Local update counter
        self.local_updates = 0
        self.sync_frequency = 10  # Sync every N steps

    def local_sgd_step(self, loss, optimizer):
        """
        Local SGD: update locally, sync periodically

        Reduces communication by 10x (if sync_frequency=10)
        """
        # Local backward pass
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        self.local_updates += 1

        # Sync periodically
        if self.local_updates % self.sync_frequency == 0:
            self.sync_models()

    def sync_models(self):
        """
        Average model parameters across all GPUs
        """
        for param in self.model.parameters():
            # All-reduce: sum then divide by world_size
            dist.all_reduce(param.data, op=dist.ReduceOp.SUM)
            param.data /= self.world_size

    def compressed_all_gather(self, embeddings, compression_ratio=0.1):
        """
        Compress embeddings before gathering to reduce bandwidth

        Args:
            embeddings: (batch_size, dim)
            compression_ratio: Fraction of dimensions to keep

        Returns:
            all_embeddings: Gathered from all GPUs (compressed)
        """
        # Top-k compression: keep only largest magnitude values
        k = int(embeddings.shape[1] * compression_ratio)

        # Find top-k values and indices
        values, indices = embeddings.topk(k, dim=1)

        # Gather compressed representations
        all_values = [torch.zeros_like(values) for _ in range(self.world_size)]
        all_indices = [torch.zeros_like(indices) for _ in range(self.world_size)]

        dist.all_gather(all_values, values)
        dist.all_gather(all_indices, indices)

        # Reconstruct (sparse)
        batch_size = embeddings.shape[0]
        embedding_dim = embeddings.shape[1]

        all_embeddings_sparse = []

        for vals, inds in zip(all_values, all_indices):
            # Reconstruct dense tensor (mostly zeros)
            dense = torch.zeros(batch_size, embedding_dim, device=embeddings.device)
            dense.scatter_(1, inds, vals)
            all_embeddings_sparse.append(dense)

        return torch.cat(all_embeddings_sparse, dim=0)
```

## Key Takeaways

- **Contrastive learning transforms embeddings into a similarity learning problem** where the model learns to distinguish similar items from dissimilar ones, requiring only pairs or triplets instead of expensive labeled data and scaling to billions of unlabeled examples

- **InfoNCE loss provides the foundation** for modern contrastive learning by treating it as a classification task: identify the positive from K negatives, with performance improving as K increases (larger batches → better embeddings)

- **Temperature parameter critically affects training dynamics**: low temperature (0.01-0.05) focuses on hardest negatives for faster convergence, medium (0.07-0.1) provides stable training, and high (0.2-0.5) handles noisy negatives better—choice depends on batch size and data quality

- **SimCLR and MoCo offer complementary approaches**: SimCLR achieves superior performance with massive batches (4096+) but requires enormous GPU memory, while MoCo uses a momentum encoder and queue to achieve similar quality with small batches (256), making it more accessible for resource-constrained environments

- **Hard negative mining dramatically improves embedding quality** by forcing the model to learn fine-grained distinctions: in-batch mining (zero overhead), queue-based mining (larger candidate pool), and offline mining with ANN (global hard negatives) each offer different trade-offs between quality and computational cost

- **Enterprise adaptations extend contrastive learning beyond research** through multi-modal support (text + image), domain-specific objectives (hierarchical, temporal), and production-scale data loading with distributed sampling and efficient caching

- **Trillion-row training requires sophisticated batch optimization**: gradient accumulation simulates large batches but loses contrastive benefits, distributed training enables truly large batches (8 GPUs × 512 = 4096 effective), mixed precision provides 2x memory savings, and smart batch composition (diversity maximization, difficulty balancing) improves training efficiency

- **Multi-node distributed architectures scale to hundreds of GPUs** across multiple machines using NCCL backend for efficient inter-node communication, all-gather operations for global negative mining, and communication optimization techniques (local SGD, gradient compression) that reduce bandwidth by 10x

- **Debiased hard negative mining prevents catastrophic failure** from false negatives through cross-encoder filtering, clustering-based filtering, and confidence thresholding that ensures true negatives while avoiding semantically similar pairs mislabeled as negatives

- **Production deployment requires memory optimization**: gradient checkpointing trades 20-30% compute for 50% memory savings, enabling larger models and batch sizes; overlap communication with computation to hide latency; and use mixed precision (FP16/BF16) universally on modern GPUs for 2-3x speedup

## Looking Ahead

Chapter 6 explores Siamese Networks, a specialized architecture that excels at learning similarity metrics for one-shot and few-shot learning scenarios—critical for applications with limited labeled data like fraud detection and rare event identification.

## Further Reading

- Chen, T., et al. (2020). "A Simple Framework for Contrastive Learning of Visual Representations." *ICML 2020* (SimCLR)
- He, K., et al. (2020). "Momentum Contrast for Unsupervised Visual Representation Learning." *CVPR 2020* (MoCo)
- Oord, A., et al. (2018). "Representation Learning with Contrastive Predictive Coding." *arXiv:1807.03748* (CPC, InfoNCE)
- Wang, T., & Isola, P. (2020). "Understanding Contrastive Representation Learning through Alignment and Uniformity on the Hypersphere." *ICML 2020*
- Gao, T., et al. (2021). "SimCSE: Simple Contrastive Learning of Sentence Embeddings." *EMNLP 2021*
- Robinson, J., et al. (2021). "Contrastive Learning with Hard Negative Samples." *ICLR 2021*
- Chuang, C., et al. (2020). "Debiased Contrastive Learning." *NeurIPS 2020*
- Chen, X., & He, K. (2021). "Exploring Simple Siamese Representation Learning." *CVPR 2021*
- Zbontar, J., et al. (2021). "Barlow Twins: Self-Supervised Learning via Redundancy Reduction." *ICML 2021*
- Grill, J., et al. (2020). "Bootstrap Your Own Latent: A New Approach to Self-Supervised Learning." *NeurIPS 2020* (BYOL)
- Khosla, P., et al. (2020). "Supervised Contrastive Learning." *NeurIPS 2020*
- Schroff, F., et al. (2015). "FaceNet: A Unified Embedding for Face Recognition and Clustering." *CVPR 2015* (Triplet Loss)
