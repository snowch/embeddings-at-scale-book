# Text Chunking for Embeddings {#sec-text-chunking}

:::{.callout-note}
## Chapter Overview
Document RAG systems don't embed individual words—they embed chunks of text that capture semantic meaning in context. This chapter explores why chunking matters, how different strategies affect retrieval quality, and practical techniques for preparing text documents for embedding systems. You'll learn fixed-size, sentence-based, semantic, and hierarchical chunking approaches, with code examples and guidance for choosing the right strategy for your use case.
:::

A common misconception about embedding-based retrieval systems is that they embed every word individually. In reality, **RAG systems embed chunks of text**—larger semantic units that preserve context and meaning. Understanding chunking is essential because it directly impacts retrieval quality: poor chunking leads to poor results, regardless of how sophisticated your embedding model or vector database might be.

## Why Chunking Matters

When building a RAG system, you face a fundamental question: what unit of text should receive its own embedding? The answer is almost never "individual words" and rarely "entire documents."

### The Problem with Word-Level Embeddings

Word embeddings (like Word2Vec or GloVe) represent individual words as vectors. While valuable for understanding vocabulary relationships, they're insufficient for retrieval:

```python
# Word embeddings: one vector per word
word_embeddings = {
    'bank': [0.2, 0.8, 0.1, ...],  # But which meaning? Financial? River?
    'river': [0.1, 0.3, 0.9, ...],
    'money': [0.8, 0.2, 0.1, ...],
}

# The word 'bank' has the same embedding regardless of context
# "I went to the bank to deposit money" vs "I sat on the river bank"
# Same vector, completely different meanings!
```

Modern embedding models solve this by processing entire passages, producing a single vector that captures the **contextual meaning** of the whole chunk:

```python
# Chunk embeddings: one vector per passage
chunk_embedding = encoder.encode(
    "I went to the bank to deposit my paycheck into savings."
)
# This single 1024-dim vector captures: financial institution,
# personal finance, banking transaction, savings context
```

### The Problem with Document-Level Embeddings

At the other extreme, embedding entire documents creates different problems:

1. **Diluted semantics**: A 50-page document covers many topics. Its embedding becomes a vague average, matching poorly with specific queries.

2. **Context window limits**: LLMs have finite context windows (4K-128K tokens). Retrieved chunks must fit within these limits alongside the query and system prompt.

3. **Retrieval granularity**: Users ask specific questions. Returning entire documents forces them to hunt for the relevant paragraph.

```python
# Document-level embedding: too coarse
doc_embedding = encoder.encode(entire_50_page_document)
# This vector represents the "average" meaning of 50 pages
# Query: "What is the return policy for electronics?"
# Result: Entire product manual returned, user must find the relevant section
```

### The Chunking Sweet Spot

Chunking finds the middle ground: units large enough to preserve context but small enough for precise retrieval and LLM consumption.

| Embedding Level | Typical Size | Context Preservation | Retrieval Precision | LLM Friendly |
|----------------|--------------|---------------------|--------------------| -------------|
| Word | 1 token | None | N/A | N/A |
| Sentence | 10-30 tokens | Low | High | Yes |
| Paragraph | 50-200 tokens | Medium | Medium | Yes |
| Chunk | 100-500 tokens | High | High | Yes |
| Document | 1000+ tokens | Complete but diluted | Low | Often too large |

: Embedding granularity trade-offs {.striped}

## Chunk Embeddings vs Word Embeddings

Let's clarify the distinction that confuses many practitioners:

### Word Embeddings (Historical Context)

Word embeddings like Word2Vec (2013) revolutionized NLP by learning dense vector representations for individual words:

```python
# Word2Vec: learns one vector per vocabulary word
# Training: predict surrounding words from center word (or vice versa)

from gensim.models import Word2Vec

sentences = [["the", "cat", "sat", "on", "mat"], ...]
model = Word2Vec(sentences, vector_size=300)

# Each word gets exactly one 300-dim vector
cat_vector = model.wv['cat']  # Always the same vector for 'cat'
```

**Key limitation**: No context sensitivity. "Bank" has one vector whether discussing finance or rivers.

### Chunk Embeddings (Modern RAG)

Modern embedding models (Sentence-BERT, OpenAI embeddings, Cohere, etc.) process entire text passages:

```python
# Modern embeddings: one vector per input passage
from sentence_transformers import SentenceTransformer

encoder = SentenceTransformer('all-MiniLM-L6-v2')

# Same word, different contexts → different chunk embeddings
chunk1 = "The bank approved my mortgage application yesterday."
chunk2 = "We had a picnic on the grassy bank beside the river."

emb1 = encoder.encode(chunk1)  # Financial context captured
emb2 = encoder.encode(chunk2)  # Nature context captured

# These embeddings are very different despite both containing 'bank'
```

### The Transformation Process

Here's what happens when you embed a chunk:

```
Input: "The quarterly financial report shows revenue increased
        by 15% compared to last year, driven primarily by
        strong performance in the cloud services division."

                    ↓ Embedding Model (e.g., all-MiniLM-L6-v2)

Output: [0.023, -0.156, 0.089, ..., 0.042]  # 384 dimensions

This single vector encodes:
- Topic: Financial/business reporting
- Sentiment: Positive (increased, strong)
- Entities: Cloud services, quarterly reports
- Relationships: Revenue growth, divisional performance
- Context: Corporate earnings, year-over-year comparison
```

The embedding model—typically a transformer—processes the entire chunk through attention layers that let every word influence every other word's representation. The final vector is a learned compression of this contextual understanding.

## Chunking Strategies

Different chunking strategies suit different use cases. Here's a comprehensive overview:

### Fixed-Size Chunking

The simplest approach: split text into chunks of N characters or tokens.

```python
{{< include /code_examples/ch17_text_chunking/fixed_size_chunking.py >}}
```

**Pros:**

- Simple to implement and understand
- Predictable chunk sizes for capacity planning
- Works for any text without structural assumptions

**Cons:**

- Breaks mid-sentence, mid-paragraph, even mid-word
- No respect for semantic boundaries
- May split critical information across chunks

**When to use:** Homogeneous text without clear structure, or as a baseline to compare against smarter strategies.

### Sentence-Based Chunking

Split on sentence boundaries, grouping sentences to reach target size.

```python
{{< include /code_examples/ch17_text_chunking/sentence_chunking.py >}}
```

**Pros:**

- Preserves complete thoughts
- Natural linguistic boundaries
- Better semantic coherence than fixed-size

**Cons:**

- Sentence detection can fail on abbreviations, URLs, code
- Variable chunk sizes
- May still split related sentences

**When to use:** Well-formed prose like articles, documentation, or reports.

### Paragraph-Based Chunking

Use paragraph breaks as natural semantic boundaries.

```python
{{< include /code_examples/ch17_text_chunking/paragraph_chunking.py >}}
```

**Pros:**

- Authors create paragraphs around coherent ideas
- Strongest natural semantic boundaries
- Often ideal chunk size naturally

**Cons:**

- Paragraph length varies wildly
- Some documents lack clear paragraphs
- Very short paragraphs may lack context

**When to use:** Well-structured documents with clear paragraph formatting.

### Semantic Chunking

Split based on topic shifts detected by embedding similarity.

```python
{{< include /code_examples/ch17_text_chunking/semantic_chunking.py >}}
```

**Pros:**

- Chunks align with actual topic boundaries
- Captures semantic coherence directly
- Adapts to content structure

**Cons:**

- Computationally expensive (requires embedding each sentence)
- Threshold tuning required
- May create very uneven chunk sizes

**When to use:** Documents with multiple topics, transcripts, or content without clear structural markers.

### Recursive/Hierarchical Chunking

Try multiple splitters in order of preference, falling back as needed.

```python
{{< include /code_examples/ch17_text_chunking/recursive_chunking.py >}}
```

**Pros:**

- Respects document hierarchy (sections → paragraphs → sentences)
- Graceful degradation for messy documents
- Flexible target sizes

**Cons:**

- More complex implementation
- Order of separators matters
- May still produce uneven chunks

**When to use:** Documents with mixed structure, or when you need consistent chunk sizes with best-effort boundary respect.

### Sliding Window with Overlap

Create overlapping chunks to preserve context at boundaries.

```python
{{< include /code_examples/ch17_text_chunking/sliding_window_chunking.py >}}
```

**Pros:**

- Information at chunk boundaries appears in multiple chunks
- Reduces risk of splitting critical context
- Better recall for boundary-spanning queries

**Cons:**

- Increases storage requirements (overlap percentage)
- May retrieve duplicate information
- Requires deduplication in results

**When to use:** When retrieval quality matters more than storage efficiency, especially for dense technical content.

## Document-Type Specific Strategies

Different document types require different chunking approaches:

### PDF Documents

PDFs present unique challenges: headers/footers on every page, multi-column layouts, embedded tables, and inconsistent text extraction.

```python
{{< include /code_examples/ch17_text_chunking/pdf_chunking.py >}}
```

### HTML Documents

HTML carries structural information that aids chunking:

```python
{{< include /code_examples/ch17_text_chunking/html_chunking.py >}}
```

### Markdown Documents

Markdown headers provide explicit hierarchy:

```python
{{< include /code_examples/ch17_text_chunking/markdown_chunking.py >}}
```

### Source Code

Code requires special handling to preserve syntactic units:

```python
{{< include /code_examples/ch17_text_chunking/code_chunking.py >}}
```

## Chunk Size Optimization

Choosing optimal chunk size involves balancing competing concerns:

### The Trade-off Triangle

```
                    CONTEXT
                      /\
                     /  \
                    /    \
                   /      \
                  /        \
                 /__________\
           PRECISION      EFFICIENCY

Larger chunks → More context, less precision, fewer chunks
Smaller chunks → Less context, more precision, more chunks
```

### Empirical Sizing Guidelines

Based on production experience across different use cases:

| Use Case | Recommended Size | Overlap | Rationale |
|----------|-----------------|---------|-----------|
| Q&A over documentation | 256-512 tokens | 10-20% | Balance context with precision |
| Legal document search | 512-1024 tokens | 20-30% | Preserve legal context and cross-references |
| Customer support | 128-256 tokens | 10% | Short, focused answers needed |
| Academic papers | 512-768 tokens | 15% | Preserve argument flow |
| Code documentation | 256-512 tokens | 0% | Function/class boundaries are natural |
| Chat/transcript search | 128-256 tokens | 20% | Conversational turns are short |

: Chunk size recommendations by use case {.striped}

### Finding Your Optimal Size

```python
{{< include /code_examples/ch17_text_chunking/chunk_size_evaluation.py >}}
```

## Metadata Preservation

Chunks without context are less useful. Preserve metadata for filtering and context:

```python
{{< include /code_examples/ch17_text_chunking/metadata_preservation.py >}}
```

## Handling Special Content

### Tables

Tables require special handling—row-by-row chunking loses context:

```python
{{< include /code_examples/ch17_text_chunking/table_chunking.py >}}
```

### Lists

Numbered and bulleted lists should stay together when possible:

```python
{{< include /code_examples/ch17_text_chunking/list_chunking.py >}}
```

### Code Blocks

Code embedded in documentation needs preservation:

```python
{{< include /code_examples/ch17_text_chunking/code_block_handling.py >}}
```

## Production Chunking Pipeline

Putting it all together into a production-ready pipeline:

```python
{{< include /code_examples/ch17_text_chunking/production_pipeline.py >}}
```

## Evaluating Chunk Quality

How do you know if your chunking strategy is working?

### Retrieval Quality Metrics

```python
{{< include /code_examples/ch17_text_chunking/chunk_evaluation.py >}}
```

### Common Failure Patterns

| Symptom | Likely Cause | Solution |
|---------|--------------|----------|
| Relevant info not retrieved | Chunks too large, query buried | Reduce chunk size |
| Retrieved chunks lack context | Chunks too small | Increase chunk size or overlap |
| Duplicate information in results | Too much overlap | Reduce overlap, add deduplication |
| Poor performance on tables | Tables split incorrectly | Use table-aware chunking |
| Code examples broken | Split mid-function | Use AST-aware code chunking |
| Headers orphaned from content | Structural chunking too aggressive | Keep headers with following content |

: Chunking troubleshooting guide {.striped}

## Key Takeaways

- **RAG systems embed chunks, not words**: Modern embedding models process entire passages to create single vectors that capture contextual meaning—this is fundamentally different from word embeddings like Word2Vec

- **Chunking directly impacts retrieval quality**: Poor chunking strategies lead to poor results regardless of embedding model quality; it's often the highest-leverage optimization available

- **Match strategy to content type**: Fixed-size for unstructured text, sentence-based for prose, paragraph-based for well-formatted documents, semantic chunking for topic-diverse content, and recursive chunking for mixed-structure documents

- **Overlap prevents boundary information loss**: 10-20% overlap ensures information at chunk boundaries appears in multiple chunks, improving recall at modest storage cost

- **Preserve metadata for filtering and context**: Source document, section headers, page numbers, and timestamps enable hybrid search and help users understand retrieved content

- **Evaluate empirically on your data**: Optimal chunk size depends on your specific content and queries; use evaluation frameworks to compare strategies systematically

## Looking Ahead

This chapter covered text chunking—preparing documents for embedding. The next chapter explores the parallel challenge for visual data: how to prepare images for embedding systems, including preprocessing, region extraction, and handling large-scale imagery like satellite photos and medical scans.

## Further Reading

- Langchain Documentation: "Text Splitters" - Comprehensive guide to chunking implementations
- Liu, N., et al. (2023). "Lost in the Middle: How Language Models Use Long Contexts." *arXiv:2307.03172*
- Shi, W., et al. (2023). "REPLUG: Retrieval-Augmented Black-Box Language Models." *arXiv:2301.12652*
- Gao, L., et al. (2023). "Precise Zero-Shot Dense Retrieval without Relevance Labels." *arXiv:2212.10496*
- Robertson, S., and Zaragoza, H. (2009). "The Probabilistic Relevance Framework: BM25 and Beyond." *Foundations and Trends in Information Retrieval*
