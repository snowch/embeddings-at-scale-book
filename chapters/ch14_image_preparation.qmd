# Image Preparation for Embeddings {#sec-image-preparation}

:::{.callout-note}
## Chapter Overview
Image embedding systems face different challenges than text: preprocessing requirements, internal patch-based processing, handling large images, and extracting regions of interest. This chapter covers how modern vision models create embeddings, practical preprocessing strategies, approaches for large-scale imagery (satellite, medical), and techniques for multi-object scenes. You'll learn to prepare images for optimal embedding quality across diverse visual domains.
:::

The previous chapter explored how text documents are chunked into semantic units for embedding. Images present a parallel but distinct challenge: while text chunking is primarily a user decision, image "chunking" often happens inside the model itself. However, image preparation decisions—preprocessing, cropping, tiling, and region extraction—significantly impact embedding quality. Understanding these choices is essential for building effective visual search and multi-modal systems.

## How Image Embedding Models Work

Before diving into preparation strategies, let's understand what happens inside modern image embedding models.

### From Pixels to Vectors

Image embedding models transform raw pixels into dense vector representations:

```
Input: RGB Image (224 × 224 × 3 = 150,528 values)
                    ↓
        Image Embedding Model
                    ↓
Output: Embedding Vector (768 or 1024 dimensions)

Compression ratio: ~150x to ~200x
```

Unlike text where chunking is explicit, image models handle spatial "chunking" internally through their architecture.

### CNN-Based Embeddings (ResNet, EfficientNet)

Convolutional Neural Networks process images through hierarchical feature extraction:

```python
{{< include /code_examples/ch12c_image_preparation/cnn_embeddings.py >}}
```

**How CNNs create embeddings:**

1. **Convolutional layers**: Detect local features (edges, textures, shapes)
2. **Pooling layers**: Reduce spatial dimensions while preserving important features
3. **Deeper layers**: Combine local features into semantic concepts
4. **Global pooling**: Collapse spatial dimensions into a single vector

```
224×224×3 → [Conv] → 112×112×64 → [Conv] → 56×56×128 → ... → 7×7×2048 → [Pool] → 2048-dim vector
   Input      Early features        Mid features           Late features    Embedding
            (edges, colors)     (textures, parts)      (objects, scenes)
```

### Transformer-Based Embeddings (ViT, CLIP)

Vision Transformers take a fundamentally different approach—they explicitly split images into patches:

```python
{{< include /code_examples/ch12c_image_preparation/vit_embeddings.py >}}
```

**How ViT creates embeddings:**

1. **Patch extraction**: Split image into fixed-size patches (typically 16×16 or 14×14 pixels)
2. **Linear projection**: Each patch becomes a token embedding
3. **Position encoding**: Add spatial position information
4. **Transformer layers**: Self-attention lets patches interact
5. **CLS token**: Special token aggregates information into final embedding

```
224×224 image → 196 patches (14×14 grid of 16×16 patches)
                    ↓
Each patch → 768-dim token (linear projection)
                    ↓
[CLS] + 196 patch tokens + position embeddings
                    ↓
Transformer layers (self-attention)
                    ↓
[CLS] token output → 768-dim embedding
```

### The Key Insight: Internal vs External Chunking

| Aspect | Text Embeddings | Image Embeddings |
|--------|-----------------|------------------|
| **User chunking** | Required (documents → chunks) | Optional (whole images often work) |
| **Model chunking** | Tokenization (subwords) | Patches (ViT) or receptive fields (CNN) |
| **Semantic units** | Sentences, paragraphs | Objects, regions, scenes |
| **Boundary decisions** | Made during preprocessing | Made by model architecture |

: Text vs image chunking comparison {.striped}

For images, the model handles spatial decomposition. Your preparation decisions focus on: input quality, scale, cropping, and whether to embed whole images or extracted regions.

## Preprocessing for Optimal Embeddings

Image preprocessing significantly impacts embedding quality. Each model expects specific input formats.

### Standard Preprocessing Pipeline

```python
{{< include /code_examples/ch12c_image_preparation/preprocessing_pipeline.py >}}
```

### Resolution and Aspect Ratio

Most models expect fixed input sizes (224×224, 384×384, etc.). How you achieve this matters:

```python
{{< include /code_examples/ch12c_image_preparation/resolution_handling.py >}}
```

| Strategy | Pros | Cons | Best For |
|----------|------|------|----------|
| **Center crop** | Preserves resolution, fast | Loses edge content | Centered subjects |
| **Resize** | Keeps all content | Distorts aspect ratio | Square-ish images |
| **Pad** | Preserves aspect ratio | Adds uninformative pixels | Varied aspect ratios |
| **Multi-crop** | Comprehensive coverage | Multiple embeddings per image | High-value images |

: Resize strategy comparison {.striped}

### Color and Normalization

```python
{{< include /code_examples/ch12c_image_preparation/color_normalization.py >}}
```

### Quality Assessment

Not all images are worth embedding. Filter low-quality inputs:

```python
{{< include /code_examples/ch12c_image_preparation/quality_assessment.py >}}
```

## Handling Large Images

Standard embedding models expect ~224×224 inputs. Large images (satellite imagery, medical scans, gigapixel pathology) require special handling.

### Tiling Strategies

Split large images into overlapping tiles, embed each, then aggregate:

```python
{{< include /code_examples/ch12c_image_preparation/tiling_strategy.py >}}
```

### Multi-Resolution Pyramids

Create embeddings at multiple scales for scale-invariant retrieval:

```python
{{< include /code_examples/ch12c_image_preparation/multi_resolution.py >}}
```

### Domain-Specific Large Image Handling

#### Satellite and Aerial Imagery

```python
{{< include /code_examples/ch12c_image_preparation/satellite_imagery.py >}}
```

#### Medical Imaging (Pathology Slides)

```python
{{< include /code_examples/ch12c_image_preparation/pathology_slides.py >}}
```

#### Document Images

```python
{{< include /code_examples/ch12c_image_preparation/document_images.py >}}
```

## Region-of-Interest Extraction

Sometimes you want embeddings for specific regions rather than whole images.

### Object Detection + Cropping

Detect objects first, then embed each separately:

```python
{{< include /code_examples/ch12c_image_preparation/object_detection_embedding.py >}}
```

### Segmentation-Based Regions

Use semantic or instance segmentation for precise region extraction:

```python
{{< include /code_examples/ch12c_image_preparation/segmentation_embedding.py >}}
```

### Attention-Guided Regions

Use model attention to identify important regions:

```python
{{< include /code_examples/ch12c_image_preparation/attention_regions.py >}}
```

## Multi-Object Scene Handling

Scenes with multiple objects present a choice: one embedding for the whole scene, or separate embeddings per object?

### Scene-Level vs Object-Level Embeddings

```python
{{< include /code_examples/ch12c_image_preparation/scene_vs_object.py >}}
```

### Hybrid Approaches

```python
{{< include /code_examples/ch12c_image_preparation/hybrid_embedding.py >}}
```

| Approach | Storage | Query Types Supported | Best For |
|----------|---------|----------------------|----------|
| Scene-only | 1× | "Show me kitchen scenes" | Scene retrieval |
| Objects-only | N× | "Find red chairs" | Object retrieval |
| Hybrid | (N+1)× | Both scene and object queries | Comprehensive search |

: Multi-object embedding strategies {.striped}

## Augmentation for Training Embeddings

When training or fine-tuning embedding models, augmentation creates diverse views of the same image—essential for contrastive learning.

### Standard Augmentation Pipeline

```python
{{< include /code_examples/ch12c_image_preparation/augmentation_pipeline.py >}}
```

### Augmentation for Contrastive Learning

```python
{{< include /code_examples/ch12c_image_preparation/contrastive_augmentation.py >}}
```

### Domain-Specific Augmentation

```python
{{< include /code_examples/ch12c_image_preparation/domain_augmentation.py >}}
```

## Video Frame Extraction

Videos require selecting which frames to embed:

```python
{{< include /code_examples/ch12c_image_preparation/video_frames.py >}}
```

## Production Image Pipeline

Putting it all together into a production system:

```python
{{< include /code_examples/ch12c_image_preparation/production_pipeline.py >}}
```

## Quality and Consistency

### Embedding Consistency Checks

```python
{{< include /code_examples/ch12c_image_preparation/consistency_checks.py >}}
```

### Batch Processing Best Practices

```python
{{< include /code_examples/ch12c_image_preparation/batch_processing.py >}}
```

## Comparing Text and Image Preparation

| Aspect | Text Chunking | Image Preparation |
|--------|---------------|-------------------|
| **Primary decision** | Chunk boundaries and size | Preprocessing and cropping strategy |
| **Model handles** | Tokenization | Patch extraction (ViT) or convolution |
| **Multi-part content** | Split into chunks | Tile large images |
| **Object-level** | Extract sentences/paragraphs | Detect and crop objects |
| **Quality filtering** | Language detection, deduplication | Blur detection, resolution checks |
| **Metadata** | Source, section, page | EXIF, geolocation, timestamp |
| **Augmentation use** | Rarely for retrieval | Essential for training |

: Text vs image preparation comparison {.striped}

## Key Takeaways

- **Image embedding models handle spatial "chunking" internally**: Unlike text where you explicitly chunk documents, CNNs use hierarchical convolutions and ViTs use patch extraction—your preparation focuses on input quality and scale

- **Preprocessing choices significantly impact embedding quality**: Resize strategy (crop vs pad vs stretch), normalization, and color handling should match model expectations and content characteristics

- **Large images require tiling with overlap**: Satellite imagery, medical scans, and gigapixel images should be split into overlapping tiles, embedded separately, with optional aggregation strategies

- **Multi-object scenes offer embedding design choices**: Whole-scene embeddings support scene queries, object-level embeddings support object queries, hybrid approaches support both at increased storage cost

- **Quality filtering prevents garbage embeddings**: Blur detection, resolution checks, and content filtering should precede embedding to avoid polluting your vector database

- **Augmentation is essential for training, optional for inference**: When training embedding models, augmentation creates diverse views for contrastive learning; for inference, consider multi-crop only for high-value retrieval scenarios

## Looking Ahead

With text and image preparation covered, you're ready to build complete retrieval systems. The next chapter explores RAG at scale—combining these preparation techniques with efficient retrieval pipelines, context assembly, and LLM integration for production question-answering systems.

## Further Reading

- Dosovitskiy, A., et al. (2020). "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale." *arXiv:2010.11929* (ViT)
- Radford, A., et al. (2021). "Learning Transferable Visual Models From Natural Language Supervision." *arXiv:2103.00020* (CLIP)
- He, K., et al. (2016). "Deep Residual Learning for Image Recognition." *CVPR* (ResNet)
- Chen, T., et al. (2020). "A Simple Framework for Contrastive Learning of Visual Representations." *ICML* (SimCLR)
- Caron, M., et al. (2021). "Emerging Properties in Self-Supervised Vision Transformers." *ICCV* (DINO)
- Campanella, G., et al. (2019). "Clinical-grade computational pathology using weakly supervised deep learning on whole slide images." *Nature Medicine*
