# Scaling Embedding Training {#sec-scaling-embedding-training}

:::{.callout-note}
## Chapter Overview
Training embedding models on trillion-row datasets requires computational infrastructure that goes far beyond single-GPU training. This chapter explores the architectures and techniques that enable embedding training at unprecedented scale: distributed training across hundreds of GPUs, gradient accumulation and mixed precision for memory efficiency, advanced memory optimization techniques, multi-GPU and multi-node coordination strategies, and cost optimization approaches that make large-scale training economically viable. These techniques transform embedding training from a multi-day single-machine task to a multi-hour distributed operation, enabling rapid iteration and larger, more powerful models.
:::

Embedding model training faces unique scaling challenges. Unlike image classification models that process fixed-size inputs, embedding models often work with variable-length sequences, sparse features, and massive vocabularies. Contrastive learning requires large batch sizes (4K-32K samples) for effective negative sampling. Self-supervised pre-training demands processing billions of documents. These requirements push standard training infrastructure to its limits, requiring specialized techniques for efficient distributed training.

## Distributed Training Architectures

Distributed training parallelizes model training across multiple devices, reducing training time from weeks to hours. However, embedding training has unique requirements that distinguish it from standard distributed training: **large batch sizes for contrastive learning**, **sparse feature handling**, **vocabulary parallelism for large embedding tables**, and **efficient negative sampling across devices**. This section explores architectures that address these challenges.

### Parallelism Strategies for Embedding Training

Modern distributed training employs multiple parallelism strategies simultaneously:

```python
"""
Parallelism Strategies for Embedding Training

1. Data Parallelism: Replicate model, split data across devices
   - Standard approach for most layers
   - Each device processes different batch
   - Gradients synchronized across devices

2. Model Parallelism: Split model across devices
   - Necessary for large embedding tables
   - Different devices hold different vocabulary ranges
   - Activations transferred between devices

3. Pipeline Parallelism: Split model into stages
   - Each stage on different device
   - Micro-batches flow through pipeline
   - Reduces bubble time (idle GPU time)

4. Tensor Parallelism: Split individual layers across devices
   - Useful for very large transformer layers
   - Intra-layer parallelism
   - High communication overhead

For embedding training, typically combine:
- Data Parallelism: For encoder/projection layers
- Model Parallelism: For large embedding tables
- Pipeline Parallelism: For deep transformer stacks
"""

import torch
import torch.nn as nn
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from typing import Optional, Dict, List, Tuple
import numpy as np

class DistributedEmbeddingTable(nn.Module):
    """
    Model-parallel embedding table for large vocabularies

    Splits vocabulary across multiple GPUs:
    - GPU 0: vocab indices 0 to N/4
    - GPU 1: vocab indices N/4 to N/2
    - GPU 2: vocab indices N/2 to 3N/4
    - GPU 3: vocab indices 3N/4 to N

    Critical for:
    - Large vocabularies (100M+ tokens)
    - Product catalogs (10M+ items)
    - User tables (100M+ users)

    Without model parallelism:
    - 100M vocab × 512 dims × 4 bytes = 200GB (exceeds single GPU memory)

    With 8-way model parallelism:
    - 12.5M vocab per GPU × 512 dims × 4 bytes = 25GB per GPU (fits on A100)
    """

    def __init__(
        self,
        total_vocab_size: int,
        embedding_dim: int,
        world_size: int,
        rank: int
    ):
        """
        Args:
            total_vocab_size: Full vocabulary size
            embedding_dim: Embedding dimension
            world_size: Number of GPUs
            rank: Current GPU rank (0 to world_size-1)
        """
        super().__init__()
        self.total_vocab_size = total_vocab_size
        self.embedding_dim = embedding_dim
        self.world_size = world_size
        self.rank = rank

        # Each GPU holds a slice of the vocabulary
        self.vocab_per_gpu = total_vocab_size // world_size
        self.vocab_start = rank * self.vocab_per_gpu
        self.vocab_end = (rank + 1) * self.vocab_per_gpu

        # Local embedding table (subset of vocabulary)
        self.embeddings = nn.Embedding(
            self.vocab_per_gpu,
            embedding_dim
        )

        print(f"Rank {rank}: Vocabulary range [{self.vocab_start}, {self.vocab_end})")
        print(f"  Local embedding size: {self.vocab_per_gpu * embedding_dim * 4 / 1e9:.2f} GB")

    def forward(self, input_ids: torch.Tensor) -> torch.Tensor:
        """
        Lookup embeddings across distributed vocabulary

        Process:
        1. Determine which GPUs hold embeddings for input_ids
        2. Send lookup requests to appropriate GPUs
        3. Gather results and return

        Args:
            input_ids: Token IDs (batch_size, seq_len)

        Returns:
            embeddings: (batch_size, seq_len, embedding_dim)
        """
        batch_size, seq_len = input_ids.shape
        device = input_ids.device

        # Initialize output
        output = torch.zeros(
            batch_size, seq_len, self.embedding_dim,
            device=device
        )

        # Mask for tokens this GPU is responsible for
        local_mask = (input_ids >= self.vocab_start) & (input_ids < self.vocab_end)

        if local_mask.any():
            # Get local token IDs (offset by vocab_start)
            local_ids = input_ids[local_mask] - self.vocab_start

            # Lookup local embeddings
            local_embeddings = self.embeddings(local_ids)

            # Place in output
            output[local_mask] = local_embeddings

        # All-reduce: Sum embeddings from all GPUs
        # Each GPU contributes embeddings for its vocabulary range
        dist.all_reduce(output, op=dist.ReduceOp.SUM)

        return output

class DistributedContrastiveEmbedding(nn.Module):
    """
    Distributed contrastive learning for embeddings

    Challenges at scale:
    1. Large batch sizes (8K-32K) for effective negative sampling
    2. All-to-all similarity computation (N² complexity)
    3. Gradient synchronization across devices
    4. Memory constraints for large batches

    Architecture:
    - Each GPU processes batch_size/world_size samples
    - Embeddings gathered across all GPUs for contrastive loss
    - Gradients computed and synchronized
    - Optimized for high-throughput, large-batch training

    Typical setup:
    - 8 GPUs × 1024 batch per GPU = 8192 total batch size
    - Contrastive loss computed on full 8192 samples
    - Enables strong negative sampling
    """

    def __init__(
        self,
        vocab_size: int,
        embedding_dim: int = 512,
        hidden_dim: int = 2048,
        temperature: float = 0.07
    ):
        super().__init__()

        # Embedding table (potentially distributed)
        self.embeddings = nn.Embedding(vocab_size, embedding_dim)

        # Projection head (data parallel)
        self.projection = nn.Sequential(
            nn.Linear(embedding_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, embedding_dim)
        )

        self.temperature = temperature

    def forward(
        self,
        anchor_ids: torch.Tensor,
        positive_ids: torch.Tensor
    ) -> torch.Tensor:
        """
        Distributed contrastive loss

        Args:
            anchor_ids: Anchor samples (local_batch_size,)
            positive_ids: Positive samples (local_batch_size,)

        Returns:
            loss: Contrastive loss
        """
        # Embed locally
        anchor_emb = self.embeddings(anchor_ids)
        positive_emb = self.embeddings(positive_ids)

        # Project
        anchor_proj = self.projection(anchor_emb)
        positive_proj = self.projection(positive_emb)

        # Normalize
        anchor_proj = F.normalize(anchor_proj, dim=-1)
        positive_proj = F.normalize(positive_proj, dim=-1)

        # Gather embeddings from all GPUs
        anchor_proj_all = self._gather_from_all_gpus(anchor_proj)
        positive_proj_all = self._gather_from_all_gpus(positive_proj)

        # Compute similarity matrix
        # (global_batch_size, global_batch_size)
        logits_aa = torch.matmul(anchor_proj_all, anchor_proj_all.T) / self.temperature
        logits_ap = torch.matmul(anchor_proj_all, positive_proj_all.T) / self.temperature

        # Contrastive loss (InfoNCE)
        # Positive pairs: (anchor_i, positive_i)
        # Negatives: All other samples in global batch
        global_batch_size = anchor_proj_all.shape[0]
        labels = torch.arange(global_batch_size, device=anchor_proj.device)

        # Loss for anchor-positive pairs
        loss_ap = F.cross_entropy(logits_ap, labels)

        return loss_ap

    def _gather_from_all_gpus(self, tensor: torch.Tensor) -> torch.Tensor:
        """
        Gather tensor from all GPUs

        Each GPU has (local_batch_size, embedding_dim)
        Result: (global_batch_size, embedding_dim) where
                global_batch_size = local_batch_size × world_size
        """
        if not dist.is_initialized():
            return tensor

        world_size = dist.get_world_size()
        tensor_list = [torch.zeros_like(tensor) for _ in range(world_size)]

        # All-gather: Each GPU gets tensors from all GPUs
        dist.all_gather(tensor_list, tensor)

        # Concatenate along batch dimension
        return torch.cat(tensor_list, dim=0)

class DistributedTrainer:
    """
    Orchestrates distributed training across multiple GPUs/nodes

    Features:
    - Automatic device placement
    - Gradient synchronization
    - Learning rate scaling
    - Checkpoint aggregation
    - Fault tolerance

    Scaling laws:
    - Linear speedup: 8 GPUs → 8x throughput (ideal)
    - Reality: 6-7x speedup (communication overhead)
    - 64 GPUs → 40-50x speedup (diminishing returns)

    Bottlenecks:
    - Communication bandwidth (gradient synchronization)
    - Load imbalance (uneven batch sizes)
    - Optimizer state synchronization
    """

    def __init__(
        self,
        model: nn.Module,
        local_rank: int,
        world_size: int,
        backend: str = 'nccl'
    ):
        """
        Args:
            model: Model to train
            local_rank: GPU rank on this node
            world_size: Total number of GPUs across all nodes
            backend: Communication backend ('nccl' for GPU, 'gloo' for CPU)
        """
        self.local_rank = local_rank
        self.world_size = world_size

        # Initialize distributed process group
        if not dist.is_initialized():
            dist.init_process_group(
                backend=backend,
                world_size=world_size,
                rank=local_rank
            )

        # Move model to GPU
        self.device = torch.device(f'cuda:{local_rank}')
        model = model.to(self.device)

        # Wrap in DistributedDataParallel
        self.model = DDP(
            model,
            device_ids=[local_rank],
            output_device=local_rank,
            find_unused_parameters=False  # Optimization: skip if all params used
        )

        print(f"Initialized DDP on rank {local_rank}/{world_size}")

    def train_step(
        self,
        batch: Dict[str, torch.Tensor],
        optimizer: torch.optim.Optimizer
    ) -> float:
        """
        Single distributed training step

        Process:
        1. Each GPU processes local batch
        2. Forward pass (communication for model-parallel layers)
        3. Backward pass (gradient synchronization)
        4. Optimizer step (local, uses synchronized gradients)

        Args:
            batch: Training batch (already on correct device)
            optimizer: Optimizer instance

        Returns:
            loss: Scalar loss value
        """
        self.model.train()

        # Forward pass
        loss = self.model(
            batch['anchor_ids'],
            batch['positive_ids']
        )

        # Backward pass (DDP automatically synchronizes gradients)
        optimizer.zero_grad()
        loss.backward()

        # Gradient clipping (before optimizer step)
        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)

        # Optimizer step
        optimizer.step()

        return loss.item()

    def save_checkpoint(self, path: str, epoch: int, optimizer: torch.optim.Optimizer):
        """
        Save checkpoint (only from rank 0 to avoid duplicate writes)

        Checkpoint includes:
        - Model state dict
        - Optimizer state dict
        - Training metadata (epoch, step, etc.)
        """
        if self.local_rank == 0:
            checkpoint = {
                'epoch': epoch,
                'model_state_dict': self.model.module.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
            }
            torch.save(checkpoint, path)
            print(f"Checkpoint saved: {path}")

        # Synchronize to ensure checkpoint written before proceeding
        dist.barrier()

    def cleanup(self):
        """Cleanup distributed training"""
        dist.destroy_process_group()

# Example: Training script for distributed embedding model
def train_distributed_embedding_model(
    rank: int,
    world_size: int,
    epochs: int = 10
):
    """
    Distributed training script

    Launch with:
    ```bash
    torchrun --nproc_per_node=8 train.py
    ```

    Or for multi-node:
    ```bash
    torchrun --nproc_per_node=8 --nnodes=4 --node_rank=0 \
             --master_addr=node0 --master_port=1234 train.py
    ```

    Args:
        rank: GPU rank (assigned by torchrun)
        world_size: Total GPUs (assigned by torchrun)
        epochs: Number of training epochs
    """

    # Initialize distributed trainer
    model = DistributedContrastiveEmbedding(
        vocab_size=100000,
        embedding_dim=512
    )

    trainer = DistributedTrainer(
        model=model,
        local_rank=rank,
        world_size=world_size
    )

    # Optimizer (scale learning rate by world size)
    base_lr = 0.001
    scaled_lr = base_lr * world_size
    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=scaled_lr
    )

    # Training loop
    for epoch in range(epochs):
        # In practice: Use DistributedSampler for data loading
        # to ensure each GPU sees different data
        for step in range(100):  # Placeholder
            # Create dummy batch
            batch = {
                'anchor_ids': torch.randint(0, 100000, (256,), device=trainer.device),
                'positive_ids': torch.randint(0, 100000, (256,), device=trainer.device)
            }

            loss = trainer.train_step(batch, optimizer)

            if rank == 0 and step % 10 == 0:
                print(f"Epoch {epoch}, Step {step}: Loss = {loss:.4f}")

        # Save checkpoint
        trainer.save_checkpoint(f"checkpoint_epoch_{epoch}.pt", epoch, optimizer)

    trainer.cleanup()
    print(f"Rank {rank}: Training complete")

# Uncomment to run (requires torchrun):
# if __name__ == '__main__':
#     import os
#     rank = int(os.environ['LOCAL_RANK'])
#     world_size = int(os.environ['WORLD_SIZE'])
#     train_distributed_embedding_model(rank, world_size)
```

:::{.callout-tip}
## Choosing the Right Parallelism Strategy

**Use Data Parallelism when:**
- Model fits on single GPU
- Batch size is primary bottleneck
- Most layers are data-parallel friendly (convolutions, transformers)

**Add Model Parallelism when:**
- Embedding tables > GPU memory (100M+ vocabulary)
- Single layer > GPU memory (very wide transformer layers)

**Add Pipeline Parallelism when:**
- Model depth > memory capacity (100+ transformer layers)
- High arithmetic intensity (can hide communication latency)

**For embedding training:**
- Start with Data Parallelism for encoder
- Add Model Parallelism for large embedding tables
- Consider Pipeline Parallelism for deep architectures (BERT-Large, GPT-3 scale)
:::

:::{.callout-warning}
## Communication Bottlenecks

Distributed training speedup is limited by communication:
- **All-reduce** (gradient sync): O(parameters × world_size)
- **All-gather** (activations): O(batch_size × hidden_dim × world_size)
- **Point-to-point** (pipeline): O(hidden_dim × micro_batch_size)

Optimizations:
- **Gradient compression**: Reduce precision (FP32 → FP16 gradients)
- **Overlap communication and computation**: Backward pass while communicating gradients
- **Hierarchical reduction**: Node-local reduction, then cross-node
- **Faster interconnect**: InfiniBand (200 Gbps) vs Ethernet (10-100 Gbps)
:::

## Gradient Accumulation and Mixed Precision

Memory is the primary constraint in deep learning training. A single NVIDIA A100 GPU has 80GB memory, yet training large embedding models with contrastive learning (32K batch size × 512 dims × 4 bytes ≈ 64GB just for embeddings) quickly exceeds capacity. **Gradient accumulation** enables large effective batch sizes by splitting batches into smaller micro-batches, while **mixed precision** reduces memory footprint and accelerates computation by using FP16 for most operations while maintaining FP32 for numerical stability.

### Gradient Accumulation for Large Batch Training

Contrastive learning benefits from large batch sizes—more negatives improve representation quality. But memory limits batch size. Gradient accumulation solves this:

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Optional

class GradientAccumulationTrainer:
    """
    Enable large effective batch sizes through gradient accumulation

    Problem:
    - Contrastive learning needs 16K-32K batch size for quality
    - Single GPU: 1024 max batch size (memory constraint)
    - Solution: Accumulate gradients over 32 micro-batches of 1024
      → Effective batch size: 32 × 1024 = 32,768

    Process:
    1. Forward pass on micro-batch
    2. Backward pass (accumulate gradients, don't update yet)
    3. Repeat for N micro-batches
    4. Optimizer step (update parameters with accumulated gradients)
    5. Zero gradients

    Benefits:
    - Train with batch sizes exceeding GPU memory
    - Identical results to full-batch training
    - Trade-off: More iterations per update (slower wall-clock time)

    Drawbacks:
    - Batch normalization statistics incorrect (computed per micro-batch)
    - Longer time per effective batch
    - May require learning rate adjustments
    """

    def __init__(
        self,
        model: nn.Module,
        accumulation_steps: int = 4
    ):
        """
        Args:
            model: Model to train
            accumulation_steps: Number of micro-batches to accumulate
        """
        self.model = model
        self.accumulation_steps = accumulation_steps

    def train_step(
        self,
        dataloader,
        optimizer: torch.optim.Optimizer,
        device: str = 'cuda'
    ) -> float:
        """
        Training step with gradient accumulation

        Args:
            dataloader: Data loader providing micro-batches
            optimizer: Optimizer instance
            device: Device to train on

        Returns:
            avg_loss: Average loss across micro-batches
        """
        self.model.train()
        optimizer.zero_grad()

        total_loss = 0.0

        for i, batch in enumerate(dataloader):
            if i >= self.accumulation_steps:
                break

            # Move batch to device
            anchor_ids = batch['anchor_ids'].to(device)
            positive_ids = batch['positive_ids'].to(device)

            # Forward pass
            loss = self.model(anchor_ids, positive_ids)

            # Scale loss by accumulation steps
            # (since we're summing gradients, not averaging)
            loss = loss / self.accumulation_steps

            # Backward pass (accumulates gradients)
            loss.backward()

            total_loss += loss.item()

        # Gradient clipping (on accumulated gradients)
        torch.nn.utils.clip_grad_norm_(
            self.model.parameters(),
            max_norm=1.0
        )

        # Optimizer step (updates parameters)
        optimizer.step()

        # Zero gradients for next accumulation cycle
        optimizer.zero_grad()

        return total_loss

# Example: Effective 32K batch with 8GB GPU
def gradient_accumulation_example():
    """
    Train with 32K effective batch size on limited memory

    Hardware: 1× A100 (80GB)
    Micro-batch: 1024 samples
    Accumulation steps: 32
    Effective batch: 32 × 1024 = 32,768 samples

    Memory usage:
    - Micro-batch: 1024 × 512 × 4 bytes = 2MB (embeddings)
    - Model: ~500MB (encoder parameters)
    - Optimizer state: ~1GB (Adam momentum)
    - Activations: ~10GB (forward/backward)
    Total: ~12GB per micro-batch (fits on A100)
    """

    model = DistributedContrastiveEmbedding(vocab_size=100000)
    model = model.to('cuda')

    optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)

    trainer = GradientAccumulationTrainer(
        model=model,
        accumulation_steps=32  # 32× micro-batches
    )

    # Dummy dataloader
    dataloader = [
        {
            'anchor_ids': torch.randint(0, 100000, (1024,)),
            'positive_ids': torch.randint(0, 100000, (1024,))
        }
        for _ in range(32)
    ]

    loss = trainer.train_step(dataloader, optimizer)
    print(f"Loss with 32K effective batch: {loss:.4f}")

# Uncomment to run:
# gradient_accumulation_example()
```

### Mixed Precision Training

Modern GPUs (Volta, Turing, Ampere architectures) have specialized Tensor Cores that accelerate FP16 matrix multiplications by 2-8×. **Mixed precision** uses FP16 for computation while maintaining FP32 for numerical stability:

```python
import torch
from torch.cuda.amp import autocast, GradScaler

class MixedPrecisionTrainer:
    """
    Automatic mixed precision (AMP) training

    FP16 (half precision):
    - 2 bytes per parameter (vs 4 bytes FP32)
    - 2-8× faster computation on Tensor Cores
    - 2× less memory usage

    Challenges:
    - Gradient underflow (very small gradients round to zero)
    - Reduced numerical range (FP16: ±65,504 vs FP32: ±3.4×10³⁸)

    Solution (Automatic Mixed Precision):
    1. Forward pass in FP16 (fast)
    2. Loss in FP32 (accuracy)
    3. Gradient scaling (prevent underflow)
    4. Backward pass in FP16 (fast)
    5. Unscale gradients to FP32
    6. Optimizer step in FP32 (stability)
    7. Copy updated FP32 parameters to FP16 for next iteration

    Speedup:
    - A100: 2-3× faster than FP32
    - V100: 2-5× faster than FP32
    - Memory: 2× reduction (enables larger batches)
    """

    def __init__(
        self,
        model: nn.Module,
        device: str = 'cuda'
    ):
        self.model = model.to(device)
        self.device = device

        # Gradient scaler (prevents underflow)
        self.scaler = GradScaler()

    def train_step(
        self,
        batch: dict,
        optimizer: torch.optim.Optimizer
    ) -> float:
        """
        Training step with automatic mixed precision

        Args:
            batch: Training batch
            optimizer: Optimizer

        Returns:
            loss: Scalar loss
        """
        self.model.train()

        anchor_ids = batch['anchor_ids'].to(self.device)
        positive_ids = batch['positive_ids'].to(self.device)

        # Zero gradients
        optimizer.zero_grad()

        # Forward pass in FP16 (autocast context)
        with autocast():
            loss = self.model(anchor_ids, positive_ids)

        # Backward pass with gradient scaling
        self.scaler.scale(loss).backward()

        # Unscale gradients and clip
        self.scaler.unscale_(optimizer)
        torch.nn.utils.clip_grad_norm_(
            self.model.parameters(),
            max_norm=1.0
        )

        # Optimizer step (with gradient scaling)
        self.scaler.step(optimizer)

        # Update scaler for next iteration
        self.scaler.update()

        return loss.item()

# Combining Gradient Accumulation + Mixed Precision
class ScalableTrainer:
    """
    Combine gradient accumulation and mixed precision

    Enables:
    - Large effective batch sizes (gradient accumulation)
    - Fast computation (mixed precision)
    - Low memory usage (both techniques reduce memory)

    Example configuration:
    - Hardware: 8× A100 GPUs
    - Micro-batch per GPU: 512
    - Accumulation steps: 8
    - Effective global batch: 8 GPUs × 512 × 8 = 32,768

    Speedup vs baseline (single GPU, FP32, batch 128):
    - 8× from multi-GPU data parallelism
    - 2.5× from mixed precision
    - 256× larger effective batch size
    Total: ~20× faster training with 256× larger batch
    """

    def __init__(
        self,
        model: nn.Module,
        accumulation_steps: int = 4,
        device: str = 'cuda'
    ):
        self.model = model.to(device)
        self.accumulation_steps = accumulation_steps
        self.device = device
        self.scaler = GradScaler()

    def train_step(
        self,
        dataloader,
        optimizer: torch.optim.Optimizer
    ) -> float:
        """
        Training step combining both techniques

        Args:
            dataloader: Micro-batch data loader
            optimizer: Optimizer

        Returns:
            avg_loss: Average loss over accumulated gradients
        """
        self.model.train()
        optimizer.zero_grad()

        total_loss = 0.0

        for i, batch in enumerate(dataloader):
            if i >= self.accumulation_steps:
                break

            anchor_ids = batch['anchor_ids'].to(self.device)
            positive_ids = batch['positive_ids'].to(self.device)

            # Forward in FP16
            with autocast():
                loss = self.model(anchor_ids, positive_ids)
                loss = loss / self.accumulation_steps

            # Backward with scaling
            self.scaler.scale(loss).backward()

            total_loss += loss.item()

        # Unscale and clip
        self.scaler.unscale_(optimizer)
        torch.nn.utils.clip_grad_norm_(
            self.model.parameters(),
            max_norm=1.0
        )

        # Step and update scaler
        self.scaler.step(optimizer)
        self.scaler.update()

        optimizer.zero_grad()

        return total_loss

def benchmark_mixed_precision():
    """
    Benchmark FP32 vs FP16 training speed

    Expected results (A100):
    - FP32: 100 samples/sec, 20GB memory
    - FP16: 250 samples/sec, 12GB memory
    """
    import time

    model = DistributedContrastiveEmbedding(vocab_size=100000)

    # FP32 baseline
    model_fp32 = model.to('cuda')
    optimizer = torch.optim.AdamW(model_fp32.parameters())

    batch = {
        'anchor_ids': torch.randint(0, 100000, (512,)).cuda(),
        'positive_ids': torch.randint(0, 100000, (512,)).cuda()
    }

    # Warmup
    for _ in range(10):
        loss = model_fp32(batch['anchor_ids'], batch['positive_ids'])
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()

    # Benchmark FP32
    torch.cuda.synchronize()
    start = time.time()
    for _ in range(100):
        loss = model_fp32(batch['anchor_ids'], batch['positive_ids'])
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()
    torch.cuda.synchronize()
    fp32_time = time.time() - start

    # FP16 with AMP
    model_fp16 = model.to('cuda')
    optimizer = torch.optim.AdamW(model_fp16.parameters())
    trainer = MixedPrecisionTrainer(model_fp16)

    # Warmup
    for _ in range(10):
        trainer.train_step(batch, optimizer)

    # Benchmark FP16
    torch.cuda.synchronize()
    start = time.time()
    for _ in range(100):
        trainer.train_step(batch, optimizer)
    torch.cuda.synchronize()
    fp16_time = time.time() - start

    print(f"FP32 time: {fp32_time:.2f}s")
    print(f"FP16 time: {fp16_time:.2f}s")
    print(f"Speedup: {fp32_time / fp16_time:.2f}×")

# Uncomment to run:
# benchmark_mixed_precision()
```

:::{.callout-tip}
## When to Use Gradient Accumulation vs Larger Hardware

**Use gradient accumulation when:**
- Memory-constrained (batch won't fit on GPU)
- Want to experiment with very large batches (64K+)
- Training on cloud instances with limited GPU memory

**Upgrade hardware when:**
- Wall-clock time is critical (accumulation is slower)
- Training very frequently (hardware cost amortizes)
- Need to scale beyond single node (distributed > accumulation)

**Use mixed precision almost always:**
- Modern GPUs (V100, A100) have Tensor Cores
- 2-3× speedup with minimal code changes
- Rarely causes numerical issues (except very deep networks)
:::

:::{.callout-warning}
## Mixed Precision Gotchas

**Gradient underflow**: Very small gradients (< 1e-7) round to zero in FP16. Gradient scaling addresses this, but extreme cases may need:
- Larger learning rates
- Loss scaling adjustments
- FP32 for sensitive layers (layer norm, softmax)

**Batch normalization**: BatchNorm statistics in FP16 can be unstable. Use FP32 for BatchNorm layers:
```python
model = model.half()  # Convert to FP16
# Keep BatchNorm in FP32
for module in model.modules():
    if isinstance(module, nn.BatchNorm1d):
        module.float()
```
:::

## Memory Optimization Techniques

Beyond mixed precision and gradient accumulation, several techniques reduce memory footprint, enabling larger models and batch sizes:

### Gradient Checkpointing

Trade computation for memory by recomputing activations during backward pass instead of storing them:

```python
from torch.utils.checkpoint import checkpoint

class CheckpointedTransformerLayer(nn.Module):
    """
    Transformer layer with gradient checkpointing

    Standard approach:
    - Forward: Compute and store all activations
    - Backward: Use stored activations for gradients
    - Memory: O(layers × sequence_length × hidden_dim)

    Gradient checkpointing:
    - Forward: Compute activations, discard most
    - Backward: Recompute activations on-the-fly
    - Memory: O(checkpoints × sequence_length × hidden_dim)

    Trade-off:
    - Memory: 10-50× reduction (depending on checkpoints)
    - Compute: 30-50% slowdown (recomputation cost)

    When to use:
    - Very deep models (50+ layers)
    - Long sequences (10K+ tokens)
    - Memory is bottleneck, compute is available
    """

    def __init__(self, hidden_dim: int, num_heads: int):
        super().__init__()
        self.attention = nn.MultiheadAttention(hidden_dim, num_heads)
        self.ffn = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim * 4),
            nn.ReLU(),
            nn.Linear(hidden_dim * 4, hidden_dim)
        )
        self.norm1 = nn.LayerNorm(hidden_dim)
        self.norm2 = nn.LayerNorm(hidden_dim)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Forward with gradient checkpointing

        Checkpoint at layer boundaries:
        - Attention block
        - FFN block
        """
        # Checkpoint attention
        def attention_forward(x):
            attn_out, _ = self.attention(x, x, x)
            return self.norm1(x + attn_out)

        x = checkpoint(attention_forward, x)

        # Checkpoint FFN
        def ffn_forward(x):
            return self.norm2(x + self.ffn(x))

        x = checkpoint(ffn_forward, x)

        return x

class MemoryEfficientEmbeddingModel(nn.Module):
    """
    Embedding model with aggressive memory optimization

    Techniques:
    1. Gradient checkpointing for transformer layers
    2. Mixed precision (FP16)
    3. Gradient accumulation
    4. In-place operations where safe

    Memory breakdown (12-layer transformer, 512 hidden dim):
    - Without optimization: ~40GB (A100 won't fit large batches)
    - With checkpointing: ~10GB (fits with batch 2048)
    - With checkpointing + FP16: ~6GB (fits with batch 4096)

    Enables training much larger models on same hardware.
    """

    def __init__(
        self,
        vocab_size: int,
        embedding_dim: int = 512,
        num_layers: int = 12,
        num_heads: int = 8
    ):
        super().__init__()

        self.embeddings = nn.Embedding(vocab_size, embedding_dim)

        # Transformer layers with checkpointing
        self.layers = nn.ModuleList([
            CheckpointedTransformerLayer(embedding_dim, num_heads)
            for _ in range(num_layers)
        ])

        self.projection = nn.Linear(embedding_dim, embedding_dim)

    def forward(self, input_ids: torch.Tensor) -> torch.Tensor:
        """Forward pass with memory-efficient operations"""

        # Embed
        x = self.embeddings(input_ids)

        # Transform (checkpointed)
        for layer in self.layers:
            x = layer(x)

        # Project
        x = self.projection(x)

        return x
```

### Optimizer State Optimization

Adam optimizer stores momentum and variance for each parameter, tripling memory usage. Optimizations:

```python
import torch.optim as optim
from typing import List

class MemoryEfficientOptimizer:
    """
    Optimize memory usage for large models

    Approaches:
    1. CPU offloading: Store optimizer state on CPU, transfer on-demand
    2. Mixed precision optimizer state: FP16 for momentum/variance
    3. 8-bit optimizers: Quantized optimizer states (bitsandbytes)
    4. Sparse updates: Only update frequently changing parameters

    Memory comparison (100M parameter model):
    - Adam (FP32): 100M params × 3 states × 4 bytes = 1.2GB
    - Adam (FP16): 100M params × 3 states × 2 bytes = 600MB
    - 8-bit Adam: 100M params × 3 states × 1 byte = 300MB
    - SGD: 100M params × 1 state × 4 bytes = 400MB (no momentum stored)
    """

    @staticmethod
    def get_optimizer(
        parameters,
        optimizer_type: str = 'adamw',
        lr: float = 0.001,
        memory_efficient: bool = True
    ):
        """
        Get memory-efficient optimizer

        Args:
            parameters: Model parameters
            optimizer_type: 'adamw', 'sgd', '8bit_adam'
            lr: Learning rate
            memory_efficient: Enable memory optimizations

        Returns:
            optimizer: Configured optimizer
        """

        if optimizer_type == 'adamw':
            if memory_efficient:
                # Use fused AdamW (faster, lower memory)
                return optim.AdamW(
                    parameters,
                    lr=lr,
                    fused=True  # Fused kernel (A100+)
                )
            else:
                return optim.AdamW(parameters, lr=lr)

        elif optimizer_type == 'sgd':
            # SGD with momentum uses less memory than Adam
            return optim.SGD(
                parameters,
                lr=lr,
                momentum=0.9,
                nesterov=True
            )

        elif optimizer_type == '8bit_adam':
            # Requires: pip install bitsandbytes
            try:
                import bitsandbytes as bnb
                return bnb.optim.Adam8bit(
                    parameters,
                    lr=lr
                )
            except ImportError:
                print("bitsandbytes not installed, falling back to AdamW")
                return optim.AdamW(parameters, lr=lr)

        else:
            raise ValueError(f"Unknown optimizer: {optimizer_type}")

def memory_usage_comparison():
    """
    Compare memory usage across optimization techniques

    Model: 100M parameters (typical BERT-Base scale)
    """

    # Create dummy model
    model = nn.Sequential(
        nn.Linear(1024, 4096),
        nn.ReLU(),
        nn.Linear(4096, 1024)
    )

    num_params = sum(p.numel() for p in model.parameters())
    print(f"Model parameters: {num_params / 1e6:.1f}M")

    # Calculate memory requirements
    param_memory_mb = num_params * 4 / 1e6  # FP32

    # Optimizer state memory
    adam_fp32_mb = num_params * 3 * 4 / 1e6  # 3 states (param, momentum, variance)
    adam_fp16_mb = num_params * 3 * 2 / 1e6
    adam_8bit_mb = num_params * 3 * 1 / 1e6
    sgd_mb = num_params * 1 * 4 / 1e6  # Only momentum

    print(f"\nMemory usage:")
    print(f"  Parameters (FP32): {param_memory_mb:.1f} MB")
    print(f"  Adam (FP32): {adam_fp32_mb:.1f} MB optimizer state")
    print(f"  Adam (FP16): {adam_fp16_mb:.1f} MB optimizer state")
    print(f"  Adam (8-bit): {adam_8bit_mb:.1f} MB optimizer state")
    print(f"  SGD: {sgd_mb:.1f} MB optimizer state")

    print(f"\nTotal memory (params + optimizer):")
    print(f"  Adam (FP32): {param_memory_mb + adam_fp32_mb:.1f} MB")
    print(f"  Adam (FP16): {param_memory_mb + adam_fp16_mb:.1f} MB")
    print(f"  Adam (8-bit): {param_memory_mb + adam_8bit_mb:.1f} MB")
    print(f"  SGD: {param_memory_mb + sgd_mb:.1f} MB")

# Uncomment to run:
# memory_usage_comparison()
```

:::{.callout-tip}
## Memory Optimization Checklist

When hitting memory limits, apply optimizations in this order:

1. **Mixed precision (FP16)**: 2× memory reduction, 2-3× speedup
2. **Gradient accumulation**: Enables larger effective batch sizes
3. **Gradient checkpointing**: 10-50× activation memory reduction
4. **Optimizer state optimization**: 8-bit Adam or SGD
5. **Model parallelism**: Split model across GPUs
6. **Batch size reduction**: Last resort (hurts contrastive learning)

Typical savings:
- FP16: 40GB → 20GB
- + Checkpointing: 20GB → 8GB
- + 8-bit optimizer: 8GB → 5GB
- Result: Fit on single A100 (80GB) with large batch
:::

## Multi-GPU and Multi-Node Strategies

Scaling beyond single GPU requires coordination across devices. This section covers practical strategies for multi-GPU (single node) and multi-node (multiple machines) training.

### Multi-GPU Training on Single Node

Single-node multi-GPU is the most common setup (8× A100 or V100 GPUs on one machine):

```python
import torch
import torch.distributed as dist
import torch.multiprocessing as mp
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data import Dataset, DataLoader
from torch.utils.data.distributed import DistributedSampler

class EmbeddingDataset(Dataset):
    """
    Simple dataset for embedding training

    In production: Load from files, databases, or data lakes
    """

    def __init__(self, num_samples: int, vocab_size: int):
        self.num_samples = num_samples
        self.vocab_size = vocab_size

    def __len__(self):
        return self.num_samples

    def __getitem__(self, idx):
        # Generate random anchor-positive pairs
        # In production: Load real pairs from storage
        return {
            'anchor_id': torch.randint(0, self.vocab_size, (1,)).item(),
            'positive_id': torch.randint(0, self.vocab_size, (1,)).item()
        }

def setup_distributed(rank: int, world_size: int):
    """
    Initialize distributed training process group

    Args:
        rank: Process rank (0 to world_size-1)
        world_size: Total number of processes
    """
    # Set device
    torch.cuda.set_device(rank)

    # Initialize process group
    dist.init_process_group(
        backend='nccl',  # NCCL for GPU communication
        init_method='tcp://localhost:12355',
        world_size=world_size,
        rank=rank
    )

def cleanup_distributed():
    """Cleanup distributed resources"""
    dist.destroy_process_group()

def train_worker(
    rank: int,
    world_size: int,
    epochs: int = 10,
    batch_size: int = 512
):
    """
    Training worker for single GPU

    Each GPU runs this function independently.
    PyTorch coordinates gradient synchronization.

    Args:
        rank: GPU rank (0 to world_size-1)
        world_size: Total GPUs
        epochs: Number of epochs
        batch_size: Batch size per GPU
    """

    # Setup distributed
    setup_distributed(rank, world_size)

    # Create model and move to GPU
    model = DistributedContrastiveEmbedding(
        vocab_size=100000,
        embedding_dim=512
    )
    model = model.to(rank)

    # Wrap in DDP
    model = DDP(model, device_ids=[rank])

    # Create dataset
    dataset = EmbeddingDataset(num_samples=100000, vocab_size=100000)

    # Distributed sampler (each GPU sees different data)
    sampler = DistributedSampler(
        dataset,
        num_replicas=world_size,
        rank=rank,
        shuffle=True
    )

    # Data loader
    dataloader = DataLoader(
        dataset,
        batch_size=batch_size,
        sampler=sampler,
        num_workers=4,
        pin_memory=True
    )

    # Optimizer (scale learning rate by world size)
    base_lr = 0.001
    scaled_lr = base_lr * world_size
    optimizer = torch.optim.AdamW(model.parameters(), lr=scaled_lr)

    # Training loop
    for epoch in range(epochs):
        # Set epoch for shuffling
        sampler.set_epoch(epoch)

        model.train()
        total_loss = 0.0

        for batch_idx, batch in enumerate(dataloader):
            # Move to GPU
            anchor_ids = torch.tensor([batch['anchor_id']], device=rank)
            positive_ids = torch.tensor([batch['positive_id']], device=rank)

            # Forward
            loss = model(anchor_ids, positive_ids)

            # Backward
            optimizer.zero_grad()
            loss.backward()

            # Gradient clipping
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

            # Optimizer step
            optimizer.step()

            total_loss += loss.item()

            if rank == 0 and batch_idx % 100 == 0:
                print(f"Epoch {epoch}, Batch {batch_idx}: Loss = {loss.item():.4f}")

        # Log epoch stats (only rank 0)
        if rank == 0:
            avg_loss = total_loss / len(dataloader)
            print(f"Epoch {epoch} complete: Avg Loss = {avg_loss:.4f}")

    # Cleanup
    cleanup_distributed()

def launch_multi_gpu_training(world_size: int = 8):
    """
    Launch multi-GPU training

    Spawns one process per GPU, each running train_worker()

    Args:
        world_size: Number of GPUs to use
    """

    # Spawn processes (one per GPU)
    mp.spawn(
        train_worker,
        args=(world_size,),
        nprocs=world_size,
        join=True
    )

# Uncomment to run:
# launch_multi_gpu_training(world_size=torch.cuda.device_count())
```

### Multi-Node Training

Multi-node training scales to hundreds of GPUs across dozens of machines:

```python
import os

def setup_multi_node(rank: int, world_size: int, master_addr: str, master_port: str):
    """
    Initialize multi-node distributed training

    Environment variables (set by job scheduler like Slurm):
    - MASTER_ADDR: IP address of rank 0 node
    - MASTER_PORT: Port for communication
    - WORLD_SIZE: Total number of processes across all nodes
    - RANK: Global rank of this process

    Typical setup:
    - Node 0 (master): Ranks 0-7 (8 GPUs)
    - Node 1: Ranks 8-15 (8 GPUs)
    - Node 2: Ranks 16-23 (8 GPUs)
    - Node 3: Ranks 24-31 (8 GPUs)
    Total: 32 GPUs across 4 nodes

    Args:
        rank: Global rank (0 to world_size-1)
        world_size: Total GPUs across all nodes
        master_addr: IP of master node
        master_port: Communication port
    """

    # Set environment variables
    os.environ['MASTER_ADDR'] = master_addr
    os.environ['MASTER_PORT'] = master_port

    # Local rank (GPU on this node)
    local_rank = rank % torch.cuda.device_count()
    torch.cuda.set_device(local_rank)

    # Initialize process group
    dist.init_process_group(
        backend='nccl',
        world_size=world_size,
        rank=rank
    )

    print(f"Initialized rank {rank}/{world_size} on device {local_rank}")

def train_multi_node():
    """
    Multi-node training script

    Launch with Slurm:
    ```bash
    sbatch --nodes=4 --gres=gpu:8 train_multi_node.sh
    ```

    Or with torchrun:
    ```bash
    # On each node, run:
    torchrun --nproc_per_node=8 \
             --nnodes=4 \
             --node_rank=$NODE_RANK \
             --master_addr=$MASTER_ADDR \
             --master_port=1234 \
             train_script.py
    ```
    """

    # Get distributed parameters from environment
    rank = int(os.environ['RANK'])
    world_size = int(os.environ['WORLD_SIZE'])
    master_addr = os.environ['MASTER_ADDR']
    master_port = os.environ['MASTER_PORT']

    # Setup multi-node training
    setup_multi_node(rank, world_size, master_addr, master_port)

    # Rest of training code (same as single-node)
    # ...

    dist.destroy_process_group()
```

:::{.callout-tip}
## Multi-GPU Best Practices

**Data loading:**
- Use `DistributedSampler` to partition data across GPUs
- Set `num_workers=4` per GPU for async data loading
- Use `pin_memory=True` for faster CPU→GPU transfer

**Learning rate scaling:**
- Scale learning rate linearly with batch size
- 1 GPU (batch 512, lr 0.001) → 8 GPUs (batch 4096, lr 0.008)
- May need warmup for large learning rates

**Synchronization:**
- Minimize `dist.barrier()` calls (blocks all GPUs)
- Overlap communication with computation
- Use `find_unused_parameters=False` in DDP when possible

**Checkpointing:**
- Only save from rank 0 to avoid duplicate writes
- Use `dist.barrier()` after saving to synchronize
- Consider sharded checkpointing for very large models
:::

:::{.callout-warning}
## Multi-Node Challenges

**Network bottlenecks:**
- Cross-node communication 10-100× slower than NVLink
- Use gradient compression or ZeRO optimizer
- Consider hierarchical all-reduce (node-local first)

**Fault tolerance:**
- Single node failure kills entire job
- Implement checkpointing every N steps
- Use elastic training frameworks (TorchElastic)

**Load imbalance:**
- Stragglers slow down entire cluster
- Monitor per-GPU utilization
- Use dynamic batch sizing if variability high
:::

## Training Cost Optimization

Large-scale training is expensive. A 100-GPU training run can cost $10K-$100K. This section covers strategies to minimize cost while maintaining quality.

### Cloud Cost Optimization

```python
from dataclasses import dataclass
from typing import Dict, List
import math

@dataclass
class GPUInstance:
    """Cloud GPU instance configuration"""
    name: str
    gpu_type: str
    num_gpus: int
    memory_gb: int
    cost_per_hour: float
    tflops_fp16: float  # Peak FP16 throughput

# Cloud GPU pricing (approximate, as of 2024)
CLOUD_INSTANCES = {
    'aws_p4d_24xlarge': GPUInstance(
        name='p4d.24xlarge',
        gpu_type='A100',
        num_gpus=8,
        memory_gb=640,
        cost_per_hour=32.77,
        tflops_fp16=1248  # 8× A100 × 156 TFLOPS
    ),
    'aws_p3_16xlarge': GPUInstance(
        name='p3.16xlarge',
        gpu_type='V100',
        num_gpus=8,
        memory_gb=488,
        cost_per_hour=24.48,
        tflops_fp16=1000  # 8× V100 × 125 TFLOPS
    ),
    'gcp_a2_ultra': GPUInstance(
        name='a2-ultragpu-8g',
        gpu_type='A100',
        num_gpus=8,
        memory_gb=680,
        cost_per_hour=30.00,
        tflops_fp16=1248
    ),
    'azure_nd96amsr_v4': GPUInstance(
        name='Standard_ND96amsr_A100_v4',
        gpu_type='A100',
        num_gpus=8,
        memory_gb=900,
        cost_per_hour=27.20,
        tflops_fp16=1248
    )
}

class CostOptimizer:
    """
    Optimize training cost across different strategies

    Levers:
    1. Instance type (A100 vs V100 vs H100)
    2. Spot vs on-demand pricing (50-90% savings)
    3. Training duration (optimize throughput)
    4. Batch size and accumulation (memory efficiency)
    5. Mixed precision (2-3× speedup)
    6. Early stopping (halt when converged)
    """

    @staticmethod
    def estimate_training_time(
        dataset_size: int,
        batch_size: int,
        epochs: int,
        samples_per_second: float
    ) -> float:
        """
        Estimate training time in hours

        Args:
            dataset_size: Number of training samples
            batch_size: Effective batch size
            epochs: Number of epochs
            samples_per_second: Throughput (depends on hardware)

        Returns:
            hours: Estimated training time
        """
        total_samples = dataset_size * epochs
        total_seconds = total_samples / samples_per_second
        return total_seconds / 3600

    @staticmethod
    def estimate_cost(
        instance: GPUInstance,
        training_hours: float,
        spot_instance: bool = False
    ) -> float:
        """
        Estimate training cost

        Args:
            instance: GPU instance configuration
            training_hours: Training duration
            spot_instance: Use spot/preemptible pricing

        Returns:
            cost: Total cost in USD
        """
        cost_per_hour = instance.cost_per_hour

        if spot_instance:
            # Spot instances typically 50-70% cheaper
            cost_per_hour *= 0.4  # 60% discount

        return cost_per_hour * training_hours

    @staticmethod
    def compare_strategies(
        dataset_size: int = 1_000_000_000,
        batch_size: int = 32768,
        epochs: int = 10
    ):
        """
        Compare cost across different instance types and strategies

        Scenario: Train embedding model on 1B samples, 10 epochs

        Args:
            dataset_size: Training samples
            batch_size: Effective batch size
            epochs: Number of epochs
        """

        print("=== Training Cost Comparison ===")
        print(f"Dataset: {dataset_size / 1e9:.1f}B samples")
        print(f"Batch size: {batch_size:,}")
        print(f"Epochs: {epochs}")
        print()

        strategies = [
            # (instance_key, throughput_samples_per_sec, use_spot)
            ('aws_p4d_24xlarge', 50000, False),  # A100, on-demand
            ('aws_p4d_24xlarge', 50000, True),   # A100, spot
            ('aws_p3_16xlarge', 30000, False),   # V100, on-demand
            ('aws_p3_16xlarge', 30000, True),    # V100, spot
        ]

        results = []

        for instance_key, throughput, use_spot in strategies:
            instance = CLOUD_INSTANCES[instance_key]

            # Estimate training time
            hours = CostOptimizer.estimate_training_time(
                dataset_size, batch_size, epochs, throughput
            )

            # Estimate cost
            cost = CostOptimizer.estimate_cost(instance, hours, use_spot)

            results.append({
                'instance': instance.name,
                'gpu': instance.gpu_type,
                'pricing': 'Spot' if use_spot else 'On-Demand',
                'hours': hours,
                'cost': cost,
                'cost_per_hour': instance.cost_per_hour if not use_spot else instance.cost_per_hour * 0.4
            })

        # Print results
        for r in results:
            print(f"{r['instance']} ({r['gpu']}, {r['pricing']}):")
            print(f"  Training time: {r['hours']:.1f} hours")
            print(f"  Cost per hour: ${r['cost_per_hour']:.2f}")
            print(f"  Total cost: ${r['cost']:.2f}")
            print()

        # Find best option
        best = min(results, key=lambda x: x['cost'])
        print(f"💰 Best option: {best['instance']} ({best['pricing']}) - ${best['cost']:.2f}")

# Uncomment to run:
# CostOptimizer.compare_strategies()
```

### Spot Instance Training

Spot instances offer 50-90% discounts but can be preempted. Strategies for resilient training:

```python
import time
import signal
from pathlib import Path

class SpotInstanceTrainer:
    """
    Training on spot instances with automatic checkpointing

    Challenges:
    - Instance can be preempted with 2-minute warning
    - Need to checkpoint frequently
    - Resume from last checkpoint on new instance

    Strategies:
    1. Checkpoint every N minutes (e.g., every 15 minutes)
    2. Listen for SIGTERM (preemption signal)
    3. Save checkpoint on SIGTERM
    4. Resume from latest checkpoint on restart
    """

    def __init__(
        self,
        model: nn.Module,
        checkpoint_dir: str = './checkpoints',
        checkpoint_interval_minutes: int = 15
    ):
        self.model = model
        self.checkpoint_dir = Path(checkpoint_dir)
        self.checkpoint_dir.mkdir(exist_ok=True)
        self.checkpoint_interval = checkpoint_interval_minutes * 60  # Convert to seconds

        self.last_checkpoint_time = time.time()

        # Register SIGTERM handler (spot instance preemption warning)
        signal.signal(signal.SIGTERM, self._handle_preemption)

    def _handle_preemption(self, signum, frame):
        """
        Handle spot instance preemption

        Called ~2 minutes before termination.
        Save checkpoint and gracefully exit.
        """
        print("⚠️  Spot instance preemption detected!")
        print("Saving emergency checkpoint...")

        self.save_checkpoint(emergency=True)

        print("✓ Emergency checkpoint saved. Exiting gracefully.")
        exit(0)

    def save_checkpoint(
        self,
        epoch: int = 0,
        step: int = 0,
        optimizer: torch.optim.Optimizer = None,
        emergency: bool = False
    ):
        """
        Save training checkpoint

        Args:
            epoch: Current epoch
            step: Current step
            optimizer: Optimizer state
            emergency: Emergency checkpoint (spot preemption)
        """
        checkpoint = {
            'epoch': epoch,
            'step': step,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict() if optimizer else None,
            'timestamp': time.time()
        }

        if emergency:
            checkpoint_path = self.checkpoint_dir / 'emergency_checkpoint.pt'
        else:
            checkpoint_path = self.checkpoint_dir / f'checkpoint_epoch_{epoch}_step_{step}.pt'

        torch.save(checkpoint, checkpoint_path)
        print(f"✓ Checkpoint saved: {checkpoint_path}")

        # Update last checkpoint time
        self.last_checkpoint_time = time.time()

    def load_latest_checkpoint(self, optimizer: torch.optim.Optimizer = None) -> Dict:
        """
        Load latest checkpoint (for resuming training)

        Returns:
            checkpoint: Checkpoint data (or None if no checkpoint exists)
        """
        # Check for emergency checkpoint first
        emergency_path = self.checkpoint_dir / 'emergency_checkpoint.pt'
        if emergency_path.exists():
            print(f"Loading emergency checkpoint: {emergency_path}")
            checkpoint = torch.load(emergency_path)
            self.model.load_state_dict(checkpoint['model_state_dict'])
            if optimizer and checkpoint['optimizer_state_dict']:
                optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            return checkpoint

        # Find latest regular checkpoint
        checkpoints = sorted(self.checkpoint_dir.glob('checkpoint_*.pt'))
        if not checkpoints:
            print("No checkpoints found, starting from scratch")
            return None

        latest = checkpoints[-1]
        print(f"Loading checkpoint: {latest}")
        checkpoint = torch.load(latest)
        self.model.load_state_dict(checkpoint['model_state_dict'])
        if optimizer and checkpoint['optimizer_state_dict']:
            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])

        return checkpoint

    def should_checkpoint(self) -> bool:
        """Check if it's time to checkpoint based on interval"""
        elapsed = time.time() - self.last_checkpoint_time
        return elapsed >= self.checkpoint_interval

    def train_with_checkpointing(
        self,
        dataloader,
        optimizer: torch.optim.Optimizer,
        epochs: int = 10,
        start_epoch: int = 0,
        start_step: int = 0
    ):
        """
        Training loop with automatic checkpointing

        Args:
            dataloader: Training data loader
            optimizer: Optimizer
            epochs: Total epochs
            start_epoch: Resume from this epoch
            start_step: Resume from this step
        """

        for epoch in range(start_epoch, epochs):
            for step, batch in enumerate(dataloader):
                if epoch == start_epoch and step < start_step:
                    continue  # Skip to resume point

                # Training step
                loss = self.model(batch['anchor_ids'], batch['positive_ids'])
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()

                # Periodic checkpointing
                if self.should_checkpoint():
                    self.save_checkpoint(epoch, step, optimizer)

            # End-of-epoch checkpoint
            self.save_checkpoint(epoch, step, optimizer)

        print("Training complete!")
```

:::{.callout-tip}
## Cost Optimization Strategies

**Immediate savings (no quality impact):**
1. **Spot instances**: 50-90% discount (with checkpointing)
2. **Mixed precision**: 2-3× speedup → 50-70% cost reduction
3. **Reserved instances**: 30-50% discount for long-term projects
4. **Multi-cloud**: Compare prices across AWS/GCP/Azure

**Advanced optimizations:**
1. **Early stopping**: Halt when validation loss plateaus
2. **Hyperparameter search efficiency**: Use Bayesian optimization, not grid search
3. **Model distillation**: Train large model, deploy small model
4. **Sparse training**: Train only subset of parameters

**Typical cost breakdown (100-GPU training):**
- Hardware: 70% (can optimize with spot instances)
- Storage: 10% (use cheaper object storage)
- Network: 10% (minimize cross-region transfer)
- Other: 10% (monitoring, logging, etc.)
:::

## Key Takeaways

- **Distributed training is essential at scale**: Data parallelism for throughput, model parallelism for large embedding tables, and pipeline parallelism for deep architectures combine to enable trillion-row training in reasonable time

- **Gradient accumulation enables large effective batch sizes**: Split large batches into micro-batches to fit memory constraints while maintaining the benefits of large-batch contrastive learning (16K-32K samples)

- **Mixed precision training provides 2-3× speedup**: FP16 computation on Tensor Cores with FP32 master weights maintains numerical stability while dramatically reducing memory usage and accelerating training

- **Memory optimization unlocks larger models**: Gradient checkpointing, optimizer state quantization (8-bit Adam), and efficient activation management reduce memory footprint by 10-50×, enabling BERT-scale models on single GPUs

- **Multi-node training scales to hundreds of GPUs**: Proper configuration of distributed samplers, learning rate scaling, and network topology awareness enable near-linear scaling to 64+ GPUs with 40-50× speedup

- **Cost optimization is critical for sustainable training**: Spot instances (50-90% savings), mixed precision speedup, and efficient checkpointing reduce training costs from $100K to $10K-$30K for large models

- **Communication is the bottleneck at scale**: Gradient synchronization, activation gathering, and cross-node communication limit speedup; overlap computation with communication and use gradient compression to mitigate

## Looking Ahead

This chapter covered the computational techniques for training embedding models at scale. Chapter 11 shifts focus from training to inference, exploring high-performance vector operations for serving embeddings in production: optimized similarity search algorithms, approximate nearest neighbor (ANN) methods, GPU acceleration for vector operations, memory-mapped storage strategies, and parallel query processing that enables sub-millisecond similarity search across billion-vector indices.

## Further Reading

### Distributed Training
- Li, Shen, et al. (2020). "PyTorch Distributed: Experiences on Accelerating Data Parallel Training." VLDB.
- Shoeybi, Mohammad, et al. (2019). "Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism." arXiv:1909.08053.
- Rajbhandari, Samyam, et al. (2020). "ZeRO: Memory Optimizations Toward Training Trillion Parameter Models." SC20.

### Mixed Precision Training
- Micikevicius, Paulius, et al. (2018). "Mixed Precision Training." ICLR.
- Narang, Sharan, et al. (2018). "Mixed Precision Training With 8-bit Floating Point." arXiv:1905.12334.

### Memory Optimization
- Chen, Tianqi, et al. (2016). "Training Deep Nets with Sublinear Memory Cost." arXiv:1604.06174.
- Sohoni, Nimit, et al. (2019). "Low-Memory Neural Network Training." arXiv:1904.10631.
- Dettmers, Tim, et al. (2022). "8-bit Optimizers via Block-wise Quantization." arXiv:2110.02861.

### Large-Scale Training Systems
- Jia, Xianyan, et al. (2018). "Highly Scalable Deep Learning Training System with Mixed-Precision." arXiv:1807.11205.
- Sergeev, Alexander, and Mike Del Balso (2018). "Horovod: Fast and Easy Distributed Deep Learning in TensorFlow." arXiv:1802.05799.
- Paszke, Adam, et al. (2019). "PyTorch: An Imperative Style, High-Performance Deep Learning Library." NeurIPS.

### Cost Optimization
- Chaudhary, Vinay, et al. (2020). "Balancing Efficiency and Flexibility for DNN Acceleration via Temporal GPU-Systolic Array Integration." DAC.
- Yang, Tianyi, et al. (2021). "Toward Efficient Deep Learning in the Cloud: Resource Provisioning and Workload Scheduling." IEEE Cloud Computing.
