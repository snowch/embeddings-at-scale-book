# Self-Supervised Learning Pipelines {#sec-self-supervised-learning}

:::{.callout-note}
## Chapter Overview
While contrastive learning (Chapter 5) and Siamese networks (Chapter 6) require labeled pairs or triplets, self-supervised learning unlocks the ability to learn from unlabeled data at unprecedented scale. This chapter explores self-supervised techniques that leverage the inherent structure of data to create powerful embeddings without manual annotation. We cover masked language modeling for domain-specific text, vision transformers for industrial imagery, time-series forecasting approaches, and multi-modal self-supervision strategies. These techniques enable enterprises to train embeddings on trillions of unlabeled documents, images, and sensor readings—data that already exists but was previously unusable for training.
:::

## Self-Supervised Learning for Unlabeled Enterprise Data

The fundamental challenge facing enterprise AI: **you have petabytes of data but almost no labels**. Traditional supervised learning requires expensive manual annotation. Self-supervised learning solves this by turning the data itself into both input and supervision.

### The Self-Supervised Paradigm

Self-supervised learning creates "pretext tasks" where the model must predict part of the input from other parts. The key insight: **by learning to solve these pretext tasks, the model develops representations that capture the underlying structure of the data**.

Common pretext tasks:
- **Masked prediction**: Predict hidden parts (BERT, MAE)
- **Next token prediction**: Predict future content (GPT, autoregressive models)
- **Contrastive prediction**: Distinguish augmented views (SimCLR, MoCo)
- **Reconstruction**: Rebuild input from transformed version (autoencoders)

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from transformers import AutoModel, AutoTokenizer
import numpy as np

class SelfSupervisedEmbeddingFramework:
    """
    Framework for self-supervised learning on enterprise data

    Supports multiple pretext tasks:
    - Masked prediction (for text, tabular data)
    - Contrastive learning (for all data types)
    - Reconstruction (for images, time-series)

    Use cases:
    - Learn from millions of unlabeled documents
    - Train on industrial imagery without manual labeling
    - Build embeddings from sensor data streams
    - Leverage historical transaction logs
    """

    def __init__(
        self,
        encoder_model,
        pretext_task='masked',
        embedding_dim=768,
        mask_probability=0.15
    ):
        """
        Args:
            encoder_model: Neural network encoder (BERT, ResNet, custom)
            pretext_task: 'masked', 'contrastive', or 'reconstruction'
            embedding_dim: Dimension of learned embeddings
            mask_probability: Probability of masking for masked prediction
        """
        self.encoder = encoder_model
        self.pretext_task = pretext_task
        self.embedding_dim = embedding_dim
        self.mask_probability = mask_probability

        # Prediction head depends on pretext task
        if pretext_task == 'masked':
            # For masked prediction: predict original tokens
            self.prediction_head = nn.Linear(embedding_dim, embedding_dim)
        elif pretext_task == 'contrastive':
            # For contrastive learning: projection head
            self.projection_head = nn.Sequential(
                nn.Linear(embedding_dim, embedding_dim),
                nn.ReLU(),
                nn.Linear(embedding_dim, 128)
            )
        elif pretext_task == 'reconstruction':
            # For reconstruction: decoder network
            self.decoder = self._build_decoder(embedding_dim)

    def _build_decoder(self, embedding_dim):
        """Build decoder for reconstruction tasks"""
        return nn.Sequential(
            nn.Linear(embedding_dim, embedding_dim * 2),
            nn.ReLU(),
            nn.Linear(embedding_dim * 2, embedding_dim * 4),
            nn.ReLU(),
            nn.Linear(embedding_dim * 4, embedding_dim)
        )

    def create_pretext_task(self, batch):
        """
        Create pretext task from unlabeled batch

        Args:
            batch: Unlabeled data (batch_size, seq_len, features)

        Returns:
            inputs: Modified inputs for pretext task
            targets: Targets for pretext task
            mask: Positions to predict (for masked tasks)
        """
        if self.pretext_task == 'masked':
            return self._create_masked_task(batch)
        elif self.pretext_task == 'contrastive':
            return self._create_contrastive_task(batch)
        elif self.pretext_task == 'reconstruction':
            return self._create_reconstruction_task(batch)

    def _create_masked_task(self, batch):
        """
        Create masked prediction task

        Randomly mask tokens and predict them from context
        """
        batch_size, seq_len, features = batch.shape

        # Create mask (True = masked position)
        mask = torch.rand(batch_size, seq_len) < self.mask_probability

        # Clone batch for inputs
        inputs = batch.clone()

        # Replace masked positions with zeros or special token
        inputs[mask] = 0

        # Targets are original values at masked positions
        targets = batch.clone()

        return inputs, targets, mask

    def _create_contrastive_task(self, batch):
        """
        Create contrastive task with data augmentations

        Generate two augmented views of each sample
        """
        # Apply augmentations (specific to data type)
        view1 = self._augment(batch)
        view2 = self._augment(batch)

        return (view1, view2), None, None

    def _create_reconstruction_task(self, batch):
        """
        Create reconstruction task

        Add noise and predict clean version
        """
        # Add noise
        noise = torch.randn_like(batch) * 0.1
        noisy_batch = batch + noise

        return noisy_batch, batch, None

    def _augment(self, batch):
        """
        Apply data augmentation (override for specific data types)

        For text: dropout, span deletion, synonym replacement
        For images: cropping, color jitter, blur
        For time-series: masking, jittering, scaling
        """
        # Simple example: add small noise
        noise = torch.randn_like(batch) * 0.05
        return batch + noise

    def forward(self, inputs):
        """
        Forward pass through encoder

        Args:
            inputs: Batch of inputs

        Returns:
            embeddings: Learned embeddings
        """
        return self.encoder(inputs)

    def compute_loss(self, inputs, targets, mask=None):
        """
        Compute loss for pretext task

        Args:
            inputs: Input batch
            targets: Target batch (may be None for contrastive)
            mask: Mask for positions to predict (may be None)

        Returns:
            loss: Scalar loss
            metrics: Dict of training metrics
        """
        if self.pretext_task == 'masked':
            return self._compute_masked_loss(inputs, targets, mask)
        elif self.pretext_task == 'contrastive':
            return self._compute_contrastive_loss(inputs)
        elif self.pretext_task == 'reconstruction':
            return self._compute_reconstruction_loss(inputs, targets)

    def _compute_masked_loss(self, inputs, targets, mask):
        """Compute loss for masked prediction"""
        # Encode inputs
        embeddings = self.encoder(inputs)

        # Predict masked tokens
        predictions = self.prediction_head(embeddings)

        # Compute loss only at masked positions
        loss = F.mse_loss(
            predictions[mask],
            targets[mask]
        )

        with torch.no_grad():
            # Compute accuracy
            accuracy = ((predictions[mask] - targets[mask]).abs() < 0.1).float().mean()

        return loss, {
            'loss': loss.item(),
            'accuracy': accuracy.item(),
            'masked_positions': mask.sum().item()
        }

    def _compute_contrastive_loss(self, views):
        """Compute contrastive loss (NT-Xent)"""
        view1, view2 = views

        # Encode both views
        embeddings1 = self.encoder(view1)
        embeddings2 = self.encoder(view2)

        # Project to contrastive space
        z1 = self.projection_head(embeddings1)
        z2 = self.projection_head(embeddings2)

        # Normalize
        z1 = F.normalize(z1, dim=1)
        z2 = F.normalize(z2, dim=1)

        # Compute similarity matrix
        batch_size = z1.shape[0]
        z = torch.cat([z1, z2], dim=0)
        similarity_matrix = torch.mm(z, z.T)

        # Temperature scaling
        temperature = 0.5
        similarity_matrix = similarity_matrix / temperature

        # Create labels: positive pairs are (i, i+batch_size)
        labels = torch.arange(batch_size).to(z1.device)
        labels = torch.cat([labels + batch_size, labels])

        # Mask out self-similarity
        mask = torch.eye(2 * batch_size, dtype=torch.bool).to(z1.device)
        similarity_matrix.masked_fill_(mask, float('-inf'))

        # Cross entropy loss
        loss = F.cross_entropy(similarity_matrix, labels)

        with torch.no_grad():
            # Top-1 accuracy
            predictions = similarity_matrix.argmax(dim=1)
            accuracy = (predictions == labels).float().mean()

        return loss, {
            'loss': loss.item(),
            'accuracy': accuracy.item()
        }

    def _compute_reconstruction_loss(self, noisy_inputs, clean_targets):
        """Compute reconstruction loss"""
        # Encode noisy inputs
        embeddings = self.encoder(noisy_inputs)

        # Decode to reconstruct
        reconstructions = self.decoder(embeddings)

        # MSE loss
        loss = F.mse_loss(reconstructions, clean_targets)

        with torch.no_grad():
            # PSNR (Peak Signal-to-Noise Ratio)
            mse = ((reconstructions - clean_targets) ** 2).mean()
            psnr = 10 * torch.log10(1.0 / mse)

        return loss, {
            'loss': loss.item(),
            'psnr': psnr.item()
        }


def train_self_supervised(
    model,
    dataloader,
    optimizer,
    device,
    num_epochs=10
):
    """
    Train self-supervised model

    Args:
        model: SelfSupervisedEmbeddingFramework
        dataloader: DataLoader with unlabeled data
        optimizer: PyTorch optimizer
        device: 'cuda' or 'cpu'
        num_epochs: Number of training epochs
    """
    model.encoder.train()

    for epoch in range(num_epochs):
        total_loss = 0
        total_accuracy = 0

        for batch_idx, batch in enumerate(dataloader):
            # Move to device
            batch = batch.to(device)

            # Create pretext task
            inputs, targets, mask = model.create_pretext_task(batch)

            # Forward pass
            optimizer.zero_grad()
            loss, metrics = model.compute_loss(inputs, targets, mask)

            # Backward pass
            loss.backward()
            optimizer.step()

            total_loss += metrics['loss']
            if 'accuracy' in metrics:
                total_accuracy += metrics['accuracy']

            if batch_idx % 100 == 0:
                print(f"Epoch {epoch}, Batch {batch_idx}, Loss: {metrics['loss']:.4f}")

        avg_loss = total_loss / len(dataloader)
        avg_accuracy = total_accuracy / len(dataloader)

        print(f"Epoch {epoch} complete. Avg Loss: {avg_loss:.4f}, Avg Accuracy: {avg_accuracy:.4f}")
```

:::{.callout-tip}
## Choosing the Right Pretext Task

**Masked prediction**: Best for structured data with natural ordering (text, sequences, time-series). Captures bidirectional context.

**Contrastive learning**: Best when you can define meaningful augmentations. Works well for images, audio, multimodal data.

**Reconstruction**: Best for high-dimensional data where reconstruction is meaningful. Good for images, sensor data.

**Rule of thumb**: If your data has natural ordering, use masked prediction. If augmentations preserve semantics, use contrastive. If neither, try reconstruction.
:::

### Enterprise Self-Supervised Pipeline

Production self-supervised learning requires careful data management and training infrastructure:

```python
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel
from torch.utils.data import DistributedSampler

class EnterpriseSelfsupervisedPipeline:
    """
    Production self-supervised learning pipeline

    Features:
    - Distributed training across multiple GPUs/nodes
    - Checkpointing and recovery
    - Monitoring and logging
    - Efficient data loading from data lake
    - Model versioning
    """

    def __init__(
        self,
        model,
        data_source,
        batch_size=256,
        num_workers=8,
        checkpoint_dir='./checkpoints',
        log_dir='./logs'
    ):
        self.model = model
        self.data_source = data_source
        self.batch_size = batch_size
        self.num_workers = num_workers
        self.checkpoint_dir = checkpoint_dir
        self.log_dir = log_dir

        # Setup distributed training if available
        self.world_size = torch.cuda.device_count()
        self.is_distributed = self.world_size > 1

    def setup_distributed(self):
        """Initialize distributed training"""
        if self.is_distributed:
            dist.init_process_group(backend='nccl')
            local_rank = dist.get_rank()
            torch.cuda.set_device(local_rank)

            # Wrap model in DDP
            self.model = DistributedDataParallel(
                self.model,
                device_ids=[local_rank]
            )

    def create_dataloader(self):
        """Create efficient dataloader for unlabeled data"""
        # In production, this would load from S3, GCS, or data lake
        dataset = UnlabeledEnterpriseDataset(self.data_source)

        if self.is_distributed:
            sampler = DistributedSampler(dataset)
        else:
            sampler = None

        dataloader = torch.utils.data.DataLoader(
            dataset,
            batch_size=self.batch_size,
            sampler=sampler,
            num_workers=self.num_workers,
            pin_memory=True,
            prefetch_factor=2
        )

        return dataloader

    def train(self, num_epochs=100, learning_rate=1e-4):
        """
        Train self-supervised model

        Args:
            num_epochs: Number of epochs
            learning_rate: Learning rate
        """
        # Setup
        self.setup_distributed()
        dataloader = self.create_dataloader()
        optimizer = torch.optim.AdamW(
            self.model.parameters(),
            lr=learning_rate,
            weight_decay=0.01
        )

        # Learning rate scheduler
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
            optimizer,
            T_max=num_epochs
        )

        # Training loop
        for epoch in range(num_epochs):
            self.model.train()

            epoch_loss = 0
            epoch_samples = 0

            for batch_idx, batch in enumerate(dataloader):
                batch = batch.cuda()

                # Create pretext task
                inputs, targets, mask = self.model.module.create_pretext_task(batch)

                # Forward and backward
                optimizer.zero_grad()
                loss, metrics = self.model.module.compute_loss(inputs, targets, mask)
                loss.backward()

                # Gradient clipping
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)

                optimizer.step()

                epoch_loss += loss.item() * batch.size(0)
                epoch_samples += batch.size(0)

            # Update learning rate
            scheduler.step()

            # Log metrics
            avg_loss = epoch_loss / epoch_samples
            self._log_metrics(epoch, avg_loss, scheduler.get_last_lr()[0])

            # Save checkpoint
            if epoch % 10 == 0:
                self.save_checkpoint(epoch, avg_loss)

    def save_checkpoint(self, epoch, loss):
        """Save model checkpoint"""
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'loss': loss
        }

        path = f"{self.checkpoint_dir}/checkpoint_epoch_{epoch}.pt"
        torch.save(checkpoint, path)
        print(f"Checkpoint saved: {path}")

    def load_checkpoint(self, path):
        """Load model checkpoint"""
        checkpoint = torch.load(path)
        self.model.load_state_dict(checkpoint['model_state_dict'])
        print(f"Checkpoint loaded from {path}")
        return checkpoint['epoch'], checkpoint['loss']

    def _log_metrics(self, epoch, loss, lr):
        """Log training metrics"""
        print(f"Epoch {epoch}: Loss={loss:.4f}, LR={lr:.6f}")

        # In production, log to MLflow, Weights & Biases, etc.


class UnlabeledEnterpriseDataset(torch.utils.data.Dataset):
    """
    Dataset for loading unlabeled enterprise data

    In production, this would:
    - Stream from S3/GCS/Azure Blob
    - Handle multiple data formats (parquet, JSON, CSV)
    - Apply preprocessing on-the-fly
    - Cache frequently accessed data
    """

    def __init__(self, data_source):
        self.data_source = data_source
        # Load metadata about available data
        self.data_files = self._discover_data()

    def _discover_data(self):
        """Discover available data files"""
        # In production: list files from data lake
        # For now, return dummy list
        return ['file1.parquet', 'file2.parquet']

    def __len__(self):
        # Return total number of samples
        return 1000000  # Dummy value

    def __getitem__(self, idx):
        # Load and preprocess sample
        # In production: load from S3, apply preprocessing
        return torch.randn(512, 768)  # Dummy data
```

:::{.callout-warning}
## Production Considerations

**Data Quality**: Self-supervised learning amplifies data quality issues. Bad data → bad embeddings. Filter corrupted samples before training.

**Compute Budget**: Training on billions of samples requires significant compute. For 100M parameters × 1B tokens, expect 100-1000 GPU-hours.

**Checkpoint Frequency**: Save checkpoints every 1-2 hours of training (not epochs). Spot instance interruptions are common.

**Monitoring**: Track loss trends, gradient norms, and embedding quality metrics. Diverging loss indicates instability.
:::

## Masked Language Modeling for Domain-Specific Text

Masked Language Modeling (MLM), popularized by BERT, is the foundation of modern NLP. For enterprises, the key is adapting MLM to domain-specific vocabulary and writing styles.

### MLM Fundamentals

The MLM objective: **predict randomly masked tokens from surrounding context**. This forces the model to learn bidirectional representations that capture semantic and syntactic patterns.

```python
from transformers import BertConfig, BertForMaskedLM, BertTokenizer
from transformers import DataCollatorForLanguageModeling
from transformers import Trainer, TrainingArguments
import torch

class DomainSpecificMLM:
    """
    Masked Language Modeling for domain-specific text

    Use cases:
    - Legal documents: Learn legal terminology and structure
    - Medical records: Capture clinical language patterns
    - Financial reports: Understand financial jargon
    - Scientific papers: Model academic writing style
    - Customer support: Learn product-specific terminology
    """

    def __init__(
        self,
        domain='general',
        vocab_size=30000,
        hidden_size=768,
        num_layers=12,
        num_heads=12
    ):
        """
        Args:
            domain: Domain name for logging
            vocab_size: Size of vocabulary (larger for specialized domains)
            hidden_size: Dimension of embeddings
            num_layers: Number of transformer layers
            num_heads: Number of attention heads
        """
        self.domain = domain

        # Configure BERT architecture
        self.config = BertConfig(
            vocab_size=vocab_size,
            hidden_size=hidden_size,
            num_hidden_layers=num_layers,
            num_attention_heads=num_heads,
            intermediate_size=hidden_size * 4,
            max_position_embeddings=512
        )

        # Initialize model
        self.model = BertForMaskedLM(self.config)
        self.tokenizer = None

    def train_tokenizer(self, text_corpus, save_path='./tokenizer'):
        """
        Train domain-specific tokenizer

        Critical for specialized domains: generic tokenizers split
        domain terms into subwords, losing semantic coherence.

        Args:
            text_corpus: Iterator of text strings
            save_path: Path to save tokenizer
        """
        from tokenizers import Tokenizer
        from tokenizers.models import BPE
        from tokenizers.trainers import BpeTrainer
        from tokenizers.pre_tokenizers import Whitespace

        # Initialize tokenizer
        tokenizer = Tokenizer(BPE(unk_token="[UNK]"))
        tokenizer.pre_tokenizer = Whitespace()

        # Configure trainer
        trainer = BpeTrainer(
            vocab_size=self.config.vocab_size,
            special_tokens=["[PAD]", "[UNK]", "[CLS]", "[SEP]", "[MASK]"],
            show_progress=True
        )

        # Train on corpus
        tokenizer.train_from_iterator(text_corpus, trainer=trainer)

        # Save
        tokenizer.save(f"{save_path}/tokenizer.json")

        # Create HuggingFace tokenizer
        self.tokenizer = BertTokenizer.from_pretrained(save_path)

        print(f"Tokenizer trained and saved to {save_path}")

    def prepare_dataset(self, texts, tokenizer):
        """
        Prepare dataset for MLM training

        Args:
            texts: List of text strings
            tokenizer: HuggingFace tokenizer

        Returns:
            Dataset ready for MLM training
        """
        from datasets import Dataset

        # Create dataset
        dataset = Dataset.from_dict({'text': texts})

        # Tokenize
        def tokenize_function(examples):
            return tokenizer(
                examples['text'],
                truncation=True,
                max_length=512,
                padding='max_length'
            )

        tokenized_dataset = dataset.map(
            tokenize_function,
            batched=True,
            remove_columns=['text']
        )

        return tokenized_dataset

    def train(
        self,
        train_dataset,
        output_dir='./mlm_model',
        num_epochs=3,
        batch_size=32,
        learning_rate=5e-5,
        mlm_probability=0.15
    ):
        """
        Train MLM model

        Args:
            train_dataset: Tokenized dataset
            output_dir: Where to save model
            num_epochs: Number of training epochs
            batch_size: Training batch size
            learning_rate: Learning rate
            mlm_probability: Probability of masking each token
        """
        # Data collator handles masking
        data_collator = DataCollatorForLanguageModeling(
            tokenizer=self.tokenizer,
            mlm=True,
            mlm_probability=mlm_probability
        )

        # Training arguments
        training_args = TrainingArguments(
            output_dir=output_dir,
            num_train_epochs=num_epochs,
            per_device_train_batch_size=batch_size,
            learning_rate=learning_rate,
            weight_decay=0.01,
            warmup_steps=500,
            logging_steps=100,
            save_steps=1000,
            save_total_limit=3,
            fp16=torch.cuda.is_available(),  # Mixed precision training
            dataloader_num_workers=4,
            remove_unused_columns=False
        )

        # Trainer
        trainer = Trainer(
            model=self.model,
            args=training_args,
            data_collator=data_collator,
            train_dataset=train_dataset
        )

        # Train
        print(f"Starting MLM training for domain: {self.domain}")
        trainer.train()

        # Save final model
        trainer.save_model(output_dir)
        print(f"Model saved to {output_dir}")

    def get_embeddings(self, texts, layer=-1):
        """
        Extract embeddings from trained model

        Args:
            texts: List of text strings
            layer: Which layer to extract from (-1 = last layer)

        Returns:
            Embeddings: (num_texts, hidden_size)
        """
        self.model.eval()

        # Tokenize
        inputs = self.tokenizer(
            texts,
            padding=True,
            truncation=True,
            max_length=512,
            return_tensors='pt'
        )

        # Forward pass
        with torch.no_grad():
            outputs = self.model.bert(
                **inputs,
                output_hidden_states=True
            )

        # Extract embeddings from specified layer
        hidden_states = outputs.hidden_states[layer]

        # Mean pooling over sequence length
        embeddings = hidden_states.mean(dim=1)

        return embeddings.numpy()


# Example: Training domain-specific MLM
def example_legal_mlm():
    """
    Example: Train MLM on legal documents

    Legal text has specialized terminology (tort, plaintiff, jurisdiction)
    and structure (citations, precedents) that generic models miss.
    """
    # Initialize
    legal_mlm = DomainSpecificMLM(
        domain='legal',
        vocab_size=32000,
        hidden_size=768,
        num_layers=12
    )

    # Sample legal texts (in production: millions of documents)
    legal_corpus = [
        "The plaintiff filed a motion for summary judgment...",
        "Under tort law, negligence requires duty, breach, causation...",
        "Precedent established in Smith v. Jones (2020) supports...",
        # ... millions more
    ]

    # Train domain-specific tokenizer
    print("Training legal tokenizer...")
    legal_mlm.train_tokenizer(legal_corpus, save_path='./legal_tokenizer')

    # Prepare dataset
    print("Preparing dataset...")
    train_dataset = legal_mlm.prepare_dataset(legal_corpus, legal_mlm.tokenizer)

    # Train MLM
    print("Training MLM...")
    legal_mlm.train(
        train_dataset,
        output_dir='./legal_mlm_model',
        num_epochs=3,
        batch_size=16
    )

    # Extract embeddings
    print("Extracting embeddings...")
    test_texts = [
        "The defendant's motion to dismiss was denied.",
        "Statutory interpretation follows the plain meaning rule."
    ]
    embeddings = legal_mlm.get_embeddings(test_texts)

    print(f"Embeddings shape: {embeddings.shape}")

    return legal_mlm
```

### Advanced MLM Techniques

For production deployments, basic MLM can be enhanced with several techniques:

```python
class AdvancedMLM:
    """
    Advanced MLM with production optimizations

    Enhancements:
    - Whole word masking: Mask entire words, not subwords
    - Dynamic masking: Different masks each epoch
    - Span masking: Mask contiguous spans
    - Entity masking: Preferentially mask named entities
    """

    def __init__(self, base_model, tokenizer):
        self.model = base_model
        self.tokenizer = tokenizer

    def whole_word_masking(self, input_ids, mlm_probability=0.15):
        """
        Mask entire words instead of subword tokens

        Better for learning word-level semantics
        """
        # Get word boundaries
        words = []
        current_word = []

        for idx, token_id in enumerate(input_ids):
            token = self.tokenizer.decode([token_id])

            if token.startswith('##'):
                # Continuation of previous word
                current_word.append(idx)
            else:
                # New word
                if current_word:
                    words.append(current_word)
                current_word = [idx]

        if current_word:
            words.append(current_word)

        # Sample words to mask
        num_words_to_mask = max(1, int(len(words) * mlm_probability))
        words_to_mask = np.random.choice(
            len(words),
            size=num_words_to_mask,
            replace=False
        )

        # Create mask
        mask = torch.zeros_like(input_ids, dtype=torch.bool)
        for word_idx in words_to_mask:
            for token_idx in words[word_idx]:
                mask[token_idx] = True

        return mask

    def span_masking(self, input_ids, span_length=3, mlm_probability=0.15):
        """
        Mask contiguous spans of tokens

        Encourages learning longer-range dependencies
        Based on SpanBERT
        """
        seq_len = len(input_ids)
        num_masks = int(seq_len * mlm_probability / span_length)

        mask = torch.zeros_like(input_ids, dtype=torch.bool)

        for _ in range(num_masks):
            # Sample span start
            start = np.random.randint(0, max(1, seq_len - span_length))

            # Mask span
            mask[start:start + span_length] = True

        return mask

    def entity_aware_masking(self, text, entities, mlm_probability=0.15):
        """
        Preferentially mask named entities

        Helps model learn domain-specific entities

        Args:
            text: Input text
            entities: List of (entity_text, entity_type) tuples
            mlm_probability: Masking probability

        Returns:
            Masked input_ids and labels
        """
        # Tokenize
        tokens = self.tokenizer.tokenize(text)
        input_ids = self.tokenizer.convert_tokens_to_ids(tokens)

        # Find entity positions
        entity_positions = []
        for entity_text, entity_type in entities:
            entity_tokens = self.tokenizer.tokenize(entity_text)

            # Find entity in tokens
            for i in range(len(tokens) - len(entity_tokens) + 1):
                if tokens[i:i+len(entity_tokens)] == entity_tokens:
                    entity_positions.extend(range(i, i + len(entity_tokens)))

        # Create mask
        mask = torch.zeros(len(input_ids), dtype=torch.bool)

        # Mask entities with higher probability
        entity_mask_prob = mlm_probability * 3  # 3x more likely to mask entities
        regular_mask_prob = mlm_probability

        for idx in range(len(input_ids)):
            prob = entity_mask_prob if idx in entity_positions else regular_mask_prob
            if np.random.rand() < prob:
                mask[idx] = True

        return input_ids, mask


class DomainAdaptiveMLM:
    """
    Domain adaptation for pre-trained MLM

    Strategy: Start with pre-trained BERT, continue training on domain data

    Benefits:
    - Faster than training from scratch
    - Retains general language understanding
    - Adapts to domain specifics
    """

    def __init__(self, pretrained_model_name='bert-base-uncased'):
        """
        Args:
            pretrained_model_name: HuggingFace model to adapt
        """
        self.model = BertForMaskedLM.from_pretrained(pretrained_model_name)
        self.tokenizer = BertTokenizer.from_pretrained(pretrained_model_name)

    def adapt_to_domain(
        self,
        domain_texts,
        output_dir='./adapted_model',
        num_epochs=3,
        learning_rate=2e-5  # Lower LR for adaptation
    ):
        """
        Adapt pre-trained model to domain

        Args:
            domain_texts: Domain-specific texts
            output_dir: Where to save adapted model
            num_epochs: Adaptation epochs (fewer than training from scratch)
            learning_rate: Lower learning rate preserves pre-training
        """
        from datasets import Dataset

        # Prepare dataset
        dataset = Dataset.from_dict({'text': domain_texts})

        def tokenize_function(examples):
            return self.tokenizer(
                examples['text'],
                truncation=True,
                max_length=512,
                padding='max_length'
            )

        tokenized_dataset = dataset.map(
            tokenize_function,
            batched=True,
            remove_columns=['text']
        )

        # Data collator
        data_collator = DataCollatorForLanguageModeling(
            tokenizer=self.tokenizer,
            mlm=True,
            mlm_probability=0.15
        )

        # Training arguments (conservative for adaptation)
        training_args = TrainingArguments(
            output_dir=output_dir,
            num_train_epochs=num_epochs,
            per_device_train_batch_size=16,
            learning_rate=learning_rate,
            weight_decay=0.01,
            warmup_ratio=0.1,
            logging_steps=100,
            save_steps=1000,
            fp16=torch.cuda.is_available()
        )

        # Train
        trainer = Trainer(
            model=self.model,
            args=training_args,
            data_collator=data_collator,
            train_dataset=tokenized_dataset
        )

        print("Adapting model to domain...")
        trainer.train()

        trainer.save_model(output_dir)
        print(f"Adapted model saved to {output_dir}")
```

:::{.callout-tip}
## MLM Training Best Practices

**Tokenizer First**: Always train a domain-specific tokenizer before MLM. Generic tokenizers fragment domain terms.

**Masking Strategy**: Use whole-word masking for semantic learning, span masking for longer dependencies.

**Adaptation vs. Scratch**: If you have < 100M tokens, adapt pre-trained model. If > 1B tokens and very specialized domain, train from scratch.

**Hyperparameters**: Standard BERT hyperparameters (lr=5e-5, batch=32, warmup=10%) work well. For adaptation, use lr=2e-5.

**Compute Budget**: 100M parameters × 1B tokens ≈ 500 GPU-hours. Use mixed precision (fp16) to reduce by 2x.
:::

## Vision Transformers for Industrial Imagery

Vision Transformers (ViTs) combined with self-supervised learning enable training on unlabeled industrial imagery—manufacturing defects, medical scans, satellite images, security footage.

### Self-Supervised Vision Transformers

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from torchvision import transforms

class MaskedAutoencoderViT(nn.Module):
    """
    Masked Autoencoder for Vision Transformers (MAE)

    Self-supervised learning for images:
    1. Mask random patches (75% of image)
    2. Encode visible patches with ViT
    3. Decode to reconstruct masked patches

    Use cases:
    - Manufacturing defect detection (learn from normal images)
    - Medical imaging (train on unlabeled scans)
    - Satellite imagery (learn from millions of images)
    - Security footage (understand visual patterns)

    Reference: "Masked Autoencoders Are Scalable Vision Learners" (He et al., 2021)
    """

    def __init__(
        self,
        img_size=224,
        patch_size=16,
        in_channels=3,
        embed_dim=768,
        depth=12,
        num_heads=12,
        decoder_embed_dim=512,
        decoder_depth=8,
        mask_ratio=0.75
    ):
        """
        Args:
            img_size: Input image size
            patch_size: Size of image patches
            in_channels: Number of input channels (3 for RGB)
            embed_dim: Embedding dimension
            depth: Number of encoder layers
            num_heads: Number of attention heads
            decoder_embed_dim: Decoder embedding dimension
            decoder_depth: Number of decoder layers
            mask_ratio: Ratio of patches to mask (0.75 = 75%)
        """
        super().__init__()

        self.patch_size = patch_size
        self.num_patches = (img_size // patch_size) ** 2
        self.mask_ratio = mask_ratio

        # Patch embedding
        self.patch_embed = nn.Conv2d(
            in_channels,
            embed_dim,
            kernel_size=patch_size,
            stride=patch_size
        )

        # Positional embedding
        self.pos_embed = nn.Parameter(
            torch.zeros(1, self.num_patches, embed_dim)
        )

        # Encoder (ViT blocks)
        self.encoder = nn.ModuleList([
            TransformerBlock(embed_dim, num_heads)
            for _ in range(depth)
        ])

        # Decoder
        self.decoder_embed = nn.Linear(embed_dim, decoder_embed_dim)
        self.decoder_pos_embed = nn.Parameter(
            torch.zeros(1, self.num_patches, decoder_embed_dim)
        )
        self.decoder = nn.ModuleList([
            TransformerBlock(decoder_embed_dim, num_heads)
            for _ in range(decoder_depth)
        ])

        # Reconstruction head
        self.decoder_pred = nn.Linear(
            decoder_embed_dim,
            patch_size ** 2 * in_channels
        )

        # Mask token (learnable)
        self.mask_token = nn.Parameter(torch.zeros(1, 1, decoder_embed_dim))

        self._init_weights()

    def _init_weights(self):
        """Initialize weights"""
        nn.init.trunc_normal_(self.pos_embed, std=0.02)
        nn.init.trunc_normal_(self.decoder_pos_embed, std=0.02)
        nn.init.trunc_normal_(self.mask_token, std=0.02)

    def patchify(self, imgs):
        """
        Convert images to patches

        Args:
            imgs: (batch_size, channels, height, width)

        Returns:
            patches: (batch_size, num_patches, patch_size^2 * channels)
        """
        batch_size = imgs.shape[0]

        # Extract patches
        patches = self.patch_embed(imgs)  # (B, embed_dim, H/P, W/P)
        patches = patches.flatten(2).transpose(1, 2)  # (B, num_patches, embed_dim)

        return patches

    def random_masking(self, x, mask_ratio):
        """
        Random masking of patches

        Args:
            x: (batch_size, num_patches, embed_dim)
            mask_ratio: Ratio of patches to mask

        Returns:
            x_masked: Visible patches only
            mask: Binary mask (1 = masked, 0 = visible)
            ids_restore: Indices to restore original order
        """
        batch_size, num_patches, embed_dim = x.shape
        num_keep = int(num_patches * (1 - mask_ratio))

        # Random shuffle
        noise = torch.rand(batch_size, num_patches, device=x.device)
        ids_shuffle = torch.argsort(noise, dim=1)
        ids_restore = torch.argsort(ids_shuffle, dim=1)

        # Keep first subset (visible patches)
        ids_keep = ids_shuffle[:, :num_keep]
        x_masked = torch.gather(
            x, dim=1,
            index=ids_keep.unsqueeze(-1).repeat(1, 1, embed_dim)
        )

        # Create mask (1 = masked, 0 = visible)
        mask = torch.ones([batch_size, num_patches], device=x.device)
        mask[:, :num_keep] = 0
        mask = torch.gather(mask, dim=1, index=ids_restore)

        return x_masked, mask, ids_restore

    def forward_encoder(self, x):
        """
        Forward pass through encoder

        Args:
            x: Input images (batch_size, channels, height, width)

        Returns:
            encoded: Encoded visible patches
            mask: Binary mask
            ids_restore: Indices for restoring order
        """
        # Patchify
        x = self.patchify(x)

        # Add positional embedding
        x = x + self.pos_embed

        # Random masking
        x, mask, ids_restore = self.random_masking(x, self.mask_ratio)

        # Encoder
        for block in self.encoder:
            x = block(x)

        return x, mask, ids_restore

    def forward_decoder(self, x, ids_restore):
        """
        Forward pass through decoder

        Args:
            x: Encoded patches
            ids_restore: Indices for restoring order

        Returns:
            reconstructed: Reconstructed patches
        """
        # Embed tokens
        x = self.decoder_embed(x)

        # Append mask tokens
        batch_size = x.shape[0]
        mask_tokens = self.mask_token.repeat(
            batch_size,
            ids_restore.shape[1] - x.shape[1],
            1
        )
        x_full = torch.cat([x, mask_tokens], dim=1)

        # Restore original order
        x_full = torch.gather(
            x_full, dim=1,
            index=ids_restore.unsqueeze(-1).repeat(1, 1, x_full.shape[2])
        )

        # Add positional embedding
        x_full = x_full + self.decoder_pos_embed

        # Decoder
        for block in self.decoder:
            x_full = block(x_full)

        # Predict patches
        reconstructed = self.decoder_pred(x_full)

        return reconstructed

    def forward(self, imgs):
        """
        Forward pass

        Args:
            imgs: Input images

        Returns:
            loss: Reconstruction loss
            reconstructed: Reconstructed images
            mask: Binary mask
        """
        # Encode
        encoded, mask, ids_restore = self.forward_encoder(imgs)

        # Decode
        reconstructed = self.forward_decoder(encoded, ids_restore)

        # Compute loss (only on masked patches)
        target = self.patchify(imgs)

        # Mean squared error on masked patches
        loss = (reconstructed - target) ** 2
        loss = (loss * mask.unsqueeze(-1)).sum() / mask.sum()

        return loss, reconstructed, mask

    def get_embeddings(self, imgs):
        """
        Extract embeddings from images

        Args:
            imgs: Input images

        Returns:
            embeddings: Image embeddings
        """
        # Encode without masking
        self.mask_ratio = 0  # Temporarily disable masking
        x = self.patchify(imgs)
        x = x + self.pos_embed

        for block in self.encoder:
            x = block(x)

        # Global average pooling
        embeddings = x.mean(dim=1)

        return embeddings


class TransformerBlock(nn.Module):
    """Transformer block for ViT"""

    def __init__(self, embed_dim, num_heads):
        super().__init__()

        self.attention = nn.MultiheadAttention(
            embed_dim,
            num_heads,
            batch_first=True
        )

        self.mlp = nn.Sequential(
            nn.Linear(embed_dim, embed_dim * 4),
            nn.GELU(),
            nn.Linear(embed_dim * 4, embed_dim)
        )

        self.norm1 = nn.LayerNorm(embed_dim)
        self.norm2 = nn.LayerNorm(embed_dim)

    def forward(self, x):
        # Attention
        x_norm = self.norm1(x)
        attn_out, _ = self.attention(x_norm, x_norm, x_norm)
        x = x + attn_out

        # MLP
        x = x + self.mlp(self.norm2(x))

        return x


def train_mae_on_industrial_images(
    image_dir,
    output_dir='./mae_model',
    num_epochs=100,
    batch_size=256
):
    """
    Train MAE on industrial imagery

    Args:
        image_dir: Directory with unlabeled images
        output_dir: Where to save model
        num_epochs: Number of training epochs
        batch_size: Training batch size
    """
    # Initialize model
    model = MaskedAutoencoderViT(
        img_size=224,
        patch_size=16,
        embed_dim=768,
        depth=12
    ).cuda()

    # Data augmentation (for self-supervised learning)
    transform = transforms.Compose([
        transforms.Resize(256),
        transforms.RandomCrop(224),
        transforms.RandomHorizontalFlip(),
        transforms.ColorJitter(0.4, 0.4, 0.4),
        transforms.ToTensor(),
        transforms.Normalize(
            mean=[0.485, 0.456, 0.406],
            std=[0.229, 0.224, 0.225]
        )
    ])

    # Load dataset
    from torchvision.datasets import ImageFolder
    dataset = ImageFolder(image_dir, transform=transform)
    dataloader = torch.utils.data.DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=8,
        pin_memory=True
    )

    # Optimizer
    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=1.5e-4,
        betas=(0.9, 0.95),
        weight_decay=0.05
    )

    # Learning rate scheduler
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
        optimizer,
        T_max=num_epochs
    )

    # Training loop
    for epoch in range(num_epochs):
        model.train()
        total_loss = 0

        for imgs, _ in dataloader:
            imgs = imgs.cuda()

            # Forward
            optimizer.zero_grad()
            loss, _, _ = model(imgs)

            # Backward
            loss.backward()
            optimizer.step()

            total_loss += loss.item()

        scheduler.step()

        avg_loss = total_loss / len(dataloader)
        print(f"Epoch {epoch}: Loss = {avg_loss:.4f}")

        # Save checkpoint
        if epoch % 10 == 0:
            torch.save(
                model.state_dict(),
                f"{output_dir}/mae_epoch_{epoch}.pt"
            )

    print(f"Training complete. Model saved to {output_dir}")
```

:::{.callout-tip}
## ViT Self-Supervision Best Practices

**Mask Ratio**: MAE uses 75% masking (aggressive!). This works because images have high redundancy. For specialized imagery (e.g., X-rays), try 50-60%.

**Patch Size**: Standard is 16x16 for 224x224 images. For higher resolution (512x512+), use 32x32 patches.

**Augmentation**: Strong augmentations (color jitter, blur) improve robustness. But avoid augmentations that change semantics (e.g., don't flip medical images if orientation matters).

**Compute**: ViT-Base with MAE requires ~100 GPU-hours for 1M images. Use ViT-Small (5.7M params) for faster prototyping.
:::

### Industrial Vision Applications

```python
class IndustrialDefectDetection:
    """
    Defect detection using self-supervised ViT

    Workflow:
    1. Train MAE on normal (defect-free) images
    2. At inference, high reconstruction error indicates anomaly
    3. No labeled defects needed for training!
    """

    def __init__(self, mae_model):
        self.model = mae_model
        self.model.eval()

        # Calibrate threshold on validation set
        self.threshold = None

    def calibrate_threshold(self, normal_images, percentile=95):
        """
        Calibrate anomaly threshold on normal images

        Args:
            normal_images: Batch of normal (non-defective) images
            percentile: Percentile for threshold (95 = 95th percentile)
        """
        reconstruction_errors = []

        with torch.no_grad():
            for img in normal_images:
                loss, _, _ = self.model(img.unsqueeze(0))
                reconstruction_errors.append(loss.item())

        # Set threshold at percentile
        self.threshold = np.percentile(reconstruction_errors, percentile)

        print(f"Threshold calibrated: {self.threshold:.4f}")

    def detect_defect(self, image, return_reconstruction=False):
        """
        Detect defects in image

        Args:
            image: Input image
            return_reconstruction: If True, return reconstructed image

        Returns:
            is_defective: Boolean
            confidence: Anomaly score
            reconstruction: (Optional) reconstructed image
        """
        with torch.no_grad():
            loss, reconstructed, mask = self.model(image.unsqueeze(0))

        # Anomaly score = reconstruction error
        anomaly_score = loss.item()

        # Compare to threshold
        is_defective = anomaly_score > self.threshold

        # Confidence = how far above threshold
        if self.threshold is not None:
            confidence = (anomaly_score - self.threshold) / self.threshold
        else:
            confidence = anomaly_score

        result = {
            'is_defective': is_defective,
            'anomaly_score': anomaly_score,
            'confidence': confidence
        }

        if return_reconstruction:
            result['reconstruction'] = reconstructed
            result['mask'] = mask

        return result


# Example: Manufacturing defect detection
def example_manufacturing_defect_detection():
    """
    Example: Detect manufacturing defects without labeled data
    """
    # 1. Train MAE on normal products
    print("Training MAE on normal products...")
    mae_model = MaskedAutoencoderViT()

    # Train on normal images (self-supervised)
    train_mae_on_industrial_images(
        image_dir='./normal_products',
        output_dir='./mae_manufacturing',
        num_epochs=100
    )

    # 2. Setup defect detector
    detector = IndustrialDefectDetection(mae_model)

    # 3. Calibrate threshold
    from torchvision.datasets import ImageFolder
    from torchvision import transforms

    transform = transforms.Compose([
        transforms.Resize(224),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ])

    normal_dataset = ImageFolder('./normal_products_val', transform=transform)
    normal_images = [normal_dataset[i][0] for i in range(100)]

    detector.calibrate_threshold(normal_images, percentile=95)

    # 4. Detect defects in new images
    test_image = normal_dataset[0][0]  # Replace with actual test image
    result = detector.detect_defect(test_image)

    print(f"Is defective: {result['is_defective']}")
    print(f"Anomaly score: {result['anomaly_score']:.4f}")
    print(f"Confidence: {result['confidence']:.2f}")
```

## Time-Series Self-Supervision

Time-series data (sensor readings, financial data, user activity logs) presents unique self-supervision opportunities due to temporal structure.

### Time-Series Pretext Tasks

```python
class TimeSeriesSelfSupervised(nn.Module):
    """
    Self-supervised learning for time-series data

    Pretext tasks:
    1. Future prediction: Predict next N steps
    2. Masked reconstruction: Predict masked time steps
    3. Contrastive temporal coding: Distinguish shuffled from real
    4. Transformation recognition: Identify applied transformation

    Use cases:
    - IoT sensor data: Learn patterns from millions of devices
    - Financial time-series: Capture market dynamics
    - User behavior logs: Model activity patterns
    - Healthcare monitoring: Learn from vital signs
    """

    def __init__(
        self,
        input_dim,
        hidden_dim=256,
        num_layers=4,
        pretext_task='forecasting'
    ):
        """
        Args:
            input_dim: Dimension of time-series features
            hidden_dim: Hidden dimension
            num_layers: Number of transformer layers
            pretext_task: 'forecasting', 'masked', or 'contrastive'
        """
        super().__init__()

        self.input_dim = input_dim
        self.hidden_dim = hidden_dim
        self.pretext_task = pretext_task

        # Encoder (Transformer or LSTM)
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=hidden_dim,
            nhead=8,
            dim_feedforward=hidden_dim * 4,
            batch_first=True
        )
        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)

        # Input projection
        self.input_proj = nn.Linear(input_dim, hidden_dim)

        # Positional encoding
        self.pos_encoder = PositionalEncoding(hidden_dim)

        # Task-specific heads
        if pretext_task == 'forecasting':
            self.forecast_head = nn.Linear(hidden_dim, input_dim)
        elif pretext_task == 'masked':
            self.reconstruction_head = nn.Linear(hidden_dim, input_dim)
        elif pretext_task == 'contrastive':
            self.projection_head = nn.Sequential(
                nn.Linear(hidden_dim, hidden_dim),
                nn.ReLU(),
                nn.Linear(hidden_dim, 128)
            )

    def forward(self, x):
        """
        Args:
            x: (batch_size, seq_len, input_dim)

        Returns:
            embeddings: (batch_size, seq_len, hidden_dim)
        """
        # Project input
        x = self.input_proj(x)

        # Add positional encoding
        x = self.pos_encoder(x)

        # Encode
        embeddings = self.encoder(x)

        return embeddings

    def forecasting_loss(self, x, forecast_steps=5):
        """
        Predict future time steps

        Args:
            x: (batch_size, seq_len, input_dim)
            forecast_steps: Number of steps to predict

        Returns:
            loss: Forecasting loss
        """
        # Use first (seq_len - forecast_steps) as context
        context = x[:, :-forecast_steps, :]
        target = x[:, -forecast_steps:, :]

        # Encode context
        embeddings = self.forward(context)

        # Predict future
        predictions = self.forecast_head(embeddings[:, -forecast_steps:, :])

        # MSE loss
        loss = F.mse_loss(predictions, target)

        return loss

    def masked_reconstruction_loss(self, x, mask_ratio=0.15):
        """
        Masked reconstruction pretext task

        Args:
            x: (batch_size, seq_len, input_dim)
            mask_ratio: Ratio of time steps to mask

        Returns:
            loss: Reconstruction loss
        """
        batch_size, seq_len, input_dim = x.shape

        # Create random mask
        mask = torch.rand(batch_size, seq_len) < mask_ratio
        mask = mask.unsqueeze(-1).expand_as(x)

        # Masked input
        x_masked = x.clone()
        x_masked[mask] = 0

        # Encode
        embeddings = self.forward(x_masked)

        # Reconstruct
        reconstructed = self.reconstruction_head(embeddings)

        # Loss only on masked positions
        loss = F.mse_loss(reconstructed[mask], x[mask])

        return loss

    def contrastive_temporal_loss(self, x):
        """
        Contrastive loss for time-series

        Strategy: Create positive pairs through augmentation,
        negative pairs from different time series
        """
        # Create two augmented views
        x1 = self.augment_time_series(x)
        x2 = self.augment_time_series(x)

        # Encode
        emb1 = self.forward(x1).mean(dim=1)  # Pool over time
        emb2 = self.forward(x2).mean(dim=1)

        # Project
        z1 = self.projection_head(emb1)
        z2 = self.projection_head(emb2)

        # Normalize
        z1 = F.normalize(z1, dim=1)
        z2 = F.normalize(z2, dim=1)

        # Contrastive loss (NT-Xent)
        batch_size = z1.shape[0]
        z = torch.cat([z1, z2], dim=0)
        similarity_matrix = torch.mm(z, z.T) / 0.5

        labels = torch.arange(batch_size).to(z1.device)
        labels = torch.cat([labels + batch_size, labels])

        mask = torch.eye(2 * batch_size, dtype=torch.bool).to(z1.device)
        similarity_matrix.masked_fill_(mask, float('-inf'))

        loss = F.cross_entropy(similarity_matrix, labels)

        return loss

    def augment_time_series(self, x):
        """
        Augment time-series data

        Augmentations:
        - Jittering: Add noise
        - Scaling: Multiply by constant
        - Time warping: Stretch/compress
        - Window slicing: Extract subsequence
        """
        # Jittering
        noise = torch.randn_like(x) * 0.05
        x_aug = x + noise

        # Scaling
        scale = torch.rand(x.shape[0], 1, 1).to(x.device) * 0.4 + 0.8
        x_aug = x_aug * scale

        return x_aug


class PositionalEncoding(nn.Module):
    """Positional encoding for sequences"""

    def __init__(self, d_model, max_len=5000):
        super().__init__()

        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(
            torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model)
        )

        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0)

        self.register_buffer('pe', pe)

    def forward(self, x):
        x = x + self.pe[:, :x.size(1), :]
        return x


# Example: IoT sensor self-supervised learning
def example_iot_sensor_ssl():
    """
    Example: Learn from unlabeled IoT sensor data
    """
    # Initialize model
    model = TimeSeriesSelfSupervised(
        input_dim=32,  # 32 sensor readings
        hidden_dim=256,
        num_layers=6,
        pretext_task='forecasting'
    ).cuda()

    # Dummy data (in production: load from time-series database)
    # Shape: (batch_size, seq_len, num_sensors)
    sensor_data = torch.randn(64, 100, 32).cuda()

    # Train with forecasting task
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)

    for epoch in range(100):
        optimizer.zero_grad()
        loss = model.forecasting_loss(sensor_data, forecast_steps=10)
        loss.backward()
        optimizer.step()

        if epoch % 10 == 0:
            print(f"Epoch {epoch}, Loss: {loss.item():.4f}")

    # Extract embeddings for downstream tasks
    with torch.no_grad():
        embeddings = model.forward(sensor_data)
        # Use embeddings for anomaly detection, classification, etc.

    print(f"Embeddings shape: {embeddings.shape}")
```

:::{.callout-tip}
## Time-Series SSL Best Practices

**Forecasting Horizon**: For high-frequency data (milliseconds), predict 5-10 steps ahead. For slow-varying data (daily), predict 1-2 steps.

**Masking Strategy**: For bursty data (event logs), use random masking. For smooth data (temperature), use contiguous span masking.

**Augmentations**: Test augmentations carefully. Ensure they preserve semantic meaning (e.g., don't shift phase of financial data).

**Architecture**: Transformers work well for long sequences (> 100 steps). For shorter sequences or limited compute, use LSTM/GRU.
:::

## Multi-Modal Self-Supervised Approaches

Multi-modal self-supervision learns from multiple data types simultaneously—text + images, audio + video, sensor + text logs.

### CLIP-Style Multi-Modal Learning

```python
class MultiModalSelfSupervised(nn.Module):
    """
    Multi-modal self-supervised learning

    Approach: Contrastive learning across modalities
    - Learn shared embedding space
    - Match corresponding pairs across modalities
    - No manual labels needed!

    Use cases:
    - Product images + descriptions
    - Medical images + clinical notes
    - Video + audio
    - IoT sensors + maintenance logs

    Reference: "Learning Transferable Visual Models From Natural Language Supervision" (CLIP)
    """

    def __init__(
        self,
        image_encoder,
        text_encoder,
        embedding_dim=512,
        temperature=0.07
    ):
        """
        Args:
            image_encoder: Image encoder (ResNet, ViT)
            text_encoder: Text encoder (BERT, RoBERTa)
            embedding_dim: Shared embedding dimension
            temperature: Temperature for contrastive loss
        """
        super().__init__()

        self.image_encoder = image_encoder
        self.text_encoder = text_encoder
        self.temperature = temperature

        # Projection heads to shared space
        self.image_projection = nn.Linear(
            image_encoder.output_dim,
            embedding_dim
        )
        self.text_projection = nn.Linear(
            text_encoder.output_dim,
            embedding_dim
        )

        # Learnable temperature
        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / temperature))

    def encode_image(self, images):
        """
        Encode images to embeddings

        Args:
            images: (batch_size, channels, height, width)

        Returns:
            embeddings: (batch_size, embedding_dim)
        """
        features = self.image_encoder(images)
        embeddings = self.image_projection(features)
        embeddings = F.normalize(embeddings, dim=-1)
        return embeddings

    def encode_text(self, texts):
        """
        Encode texts to embeddings

        Args:
            texts: List of text strings or tokenized inputs

        Returns:
            embeddings: (batch_size, embedding_dim)
        """
        features = self.text_encoder(texts)
        embeddings = self.text_projection(features)
        embeddings = F.normalize(embeddings, dim=-1)
        return embeddings

    def forward(self, images, texts):
        """
        Forward pass: Compute contrastive loss

        Args:
            images: Batch of images
            texts: Corresponding text descriptions

        Returns:
            loss: Contrastive loss
            accuracy: Top-1 accuracy
        """
        # Encode both modalities
        image_embeddings = self.encode_image(images)
        text_embeddings = self.encode_text(texts)

        # Compute similarity matrix
        logit_scale = self.logit_scale.exp()
        logits_per_image = logit_scale * image_embeddings @ text_embeddings.T
        logits_per_text = logits_per_image.T

        # Labels: diagonal elements are positive pairs
        batch_size = image_embeddings.shape[0]
        labels = torch.arange(batch_size).to(images.device)

        # Contrastive loss (symmetric)
        loss_image = F.cross_entropy(logits_per_image, labels)
        loss_text = F.cross_entropy(logits_per_text, labels)
        loss = (loss_image + loss_text) / 2

        # Accuracy
        with torch.no_grad():
            pred_image = logits_per_image.argmax(dim=1)
            pred_text = logits_per_text.argmax(dim=1)
            accuracy = ((pred_image == labels).float().mean() +
                       (pred_text == labels).float().mean()) / 2

        return loss, accuracy.item()

    def find_similar_texts(self, image, text_candidates):
        """
        Find most similar texts for an image

        Args:
            image: Query image
            text_candidates: List of candidate texts

        Returns:
            ranked_texts: Texts ranked by similarity
            similarities: Similarity scores
        """
        image_embedding = self.encode_image(image.unsqueeze(0))
        text_embeddings = self.encode_text(text_candidates)

        # Compute similarities
        similarities = (image_embedding @ text_embeddings.T).squeeze(0)

        # Rank by similarity
        ranked_indices = similarities.argsort(descending=True)
        ranked_texts = [text_candidates[i] for i in ranked_indices]
        ranked_similarities = similarities[ranked_indices]

        return ranked_texts, ranked_similarities.tolist()

    def find_similar_images(self, text, image_candidates):
        """
        Find most similar images for a text query

        Args:
            text: Query text
            image_candidates: Batch of candidate images

        Returns:
            ranked_indices: Indices of images ranked by similarity
            similarities: Similarity scores
        """
        text_embedding = self.encode_text([text])
        image_embeddings = self.encode_image(image_candidates)

        # Compute similarities
        similarities = (text_embedding @ image_embeddings.T).squeeze(0)

        # Rank by similarity
        ranked_indices = similarities.argsort(descending=True)
        ranked_similarities = similarities[ranked_indices]

        return ranked_indices.tolist(), ranked_similarities.tolist()


# Example: Training multi-modal embeddings
def train_multimodal_ssl(
    image_text_pairs,
    num_epochs=100,
    batch_size=256
):
    """
    Train multi-modal self-supervised model

    Args:
        image_text_pairs: Dataset of (image, text) pairs
        num_epochs: Number of training epochs
        batch_size: Batch size
    """
    # Initialize encoders
    from torchvision.models import resnet50
    from transformers import AutoModel

    # Image encoder
    image_encoder = resnet50(pretrained=True)
    image_encoder.output_dim = 2048

    # Text encoder
    text_encoder = AutoModel.from_pretrained('bert-base-uncased')
    text_encoder.output_dim = 768

    # Multi-modal model
    model = MultiModalSelfSupervised(
        image_encoder,
        text_encoder,
        embedding_dim=512
    ).cuda()

    # Optimizer
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)

    # DataLoader
    dataloader = torch.utils.data.DataLoader(
        image_text_pairs,
        batch_size=batch_size,
        shuffle=True,
        num_workers=8
    )

    # Training loop
    for epoch in range(num_epochs):
        total_loss = 0
        total_accuracy = 0

        for images, texts in dataloader:
            images = images.cuda()

            optimizer.zero_grad()
            loss, accuracy = model(images, texts)
            loss.backward()
            optimizer.step()

            total_loss += loss.item()
            total_accuracy += accuracy

        avg_loss = total_loss / len(dataloader)
        avg_accuracy = total_accuracy / len(dataloader)

        print(f"Epoch {epoch}: Loss={avg_loss:.4f}, Accuracy={avg_accuracy:.4f}")

    return model


# Enterprise application: Product matching
class ProductMatchingSystem:
    """
    Multi-modal product matching

    Use case: Match product images with descriptions
    across different catalogs (e-commerce, inventory systems)
    """

    def __init__(self, multimodal_model):
        self.model = multimodal_model
        self.product_embeddings = {}

    def index_products(self, products):
        """
        Index products (images + descriptions)

        Args:
            products: List of (product_id, image, description) tuples
        """
        for product_id, image, description in products:
            # Encode both modalities
            image_emb = self.model.encode_image(image.unsqueeze(0))
            text_emb = self.model.encode_text([description])

            # Average embeddings for robust matching
            combined_emb = (image_emb + text_emb) / 2

            self.product_embeddings[product_id] = combined_emb.cpu()

    def find_matches(self, query_image=None, query_text=None, top_k=10):
        """
        Find matching products

        Args:
            query_image: Query image (optional)
            query_text: Query text (optional)
            top_k: Number of results

        Returns:
            matches: List of (product_id, similarity) tuples
        """
        # Encode query
        if query_image is not None:
            query_emb = self.model.encode_image(query_image.unsqueeze(0))
        elif query_text is not None:
            query_emb = self.model.encode_text([query_text])
        else:
            raise ValueError("Must provide query_image or query_text")

        # Compute similarities
        similarities = {}
        for product_id, product_emb in self.product_embeddings.items():
            sim = F.cosine_similarity(
                query_emb,
                product_emb.to(query_emb.device),
                dim=1
            ).item()
            similarities[product_id] = sim

        # Rank by similarity
        matches = sorted(
            similarities.items(),
            key=lambda x: x[1],
            reverse=True
        )[:top_k]

        return matches
```

:::{.callout-tip}
## Multi-Modal SSL Best Practices

**Pairing Quality**: The quality of modality pairs matters more than quantity. 10M high-quality pairs > 100M noisy pairs.

**Batch Size**: Larger batches provide more negative samples. Use at least 256, ideally 1024+ with gradient accumulation.

**Temperature**: Start with 0.07. Lower (0.01) for fine-grained matching, higher (0.2) for coarse similarity.

**Modality Balance**: If one modality is much noisier, consider weighted loss or filtering poor pairs.

**Compute**: CLIP-scale training (400M pairs) requires thousands of GPU-hours. For enterprise, 1M-10M pairs often sufficient.
:::

## Key Takeaways

- **Self-supervised learning unlocks unlabeled data** at unprecedented scale. No manual annotation needed—data structure provides supervision through pretext tasks.

- **Masked Language Modeling** is the foundation for domain-specific text embeddings. Always train a domain-specific tokenizer first, then adapt or train MLM on your corpus.

- **Vision Transformers with Masked Autoencoding (MAE)** enable learning from unlabeled images with 75% masking. Ideal for manufacturing defects, medical imaging, and satellite imagery where labels are scarce.

- **Time-series self-supervision** uses forecasting, masked reconstruction, or contrastive tasks. Choose based on data characteristics: forecasting for ordered data, contrastive for augmentable data.

- **Multi-modal self-supervision** creates shared embedding spaces across text, images, audio, and sensors without paired labels. Contrastive learning between modalities is highly effective.

- **Production deployment requires distributed training**, checkpointing, and careful data management. For 100M parameters × 1B samples, expect 500-1000 GPU-hours.

## Looking Ahead

In Chapter 8, we explore advanced embedding techniques that push beyond standard architectures—hierarchical embeddings for taxonomies, dynamic embeddings that evolve over time, compositional embeddings for combinatorial spaces, and quantum-inspired embeddings for ultra-high-dimensional data. These techniques unlock capabilities impossible with standard approaches.

## Further Reading

- Devlin, J., et al. (2018). "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding." NAACL.
- He, K., et al. (2021). "Masked Autoencoders Are Scalable Vision Learners." CVPR.
- Radford, A., et al. (2021). "Learning Transferable Visual Models From Natural Language Supervision." ICML.
- Chen, T., et al. (2020). "A Simple Framework for Contrastive Learning of Visual Representations." ICML.
- Oord, A., Li, Y., & Vinyals, O. (2018). "Representation Learning with Contrastive Predictive Coding." arXiv:1807.03748.
- Liu, Y., et al. (2019). "RoBERTa: A Robustly Optimized BERT Pretraining Approach." arXiv:1907.11692.
- Dosovitskiy, A., et al. (2020). "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale." ICLR.
- Franceschi, J.Y., et al. (2019). "Unsupervised Scalable Representation Learning for Multivariate Time Series." NeurIPS.
