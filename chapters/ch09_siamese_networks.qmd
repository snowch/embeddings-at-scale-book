# Siamese Networks for Specialized Use Cases {#sec-siamese-networks}

:::{.callout-note}
## Chapter Overview
While contrastive learning (Chapter 5) taught us how to train embeddings that distinguish similar from dissimilar, Siamese networks provide the architectural foundation for similarity-based learning at enterprise scale. This chapter explores Siamese architectures—twin neural networks that excel at learning similarity metrics for specialized use cases including one-shot learning, anomaly detection, and verification systems. We cover the architectural patterns, triplet loss optimization, strategies for rare event handling, threshold calibration techniques, and production deployment patterns that enable Siamese networks to scale to trillion-row deployments.
:::

## Siamese Architecture for Enterprise Similarity

Siamese networks solve a fundamental challenge: **how do you learn similarity when you have few examples per class, unbalanced distributions, or continuously evolving categories?** Traditional classifiers fail in these scenarios. Siamese networks succeed by learning to compare rather than classify.

### The Siamese Paradigm

Named after Siamese twins, a Siamese network consists of two or more identical neural networks (sharing weights) that process different inputs and compare their outputs. The key insight: **instead of learning "what is X?", learn "how similar are X and Y?"**

This shift enables:

- **Few-shot learning**: Learn from 1-5 examples per class
- **Open-set recognition**: Handle classes not seen during training
- **Verification tasks**: "Are these the same?" vs "What is this?"
- **Similarity search**: Find nearest neighbors in learned space

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class SiameseNetwork(nn.Module):
    """
    Siamese Network for learning similarity metrics

    Architecture: Two identical networks (shared weights) process different
    inputs, producing embeddings that are compared using a distance metric.

    Use cases:
    - Face verification: "Is this the same person?"
    - Document similarity: "Are these papers related?"
    - Product matching: "Are these the same item?"
    - Anomaly detection: "Is this different from normal?"
    """

    def __init__(self, embedding_net, embedding_dim=512):
        """
        Args:
            embedding_net: The base network for creating embeddings
                          (e.g., ResNet, BERT, custom architecture)
            embedding_dim: Dimension of output embeddings
        """
        super().__init__()
        self.embedding_net = embedding_net
        self.embedding_dim = embedding_dim

    def forward(self, x1, x2):
        """
        Forward pass through Siamese network

        Args:
            x1: First input (batch_size, ...)
            x2: Second input (batch_size, ...)

        Returns:
            embedding1: Embeddings for x1 (batch_size, embedding_dim)
            embedding2: Embeddings for x2 (batch_size, embedding_dim)
        """
        # Both inputs go through the SAME network (shared weights)
        embedding1 = self.embedding_net(x1)
        embedding2 = self.embedding_net(x2)

        return embedding1, embedding2

    def get_embedding(self, x):
        """Get embedding for a single input"""
        return self.embedding_net(x)


class EmbeddingNet(nn.Module):
    """
    Example embedding network for structured/tabular data

    For images: Use ResNet, EfficientNet, Vision Transformer
    For text: Use BERT, RoBERTa, sentence transformers
    For multimodal: Use CLIP-style architectures
    """

    def __init__(self, input_dim, embedding_dim=512, hidden_dims=[1024, 512]):
        super().__init__()

        layers = []
        prev_dim = input_dim

        for hidden_dim in hidden_dims:
            layers.extend([
                nn.Linear(prev_dim, hidden_dim),
                nn.BatchNorm1d(hidden_dim),
                nn.ReLU(),
                nn.Dropout(0.3)
            ])
            prev_dim = hidden_dim

        # Final embedding layer
        layers.append(nn.Linear(prev_dim, embedding_dim))

        self.network = nn.Sequential(*layers)

    def forward(self, x):
        """
        Args:
            x: Input features (batch_size, input_dim)

        Returns:
            embeddings: L2-normalized embeddings (batch_size, embedding_dim)
        """
        embeddings = self.network(x)
        # L2 normalization for cosine similarity
        return F.normalize(embeddings, p=2, dim=1)


# Example: Building a Siamese network for enterprise use
def create_enterprise_siamese_network(input_type='tabular', input_dim=None):
    """
    Factory function for creating Siamese networks

    Args:
        input_type: 'tabular', 'image', 'text', or 'multimodal'
        input_dim: Input dimension (for tabular data)

    Returns:
        SiameseNetwork instance configured for the input type
    """

    if input_type == 'tabular':
        if input_dim is None:
            raise ValueError("input_dim required for tabular data")
        embedding_net = EmbeddingNet(
            input_dim=input_dim,
            embedding_dim=512,
            hidden_dims=[1024, 768, 512]
        )

    elif input_type == 'image':
        # Use pre-trained ResNet
        import torchvision.models as models
        resnet = models.resnet50(pretrained=True)
        # Remove classification head
        embedding_net = nn.Sequential(*list(resnet.children())[:-1])

    elif input_type == 'text':
        # Use transformer-based encoder
        from transformers import AutoModel
        embedding_net = AutoModel.from_pretrained('bert-base-uncased')

    else:
        raise ValueError(f"Unknown input_type: {input_type}")

    return SiameseNetwork(embedding_net, embedding_dim=512)
```

### Contrastive Loss for Siamese Networks

The classic Siamese network uses **contrastive loss** to bring similar pairs together and push dissimilar pairs apart:

```python
{{< include /code_examples/ch09_siamese_networks/contrastiveloss.py >}}
```

:::{.callout-tip}
## Choosing Distance Metrics

**Euclidean distance** works well for normalized embeddings in low dimensions (< 128).

**Cosine distance** (1 - cosine similarity) is preferred for:

- High-dimensional embeddings (> 128)
- Text embeddings
- When magnitude isn't meaningful

**Learned distance metrics** (e.g., Mahalanobis) can capture domain-specific similarity but require more data and computation.
:::

### Enterprise Siamese Architecture Patterns

For production systems handling billions of comparisons daily, architecture choices matter:

```python
{{< include /code_examples/ch09_siamese_networks/enterpriseoptimizedsiamesenetwork.py >}}
```

:::{.callout-warning}
## Production Considerations

**Memory Management**: For large models (> 1B parameters), gradient checkpointing is essential. It trades 30% more compute for 50% less memory.

**Batch Size Selection**: Larger batches (256-1024) improve training stability for Siamese networks. Use gradient accumulation if GPU memory is limited.

**Learning Rate**: Start with 1e-4 for fine-tuning pre-trained models, 1e-3 for training from scratch. Use warmup for stability.
:::

## Triplet Loss Optimization Techniques

While contrastive loss works with pairs, **triplet loss** works with triplets: (anchor, positive, negative). This provides more information per training example and often leads to better embeddings.

### Triplet Loss Fundamentals

Triplet loss ensures that anchor-positive distance is smaller than anchor-negative distance by at least a margin:

**Loss = max(d(anchor, positive) - d(anchor, negative) + margin, 0)**

```python
{{< include /code_examples/ch09_siamese_networks/tripletloss.py >}}
```

### Advanced Triplet Loss Variants

For enterprise scale, basic triplet loss isn't enough. Here are production-tested variants:

```python
{{< include /code_examples/ch09_siamese_networks/advancedtripletloss.py >}}
```

:::{.callout-tip}
## Mining Strategy Selection

**Hard negative mining**: Best for well-separated classes. Can cause training instability if classes overlap.

**Semi-hard negative mining**: Recommended for production. Balances learning speed with stability. Use when classes have some overlap.

**All triplets**: Only for small datasets (< 10K examples) or final fine-tuning. Computationally expensive.

**Rule of thumb**: Start with semi-hard, switch to hard if training plateaus after 70% of epochs.
:::

### Batch Construction for Efficient Triplet Training

Efficient triplet mining requires careful batch construction:

```python
{{< include /code_examples/ch09_siamese_networks/balancedbatchsampler.py >}}
```

:::{.callout-warning}
## Production Batch Sizing

**Memory constraints**: P × K = batch_size. Larger batches provide more triplets but require more memory.

**Recommended configurations**:

- Small models (< 100M params): P=16, K=8, batch_size=128
- Medium models (100M-1B params): P=10, K=5, batch_size=50
- Large models (> 1B params): P=8, K=4, batch_size=32

**GPU utilization**: Use gradient accumulation to simulate larger batches if needed.
:::

## One-Shot Learning for Rare Events

One-shot learning—learning from a single example—is critical for enterprise scenarios where rare events are important but examples are scarce: fraud detection, manufacturing defects, zero-day threats, rare diseases.

### One-Shot Learning Fundamentals

Traditional ML fails with one example per class. Siamese networks succeed by:

1. **Learning similarity during training** on abundant data
2. **Applying similarity at inference** to new classes with few examples
3. **Comparing rather than classifying** new inputs

```python
{{< include /code_examples/ch09_siamese_networks/oneshotclassifier.py >}}
```

:::{.callout-tip}
## When One-Shot Learning Works Best

**Ideal scenarios**:

- High-quality training data (even if small)
- Well-defined similarity metric
- Rare event detection (fraud, anomalies, defects)
- Rapidly evolving categories (new threats, trends)

**Challenging scenarios**:

- Noisy data (single example may be unrepresentative)
- Complex decision boundaries
- Classes that require multiple features to distinguish

**Best practice**: Collect 3-5 examples per class when possible. Average their embeddings for more robust representation.
:::

### Few-Shot Learning Extensions

When you have 2-10 examples per class (few-shot), you can use more sophisticated techniques:

```python
{{< include /code_examples/ch09_siamese_networks/prototypicalnetworkclassifier.py >}}
```

## Similarity Threshold Calibration

A critical but often overlooked challenge: **How do you set the threshold for "similar enough"?** Too low and you get false positives. Too high and you miss true matches.

### The Threshold Calibration Challenge

```python
class ThresholdCalibrator:
    """
    Calibrate similarity thresholds for production deployment

    Challenge: The optimal threshold depends on:
    - Distribution of true positives vs negatives
    - Business costs of false positives vs false negatives
    - Dataset characteristics (intra-class vs inter-class variance)

    This class provides multiple calibration strategies.
    """

    def __init__(self, siamese_model):
        self.model = siamese_model
        self.threshold = None
        self.calibration_metrics = {}

    def calibrate_on_validation_set(
        self,
        validation_pairs,
        validation_labels,
        metric='f1',
        plot=False
    ):
        """
        Calibrate threshold on validation set to optimize a metric

        Args:
            validation_pairs: List of (item1, item2) pairs
            validation_labels: 1 if similar, 0 if dissimilar
            metric: 'f1', 'precision', 'recall', or 'accuracy'
            plot: If True, plot threshold vs metric curve

        Returns:
            Optimal threshold value
        """
        # Compute distances for all pairs
        distances = []

        with torch.no_grad():
            self.model.eval()

            for item1, item2 in validation_pairs:
                embedding1 = self.model.get_embedding(item1.unsqueeze(0))
                embedding2 = self.model.get_embedding(item2.unsqueeze(0))

                distance = F.pairwise_distance(embedding1, embedding2).item()
                distances.append(distance)

        distances = np.array(distances)
        validation_labels = np.array(validation_labels)

        # Try different thresholds
        thresholds = np.linspace(distances.min(), distances.max(), 100)
        metrics_by_threshold = []

        for threshold in thresholds:
            # Predict: similar if distance < threshold
            predictions = (distances < threshold).astype(int)

            # Compute metrics
            tp = ((predictions == 1) & (validation_labels == 1)).sum()
            fp = ((predictions == 1) & (validation_labels == 0)).sum()
            tn = ((predictions == 0) & (validation_labels == 0)).sum()
            fn = ((predictions == 0) & (validation_labels == 1)).sum()

            precision = tp / (tp + fp) if (tp + fp) > 0 else 0
            recall = tp / (tp + fn) if (tp + fn) > 0 else 0
            f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
            accuracy = (tp + tn) / len(validation_labels)

            metrics_by_threshold.append({
                'threshold': threshold,
                'precision': precision,
                'recall': recall,
                'f1': f1,
                'accuracy': accuracy
            })

        # Find threshold that maximizes chosen metric
        best_idx = max(
            range(len(metrics_by_threshold)),
            key=lambda i: metrics_by_threshold[i][metric]
        )

        self.threshold = metrics_by_threshold[best_idx]['threshold']
        self.calibration_metrics = metrics_by_threshold[best_idx]

        if plot:
            self._plot_calibration_curve(metrics_by_threshold, metric)

        return self.threshold

    def calibrate_with_business_costs(
        self,
        validation_pairs,
        validation_labels,
        false_positive_cost=1.0,
        false_negative_cost=1.0
    ):
        """
        Calibrate threshold based on business costs

        Args:
            validation_pairs: List of (item1, item2) pairs
            validation_labels: 1 if similar, 0 if dissimilar
            false_positive_cost: Cost of incorrectly marking as similar
            false_negative_cost: Cost of missing a true match

        Returns:
            Cost-optimal threshold

        Example costs:
        - Fraud detection: FN cost >> FP cost (missing fraud is expensive)
        - Product matching: FP cost >> FN cost (wrong matches annoy users)
        """
        # Compute distances
        distances = []

        with torch.no_grad():
            self.model.eval()

            for item1, item2 in validation_pairs:
                embedding1 = self.model.get_embedding(item1.unsqueeze(0))
                embedding2 = self.model.get_embedding(item2.unsqueeze(0))

                distance = F.pairwise_distance(embedding1, embedding2).item()
                distances.append(distance)

        distances = np.array(distances)
        validation_labels = np.array(validation_labels)

        # Try different thresholds
        thresholds = np.linspace(distances.min(), distances.max(), 100)
        costs = []

        for threshold in thresholds:
            predictions = (distances < threshold).astype(int)

            fp = ((predictions == 1) & (validation_labels == 0)).sum()
            fn = ((predictions == 0) & (validation_labels == 1)).sum()

            total_cost = fp * false_positive_cost + fn * false_negative_cost
            costs.append(total_cost)

        # Find threshold that minimizes cost
        best_idx = np.argmin(costs)
        self.threshold = thresholds[best_idx]

        self.calibration_metrics = {
            'threshold': self.threshold,
            'expected_cost': costs[best_idx],
            'false_positive_cost': false_positive_cost,
            'false_negative_cost': false_negative_cost
        }

        return self.threshold

    def calibrate_for_precision_target(
        self,
        validation_pairs,
        validation_labels,
        target_precision=0.95
    ):
        """
        Calibrate to achieve target precision

        Use when false positives are unacceptable (e.g., financial matching)

        Args:
            validation_pairs: List of (item1, item2) pairs
            validation_labels: 1 if similar, 0 if dissimilar
            target_precision: Desired precision (0-1)

        Returns:
            Threshold that achieves target precision (or closest possible)
        """
        # Compute distances
        distances = []

        with torch.no_grad():
            self.model.eval()

            for item1, item2 in validation_pairs:
                embedding1 = self.model.get_embedding(item1.unsqueeze(0))
                embedding2 = self.model.get_embedding(item2.unsqueeze(0))

                distance = F.pairwise_distance(embedding1, embedding2).item()
                distances.append(distance)

        distances = np.array(distances)
        validation_labels = np.array(validation_labels)

        # Try different thresholds
        thresholds = np.linspace(distances.min(), distances.max(), 100)

        best_threshold = None
        best_precision = 0
        best_recall = 0

        for threshold in thresholds:
            predictions = (distances < threshold).astype(int)

            tp = ((predictions == 1) & (validation_labels == 1)).sum()
            fp = ((predictions == 1) & (validation_labels == 0)).sum()
            fn = ((predictions == 0) & (validation_labels == 1)).sum()

            precision = tp / (tp + fp) if (tp + fp) > 0 else 0
            recall = tp / (tp + fn) if (tp + fn) > 0 else 0

            # Find threshold closest to target precision
            if precision >= target_precision:
                if best_threshold is None or recall > best_recall:
                    best_threshold = threshold
                    best_precision = precision
                    best_recall = recall

        if best_threshold is None:
            # Can't achieve target, return threshold with highest precision
            for threshold in thresholds:
                predictions = (distances < threshold).astype(int)
                tp = ((predictions == 1) & (validation_labels == 1)).sum()
                fp = ((predictions == 1) & (validation_labels == 0)).sum()
                precision = tp / (tp + fp) if (tp + fp) > 0 else 0

                if precision > best_precision:
                    best_precision = precision
                    best_threshold = threshold

        self.threshold = best_threshold
        self.calibration_metrics = {
            'threshold': best_threshold,
            'achieved_precision': best_precision,
            'achieved_recall': best_recall,
            'target_precision': target_precision
        }

        return self.threshold

    def _plot_calibration_curve(self, metrics_by_threshold, target_metric):
        """Plot threshold vs metric curve"""
        import matplotlib.pyplot as plt

        thresholds = [m['threshold'] for m in metrics_by_threshold]
        values = [m[target_metric] for m in metrics_by_threshold]

        plt.figure(figsize=(10, 6))
        plt.plot(thresholds, values)
        plt.axvline(self.threshold, color='r', linestyle='--',
                   label=f'Optimal: {self.threshold:.3f}')
        plt.xlabel('Threshold')
        plt.ylabel(target_metric.capitalize())
        plt.title(f'Threshold Calibration: {target_metric.capitalize()}')
        plt.legend()
        plt.grid(True)
        plt.show()
```

:::{.callout-warning}
## Threshold Calibration Best Practices

**Re-calibrate regularly**: Data distributions drift. Re-calibrate quarterly or when you detect performance degradation.

**Use stratified validation**: Ensure your validation set represents production distribution. Unbalanced calibration data leads to suboptimal thresholds.

**Monitor threshold effectiveness**: Track precision/recall in production. Alert if metrics deviate > 5% from calibration values.

**Business cost alignment**: Work with stakeholders to quantify FP and FN costs. Technical metrics (F1) may not align with business value.
:::

### Dynamic Threshold Adaptation

For production systems, static thresholds aren't enough. Implement dynamic adaptation:

```python
{{< include /code_examples/ch09_siamese_networks/adaptivethresholdmanager.py >}}
```

## Production Deployment Patterns

Deploying Siamese networks at scale requires careful architecture design. Here are battle-tested patterns from trillion-row deployments:

### Pattern 1: Embedding Cache Architecture

```python
{{< include /code_examples/ch09_siamese_networks/siameseembeddingservice.py >}}
```

### Pattern 2: Approximate Nearest Neighbor Integration

For billion-scale similarity search, integrate with ANN indexes:

```python
{{< include /code_examples/ch09_siamese_networks/siameseannservice.py >}}
```

### Pattern 3: Multi-Stage Verification Pipeline

For high-precision applications (fraud, compliance), use multi-stage verification:

```python
class MultiStageVerificationPipeline:
    """
    Multi-stage verification using Siamese networks

    Stage 1: Fast filtering with loose threshold
    Stage 2: Detailed verification with strict threshold
    Stage 3: Human review for borderline cases

    Reduces compute cost while maintaining high accuracy.
    """

    def __init__(
        self,
        siamese_service,
        stage1_threshold=0.7,  # Recall-optimized
        stage2_threshold=0.9,  # Precision-optimized
        use_ann=True
    ):
        self.siamese_service = siamese_service
        self.stage1_threshold = stage1_threshold
        self.stage2_threshold = stage2_threshold

        if use_ann:
            self.ann_service = SiameseANNService(
                siamese_service,
                embedding_dim=512
            )
        else:
            self.ann_service = None

        self.stage1_candidates = 0
        self.stage2_matches = 0
        self.human_review_cases = 0

    def verify(self, query, candidate_pool=None, candidate_ids=None):
        """
        Multi-stage verification

        Args:
            query: Item to verify
            candidate_pool: Pool of candidates to check against
                          (or None to use ANN search)
            candidate_ids: IDs for candidates (if using candidate_pool)

        Returns:
            Dict with:
            - matched: Boolean or 'needs_review'
            - match_id: ID of matched item (if any)
            - confidence: Similarity score
            - stage: Which stage made the decision
        """

        # Stage 1: Fast filtering
        if self.ann_service is not None and candidate_pool is None:
            # Use ANN search for fast filtering
            stage1_results = self.ann_service.search(query, top_k=100)
            stage1_candidates = [
                (item_id, sim) for item_id, sim in stage1_results
                if sim >= self.stage1_threshold
            ]
        else:
            # Linear search through candidate pool
            if candidate_pool is None:
                raise ValueError("Must provide candidate_pool or use ANN")

            query_embedding = self.siamese_service.get_embedding(query)
            candidate_embeddings = self.siamese_service.get_embeddings_batch(
                candidate_pool
            )

            similarities = F.cosine_similarity(
                query_embedding.unsqueeze(0),
                candidate_embeddings,
                dim=1
            )

            stage1_candidates = [
                (candidate_ids[i], sim.item())
                for i, sim in enumerate(similarities)
                if sim.item() >= self.stage1_threshold
            ]

        self.stage1_candidates += len(stage1_candidates)

        if len(stage1_candidates) == 0:
            return {
                'matched': False,
                'match_id': None,
                'confidence': 0.0,
                'stage': 1
            }

        # Stage 2: Detailed verification
        # For production, this might involve:
        # - More expensive model
        # - Feature-level comparison
        # - Additional business logic

        best_match = max(stage1_candidates, key=lambda x: x[1])
        match_id, similarity = best_match

        if similarity >= self.stage2_threshold:
            # High confidence match
            self.stage2_matches += 1
            return {
                'matched': True,
                'match_id': match_id,
                'confidence': similarity,
                'stage': 2
            }
        else:
            # Borderline case - needs human review
            self.human_review_cases += 1
            return {
                'matched': 'needs_review',
                'match_id': match_id,
                'confidence': similarity,
                'stage': 2,
                'review_reason': 'confidence_below_threshold'
            }

    def get_statistics(self):
        """Get pipeline statistics"""
        return {
            'stage1_candidates': self.stage1_candidates,
            'stage2_matches': self.stage2_matches,
            'human_review_cases': self.human_review_cases,
            'human_review_rate': self.human_review_cases / max(self.stage1_candidates, 1)
        }
```

:::{.callout-tip}
## Production Deployment Checklist

**Before deploying Siamese networks to production**:

- [ ] Model performance validated on production-like data
- [ ] Thresholds calibrated using business metrics
- [ ] Embedding cache sized appropriately (monitor hit rate > 70%)
- [ ] ANN index configured for scale (test with 10x expected load)
- [ ] Monitoring dashboards for similarity distributions
- [ ] Alerting on performance degradation (precision/recall < thresholds)
- [ ] A/B testing framework for model updates
- [ ] Rollback plan for model failures
- [ ] Load testing at peak + 50% capacity
- [ ] Cost optimization: GPU utilization > 80%

**Ongoing maintenance**:

- Re-calibrate thresholds quarterly
- Retrain on recent data every 3-6 months
- Monitor for data drift (distributional shifts)
- Collect hard negative examples for continuous improvement
:::

## Key Takeaways

- **Siamese networks learn similarity rather than classification**, enabling few-shot learning, verification tasks, and open-set recognition without retraining.

- **Triplet loss with hard negative mining** provides better gradients than contrastive loss for most enterprise applications. Use semi-hard mining for stable training.

- **One-shot learning enables immediate adaptation to new categories** from single examples—critical for fraud detection, rare defects, and rapidly evolving threats.

- **Threshold calibration is not optional**. Use validation data to calibrate thresholds based on business metrics (precision/recall) or costs (FP/FN costs). Re-calibrate quarterly.

- **Production deployment requires caching and ANN integration** to achieve sub-millisecond similarity search at billion-scale. Multi-stage pipelines balance cost and accuracy.

- **Monitor similarity distributions in production**. Shifts indicate data drift or model degradation. Alert when mean similarity changes > 10% from baseline.

## Looking Ahead

In Chapter 7, we expand beyond supervised and Siamese approaches to self-supervised learning—techniques that leverage the structure of unlabeled data itself to train powerful embeddings. We'll explore masked language modeling, vision transformers, and multi-modal self-supervision strategies that enable learning from trillions of unlabeled examples across text, images, time-series, and more.

## Further Reading

- Bromley, J., et al. (1993). "Signature Verification using a Siamese Time Delay Neural Network." NIPS.
- Schroff, F., Kalenichenko, D., & Philbin, J. (2015). "FaceNet: A Unified Embedding for Face Recognition and Clustering." CVPR.
- Snell, J., Swersky, K., & Zemel, R. (2017). "Prototypical Networks for Few-shot Learning." NeurIPS.
- Koch, G., Zemel, R., & Salakhutdinov, R. (2015). "Siamese Neural Networks for One-shot Image Recognition." ICML Workshop.
- Wang, J., et al. (2017). "Deep Metric Learning with Angular Loss." ICCV.
- Hermans, A., Beyer, L., & Leibe, B. (2017). "In Defense of the Triplet Loss for Person Re-Identification." arXiv:1703.07737.
