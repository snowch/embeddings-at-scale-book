# Foundational Embedding Types {#sec-foundational-embedding-types}

::: callout-note
## Chapter Overview

This chapter provides a comprehensive tour of foundational embedding types across different data modalities. We examine text, image, audio, video, multi-modal, graph, time-series, and code embeddings—understanding what makes each unique, when to use them, and how they're created. These foundational types form the building blocks that production systems combine and extend using the advanced patterns covered in @sec-advanced-embedding-patterns.
:::

## The Embedding Landscape

Every type of data can be converted to embeddings, but different data types require different approaches. The key insight is that **the embedding architecture must match the structure of your data**:

- **Text** (words, sentences, documents): Transformer models capture context and semantic meaning
- **Images**: CNNs and Vision Transformers capture spatial patterns and visual features
- **Audio** (speech, music, sounds): Spectral models process frequency patterns over time
- **Video**: Temporal models combine frame-level features with motion understanding
- **Multi-modal** (text + images): Alignment models map different modalities to a shared space
- **Graphs** (networks, relationships): Message-passing models aggregate neighborhood information
- **Time-series** (sensors, sequences): Recurrent and convolutional models capture temporal patterns
- **Code** (programs, functions): Specialized models understand syntax and program semantics

Let's explore each embedding type in depth.

## Text Embeddings {#sec-text-embedding-types}

Text embeddings are the most mature and widely used embedding type. They've evolved through several generations:

### Word Embeddings

The foundation of modern NLP, word embeddings map individual words to vectors:

```{python}
#| code-fold: false

"""
Word Embeddings: From Words to Vectors

Word embeddings capture semantic relationships between individual words.
Words with similar meanings cluster together in the embedding space.
"""

from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

# Use a sentence model to embed individual words
model = SentenceTransformer('all-MiniLM-L6-v2')

# Embed words across different categories
words = {
    'animals': ['cat', 'dog', 'elephant', 'whale'],
    'vehicles': ['car', 'truck', 'airplane', 'boat'],
    'colors': ['red', 'blue', 'green', 'yellow'],
}

all_words = [w for group in words.values() for w in group]
embeddings = model.encode(all_words)

# Show that words cluster by category
print("Word similarities (same category = higher similarity):\n")
print("Within categories:")
print(f"  cat ↔ dog:      {cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]:.3f}")
print(f"  car ↔ truck:    {cosine_similarity([embeddings[4]], [embeddings[5]])[0][0]:.3f}")
print(f"  red ↔ blue:     {cosine_similarity([embeddings[8]], [embeddings[9]])[0][0]:.3f}")

print("\nAcross categories:")
print(f"  cat ↔ car:      {cosine_similarity([embeddings[0]], [embeddings[4]])[0][0]:.3f}")
print(f"  dog ↔ red:      {cosine_similarity([embeddings[1]], [embeddings[8]])[0][0]:.3f}")
```

**Key characteristics:**

- One vector per word (static, context-independent in classic models)
- Typically 100-300 dimensions
- Captures synonyms, analogies, and semantic relationships

### Sentence and Document Embeddings

Modern applications need to embed entire sentences or documents:

```{python}
#| code-fold: false

"""
Sentence Embeddings: Capturing Complete Thoughts

Sentence embeddings represent the meaning of entire sentences,
enabling semantic search and similarity comparison.
"""

from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity

model = SentenceTransformer('all-MiniLM-L6-v2')

# Sentences with similar meaning but different words
sentences = [
    "The quick brown fox jumps over the lazy dog",
    "A fast auburn fox leaps above a sleepy canine",
    "Machine learning models require lots of training data",
    "AI systems need substantial amounts of examples to learn",
]

embeddings = model.encode(sentences)

print("Sentence similarities:\n")
print("Similar meaning (paraphrases):")
print(f"  Sentence 1 ↔ 2: {cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]:.3f}")
print(f"  Sentence 3 ↔ 4: {cosine_similarity([embeddings[2]], [embeddings[3]])[0][0]:.3f}")

print("\nDifferent topics:")
print(f"  Sentence 1 ↔ 3: {cosine_similarity([embeddings[0]], [embeddings[2]])[0][0]:.3f}")
```

**When to use text embeddings:**

- Text classification, clustering, and sentiment analysis (see @sec-text-classification-clustering)
- Question answering and RAG systems (see @sec-rag-at-scale)
- Chatbots and conversational AI—intent matching, response selection (see @sec-conversational-ai)
- Summarization—finding representative sentences (see @sec-embedding-summarization)
- Semantic search—finding documents by meaning (see @sec-semantic-search)
- Recommendation systems—content-based filtering (see @sec-recommendation-systems)
- Customer support—ticket routing, finding similar issues (see @sec-cross-industry-patterns)
- Content moderation—detecting similar problematic content (see @sec-content-moderation)
- Duplicate and near-duplicate detection (see @sec-entity-resolution)
- Entity resolution—matching names and descriptions (see @sec-entity-resolution)
- Machine translation—cross-lingual embeddings (see @sec-defense-intelligence)

**Popular models:**

| Model | Dimensions | Speed | Quality | Best For |
|-------|-----------|-------|---------|----------|
| [all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) | 384 | Fast | Good | General purpose |
| [all-mpnet-base-v2](https://huggingface.co/sentence-transformers/all-mpnet-base-v2) | 768 | Medium | Better | Higher quality needs |
| [text-embedding-3-small](https://platform.openai.com/docs/guides/embeddings) | 1536 | API | Excellent | Production systems |
| [text-embedding-3-large](https://platform.openai.com/docs/guides/embeddings) | 3072 | API | Best | Maximum quality |

: Popular text embedding models {.striped}

See @sec-sentence-transformers for details on how these models are trained.

### Classification, Clustering, and Sentiment Analysis {#sec-text-classification-clustering}

Once you have text embeddings, three foundational tasks become straightforward: **classification** (assigning labels), **clustering** (discovering groups), and **sentiment analysis** (a special case of classification). All three leverage the same principle—similar texts have similar embeddings.

**Classification with embeddings:**

Train a simple classifier on top of frozen embeddings, or use nearest-neighbor approaches. The k-NN (k-nearest neighbors) method shown below works as follows: during training, embed each text and store the embedding alongside its label. To predict a new text's label, embed it, find the k training embeddings most similar to it (using cosine similarity), and return the most common label among those neighbors.

```{python}
#| echo: true
#| code-fold: true
#| code-summary: "Show Text Classifier"
import numpy as np
from collections import Counter


class EmbeddingClassifier:
    """Simple k-NN classifier using embeddings."""

    def __init__(self, encoder, k: int = 5):
        self.encoder = encoder
        self.k = k
        self.embeddings = []
        self.labels = []

    def fit(self, texts: list, labels: list):
        """Store training examples."""
        self.embeddings = [self.encoder.encode(text) for text in texts]
        self.labels = labels

    def predict(self, text: str) -> str:
        """Predict label using k-NN."""
        query_emb = self.encoder.encode(text)

        # Calculate distances to all training examples
        distances = []
        for i, emb in enumerate(self.embeddings):
            dist = np.dot(query_emb, emb) / (np.linalg.norm(query_emb) * np.linalg.norm(emb))
            distances.append((dist, self.labels[i]))

        # Get k nearest neighbors
        distances.sort(reverse=True)
        k_nearest = [label for _, label in distances[: self.k]]

        # Return most common label
        return Counter(k_nearest).most_common(1)[0][0]


# Example: Sentiment classification
from sentence_transformers import SentenceTransformer
encoder = SentenceTransformer('all-MiniLM-L6-v2')

classifier = EmbeddingClassifier(encoder, k=3)
classifier.fit(
    texts=["Great product!", "Loved it", "Terrible", "Waste of money", "Amazing quality"],
    labels=["positive", "positive", "negative", "negative", "positive"],
)
print(f"Prediction: {classifier.predict('This is wonderful!')}")
```

**Clustering with embeddings:**

Discover natural groups in your data using standard clustering algorithms:

```{python}
#| echo: true
#| code-fold: true
#| code-summary: "Show Text Clustering"
import numpy as np
from typing import List, Dict


class EmbeddingClusterer:
    """K-means clustering on text embeddings."""

    def __init__(self, encoder, n_clusters: int = 3):
        self.encoder = encoder
        self.n_clusters = n_clusters
        self.centroids = None

    def fit(self, texts: List[str], max_iters: int = 100):
        """Cluster texts and return assignments."""
        embeddings = np.array([self.encoder.encode(text) for text in texts])

        # Initialize centroids randomly
        indices = np.random.choice(len(embeddings), self.n_clusters, replace=False)
        self.centroids = embeddings[indices].copy()

        for _ in range(max_iters):
            # Assign points to nearest centroid
            assignments = []
            for emb in embeddings:
                distances = [np.linalg.norm(emb - c) for c in self.centroids]
                assignments.append(np.argmin(distances))

            # Update centroids
            new_centroids = []
            for i in range(self.n_clusters):
                cluster_points = embeddings[np.array(assignments) == i]
                if len(cluster_points) > 0:
                    new_centroids.append(cluster_points.mean(axis=0))
                else:
                    new_centroids.append(self.centroids[i])

            self.centroids = np.array(new_centroids)

        return assignments

    def get_cluster_examples(self, texts: List[str], assignments: List[int]) -> Dict[int, List[str]]:
        """Group texts by cluster."""
        clusters = {i: [] for i in range(self.n_clusters)}
        for text, cluster_id in zip(texts, assignments):
            clusters[cluster_id].append(text)
        return clusters


# Example: Topic discovery
texts = [
    "The stock market rose today",
    "Investors are optimistic about tech",
    "The basketball game was exciting",
    "Football season starts next week",
    "New AI model breaks records",
    "Machine learning advances continue",
]

clusterer = EmbeddingClusterer(encoder, n_clusters=3)
assignments = clusterer.fit(texts)
clusters = clusterer.get_cluster_examples(texts, assignments)
for cluster_id, examples in clusters.items():
    print(f"Cluster {cluster_id}: {examples[:2]}")
```

**Sentiment analysis:**

Sentiment is classification with polarity labels. For nuanced analysis, use the embedding's position relative to sentiment anchors:

```{python}
#| echo: true
#| code-fold: true
#| code-summary: "Show Sentiment Analyzer"
import numpy as np
from typing import Tuple


class SentimentAnalyzer:
    """Embedding-based sentiment analysis using anchor texts."""

    def __init__(self, encoder):
        self.encoder = encoder
        # Anchor texts define the sentiment space
        positive_anchors = ["excellent", "amazing", "wonderful", "fantastic", "love it"]
        negative_anchors = ["terrible", "awful", "horrible", "hate it", "worst ever"]

        # Compute anchor centroids
        self.positive_centroid = np.mean(
            [encoder.encode(t) for t in positive_anchors], axis=0
        )
        self.negative_centroid = np.mean(
            [encoder.encode(t) for t in negative_anchors], axis=0
        )

    def analyze(self, text: str) -> Tuple[str, float]:
        """
        Return sentiment label and confidence score.
        Score ranges from -1 (negative) to +1 (positive).
        """
        emb = self.encoder.encode(text)

        # Cosine similarity to each centroid
        pos_sim = np.dot(emb, self.positive_centroid) / (
            np.linalg.norm(emb) * np.linalg.norm(self.positive_centroid)
        )
        neg_sim = np.dot(emb, self.negative_centroid) / (
            np.linalg.norm(emb) * np.linalg.norm(self.negative_centroid)
        )

        # Score: positive if closer to positive centroid
        score = pos_sim - neg_sim
        label = "positive" if score > 0 else "negative"
        confidence = abs(score)

        return label, confidence


# Example usage
analyzer = SentimentAnalyzer(encoder)
for text in ["This product exceeded expectations!", "Complete waste of money"]:
    label, conf = analyzer.analyze(text)
    print(f"'{text[:30]}...' -> {label} ({conf:.2f})")
```

:::{.callout-tip}
## Classification and Clustering Best Practices

**For classification:**

- **Few-shot is often enough**: With good embeddings, 10-50 examples per class often suffices
- **k-NN for simplicity**: No training required, just store examples
- **Logistic regression for speed**: Train a simple linear classifier on embeddings
- **Fine-tune for best quality**: When you have thousands of examples, fine-tune the embedding model itself (see @sec-custom-embedding-strategies)

**For clustering:**

- **Choose k carefully**: Use elbow method or silhouette scores to find optimal cluster count
- **HDBSCAN for unknown k**: Automatically determines cluster count and handles outliers
- **Reduce dimensions first**: For visualization, use UMAP or t-SNE on embeddings
- **Label clusters post-hoc**: Examine cluster members to assign meaningful names

**For sentiment:**

- **Domain matters**: Financial sentiment differs from product reviews—use domain-specific anchors
- **Beyond binary**: Use multiple anchors (joy, anger, sadness) for fine-grained emotion detection
- **Aspect-based**: Extract aspects first ("battery life", "screen quality"), then analyze sentiment per aspect
:::

## Image Embeddings {#sec-image-embedding-types}

Image embeddings convert visual content into vectors that capture visual semantics:

```{python}
#| code-fold: false

"""
Image Embeddings: Visual Content as Vectors

Image embeddings capture visual features like shapes, colors, textures,
and objects, enabling visual similarity search.
"""

import torch
import numpy as np
from PIL import Image
from torchvision import models, transforms
from sklearn.metrics.pairwise import cosine_similarity

# Suppress download messages
import logging
logging.getLogger('torch').setLevel(logging.ERROR)

from torchvision.models import ResNet50_Weights

# Load pretrained ResNet50 as feature extractor
weights = ResNet50_Weights.IMAGENET1K_V1
model = models.resnet50(weights=None)
model.load_state_dict(weights.get_state_dict(progress=False))
model.eval()

# Remove classification head to get embeddings
feature_extractor = torch.nn.Sequential(*list(model.children())[:-1])

# Standard ImageNet preprocessing
preprocess = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

def get_image_embedding(image):
    """Extract embedding from PIL Image."""
    tensor = preprocess(image).unsqueeze(0)
    with torch.no_grad():
        embedding = feature_extractor(tensor)
    return embedding.squeeze().numpy()

# Create synthetic test images with different color patterns
np.random.seed(42)
images = {
    'red_pattern': Image.fromarray(
        np.random.randint([180, 0, 0], [255, 80, 80], (224, 224, 3), dtype=np.uint8)
    ),
    'blue_pattern': Image.fromarray(
        np.random.randint([0, 0, 180], [80, 80, 255], (224, 224, 3), dtype=np.uint8)
    ),
    'orange_pattern': Image.fromarray(
        np.random.randint([200, 100, 0], [255, 150, 50], (224, 224, 3), dtype=np.uint8)
    ),
}

# Get embeddings
embeddings = {name: get_image_embedding(img) for name, img in images.items()}

print("Image embedding similarities:\n")
print("Red and orange (similar warm colors) should be more similar than red and blue:")
red_orange = cosine_similarity([embeddings['red_pattern']], [embeddings['orange_pattern']])[0][0]
red_blue = cosine_similarity([embeddings['red_pattern']], [embeddings['blue_pattern']])[0][0]
print(f"  red ↔ orange: {red_orange:.3f}")
print(f"  red ↔ blue:   {red_blue:.3f}")
```

**Image embedding architectures:**

| Architecture | Type | Strengths | Use Cases |
|-------------|------|-----------|-----------|
| ResNet | CNN | Fast, proven | General visual search |
| EfficientNet | CNN | Efficient, accurate | Mobile/edge deployment |
| ViT | Transformer | Best accuracy | High-quality requirements |
| CLIP | Multi-modal | Text-image alignment | Zero-shot classification |

: Image embedding architectures {.striped}

**When to use image embeddings:**

- Visual similarity search (find similar products)
- Duplicate/near-duplicate detection
- Content-based image retrieval
- Visual clustering and organization
- Reverse image search

## Audio Embeddings {#sec-audio-embedding-types}

Audio embeddings capture acoustic features from speech, music, and environmental sounds. Let's create embeddings from real audio using MFCC (Mel-frequency cepstral coefficients) features:

```{python}
#| code-fold: false

"""
Audio Embeddings: Real Audio to Vectors

We'll use librosa to extract MFCC features from actual audio,
creating embeddings that capture timbral characteristics.
"""

import numpy as np
import librosa
from sklearn.metrics.pairwise import cosine_similarity

def audio_to_embedding(audio, sr, n_mfcc=20):
    """Convert audio waveform to a fixed-size embedding using MFCCs."""
    mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=n_mfcc)
    # Aggregate over time: mean and std of each coefficient
    return np.concatenate([mfccs.mean(axis=1), mfccs.std(axis=1)])

# Load librosa's built-in trumpet sample
audio, sr = librosa.load(librosa.ex('trumpet'))
trumpet_embedding = audio_to_embedding(audio, sr)

# Create variations to demonstrate similarity
trumpet_slow = librosa.effects.time_stretch(audio, rate=0.8)
trumpet_pitch_up = librosa.effects.pitch_shift(audio, sr=sr, n_steps=4)

# Generate embeddings for each variation
embeddings = {
    'trumpet_original': trumpet_embedding,
    'trumpet_slower': audio_to_embedding(trumpet_slow, sr),
    'trumpet_higher_pitch': audio_to_embedding(trumpet_pitch_up, sr),
}

print(f"Embedding dimension: {len(trumpet_embedding)} (20 MFCCs × 2 stats)\n")

# Compare similarities
print("Audio embedding similarities:")
sim_slow = cosine_similarity(
    [embeddings['trumpet_original']], [embeddings['trumpet_slower']]
)[0][0]
print(f"  Original ↔ Slower tempo:   {sim_slow:.3f}")

sim_pitch = cosine_similarity(
    [embeddings['trumpet_original']], [embeddings['trumpet_higher_pitch']]
)[0][0]
print(f"  Original ↔ Higher pitch:   {sim_pitch:.3f}")

sim_variations = cosine_similarity(
    [embeddings['trumpet_slower']], [embeddings['trumpet_higher_pitch']]
)[0][0]
print(f"  Slower ↔ Higher pitch:     {sim_variations:.3f}")
```

The tempo change preserves timbre (high similarity), while pitch shifting alters the spectral characteristics more significantly.

**Popular audio embedding models:**

- **Wav2Vec2**: Self-supervised speech representations
- **Whisper**: Multi-task speech model (transcription + embeddings)
- **CLAP**: Audio-text alignment (like CLIP for audio)
- **VGGish**: Audio event classification embeddings

**When to use audio embeddings:**

- Voice search and speaker identification
- Music recommendation and similarity
- Sound event detection and classification
- Audio fingerprinting
- Podcast/video content search

## Video Embeddings {#sec-video-embedding-types}

Video embeddings must capture both spatial (visual) and temporal (motion) information:

```{python}
#| code-fold: false

"""
Video Embeddings: Motion and Time

Video embeddings extend image embeddings to capture temporal dynamics:
actions, transitions, and narrative flow.

Approaches:
1. Frame-level: Embed individual frames, aggregate (mean, max, attention)
2. Clip-level: Models like X3D, SlowFast process multiple frames together
3. Two-stream: Separate spatial (RGB) and temporal (optical flow) processing
"""

import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

# Simulated video embeddings capturing action types
np.random.seed(42)

video_embeddings = {
    'running_outdoor': np.random.randn(512) + np.array([1, 0, 0] + [0]*509),
    'jogging_park': np.random.randn(512) + np.array([0.9, 0.1, 0] + [0]*509),
    'cooking_kitchen': np.random.randn(512) + np.array([0, 1, 0] + [0]*509),
    'baking_cake': np.random.randn(512) + np.array([0.1, 0.9, 0.2] + [0]*509),
    'cat_playing': np.random.randn(512) + np.array([0, 0, 1] + [0]*509),
}

print("Simulated video embedding similarities:\n")
print("Similar actions cluster together:")
run_jog = cosine_similarity(
    [video_embeddings['running_outdoor']],
    [video_embeddings['jogging_park']]
)[0][0]
cook_bake = cosine_similarity(
    [video_embeddings['cooking_kitchen']],
    [video_embeddings['baking_cake']]
)[0][0]
run_cook = cosine_similarity(
    [video_embeddings['running_outdoor']],
    [video_embeddings['cooking_kitchen']]
)[0][0]

print(f"  Running ↔ Jogging: {run_jog:.3f}")
print(f"  Cooking ↔ Baking:  {cook_bake:.3f}")
print(f"  Running ↔ Cooking: {run_cook:.3f}")
```

**Video embedding strategies:**

| Approach | Description | Pros | Cons |
|----------|-------------|------|------|
| Frame sampling | Embed N frames, average | Simple, fast | Misses motion |
| 3D CNNs | Process frame volumes | Captures motion | Computationally heavy |
| Two-stream | RGB + optical flow | Best accuracy | Complex pipeline |
| Video transformers | Attention over frames | State-of-the-art | Very expensive |

: Video embedding approaches {.striped}

**When to use video embeddings:**

- Action recognition and search
- Video recommendation
- Content moderation
- Surveillance and anomaly detection
- Video summarization

## Multi-Modal Embeddings {#sec-multimodal-embedding-types}

Multi-modal embeddings align different data types in a shared space:

```{python}
#| code-fold: false

"""
Multi-Modal Embeddings: Bridging Modalities

Multi-modal models like CLIP learn a shared embedding space where
text and images can be directly compared. This enables:
- Text-to-image search ("find images of cats")
- Image-to-text search (find descriptions for an image)
- Zero-shot classification (classify without training examples)
"""

import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

# Simulated CLIP-style embeddings where text and images share a space
np.random.seed(42)

# In a real CLIP model, these would be 512-768 dimensional
# and text/image of the same concept would have similar vectors
embeddings = {
    # Text embeddings
    'text: a photo of a cat': np.array([0.8, 0.1, 0.2, 0.1]),
    'text: a picture of a dog': np.array([0.7, 0.3, 0.2, 0.1]),
    'text: a red sports car': np.array([0.1, 0.1, 0.9, 0.3]),
    # Image embeddings (would come from image encoder)
    'image: cat_photo.jpg': np.array([0.75, 0.15, 0.25, 0.1]),
    'image: dog_photo.jpg': np.array([0.65, 0.35, 0.15, 0.15]),
    'image: car_photo.jpg': np.array([0.15, 0.1, 0.85, 0.35]),
}

print("Multi-modal embedding alignment:\n")
print("Text queries matched to images:")

text_cat = embeddings['text: a photo of a cat']
for name, emb in embeddings.items():
    if name.startswith('image:'):
        sim = cosine_similarity([text_cat], [emb])[0][0]
        print(f"  'a photo of a cat' ↔ {name}: {sim:.3f}")
```

**Multi-modal embedding models:**

- **CLIP** (OpenAI): Text-image alignment, 400M image-text pairs
- **BLIP/BLIP-2**: Image captioning + retrieval
- **ImageBind** (Meta): Aligns 6 modalities (image, text, audio, depth, thermal, IMU)
- **LLaVA**: Large language model with vision

**When to use multi-modal embeddings:**

- Cross-modal search (text→image, image→text)
- Zero-shot image classification
- Image captioning
- Visual question answering
- Product search with text and images

## Graph Embeddings {#sec-graph-embedding-types}

Graph embeddings represent nodes, edges, and subgraphs in vector space:

```{python}
#| code-fold: false

"""
Graph Embeddings: Relationships as Vectors

Graph embeddings capture structural relationships between entities.
Nodes that are connected or share neighbors get similar embeddings.

Applications: Knowledge graphs, social networks, molecule structures
"""

import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

# Simulated node embeddings from a social network
# In practice, use models like Node2Vec, GraphSAGE, or GNN-based approaches
np.random.seed(42)

# Imagine a graph where:
# - Alice, Bob, Carol are in the same friend group
# - Xavier, Yuki, Zara are in a different friend group
# - Nodes in the same community should have similar embeddings

node_embeddings = {
    'Alice': np.random.randn(64) + np.array([1, 0] + [0]*62),
    'Bob': np.random.randn(64) + np.array([0.9, 0.1] + [0]*62),
    'Carol': np.random.randn(64) + np.array([0.8, 0.2] + [0]*62),
    'Xavier': np.random.randn(64) + np.array([0, 1] + [0]*62),
    'Yuki': np.random.randn(64) + np.array([0.1, 0.9] + [0]*62),
    'Zara': np.random.randn(64) + np.array([0.2, 0.8] + [0]*62),
}

print("Graph embedding similarities (same community = higher):\n")
print("Within community 1:")
ab = cosine_similarity([node_embeddings['Alice']], [node_embeddings['Bob']])[0][0]
ac = cosine_similarity([node_embeddings['Alice']], [node_embeddings['Carol']])[0][0]
print(f"  Alice ↔ Bob:   {ab:.3f}")
print(f"  Alice ↔ Carol: {ac:.3f}")

print("\nAcross communities:")
ax = cosine_similarity([node_embeddings['Alice']], [node_embeddings['Xavier']])[0][0]
print(f"  Alice ↔ Xavier: {ax:.3f}")
```

**Graph embedding methods:**

| Method | Approach | Best For |
|--------|----------|----------|
| Node2Vec | Random walks + Word2Vec | Homogeneous graphs |
| GraphSAGE | Neighborhood aggregation | Inductive learning |
| GAT | Attention over neighbors | Weighted importance |
| Knowledge Graph Embeddings | TransE, RotatE, etc. | Link prediction |

: Graph embedding methods {.striped}

**When to use graph embeddings:**

- Social network analysis
- Recommendation systems (user-item graphs)
- Knowledge graph completion
- Fraud detection (transaction graphs)
- Drug discovery (molecular graphs)

## Time-Series Embeddings {#sec-timeseries-embedding-types}

Time-series embeddings capture temporal patterns and dynamics:

```{python}
#| code-fold: false

"""
Time-Series Embeddings: Patterns Over Time

Time-series embeddings capture temporal dynamics, trends, and patterns.
Similar time-series (e.g., similar sensor readings) cluster together.
"""

import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

np.random.seed(42)

# Simulate different time-series patterns
def generate_pattern(pattern_type, length=100):
    t = np.linspace(0, 4*np.pi, length)
    if pattern_type == 'sine':
        return np.sin(t) + np.random.randn(length) * 0.1
    elif pattern_type == 'cosine':
        return np.cos(t) + np.random.randn(length) * 0.1
    elif pattern_type == 'increasing':
        return t/10 + np.random.randn(length) * 0.2
    elif pattern_type == 'decreasing':
        return -t/10 + 5 + np.random.randn(length) * 0.2
    else:
        return np.random.randn(length)

# Simple embedding: statistical features (real systems use learned embeddings)
def simple_ts_embedding(series):
    return np.array([
        np.mean(series),
        np.std(series),
        np.max(series) - np.min(series),
        np.mean(np.diff(series)),  # trend
        np.corrcoef(series[:-1], series[1:])[0,1],  # autocorrelation
    ])

# Generate and embed time series
patterns = {
    'sine_wave_1': generate_pattern('sine'),
    'sine_wave_2': generate_pattern('sine'),
    'trend_up_1': generate_pattern('increasing'),
    'trend_up_2': generate_pattern('increasing'),
    'random_1': generate_pattern('random'),
}

embeddings = {name: simple_ts_embedding(ts) for name, ts in patterns.items()}

print("Time-series embedding similarities:\n")
print("Similar patterns cluster together:")
sine_sim = cosine_similarity([embeddings['sine_wave_1']], [embeddings['sine_wave_2']])[0][0]
trend_sim = cosine_similarity([embeddings['trend_up_1']], [embeddings['trend_up_2']])[0][0]
cross_sim = cosine_similarity([embeddings['sine_wave_1']], [embeddings['trend_up_1']])[0][0]

print(f"  Sine wave 1 ↔ Sine wave 2:  {sine_sim:.3f}")
print(f"  Trend up 1 ↔ Trend up 2:    {trend_sim:.3f}")
print(f"  Sine wave ↔ Trend up:       {cross_sim:.3f}")
```

**Time-series embedding approaches:**

- **Learned features**: LSTMs, Transformers, TCNs trained on time-series
- **Self-supervised**: Contrastive learning on augmented time-series
- **Statistical**: Hand-crafted features (mean, variance, entropy, etc.)
- **Frequency domain**: FFT-based representations

**When to use time-series embeddings:**

- Anomaly detection in sensor data
- Predictive maintenance
- Financial pattern recognition
- Health monitoring (ECG, EEG)
- IoT device fingerprinting

## Code Embeddings {#sec-code-embedding-types}

Code embeddings represent programs, functions, and code snippets:

```{python}
#| code-fold: false

"""
Code Embeddings: Programs as Vectors

Code embeddings capture the semantics of source code, enabling:
- Code search (find code by description)
- Clone detection (find duplicate/similar code)
- Bug detection (identify anomalous patterns)
"""

from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity

# Use a code-aware model (or general sentence transformer for demo)
model = SentenceTransformer('all-MiniLM-L6-v2')

# Code snippets with similar functionality but different implementations
code_snippets = {
    'sum_loop': '''
def sum_numbers(nums):
    total = 0
    for n in nums:
        total += n
    return total
''',
    'sum_builtin': '''
def sum_numbers(numbers):
    return sum(numbers)
''',
    'reverse_loop': '''
def reverse_list(lst):
    result = []
    for i in range(len(lst)-1, -1, -1):
        result.append(lst[i])
    return result
''',
    'reverse_slice': '''
def reverse_list(items):
    return items[::-1]
''',
}

embeddings = {name: model.encode(code) for name, code in code_snippets.items()}

print("Code embedding similarities:\n")
print("Functionally similar code clusters together:")
sum_sim = cosine_similarity([embeddings['sum_loop']], [embeddings['sum_builtin']])[0][0]
rev_sim = cosine_similarity([embeddings['reverse_loop']], [embeddings['reverse_slice']])[0][0]
cross_sim = cosine_similarity([embeddings['sum_loop']], [embeddings['reverse_loop']])[0][0]

print(f"  sum (loop) ↔ sum (builtin):     {sum_sim:.3f}")
print(f"  reverse (loop) ↔ reverse (slice): {rev_sim:.3f}")
print(f"  sum ↔ reverse:                   {cross_sim:.3f}")
```

**Code embedding models:**

- **CodeBERT**: BERT-style model trained on code
- **GraphCodeBERT**: Incorporates data flow graphs
- **StarCoder**: Large code model with embeddings
- **Codex/GPT-4**: Can generate embeddings via API

**When to use code embeddings:**

- Semantic code search
- Code clone detection
- Vulnerability detection
- Code recommendation
- Repository organization

## Choosing the Right Embedding Type

Here's a decision framework for selecting embedding types:

```
                    What is your primary data type?
                              │
        ┌──────────┬──────────┼──────────┬──────────┐
        │          │          │          │          │
      Text      Images     Audio/    Multiple   Structured/
        │          │       Video     Modalities   Relational
        │          │          │          │          │
        ▼          ▼          ▼          ▼          ▼
   Sentence    CNN/ViT    Domain-    CLIP/      Graph
  Transformers  CLIP     specific  ImageBind  Embeddings
```

| Scenario | Recommended Approach |
|----------|---------------------|
| Search documents by meaning | Text embeddings (sentence transformers) |
| Find visually similar images | Image embeddings (ResNet, CLIP) |
| Match images to text queries | Multi-modal (CLIP) |
| Find similar entities in a network | Graph embeddings (GraphSAGE) |
| Detect anomalies in sensor data | Time-series embeddings |
| Search code by functionality | Code embeddings (CodeBERT) |
| Match voice to speaker | Audio embeddings (speaker verification) |

: Embedding type selection guide {.striped}

## Key Takeaways

- **Different data types require different embedding architectures** that match the structure of the data

- **Text embeddings** are the most mature, with sentence transformers providing excellent quality for most applications

- **Image embeddings** use CNNs or Vision Transformers to capture visual semantics

- **Multi-modal embeddings** like CLIP enable cross-modal search and zero-shot classification

- **Graph embeddings** capture relational structure, essential for social networks and knowledge graphs

- **Time-series embeddings** encode temporal patterns for anomaly detection and similarity search

- **Code embeddings** understand program semantics, enabling semantic code search

## Looking Ahead

Now that you understand the foundational embedding types, @sec-advanced-embedding-patterns covers advanced patterns used in production systems—hybrid vectors that combine multiple feature types, multi-vector representations, learned sparse embeddings, and more. For deep dives into how specific models work, @sec-embedding-model-fundamentals explains the underlying architectures. Then @sec-strategic-architecture covers how to architect embedding systems that can handle multiple modalities at scale.

## Further Reading

- Reimers, N. & Gurevych, I. (2019). "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks." *arXiv:1908.10084*
- Radford, A., et al. (2021). "Learning Transferable Visual Models From Natural Language Supervision." *arXiv:2103.00020* (CLIP)
- Grover, A. & Leskovec, J. (2016). "node2vec: Scalable Feature Learning for Networks." *KDD*
- Baevski, A., et al. (2020). "wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations." *NeurIPS*
- Feng, Z., et al. (2020). "CodeBERT: A Pre-Trained Model for Programming and Natural Languages." *EMNLP*
