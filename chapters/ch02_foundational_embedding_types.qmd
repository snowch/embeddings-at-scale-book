# Foundational Embedding Types {#sec-foundational-embedding-types}

::: callout-note
## Chapter Overview

This chapter provides a comprehensive tour of foundational embedding types across different data modalities. We examine text, image, audio, video, multi-modal, graph, time-series, and code embeddings—understanding what makes each unique, when to use them, and how they're created. These foundational types form the building blocks that production systems combine and extend using the advanced patterns covered in @sec-advanced-embedding-patterns.
:::

## The Embedding Landscape

Every type of data can be converted to embeddings, but different data types require different approaches. The key insight is that **the embedding architecture must match the structure of your data**:

- **Text** (words, sentences, documents): Transformer models capture context and semantic meaning
- **Images**: CNNs and Vision Transformers capture spatial patterns and visual features
- **Audio** (speech, music, sounds): Spectral models process frequency patterns over time
- **Video**: Temporal models combine frame-level features with motion understanding
- **Multi-modal** (text + images): Alignment models map different modalities to a shared space
- **Graphs** (networks, relationships): Message-passing models aggregate neighborhood information
- **Time-series** (sensors, sequences): Recurrent and convolutional models capture temporal patterns
- **Code** (programs, functions): Specialized models understand syntax and program semantics

Let's explore each embedding type in depth.

## Text Embeddings {#sec-text-embedding-types}

Text embeddings are the most mature and widely used embedding type. They've evolved through several generations:

### Word Embeddings

The foundation of modern NLP, word embeddings map individual words to vectors:

```{python}
#| code-fold: false

"""
Word Embeddings: From Words to Vectors

Word embeddings capture semantic relationships between individual words.
Words with similar meanings cluster together in the embedding space.
"""

import os
os.environ["TOKENIZERS_PARALLELISM"] = "false"

from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

# Use a sentence model to embed individual words
model = SentenceTransformer('all-MiniLM-L6-v2')

# Embed words across different categories
words = {
    'animals': ['cat', 'dog', 'elephant', 'whale'],
    'vehicles': ['car', 'truck', 'airplane', 'boat'],
    'colors': ['red', 'blue', 'green', 'yellow'],
}

all_words = [w for group in words.values() for w in group]
embeddings = model.encode(all_words)

# Show that words cluster by category
print("Word similarities (same category = higher similarity):\n")
print("Within categories:")
print(f"  cat ↔ dog:      {cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]:.3f}")
print(f"  car ↔ truck:    {cosine_similarity([embeddings[4]], [embeddings[5]])[0][0]:.3f}")
print(f"  red ↔ blue:     {cosine_similarity([embeddings[8]], [embeddings[9]])[0][0]:.3f}")

print("\nAcross categories:")
print(f"  cat ↔ car:      {cosine_similarity([embeddings[0]], [embeddings[4]])[0][0]:.3f}")
print(f"  dog ↔ red:      {cosine_similarity([embeddings[1]], [embeddings[8]])[0][0]:.3f}")
```

**Key characteristics:**

- One vector per word (static, context-independent in classic models)
- Typically 100-300 dimensions
- Captures synonyms, analogies, and semantic relationships

### Sentence and Document Embeddings

Modern applications need to embed entire sentences or documents:

```{python}
#| code-fold: false

"""
Sentence Embeddings: Capturing Complete Thoughts

Sentence embeddings represent the meaning of entire sentences,
enabling semantic search and similarity comparison.
"""

from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity

model = SentenceTransformer('all-MiniLM-L6-v2')

# Sentences with similar meaning but different words
sentences = [
    "The quick brown fox jumps over the lazy dog",
    "A fast auburn fox leaps above a sleepy canine",
    "Machine learning models require lots of training data",
    "AI systems need substantial amounts of examples to learn",
]

embeddings = model.encode(sentences)

print("Sentence similarities:\n")
print("Similar meaning (paraphrases):")
print(f"  Sentence 1 ↔ 2: {cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]:.3f}")
print(f"  Sentence 3 ↔ 4: {cosine_similarity([embeddings[2]], [embeddings[3]])[0][0]:.3f}")

print("\nDifferent topics:")
print(f"  Sentence 1 ↔ 3: {cosine_similarity([embeddings[0]], [embeddings[2]])[0][0]:.3f}")
```

**When to use text embeddings:**

- Text classification, clustering, and sentiment analysis (see @sec-text-classification-clustering)
- Question answering and RAG systems (see @sec-rag-at-scale)
- Chatbots and conversational AI—intent matching, response selection (see @sec-conversational-ai)
- Summarization—finding representative sentences (see @sec-embedding-summarization)
- Semantic search—finding documents by meaning (see @sec-semantic-search)
- Recommendation systems—content-based filtering (see @sec-recommendation-systems)
- Customer support—ticket routing, finding similar issues (see @sec-cross-industry-patterns)
- Content moderation—detecting similar problematic content (see @sec-content-moderation)
- Duplicate and near-duplicate detection (see @sec-entity-resolution)
- Entity resolution—matching names and descriptions (see @sec-entity-resolution)
- Machine translation—cross-lingual embeddings (see @sec-defense-intelligence)

**Popular models:**

| Model | Dimensions | Speed | Quality | Best For |
|-------|-----------|-------|---------|----------|
| [all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) | 384 | Fast | Good | General purpose |
| [all-mpnet-base-v2](https://huggingface.co/sentence-transformers/all-mpnet-base-v2) | 768 | Medium | Better | Higher quality needs |
| [text-embedding-3-small](https://platform.openai.com/docs/guides/embeddings) | 1536 | API | Excellent | Production systems |
| [text-embedding-3-large](https://platform.openai.com/docs/guides/embeddings) | 3072 | API | Best | Maximum quality |

: Popular text embedding models {.striped}

See @sec-sentence-transformers for details on how these models are trained.

### Classification, Clustering, and Sentiment Analysis {#sec-text-classification-clustering}

Once you have text embeddings, three foundational tasks become straightforward: **classification** (assigning labels), **clustering** (discovering groups), and **sentiment analysis** (a special case of classification). All three leverage the same principle—similar texts have similar embeddings.

**Classification with embeddings:**

Train a simple classifier on top of frozen embeddings, or use nearest-neighbor approaches. The k-NN (k-nearest neighbors) method shown below works as follows: during training, embed each text and store the embedding alongside its label. To predict a new text's label, embed it, find the k training embeddings most similar to it (using cosine similarity), and return the most common label among those neighbors.

```{python}
#| echo: true
#| code-fold: true
#| code-summary: "Show Text Classifier"
import numpy as np
from collections import Counter


class EmbeddingClassifier:
    """Simple k-NN classifier using embeddings."""

    def __init__(self, encoder, k: int = 5):
        self.encoder = encoder
        self.k = k
        self.embeddings = []
        self.labels = []

    def fit(self, texts: list, labels: list):
        """Embed texts and store with their labels."""
        self.embeddings = [self.encoder.encode(text) for text in texts]
        self.labels = labels

    def predict(self, text: str) -> str:
        """Predict label using k-NN."""
        query_emb = self.encoder.encode(text)

        # Cosine similarity: (A · B) / (||A|| × ||B||)
        distances = []
        for i, emb in enumerate(self.embeddings):
            dist = np.dot(query_emb, emb) / (np.linalg.norm(query_emb) * np.linalg.norm(emb))
            distances.append((dist, self.labels[i]))

        # Get k nearest neighbors
        distances.sort(reverse=True)
        k_nearest = [label for _, label in distances[: self.k]]

        # Return most common label
        return Counter(k_nearest).most_common(1)[0][0]


# Example: Sentiment classification
from sentence_transformers import SentenceTransformer
encoder = SentenceTransformer('all-MiniLM-L6-v2')

classifier = EmbeddingClassifier(encoder, k=3)
classifier.fit(
    texts=["Great product!", "Loved it", "Terrible", "Waste of money", "Amazing quality"],
    labels=["positive", "positive", "negative", "negative", "positive"],
)
print(f"Prediction: {classifier.predict('This is wonderful!')}")
```

**Clustering with embeddings:**

Clustering discovers natural groups in your data without predefined labels. Since similar texts have similar embeddings, texts on the same topic will cluster together in embedding space.

K-means is a popular clustering algorithm. You specify k (the number of clusters), and the algorithm finds k groups by positioning a *centroid* (center point) for each cluster. Each text belongs to the cluster whose centroid is closest to its embedding.

The algorithm works as follows: first, embed all texts. Then pick k random embeddings as initial centroids—the algorithm needs starting points before it can begin refining. Next, iterate until convergence: (1) assign each embedding to its nearest centroid (measured by Euclidean distance), and (2) update each centroid to be the mean of its assigned embeddings. The algorithm converges when assignments stop changing.

```{python}
#| echo: false
#| label: fig-kmeans-clustering
#| fig-cap: "K-means clustering in 2D embedding space. Points are colored by cluster assignment, with centroids marked as X."
import matplotlib.pyplot as plt
import numpy as np

np.random.seed(42)

# Generate sample clusters
cluster1 = np.random.randn(15, 2) * 0.5 + [2, 2]
cluster2 = np.random.randn(15, 2) * 0.5 + [-1, -1]
cluster3 = np.random.randn(15, 2) * 0.5 + [2, -1.5]
points = np.vstack([cluster1, cluster2, cluster3])

# Compute centroids
centroids = np.array([cluster1.mean(axis=0), cluster2.mean(axis=0), cluster3.mean(axis=0)])

# Assign colors
colors = ['#4C72B0'] * 15 + ['#DD8452'] * 15 + ['#55A868'] * 15

fig, ax = plt.subplots(figsize=(6, 5))
ax.scatter(points[:, 0], points[:, 1], c=colors, s=60, alpha=0.7)
ax.scatter(centroids[:, 0], centroids[:, 1], c=['#4C72B0', '#DD8452', '#55A868'],
           s=200, marker='X', edgecolors='black', linewidths=2)
ax.set_xlabel('Embedding dimension 1')
ax.set_ylabel('Embedding dimension 2')
ax.set_title('K-means: 3 clusters with centroids')
plt.tight_layout()
plt.show()
```

```{python}
#| echo: true
#| code-fold: true
#| code-summary: "Show Text Clustering"
import numpy as np
from typing import List, Dict


class EmbeddingClusterer:
    """K-means clustering on text embeddings."""

    def __init__(self, encoder, n_clusters: int = 3):
        self.encoder = encoder
        self.n_clusters = n_clusters
        self.centroids = None

    def fit(self, texts: List[str], max_iters: int = 100):
        """Cluster texts and return assignments."""
        embeddings = np.array([self.encoder.encode(text) for text in texts])

        # Initialize centroids by picking k random embeddings as starting points
        indices = np.random.choice(len(embeddings), self.n_clusters, replace=False)
        self.centroids = embeddings[indices].copy()

        for _ in range(max_iters):
            # Assign points to nearest centroid
            assignments = []
            for emb in embeddings:
                distances = [np.linalg.norm(emb - c) for c in self.centroids]
                assignments.append(np.argmin(distances))

            # Update centroids
            new_centroids = []
            for i in range(self.n_clusters):
                cluster_points = embeddings[np.array(assignments) == i]
                if len(cluster_points) > 0:
                    new_centroids.append(cluster_points.mean(axis=0))
                else:
                    new_centroids.append(self.centroids[i])

            self.centroids = np.array(new_centroids)

        return assignments

    def get_cluster_examples(self, texts: List[str], assignments: List[int]) -> Dict[int, List[str]]:
        """Group texts by cluster."""
        clusters = {i: [] for i in range(self.n_clusters)}
        for text, cluster_id in zip(texts, assignments):
            clusters[cluster_id].append(text)
        return clusters


# Example: Topic discovery
texts = [
    # Cooking
    "Chop the onions and garlic finely",
    "Simmer the sauce for twenty minutes",
    "Season with salt and pepper to taste",
    "Preheat the oven to 350 degrees",
    # Space
    "The telescope discovered a new exoplanet",
    "Astronauts completed their spacewalk today",
    "The Mars rover collected soil samples",
    "A new comet is visible this month",
    # Weather
    "Heavy rain expected throughout the weekend",
    "Temperatures will drop below freezing tonight",
    "A warm front is moving in from the south",
    "Clear skies and sunshine forecast for Monday",
]

np.random.seed(42)  # For reproducible results
clusterer = EmbeddingClusterer(encoder, n_clusters=3)
assignments = clusterer.fit(texts)
clusters = clusterer.get_cluster_examples(texts, assignments)
for cluster_id, examples in clusters.items():
    print(f"\nCluster {cluster_id}:")
    for text in examples:
        print(f"  - {text}")
```

Notice that some items may appear in unexpected clusters. Embeddings capture semantic similarity that doesn't always match our intuitive topic categories—"Preheat the oven to 350 degrees" mentions temperature, which may pull it toward weather texts, while "A warm front is moving in" uses directional language similar to space descriptions. This is a feature, not a bug: embeddings capture meaning patterns that humans might overlook.

**Sentiment analysis:**

Sentiment analysis determines whether text expresses positive, negative, or neutral opinions. While you could treat this as classification (train on labeled examples), an elegant alternative uses *anchor texts*—words or phrases with known sentiment.

The approach works as follows: embed a set of clearly positive words ("excellent", "amazing", "love it") and compute their centroid. Do the same for negative words. To analyze new text, embed it and measure which centroid it's closer to. The difference in distances gives both a label and a confidence score—text much closer to the positive centroid is strongly positive.

```{python}
#| echo: false
#| label: fig-sentiment-anchors
#| fig-cap: "Sentiment analysis using anchor texts. Positive and negative anchor words form centroids. New text is classified by which centroid it's closer to."
import matplotlib.pyplot as plt
import numpy as np

np.random.seed(123)

# Positive anchors (right side)
pos_anchors = np.random.randn(5, 2) * 0.3 + [2, 0.5]
pos_centroid = pos_anchors.mean(axis=0)

# Negative anchors (left side)
neg_anchors = np.random.randn(5, 2) * 0.3 + [-2, 0.5]
neg_centroid = neg_anchors.mean(axis=0)

# New text to classify (closer to positive)
new_text = np.array([0.8, 0.3])

fig, ax = plt.subplots(figsize=(8, 5))

# Plot anchors
ax.scatter(pos_anchors[:, 0], pos_anchors[:, 1], c='#55A868', s=80, alpha=0.6, label='Positive anchors')
ax.scatter(neg_anchors[:, 0], neg_anchors[:, 1], c='#C44E52', s=80, alpha=0.6, label='Negative anchors')

# Plot centroids
ax.scatter(*pos_centroid, c='#55A868', s=250, marker='X', edgecolors='black', linewidths=2, zorder=5)
ax.scatter(*neg_centroid, c='#C44E52', s=250, marker='X', edgecolors='black', linewidths=2, zorder=5)

# Plot new text
ax.scatter(*new_text, c='#4C72B0', s=150, marker='s', edgecolors='black', linewidths=2, label='New text', zorder=5)

# Draw distance lines
ax.plot([new_text[0], pos_centroid[0]], [new_text[1], pos_centroid[1]], 'g--', alpha=0.7, linewidth=2)
ax.plot([new_text[0], neg_centroid[0]], [new_text[1], neg_centroid[1]], 'r--', alpha=0.7, linewidth=2)

# Labels
ax.annotate('Positive\ncentroid', pos_centroid + [0.1, 0.4], fontsize=10, ha='center')
ax.annotate('Negative\ncentroid', neg_centroid + [0.1, 0.4], fontsize=10, ha='center')
ax.annotate('"Great product!"', new_text + [0, -0.4], fontsize=10, ha='center', style='italic')

ax.set_xlabel('Embedding dimension 1')
ax.set_ylabel('Embedding dimension 2')
ax.set_title('Sentiment: closer to positive centroid → positive sentiment')
ax.legend(loc='upper right')
plt.tight_layout()
plt.show()
```

```{python}
#| echo: true
#| code-fold: true
#| code-summary: "Show Sentiment Analyzer"
import numpy as np
from typing import Tuple


class SentimentAnalyzer:
    """Embedding-based sentiment analysis using anchor texts."""

    def __init__(self, encoder):
        self.encoder = encoder
        # Anchor texts define the sentiment space
        positive_anchors = ["excellent", "amazing", "wonderful", "fantastic", "love it"]
        negative_anchors = ["terrible", "awful", "horrible", "hate it", "worst ever"]

        # Compute anchor centroids: average the embeddings of all anchor words
        # to find the "center" of positive/negative regions in embedding space.
        # Using multiple anchors makes the centroid more robust than any single word.
        self.positive_centroid = np.mean(
            [encoder.encode(t) for t in positive_anchors], axis=0
        )
        self.negative_centroid = np.mean(
            [encoder.encode(t) for t in negative_anchors], axis=0
        )

    def analyze(self, text: str) -> Tuple[str, float]:
        """
        Return sentiment label and confidence score.
        Score ranges from -1 (negative) to +1 (positive).
        """
        emb = self.encoder.encode(text)

        # Cosine similarity: (A · B) / (||A|| × ||B||)
        pos_sim = np.dot(emb, self.positive_centroid) / (
            np.linalg.norm(emb) * np.linalg.norm(self.positive_centroid)
        )
        neg_sim = np.dot(emb, self.negative_centroid) / (
            np.linalg.norm(emb) * np.linalg.norm(self.negative_centroid)
        )

        # Score: positive if closer to positive centroid
        score = pos_sim - neg_sim
        label = "positive" if score > 0 else "negative"
        confidence = abs(score)

        return label, confidence


# Example usage
np.random.seed(42)
analyzer = SentimentAnalyzer(encoder)
for text in ["This product exceeded expectations!", "Complete waste of money"]:
    label, conf = analyzer.analyze(text)
    print(f"'{text[:30]}...' -> {label} ({conf:.2f})")
```

The number in parentheses is the confidence score—the difference between cosine similarities to the positive and negative centroids. These values appear low because general-purpose embeddings capture broad semantics, not just sentiment. The key insight is that the *relative* scores still correctly distinguish positive from negative text, even when absolute differences are small. For production sentiment analysis, you'd typically fine-tune embeddings on sentiment-labeled data (see @sec-custom-embedding-strategies).

:::{.callout-tip}
## Classification and Clustering Best Practices

**For classification:**

- **Few-shot is often enough**: With good embeddings, 10-50 examples per class often suffices (see @sec-siamese-networks for few-shot techniques)
- **k-NN for simplicity**: No training required, just store examples
- **Logistic regression for speed**: Train a simple linear classifier on embeddings
- **Fine-tune for best quality**: When you have thousands of examples, fine-tune the embedding model itself (see @sec-custom-embedding-strategies)

**For clustering:**

- **Choose k carefully**: Use elbow method or silhouette scores to find optimal cluster count
- **[HDBSCAN](https://hdbscan.readthedocs.io/) for unknown k**: Unlike k-means, HDBSCAN doesn't require specifying cluster count upfront—it discovers clusters based on density and labels sparse points as outliers rather than forcing them into clusters
- **Reduce dimensions first**: For visualization, use UMAP or t-SNE on embeddings
- **Label clusters post-hoc**: Examine cluster members to assign meaningful names

**For sentiment:**

- **Domain matters**: Financial sentiment differs from product reviews—use domain-specific anchors (see @sec-financial-services for financial sentiment)
- **Beyond binary**: Instead of just positive/negative centroids, create centroids for multiple emotions (joy, anger, sadness, fear, surprise). Measure distance to each and return the closest emotion, or return a distribution across all emotions for nuanced analysis.
- **Aspect-based**: Reviews often mix sentiment across topics ("great battery, terrible screen"). First extract aspects (product features, service elements), then run sentiment analysis on each aspect separately to understand what users love and hate.
:::

## Image Embeddings {#sec-image-embedding-types}

Image embeddings convert visual content into vectors that capture visual semantics—shapes, colors, textures, objects, and spatial relationships. Unlike pixel-by-pixel comparison, embeddings understand that two photos of the same cat are similar even if taken from different angles or lighting conditions.

The example below uses ResNet50 [@he2016resnet], a CNN pre-trained on ImageNet's 1.4 million images. ResNet learns hierarchical visual features—early layers detect edges and textures, middle layers recognize shapes and parts, and deep layers understand objects and scenes. We remove the final classification layer to extract the 2048-dimensional feature vector as our embedding.

Digital images are stored as 3D arrays with shape (height, width, 3)—the third dimension holds Red, Green, and Blue color intensities for each pixel:

```{python}
#| echo: false
#| label: fig-image-rgb-cube
#| fig-cap: "An image is a 3D array: height × width × 3 color channels (Red, Green, Blue). Each pixel has three values (0-255)."
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d.art3d import Poly3DCollection
import numpy as np

fig = plt.figure(figsize=(8, 5))
ax = fig.add_subplot(111, projection='3d')

w, h, c = 224, 224, 3
scale = 1/224
sw, sh, sc = w * scale, h * scale, c * 0.3

vertices = [
    [0, 0, 0], [sw, 0, 0], [sw, sh, 0], [0, sh, 0],
    [0, 0, sc], [sw, 0, sc], [sw, sh, sc], [0, sh, sc]
]

colors = ['#FF6B6B', '#6BCB77', '#4D96FF']
labels = ['Red channel', 'Green channel', 'Blue channel']

for i, (color, label) in enumerate(zip(colors, labels)):
    z = i * sc / 3 + sc / 6
    slice_verts = [[0, 0, z], [sw, 0, z], [sw, sh, z], [0, sh, z]]
    ax.add_collection3d(Poly3DCollection([slice_verts], alpha=0.7, facecolor=color, edgecolor='black', linewidth=1))
    ax.text(sw + 0.1, sh/2, z, label, fontsize=9)

for i, j in [(0,1), (1,2), (2,3), (3,0), (4,5), (5,6), (6,7), (7,4), (0,4), (1,5), (2,6), (3,7)]:
    ax.plot3D(*zip(vertices[i], vertices[j]), 'k-', alpha=0.3, linewidth=0.5)

ax.set_xlabel('Width (224 px)', fontsize=10)
ax.set_ylabel('Height (224 px)', fontsize=10)
ax.set_zlabel('Channels', fontsize=10)
ax.set_title('Image as a 3D array: (224, 224, 3)', fontsize=11, fontweight='bold')
ax.view_init(elev=20, azim=45)
ax.set_box_aspect([1, 1, 0.3])
ax.set_xticks([])
ax.set_yticks([])
ax.set_zticks([])
plt.tight_layout()
plt.show()
```

```{python}
#| code-fold: false

"""
Image Embeddings: Visual Content as Vectors
"""

import torch
import numpy as np
from PIL import Image
from torchvision import models, transforms
from sklearn.metrics.pairwise import cosine_similarity

# Suppress download messages
import logging
logging.getLogger('torch').setLevel(logging.ERROR)

from torchvision.models import ResNet50_Weights

# Load pretrained ResNet50 as feature extractor
weights = ResNet50_Weights.IMAGENET1K_V1
model = models.resnet50(weights=None)
model.load_state_dict(weights.get_state_dict(progress=False))
model.eval()

# Remove classification head to get embeddings
feature_extractor = torch.nn.Sequential(*list(model.children())[:-1])

# Standard ImageNet preprocessing
preprocess = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

def get_image_embedding(image):
    """Extract embedding from PIL Image."""
    tensor = preprocess(image).unsqueeze(0)
    with torch.no_grad():
        embedding = feature_extractor(tensor)
    return embedding.squeeze().numpy()

# Create synthetic test images with different color patterns
# np.random.randint([low_r, low_g, low_b], [high_r, high_g, high_b], shape)
# generates random RGB values within the specified range for each pixel
np.random.seed(42)
images = {
    'red_pattern': Image.fromarray(
        np.random.randint([180, 0, 0], [255, 80, 80], (224, 224, 3), dtype=np.uint8)
    ),
    'blue_pattern': Image.fromarray(
        np.random.randint([0, 0, 180], [80, 80, 255], (224, 224, 3), dtype=np.uint8)
    ),
    'orange_pattern': Image.fromarray(
        np.random.randint([200, 100, 0], [255, 150, 50], (224, 224, 3), dtype=np.uint8)
    ),
}

# Get embeddings
embeddings = {name: get_image_embedding(img) for name, img in images.items()}

print("Image embedding similarities:\n")
print("Red and orange (similar warm colors) should be more similar than red and blue:")
red_orange = cosine_similarity([embeddings['red_pattern']], [embeddings['orange_pattern']])[0][0]
red_blue = cosine_similarity([embeddings['red_pattern']], [embeddings['blue_pattern']])[0][0]
print(f"  red ↔ orange: {red_orange:.3f}")
print(f"  red ↔ blue:   {red_blue:.3f}")
```

```{python}
#| echo: true
#| code-fold: true
#| code-summary: "Show RGB channel visualization code"
#| label: fig-rgb-channels
#| fig-cap: "Each image is a 3D array (224 × 224 × 3). The combined color equals the sum of Red, Green, and Blue channel intensities."
import matplotlib.pyplot as plt
import matplotlib.gridspec as gridspec

fig = plt.figure(figsize=(12, 4))
gs = gridspec.GridSpec(3, 7, width_ratios=[3, 0.5, 3, 0.5, 3, 0.5, 3], hspace=0.1, wspace=0.05)

for row, (name, img) in enumerate(images.items()):
    img_array = np.array(img)
    label = name.replace("_pattern", "").title()

    ax = fig.add_subplot(gs[row, 0])
    ax.imshow(img)
    if row == 0:
        ax.set_title(label, fontsize=10)
    ax.axis('off')

    ax = fig.add_subplot(gs[row, 1])
    ax.text(0.5, 0.5, '=', fontsize=16, ha='center', va='center', fontweight='bold')
    ax.axis('off')

    ax = fig.add_subplot(gs[row, 2])
    ax.imshow(img_array[:,:,0], cmap='Reds', vmin=0, vmax=255)
    if row == 0:
        ax.set_title('R', fontsize=10)
    ax.axis('off')

    ax = fig.add_subplot(gs[row, 3])
    ax.text(0.5, 0.5, '+', fontsize=16, ha='center', va='center', fontweight='bold')
    ax.axis('off')

    ax = fig.add_subplot(gs[row, 4])
    ax.imshow(img_array[:,:,1], cmap='Greens', vmin=0, vmax=255)
    if row == 0:
        ax.set_title('G', fontsize=10)
    ax.axis('off')

    ax = fig.add_subplot(gs[row, 5])
    ax.text(0.5, 0.5, '+', fontsize=16, ha='center', va='center', fontweight='bold')
    ax.axis('off')

    ax = fig.add_subplot(gs[row, 6])
    ax.imshow(img_array[:,:,2], cmap='Blues', vmin=0, vmax=255)
    if row == 0:
        ax.set_title('B', fontsize=10)
    ax.axis('off')

plt.suptitle('RGB: Three 2D channels combine to create color images', fontsize=11, fontweight='bold', y=0.98)
plt.show()
```

When comparing image embeddings, we use cosine similarity just like with text. Images with similar visual features—colors, textures, shapes, or objects—will have embeddings that point in similar directions, yielding high similarity scores. The red and orange patterns share warm color features, so their embeddings are closer together than red and blue. In practice, this means a photo of a red dress will be more similar to an orange dress than a blue one, even though all three are "dresses."

How does the model "understand" colors? As @fig-rgb-channels shows, each image is stored as three 2D arrays—one for Red, Green, and Blue intensity. The red pattern has high values in the R channel and low values in G and B. Orange combines high R with medium G (red + green = orange).

::: {.callout-note}
## RGB Input ≠ Embedding Output
Don't confuse the 3-channel RGB input with the embedding output. ResNet50 takes the 3 RGB channels as input but produces a **2048-dimensional** embedding vector that captures far more than color: edges, textures, shapes, and high-level visual concepts learned from millions of images.
:::

The model receives the three RGB channels as input, and early CNN layers learn filters that activate for specific patterns—some respond to warm tones, others to edges or textures. As layers get deeper, the network combines these low-level features into increasingly abstract representations. By training on millions of labeled images, the model learns that red and orange often appear together (sunsets, autumn leaves, fire) more frequently than red and blue, encoding this statistical relationship across all 2048 dimensions.

Beyond colors, early CNN layers also learn edge detectors—filters that respond to boundaries between light and dark regions. For a hands-on introduction to how a single neuron learns to detect edges, see [How Neural Networks Learn to See](https://snowch.github.io/nn_edge_detector_blog.html).

**When to use image embeddings:**

- Visual recommendation systems—suggest visually similar items (see @sec-recommendation-systems)
- Content moderation—detect variations of prohibited images (see @sec-content-moderation)
- Forensic video search—find specific people or objects in footage (see @sec-video-surveillance) *(reverse image lookup)*
- Duplicate and near-duplicate detection—identify copied or modified images (see @sec-entity-resolution) *(reverse image lookup)*
- Medical imaging—find similar X-rays, scans, or pathology slides (see @sec-healthcare-life-sciences)
- Visual product search—find products similar to a photo (see @sec-retail-ecommerce) *(reverse image lookup)*
- Quality control—detect defects by comparing to reference images (see @sec-manufacturing-industry40)
- Face recognition—identify or verify individuals from photos (see @sec-video-surveillance)

There are dozens of other applications including art style matching, stock photo search, image classification, trademark and logo detection, scene recognition, wildlife identification, satellite imagery analysis, document scanning, and autonomous vehicle perception.

**Popular architectures:**

| Architecture | Type | Strengths | Use Cases |
|-------------|------|-----------|-----------|
| [ResNet](https://pytorch.org/vision/main/models/resnet.html) | CNN | Fast, proven | General visual search |
| [EfficientNet](https://pytorch.org/vision/main/models/efficientnet.html) | CNN | Efficient, accurate | Mobile/edge deployment |
| [ViT](https://pytorch.org/vision/main/models/vision_transformer.html) | Transformer | Best accuracy | High-quality requirements |
| [CLIP](https://github.com/openai/CLIP) | Multi-modal | Text-image alignment | Zero-shot classification |

: Image embedding architectures {.striped}

See @sec-image-embedding-models for details on how these architectures work.

## Audio Embeddings {#sec-audio-embedding-types}

Audio embeddings convert sound into vectors that capture acoustic properties—pitch, timbre, rhythm, and spectral characteristics. Unlike raw waveform comparison, embeddings understand that two recordings of the same spoken word are similar even with different speakers, background noise, or recording equipment.

The example below uses MFCC (Mel-frequency cepstral coefficients), a classic signal processing technique—not a neural network, but a fixed mathematical transformation that extracts acoustic features. MFCCs transform audio into a representation that mimics human hearing—emphasizing frequencies we're sensitive to while compressing less important details. While modern systems use learned embeddings from models like Wav2Vec2, MFCCs remain useful as a baseline and for understanding what acoustic features matter.

We extract 20 values per time frame, called *coefficients*—each describes a different aspect of the sound's frequency content (overall loudness, balance between low and high frequencies, etc.). Since audio clips vary in length (a 3-second clip has more frames than a 1-second clip), we aggregate across time using mean and standard deviation to create a fixed-size embedding that works regardless of duration:

```{python}
#| echo: false
#| label: fig-mfcc-aggregation
#| fig-cap: "MFCC aggregation: 20 coefficients per frame become a 40-dimensional embedding via mean and standard deviation."
import matplotlib.pyplot as plt
from matplotlib.patches import FancyBboxPatch

fig, ax = plt.subplots(figsize=(10, 5))
ax.set_xlim(0, 10)
ax.set_ylim(0.5, 7.5)
ax.axis('off')

# Consistent spacing: boxes are 0.8 tall, gaps are 0.6, arrows span the gaps
box_height = 0.8
gap = 0.6

# Box 1: Audio file
y1 = 6.4
box1 = FancyBboxPatch((3.5, y1), 3, box_height, boxstyle="round,pad=0.05",
                       facecolor='#E8F4FD', edgecolor='#2196F3', linewidth=2)
ax.add_patch(box1)
ax.text(5, y1 + box_height/2, 'Audio file (e.g. mp3, wav)', ha='center', va='center', fontsize=11, fontweight='bold')

# Arrow 1
ax.annotate('', xy=(5, y1 - gap + 0.1), xytext=(5, y1 - 0.1), arrowprops=dict(arrowstyle='->', color='gray', lw=2))
ax.text(5.8, y1 - gap/2, 'split into ~25ms frames', ha='left', va='center', fontsize=9, style='italic', color='#666')

# Box 2: Frames (y = 4.6 to 5.6)
y2 = y1 - gap - box_height - 0.2
box2 = FancyBboxPatch((1, y2), 8, box_height + 0.2, boxstyle="round,pad=0.05",
                       facecolor='#FFF3E0', edgecolor='#FF9800', linewidth=2)
ax.add_patch(box2)
ax.text(1.8, y2 + 0.5, 'Frame 1\n[20 vals]', ha='center', va='center', fontsize=9)
ax.text(3.6, y2 + 0.5, 'Frame 2\n[20 vals]', ha='center', va='center', fontsize=9)
ax.text(5.4, y2 + 0.5, 'Frame 3\n[20 vals]', ha='center', va='center', fontsize=9)
ax.text(6.6, y2 + 0.5, '...', ha='center', va='center', fontsize=12)
ax.text(8, y2 + 0.5, 'Frame N\n[20 vals]', ha='center', va='center', fontsize=9)

# Arrow 2
ax.annotate('', xy=(5, y2 - gap + 0.1), xytext=(5, y2 - 0.1), arrowprops=dict(arrowstyle='->', color='gray', lw=2))
ax.text(5.8, y2 - gap/2, 'aggregate across frames', ha='left', va='center', fontsize=9, style='italic', color='#666')

# Box 3: Mean/Std (y = 2.8 to 3.8)
y3 = y2 - gap - box_height - 0.2
box3 = FancyBboxPatch((1.5, y3), 7, box_height + 0.2, boxstyle="round,pad=0.05",
                       facecolor='#E8F5E9', edgecolor='#4CAF50', linewidth=2)
ax.add_patch(box3)
ax.text(5, y3 + 0.65, 'Mean across all frames:  [20 values]', ha='center', va='center', fontsize=10)
ax.text(5, y3 + 0.25, 'Std across all frames:     [20 values]', ha='center', va='center', fontsize=10)

# Arrow 3
ax.annotate('', xy=(5, y3 - gap + 0.1), xytext=(5, y3 - 0.1), arrowprops=dict(arrowstyle='->', color='gray', lw=2))
ax.text(5.8, y3 - gap/2, 'concatenate', ha='left', va='center', fontsize=9, style='italic', color='#666')

# Box 4: Final embedding (y = 1.0 to 1.8)
y4 = y3 - gap - box_height
box4 = FancyBboxPatch((2.5, y4), 5, box_height, boxstyle="round,pad=0.05",
                       facecolor='#F3E5F5', edgecolor='#9C27B0', linewidth=2)
ax.add_patch(box4)
ax.text(5, y4 + box_height/2, 'Audio embedding: [40 values]', ha='center', va='center', fontsize=11, fontweight='bold')

plt.tight_layout()
plt.show()
```

```{python}
#| code-fold: false

"""
Audio Embeddings: Sound as Vectors
"""

import numpy as np
import librosa
from sklearn.metrics.pairwise import cosine_similarity

def audio_to_embedding(audio, sr, n_mfcc=20):
    """Convert audio waveform to a fixed-size embedding using MFCCs."""
    # Extract MFCCs: 20 coefficients per time frame
    mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=n_mfcc)
    # Aggregate over time: mean captures average timbre, std captures variation
    return np.concatenate([mfccs.mean(axis=1), mfccs.std(axis=1)])

# Load librosa's built-in trumpet sample
audio, sr = librosa.load(librosa.ex('trumpet'))
trumpet_embedding = audio_to_embedding(audio, sr)

# Create variations to demonstrate similarity
trumpet_slow = librosa.effects.time_stretch(audio, rate=0.8)
trumpet_pitch_up = librosa.effects.pitch_shift(audio, sr=sr, n_steps=12)  # One octave up

# Generate embeddings for each variation
embeddings = {
    'trumpet_original': trumpet_embedding,
    'trumpet_slower': audio_to_embedding(trumpet_slow, sr),
    'trumpet_higher_pitch': audio_to_embedding(trumpet_pitch_up, sr),
}

print(f"Embedding dimension: {len(trumpet_embedding)} (20 MFCCs × 2 stats)\n")

# Compare similarities
print("Audio embedding similarities:")
sim_slow = cosine_similarity(
    [embeddings['trumpet_original']], [embeddings['trumpet_slower']]
)[0][0]
print(f"  Original ↔ Slower tempo:   {sim_slow:.3f}")

sim_pitch = cosine_similarity(
    [embeddings['trumpet_original']], [embeddings['trumpet_higher_pitch']]
)[0][0]
print(f"  Original ↔ Higher pitch:   {sim_pitch:.3f}")

sim_variations = cosine_similarity(
    [embeddings['trumpet_slower']], [embeddings['trumpet_higher_pitch']]
)[0][0]
print(f"  Slower ↔ Higher pitch:     {sim_variations:.3f}")
```

When comparing audio embeddings, cosine similarity measures how acoustically similar two sounds are. Notice that all similarities are high (>0.97)—this is by design. MFCCs capture timbre (the trumpet's characteristic "brassy" quality) rather than absolute pitch or tempo. The tempo change preserves timbre almost perfectly (1.000), while shifting pitch by an octave causes only a small drop (~0.98) because the overall spectral shape remains trumpet-like. This robustness is useful for applications like speaker identification, where you want to match voices regardless of speaking speed or emotional pitch variations.

How do MFCCs capture sound? Audio is first split into short overlapping frames (typically 25ms). For each frame, we compute the frequency spectrum, then apply a mel filterbank that groups frequencies into bands matching human perception—more resolution at low frequencies where we hear pitch differences, less at high frequencies. The cepstral coefficients compress this further, capturing the overall "shape" of the spectrum. The result: a compact representation of timbre that's robust to volume changes.

**When to use audio embeddings:** Content moderation, audio fingerprinting, speaker identification, music recommendation, podcast and video search, sound event detection, voice cloning detection, acoustic quality control, wildlife monitoring, and medical diagnostics from sounds (coughs, heartbeats).

This book covers music recommendation with audio embeddings in @sec-recommendation-systems. If you'd like to see other audio applications covered in future editions, reach out to the author.

**Popular architectures:**

| Architecture | Type | Strengths | Use Cases |
|-------------|------|-----------|-----------|
| [Wav2Vec2](https://huggingface.co/facebook/wav2vec2-base) | Self-supervised | Rich speech features | Speech recognition, speaker ID |
| [Whisper](https://github.com/openai/whisper) | Multi-task | Transcription + embeddings | Speech search, subtitles |
| [CLAP](https://github.com/LAION-AI/CLAP) | Multi-modal | Audio-text alignment | Zero-shot audio classification |
| [VGGish](https://github.com/tensorflow/models/tree/master/research/audioset/vggish) | CNN | General audio events | Sound classification |

: Audio embedding architectures {.striped}

See @sec-embedding-model-fundamentals for details on how text and image embedding models work. For audio models, the linked documentation above provides architecture details, or explore the [Hugging Face Audio Course](https://huggingface.co/learn/audio-course) for a comprehensive introduction.

## Video Embeddings {#sec-video-embedding-types}

Video embeddings convert video clips into vectors that capture both visual content and temporal dynamics—actions, motion patterns, scene transitions, and narrative flow. Unlike image embeddings that capture a single moment, video embeddings understand that "a person sitting down" and "a person standing up" are different actions even though individual frames might look similar.

The challenge with video is combining spatial information (what's in each frame) with temporal information (how things change over time). The simplest approach extracts image embeddings from sampled frames and aggregates them. More sophisticated models process multiple frames together to capture motion directly.

The example below demonstrates the frame sampling approach: we extract image embeddings from frames sampled throughout a video, then average them to create a single video embedding. Since videos vary in length, aggregation (like we saw with audio) produces a fixed-size embedding regardless of duration.

```{python}
#| echo: false
#| label: fig-video-aggregation
#| fig-cap: "Video embedding via frame sampling: extract image embeddings from sampled frames, then aggregate into a single video embedding."
import matplotlib.pyplot as plt
from matplotlib.patches import FancyBboxPatch

fig, ax = plt.subplots(figsize=(10, 5.5))
ax.set_xlim(0, 10)
ax.set_ylim(0, 7.5)
ax.axis('off')

# Consistent spacing
box_height = 0.8
gap = 0.6

# Box 1: Video file
y1 = 6.4
box1 = FancyBboxPatch((3, y1), 4, box_height, boxstyle="round,pad=0.05",
                       facecolor='#E8F4FD', edgecolor='#2196F3', linewidth=2)
ax.add_patch(box1)
ax.text(5, y1 + box_height/2, 'Video file (e.g. mp4, avi)', ha='center', va='center', fontsize=11, fontweight='bold')

# Arrow 1
ax.annotate('', xy=(5, y1 - gap + 0.1), xytext=(5, y1 - 0.1), arrowprops=dict(arrowstyle='->', color='gray', lw=2))
ax.text(5.8, y1 - gap/2, 'sample frames (e.g. 1 per second)', ha='left', va='center', fontsize=9, style='italic', color='#666')

# Box 2: Frames
y2 = y1 - gap - box_height - 0.2
box2 = FancyBboxPatch((1, y2), 8, box_height + 0.2, boxstyle="round,pad=0.05",
                       facecolor='#FFF3E0', edgecolor='#FF9800', linewidth=2)
ax.add_patch(box2)
ax.text(1.8, y2 + 0.5, 'Frame 1', ha='center', va='center', fontsize=9)
ax.text(3.6, y2 + 0.5, 'Frame 2', ha='center', va='center', fontsize=9)
ax.text(5.4, y2 + 0.5, 'Frame 3', ha='center', va='center', fontsize=9)
ax.text(6.6, y2 + 0.5, '...', ha='center', va='center', fontsize=12)
ax.text(8, y2 + 0.5, 'Frame N', ha='center', va='center', fontsize=9)

# Arrow 2
ax.annotate('', xy=(5, y2 - gap + 0.1), xytext=(5, y2 - 0.1), arrowprops=dict(arrowstyle='->', color='gray', lw=2))
ax.text(5.8, y2 - gap/2, 'embed each frame (e.g. ResNet)', ha='left', va='center', fontsize=9, style='italic', color='#666')

# Box 3: Frame embeddings
y3 = y2 - gap - box_height - 0.2
box3 = FancyBboxPatch((1, y3), 8, box_height + 0.2, boxstyle="round,pad=0.05",
                       facecolor='#E8F5E9', edgecolor='#4CAF50', linewidth=2)
ax.add_patch(box3)
ax.text(1.8, y3 + 0.5, '[2048 vals]', ha='center', va='center', fontsize=9)
ax.text(3.6, y3 + 0.5, '[2048 vals]', ha='center', va='center', fontsize=9)
ax.text(5.4, y3 + 0.5, '[2048 vals]', ha='center', va='center', fontsize=9)
ax.text(6.6, y3 + 0.5, '...', ha='center', va='center', fontsize=12)
ax.text(8, y3 + 0.5, '[2048 vals]', ha='center', va='center', fontsize=9)

# Arrow 3
ax.annotate('', xy=(5, y3 - gap + 0.1), xytext=(5, y3 - 0.1), arrowprops=dict(arrowstyle='->', color='gray', lw=2))
ax.text(5.8, y3 - gap/2, 'aggregate (mean across frames)', ha='left', va='center', fontsize=9, style='italic', color='#666')

# Box 4: Video embedding
y4 = y3 - gap - box_height
box4 = FancyBboxPatch((2.5, y4), 5, box_height, boxstyle="round,pad=0.05",
                       facecolor='#F3E5F5', edgecolor='#9C27B0', linewidth=2)
ax.add_patch(box4)
ax.text(5, y4 + box_height/2, 'Video embedding: [2048 values]', ha='center', va='center', fontsize=11, fontweight='bold')

plt.tight_layout()
plt.show()
```

```{python}
#| code-fold: false

"""
Video Embeddings: From Clips to Vectors
"""

import torch
import numpy as np
from torchvision import models, transforms
from torchvision.models import ResNet50_Weights
from sklearn.metrics.pairwise import cosine_similarity

# Suppress download messages
import logging
logging.getLogger('torch').setLevel(logging.ERROR)

# Load pretrained ResNet50 as frame feature extractor
weights = ResNet50_Weights.IMAGENET1K_V1
model = models.resnet50(weights=None)
model.load_state_dict(weights.get_state_dict(progress=False))
model.eval()
feature_extractor = torch.nn.Sequential(*list(model.children())[:-1])

preprocess = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

def frames_to_video_embedding(frames):
    """Convert a list of frames (as numpy arrays) to a video embedding."""
    frame_embeddings = []
    for frame in frames:
        # Convert numpy array to tensor and preprocess
        if frame.dtype == np.uint8:
            frame = frame.astype(np.float32) / 255.0
        tensor = preprocess(frame).unsqueeze(0)
        with torch.no_grad():
            emb = feature_extractor(tensor).squeeze().numpy()
        frame_embeddings.append(emb)
    # Aggregate: mean across all frames
    return np.mean(frame_embeddings, axis=0)

# Simulate video frames as colored images (224x224 RGB)
# "Running video": frames transition from green (grass) to blue (sky) - outdoor motion
running_frames = [
    np.full((224, 224, 3), [0.2, 0.6 + i*0.05, 0.2], dtype=np.float32)  # Green to lighter
    for i in range(5)
]

# "Cooking video": frames stay warm orange/red tones - kitchen scene
cooking_frames = [
    np.full((224, 224, 3), [0.8, 0.4 + i*0.02, 0.1], dtype=np.float32)  # Warm tones
    for i in range(5)
]

# "Jogging video": similar to running - outdoor greens and blues
jogging_frames = [
    np.full((224, 224, 3), [0.25, 0.55 + i*0.05, 0.25], dtype=np.float32)  # Similar greens
    for i in range(5)
]

# Generate video embeddings
embeddings = {
    'running': frames_to_video_embedding(running_frames),
    'cooking': frames_to_video_embedding(cooking_frames),
    'jogging': frames_to_video_embedding(jogging_frames),
}

print(f"Video embedding dimension: {len(embeddings['running'])}\n")
print("Video embedding similarities:")

run_jog = cosine_similarity([embeddings['running']], [embeddings['jogging']])[0][0]
run_cook = cosine_similarity([embeddings['running']], [embeddings['cooking']])[0][0]
jog_cook = cosine_similarity([embeddings['jogging']], [embeddings['cooking']])[0][0]

print(f"  Running ↔ Jogging: {run_jog:.3f}  (similar outdoor scenes)")
print(f"  Running ↔ Cooking: {run_cook:.3f}  (different scenes)")
print(f"  Jogging ↔ Cooking: {jog_cook:.3f}  (different scenes)")
```

When comparing video embeddings, similar activities and scenes cluster together. Running and jogging videos both contain outdoor scenes with similar color palettes (greens, blues), so their embeddings are close. Cooking videos have warm indoor tones (oranges, reds) that differ significantly from outdoor activities.

The frame sampling approach shown above is simple but misses motion information—it can't distinguish "sitting down" from "standing up" since both might have similar individual frames. More advanced approaches process frames together.

**When to use video embeddings:** Action recognition and search, video recommendation, content moderation, surveillance and anomaly detection, video summarization, sports analytics, and gesture recognition.

This book covers video surveillance applications in @sec-video-surveillance. If you'd like to see other video applications covered in future editions, reach out to the author.

**Popular architectures:**

| Architecture | Type | Strengths | Use Cases |
|-------------|------|-----------|-----------|
| Frame sampling + CNN | Aggregated image embeddings | Simple, fast (no motion) | Scene classification |
| [SlowFast](https://github.com/facebookresearch/SlowFast) | Two-pathway 3D CNN | Captures fast and slow motion | Action recognition |
| [X3D](https://github.com/facebookresearch/SlowFast) | Efficient 3D CNN | Mobile-friendly | Real-time applications |
| [Video Swin](https://github.com/SwinTransformer/Video-Swin-Transformer) | Video transformer | State-of-the-art accuracy | High-quality requirements |

: Video embedding architectures {.striped}

See @sec-embedding-model-fundamentals for details on how text and image embedding models work. For video models, the linked documentation above provides architecture details.

## Multi-Modal Embeddings {#sec-multimodal-embedding-types}

Multi-modal embeddings map different data types—text, images, audio—into a shared vector space where they can be directly compared. This enables powerful cross-modal capabilities: searching images with text queries, finding text descriptions for images, or classifying images without any training examples.

The key insight is training two encoders (e.g., one for text, one for images) so that matching pairs produce similar vectors. CLIP, trained on 400 million image-text pairs from the internet, learns that "a photo of a cat" and an actual cat photo should have nearby embeddings.

```{python}
#| echo: false
#| label: fig-multimodal-space
#| fig-cap: "Multi-modal embeddings: text and images mapped to a shared space where matching concepts are close together."
import matplotlib.pyplot as plt
import numpy as np

fig, ax = plt.subplots(figsize=(8, 5))

# Simulated 2D positions for text and images
np.random.seed(42)
concepts = {
    'cat': {'text': (1.0, 2.5), 'image': (1.2, 2.3)},
    'dog': {'text': (1.8, 2.0), 'image': (1.6, 2.2)},
    'car': {'text': (4.0, 1.0), 'image': (3.8, 1.2)},
}

# Plot text embeddings (squares)
for concept, positions in concepts.items():
    ax.scatter(*positions['text'], marker='s', s=150, c='#2196F3', edgecolors='black', linewidths=1.5, zorder=3)
    ax.annotate(f'"{concept}"', positions['text'], xytext=(10, 10), textcoords='offset points', fontsize=10)

# Plot image embeddings (circles)
for concept, positions in concepts.items():
    ax.scatter(*positions['image'], marker='o', s=150, c='#FF9800', edgecolors='black', linewidths=1.5, zorder=3)
    ax.annotate(f'{concept}.jpg', positions['image'], xytext=(10, -15), textcoords='offset points', fontsize=10)

# Draw lines connecting matching text-image pairs
for concept, positions in concepts.items():
    ax.plot([positions['text'][0], positions['image'][0]],
            [positions['text'][1], positions['image'][1]],
            'g--', alpha=0.5, linewidth=2)

# Legend
ax.scatter([], [], marker='s', s=100, c='#2196F3', edgecolors='black', label='Text embeddings')
ax.scatter([], [], marker='o', s=100, c='#FF9800', edgecolors='black', label='Image embeddings')
ax.plot([], [], 'g--', alpha=0.5, linewidth=2, label='Matching pairs (close)')

ax.set_xlabel('Embedding dimension 1')
ax.set_ylabel('Embedding dimension 2')
ax.set_title('Shared embedding space: text and images aligned')
ax.legend(loc='upper right')
ax.set_xlim(0, 5)
ax.set_ylim(0, 3.5)
plt.tight_layout()
plt.show()
```

```{python}
#| code-fold: false

"""
Multi-Modal Embeddings: Text and Images in Shared Space
"""

import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

# Simulated CLIP-style embeddings (real CLIP uses 512-768 dimensions)
# Key property: matching text-image pairs have similar vectors
np.random.seed(42)

embeddings = {
    # Text embeddings (from text encoder)
    'text: a photo of a cat': np.array([0.8, 0.1, 0.2, 0.1]),
    'text: a picture of a dog': np.array([0.7, 0.3, 0.2, 0.1]),
    'text: a red sports car': np.array([0.1, 0.1, 0.9, 0.3]),
    # Image embeddings (from image encoder)
    'image: cat_photo.jpg': np.array([0.75, 0.15, 0.25, 0.1]),
    'image: dog_photo.jpg': np.array([0.65, 0.35, 0.15, 0.15]),
    'image: car_photo.jpg': np.array([0.15, 0.1, 0.85, 0.35]),
}

print("Multi-modal text-to-image search:\n")
print("Query: 'a photo of a cat'\n")

text_cat = embeddings['text: a photo of a cat']
results = []
for name, emb in embeddings.items():
    if name.startswith('image:'):
        sim = cosine_similarity([text_cat], [emb])[0][0]
        results.append((name, sim))

# Sort by similarity
for name, sim in sorted(results, key=lambda x: -x[1]):
    print(f"  {sim:.3f}  {name.replace('image: ', '')}")
```

The cat image ranks highest for the cat query because CLIP learned to align text descriptions with matching images. This enables zero-shot image classification: to classify an image, compare it against text embeddings like "a photo of a cat", "a photo of a dog", etc., and pick the highest similarity.

**When to use multi-modal embeddings:** Cross-modal search (text→image, image→text), zero-shot image classification, image captioning, visual question answering, and product search with text and images.

This book covers multi-modal search in @sec-semantic-search. If you'd like to see other multi-modal applications covered in future editions, reach out to the author.

**Popular architectures:**

| Architecture | Type | Strengths | Use Cases |
|-------------|------|-----------|-----------|
| [CLIP](https://github.com/openai/CLIP) | Text-image | Fast, versatile | Search, classification |
| [BLIP-2](https://github.com/salesforce/LAVIS) | Text-image | Captioning + retrieval | Image understanding |
| [ImageBind](https://github.com/facebookresearch/ImageBind) | 6 modalities | Audio, depth, thermal | Multi-sensor fusion |
| [LLaVA](https://github.com/haotian-liu/LLaVA) | Vision-language | Conversational | Visual Q&A |

: Multi-modal embedding architectures {.striped}

See @sec-embedding-model-fundamentals for details on how text and image embedding models work.

## Graph Embeddings {#sec-graph-embedding-types}

Graph embeddings convert nodes, edges, and subgraphs into vectors that capture structural relationships. Unlike text or images where data is sequential or grid-like, graphs have arbitrary connectivity—a social network node might have 3 friends or 3,000. Graph embeddings learn representations where connected nodes (or nodes with similar neighborhoods) have similar vectors.

The key insight is that a node's meaning comes from its connections. In a social network, people with similar friends likely have similar interests. In a molecule, atoms with similar bonding patterns have similar chemical properties. Graph embedding algorithms capture this by aggregating information from neighbors.

```{python}
#| echo: false
#| label: fig-graph-embedding
#| fig-cap: "Graph embeddings: nodes in the same community (densely connected) map to nearby points in embedding space."
import matplotlib.pyplot as plt
import numpy as np

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))

# Left: Graph structure
ax1.set_title('Graph structure', fontsize=11, fontweight='bold')
# Community 1 (blue)
community1_pos = {'Alice': (1, 2), 'Bob': (1.5, 2.8), 'Carol': (2, 2)}
for name, pos in community1_pos.items():
    ax1.scatter(*pos, s=200, c='#2196F3', edgecolors='black', linewidths=1.5, zorder=3)
    ax1.annotate(name, pos, xytext=(0, -20), textcoords='offset points', ha='center', fontsize=9)
# Edges within community 1
ax1.plot([1, 1.5], [2, 2.8], 'k-', alpha=0.5)
ax1.plot([1.5, 2], [2.8, 2], 'k-', alpha=0.5)
ax1.plot([1, 2], [2, 2], 'k-', alpha=0.5)

# Community 2 (orange)
community2_pos = {'Xavier': (3.5, 2), 'Yuki': (4, 2.8), 'Zara': (4.5, 2)}
for name, pos in community2_pos.items():
    ax1.scatter(*pos, s=200, c='#FF9800', edgecolors='black', linewidths=1.5, zorder=3)
    ax1.annotate(name, pos, xytext=(0, -20), textcoords='offset points', ha='center', fontsize=9)
# Edges within community 2
ax1.plot([3.5, 4], [2, 2.8], 'k-', alpha=0.5)
ax1.plot([4, 4.5], [2.8, 2], 'k-', alpha=0.5)
ax1.plot([3.5, 4.5], [2, 2], 'k-', alpha=0.5)

# Weak link between communities
ax1.plot([2, 3.5], [2, 2], 'k--', alpha=0.3)

ax1.set_xlim(0.5, 5)
ax1.set_ylim(1, 3.5)
ax1.axis('off')

# Right: Embedding space
ax2.set_title('Embedding space', fontsize=11, fontweight='bold')
# Community 1 embeddings (clustered)
emb1 = {'Alice': (1.2, 2.5), 'Bob': (1.4, 2.3), 'Carol': (1.1, 2.2)}
for name, pos in emb1.items():
    ax2.scatter(*pos, s=200, c='#2196F3', edgecolors='black', linewidths=1.5, zorder=3)
    ax2.annotate(name, pos, xytext=(5, 5), textcoords='offset points', fontsize=9)

# Community 2 embeddings (clustered separately)
emb2 = {'Xavier': (3.2, 1.2), 'Yuki': (3.4, 1.4), 'Zara': (3.1, 1.0)}
for name, pos in emb2.items():
    ax2.scatter(*pos, s=200, c='#FF9800', edgecolors='black', linewidths=1.5, zorder=3)
    ax2.annotate(name, pos, xytext=(5, 5), textcoords='offset points', fontsize=9)

ax2.set_xlabel('Embedding dim 1')
ax2.set_ylabel('Embedding dim 2')
ax2.set_xlim(0.5, 4)
ax2.set_ylim(0.5, 3)

plt.tight_layout()
plt.show()
```

```{python}
#| code-fold: false

"""
Graph Embeddings: Network Structure as Vectors
"""

import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

# Simulated node embeddings from a social network
# Real systems use Node2Vec, GraphSAGE, or GNN-based approaches
np.random.seed(42)

# Two friend groups: nodes in the same group have similar embeddings
node_embeddings = {
    # Community 1: Alice, Bob, Carol (densely connected)
    'Alice': np.random.randn(64) + np.array([1, 0] + [0]*62),
    'Bob': np.random.randn(64) + np.array([0.9, 0.1] + [0]*62),
    'Carol': np.random.randn(64) + np.array([0.8, 0.2] + [0]*62),
    # Community 2: Xavier, Yuki, Zara (densely connected)
    'Xavier': np.random.randn(64) + np.array([0, 1] + [0]*62),
    'Yuki': np.random.randn(64) + np.array([0.1, 0.9] + [0]*62),
    'Zara': np.random.randn(64) + np.array([0.2, 0.8] + [0]*62),
}

print("Graph embedding similarities:\n")
print("Within community (friends):")
ab = cosine_similarity([node_embeddings['Alice']], [node_embeddings['Bob']])[0][0]
ac = cosine_similarity([node_embeddings['Alice']], [node_embeddings['Carol']])[0][0]
print(f"  Alice ↔ Bob:   {ab:.3f}")
print(f"  Alice ↔ Carol: {ac:.3f}")

print("\nAcross communities (not connected):")
ax = cosine_similarity([node_embeddings['Alice']], [node_embeddings['Xavier']])[0][0]
print(f"  Alice ↔ Xavier: {ax:.3f}")
```

Nodes in the same community have high similarity because they share connections. Alice, Bob, and Carol are all friends with each other, so their embeddings cluster together. Xavier is in a different friend group with no direct connection to Alice, resulting in lower similarity.

**When to use graph embeddings:** Social network analysis, recommendation systems (user-item graphs), knowledge graph completion, fraud detection (transaction graphs), and drug discovery (molecular graphs).

This book covers entity resolution with graph techniques in @sec-entity-resolution. If you'd like to see other graph applications covered in future editions, reach out to the author.

**Popular architectures:**

| Architecture | Type | Strengths | Use Cases |
|-------------|------|-----------|-----------|
| [Node2Vec](https://github.com/aditya-grover/node2vec) | Random walks | Simple, scalable | Homogeneous graphs |
| [GraphSAGE](https://github.com/williamleif/GraphSAGE) | Neighborhood aggregation | Inductive learning | New nodes |
| [GAT](https://github.com/PetarV-/GAT) | Graph attention | Weighted neighbors | Heterogeneous graphs |
| [TransE](https://github.com/thunlp/OpenKE) | Translation-based | Link prediction | Knowledge graphs |

: Graph embedding architectures {.striped}

See @sec-embedding-model-fundamentals for details on how embedding models learn.

## Time-Series Embeddings {#sec-timeseries-embedding-types}

Time-series embeddings convert sequences of measurements over time into vectors that capture temporal patterns—trends, seasonality, cycles, and anomalies. Two sensors with similar behavior patterns (e.g., both showing daily cycles) will have similar embeddings, even if their absolute values differ.

The challenge with time-series is capturing patterns at multiple scales: short-term fluctuations, medium-term trends, and long-term seasonality. Like audio, time-series vary in length, so embeddings must aggregate temporal information into fixed-size vectors.

The example below demonstrates a simple statistical approach: extract features that summarize the time-series behavior. More sophisticated methods use learned representations from LSTMs, Transformers, or contrastive learning.

```{python}
#| echo: false
#| label: fig-timeseries-patterns
#| fig-cap: "Time-series patterns: sine waves and upward trends have distinct signatures that embeddings capture."
import matplotlib.pyplot as plt
import numpy as np

np.random.seed(42)
t = np.linspace(0, 4*np.pi, 100)

fig, axes = plt.subplots(1, 3, figsize=(10, 3))

# Sine wave
axes[0].plot(t, np.sin(t) + np.random.randn(100) * 0.1, color='#2196F3', linewidth=1.5)
axes[0].set_title('Sine wave pattern', fontsize=10)
axes[0].set_xlabel('Time')
axes[0].set_ylabel('Value')

# Upward trend
axes[1].plot(t, t/10 + np.random.randn(100) * 0.2, color='#4CAF50', linewidth=1.5)
axes[1].set_title('Upward trend', fontsize=10)
axes[1].set_xlabel('Time')

# Random noise
axes[2].plot(t, np.random.randn(100), color='#FF9800', linewidth=1.5)
axes[2].set_title('Random noise', fontsize=10)
axes[2].set_xlabel('Time')

plt.tight_layout()
plt.show()
```

```{python}
#| code-fold: false

"""
Time-Series Embeddings: Temporal Patterns as Vectors
"""

import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

np.random.seed(42)

def generate_pattern(pattern_type, length=100):
    """Generate synthetic time-series with specific patterns."""
    t = np.linspace(0, 4*np.pi, length)
    if pattern_type == 'sine':
        return np.sin(t) + np.random.randn(length) * 0.1
    elif pattern_type == 'increasing':
        return t/10 + np.random.randn(length) * 0.2
    else:
        return np.random.randn(length)

def timeseries_embedding(series):
    """Extract statistical features as a simple embedding."""
    return np.array([
        np.mean(series),                              # level
        np.std(series),                               # variability
        np.max(series) - np.min(series),              # range
        np.mean(np.diff(series)),                     # trend (avg change)
        np.corrcoef(series[:-1], series[1:])[0, 1],   # autocorrelation
    ])

# Generate time-series with different patterns
patterns = {
    'sine_wave_1': generate_pattern('sine'),
    'sine_wave_2': generate_pattern('sine'),
    'trend_up_1': generate_pattern('increasing'),
    'trend_up_2': generate_pattern('increasing'),
}

embeddings = {name: timeseries_embedding(ts) for name, ts in patterns.items()}

print(f"Embedding dimension: {len(embeddings['sine_wave_1'])} (5 statistical features)\n")
print("Time-series embedding similarities:\n")
print("Same pattern type:")
sine_sim = cosine_similarity([embeddings['sine_wave_1']], [embeddings['sine_wave_2']])[0][0]
trend_sim = cosine_similarity([embeddings['trend_up_1']], [embeddings['trend_up_2']])[0][0]
print(f"  Sine wave 1 ↔ Sine wave 2: {sine_sim:.3f}")
print(f"  Trend up 1 ↔ Trend up 2:   {trend_sim:.3f}")

print("\nDifferent pattern types:")
cross_sim = cosine_similarity([embeddings['sine_wave_1']], [embeddings['trend_up_1']])[0][0]
print(f"  Sine wave ↔ Trend up:      {cross_sim:.3f}")
```

Time-series with the same pattern type have high similarity because they share statistical properties (similar autocorrelation for sine waves, similar trend for increasing patterns). Different pattern types have lower similarity because their fundamental characteristics differ.

The statistical approach above is simple but limited—it can't capture complex patterns like "spike followed by gradual recovery." Modern approaches use learned embeddings from neural networks trained on large time-series datasets.

**When to use time-series embeddings:** Anomaly detection in sensor data, predictive maintenance, financial pattern recognition, health monitoring (ECG, EEG), and IoT device fingerprinting.

This book covers time-series applications in manufacturing (@sec-manufacturing-industry40) and financial services (@sec-financial-services). If you'd like to see other time-series applications covered in future editions, reach out to the author.

**Popular architectures:**

| Architecture | Type | Strengths | Use Cases |
|-------------|------|-----------|-----------|
| Statistical features | Hand-crafted | Simple, interpretable | Baseline, small data |
| [TSFresh](https://github.com/blue-yonder/tsfresh) | Auto feature extraction | Comprehensive | General purpose |
| LSTM/GRU | Recurrent | Captures sequences | Variable length |
| [Temporal Fusion Transformer](https://github.com/google-research/google-research/tree/master/tft) | Attention | Multi-horizon | Forecasting |

: Time-series embedding architectures {.striped}

See @sec-embedding-model-fundamentals for details on how embedding models learn.

## Code Embeddings {#sec-code-embedding-types}

Code embeddings convert source code into vectors that capture program semantics—what the code does, not just how it's written. Two functions that sum a list of numbers should have similar embeddings whether implemented with a loop or the built-in `sum()` function.

The challenge with code is that syntax varies widely while functionality remains the same. Variable names, formatting, and implementation choices differ between programmers, but the underlying logic may be identical. Code embeddings must see through surface differences to capture semantic similarity.

The example below uses a general text model for demonstration. Production systems use specialized code models (CodeBERT, StarCoder) trained on millions of code repositories that understand programming language syntax and semantics.

```{python}
#| code-fold: false

"""
Code Embeddings: Source Code as Vectors
"""

from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity

# General text model for demo (production: use CodeBERT, StarCoder, etc.)
model = SentenceTransformer('all-MiniLM-L6-v2')

# Same functionality, different implementations
code_snippets = {
    'sum_loop': '''
def sum_numbers(nums):
    total = 0
    for n in nums:
        total += n
    return total
''',
    'sum_builtin': '''
def sum_numbers(numbers):
    return sum(numbers)
''',
    'reverse_loop': '''
def reverse_list(lst):
    result = []
    for i in range(len(lst)-1, -1, -1):
        result.append(lst[i])
    return result
''',
    'reverse_slice': '''
def reverse_list(items):
    return items[::-1]
''',
}

embeddings = {name: model.encode(code) for name, code in code_snippets.items()}

print(f"Embedding dimension: {len(embeddings['sum_loop'])}\n")
print("Code embedding similarities:\n")
print("Same functionality, different implementation:")
sum_sim = cosine_similarity([embeddings['sum_loop']], [embeddings['sum_builtin']])[0][0]
rev_sim = cosine_similarity([embeddings['reverse_loop']], [embeddings['reverse_slice']])[0][0]
print(f"  sum (loop) ↔ sum (builtin):       {sum_sim:.3f}")
print(f"  reverse (loop) ↔ reverse (slice): {rev_sim:.3f}")

print("\nDifferent functionality:")
cross_sim = cosine_similarity([embeddings['sum_loop']], [embeddings['reverse_loop']])[0][0]
print(f"  sum ↔ reverse:                    {cross_sim:.3f}")
```

Functions with the same purpose cluster together even with different implementations. The two sum functions are more similar to each other than to the reverse functions, and vice versa. This enables powerful applications like "find code similar to this function" or "detect if this code was copied from somewhere."

**When to use code embeddings:** Semantic code search, code clone detection, vulnerability detection, code recommendation, and repository organization.

This book doesn't include dedicated code embedding chapters. If you'd like to see code applications covered in future editions, reach out to the author.

**Popular architectures:**

| Architecture | Type | Strengths | Use Cases |
|-------------|------|-----------|-----------|
| [CodeBERT](https://github.com/microsoft/CodeBERT) | BERT-style | Multi-language | Search, clone detection |
| [GraphCodeBERT](https://github.com/microsoft/CodeBERT) | Graph-enhanced | Data flow awareness | Bug detection |
| [StarCoder](https://github.com/bigcode-project/starcoder) | Large model | 80+ languages | Code generation |
| [CodeT5](https://github.com/salesforce/CodeT5) | Encoder-decoder | Understanding + generation | Code summarization |

: Code embedding architectures {.striped}

See @sec-embedding-model-fundamentals for details on how embedding models learn.

## Choosing the Right Embedding Type

Here's a decision framework for selecting embedding types:

```
                    What is your primary data type?
                              │
        ┌──────────┬──────────┼──────────┬──────────┐
        │          │          │          │          │
      Text      Images     Audio/    Multiple   Structured/
        │          │       Video     Modalities   Relational
        │          │          │          │          │
        ▼          ▼          ▼          ▼          ▼
   Sentence    CNN/ViT    Domain-    CLIP/      Graph
  Transformers  CLIP     specific  ImageBind  Embeddings
```

| Scenario | Recommended Approach |
|----------|---------------------|
| Search documents by meaning | Text embeddings (sentence transformers) |
| Find visually similar images | Image embeddings (ResNet, CLIP) |
| Match images to text queries | Multi-modal (CLIP) |
| Find similar entities in a network | Graph embeddings (GraphSAGE) |
| Detect anomalies in sensor data | Time-series embeddings |
| Search code by functionality | Code embeddings (CodeBERT) |
| Match voice to speaker | Audio embeddings (speaker verification) |

: Embedding type selection guide {.striped}

## Key Takeaways

- **Different data types require different embedding architectures** that match the structure of the data

- **Text embeddings** are the most mature, with sentence transformers providing excellent quality for most applications

- **Image embeddings** use CNNs or Vision Transformers to capture visual semantics

- **Multi-modal embeddings** like CLIP enable cross-modal search and zero-shot classification

- **Graph embeddings** capture relational structure, essential for social networks and knowledge graphs

- **Time-series embeddings** encode temporal patterns for anomaly detection and similarity search

- **Code embeddings** understand program semantics, enabling semantic code search

## Looking Ahead

Now that you understand the foundational embedding types, @sec-advanced-embedding-patterns covers advanced patterns used in production systems—hybrid vectors that combine multiple feature types, multi-vector representations, learned sparse embeddings, and more. For deep dives into how specific models work, @sec-embedding-model-fundamentals explains the underlying architectures. Then @sec-strategic-architecture covers how to architect embedding systems that can handle multiple modalities at scale.

## Further Reading

- Reimers, N. & Gurevych, I. (2019). "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks." *arXiv:1908.10084*
- Radford, A., et al. (2021). "Learning Transferable Visual Models From Natural Language Supervision." *arXiv:2103.00020* (CLIP)
- Grover, A. & Leskovec, J. (2016). "node2vec: Scalable Feature Learning for Networks." *KDD*
- Baevski, A., et al. (2020). "wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations." *NeurIPS*
- Feng, Z., et al. (2020). "CodeBERT: A Pre-Trained Model for Programming and Natural Languages." *EMNLP*
