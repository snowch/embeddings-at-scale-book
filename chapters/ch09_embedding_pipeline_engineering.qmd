# Embedding Pipeline Engineering {#sec-embedding-pipeline-engineering}

:::{.callout-note}
## Chapter Overview
Moving from custom embedding development to production deployment requires robust engineering practices. This chapter explores the operational infrastructure needed to deploy, monitor, and maintain embedding systems at trillion-row scale. We'll cover MLOps practices specific to embeddings, the trade-offs between real-time and batch processing, versioning strategies that enable safe rollouts and rollbacks, A/B testing methodologies for embedding models, and comprehensive monitoring approaches to detect drift and degradation. These practices ensure embedding systems remain reliable, performant, and maintainable as they scale from prototype to production.
:::

The journey from a successful embedding model to a production-ready system involves significant engineering challenges. Unlike traditional ML models that produce discrete predictions, embedding systems integrate into search pipelines, recommendation engines, and real-time decision systems where latency, freshness, and consistency are critical. This chapter provides the operational toolkit for building embedding pipelines that scale to hundreds of millions of queries per day across trillion-row datasets.

## MLOps for Embedding Production

Embedding systems have unique MLOps requirements that distinguish them from traditional ML deployments. While a classification model serves predictions on demand, an embedding system must **continuously generate and update vectors for massive datasets, maintain multiple indices for fast retrieval, serve both embedding generation and similarity search, and coordinate versioning across embedding models and vector indices**.

### The Embedding Production Stack

A production embedding system comprises multiple interconnected components:

```python
"""
Production Embedding System Architecture

Components:
1. Training Pipeline: Generates embedding models
2. Inference Pipeline: Applies models to data at scale
3. Vector Store: Stores and indexes embeddings
4. Serving Layer: Handles real-time queries
5. Monitoring System: Tracks quality and performance
6. Orchestration: Coordinates updates and rollouts

Data flow:
Raw Data → Feature Engineering → Model Training → Model Registry
Raw Data → Feature Engineering → Batch Inference → Vector Store → Serving Layer
Real-time Data → Feature Engineering → Online Inference → Vector Store → Serving Layer
"""

import torch
import torch.nn as nn
from typing import Dict, List, Optional, Union
from dataclasses import dataclass
from datetime import datetime
import numpy as np
import json

@dataclass
class EmbeddingModelMetadata:
    """
    Metadata for tracking embedding models in production

    Critical for:
    - Version management across pipeline stages
    - Reproducing embeddings for debugging
    - Audit trails for compliance
    - Impact analysis for changes
    """
    model_id: str  # Unique identifier (e.g., "user-embeddings-v1.2.3")
    model_version: str  # Semantic version
    training_date: datetime
    training_data_hash: str  # Hash of training data for reproducibility
    hyperparameters: Dict
    performance_metrics: Dict  # Validation metrics
    embedding_dimension: int
    input_features: List[str]
    preprocessing_config: Dict

    # Deployment tracking
    deployed_to_staging: Optional[datetime] = None
    deployed_to_production: Optional[datetime] = None
    rollback_model_id: Optional[str] = None  # Previous version to rollback to

    # Model lineage
    parent_model_id: Optional[str] = None  # For fine-tuned models
    training_commit_hash: str = ""  # Git commit of training code

    def to_dict(self) -> Dict:
        """Serialize for storage in model registry"""
        return {
            'model_id': self.model_id,
            'model_version': self.model_version,
            'training_date': self.training_date.isoformat(),
            'training_data_hash': self.training_data_hash,
            'hyperparameters': self.hyperparameters,
            'performance_metrics': self.performance_metrics,
            'embedding_dimension': self.embedding_dimension,
            'input_features': self.input_features,
            'preprocessing_config': self.preprocessing_config,
            'deployed_to_staging': self.deployed_to_staging.isoformat() if self.deployed_to_staging else None,
            'deployed_to_production': self.deployed_to_production.isoformat() if self.deployed_to_production else None,
            'rollback_model_id': self.rollback_model_id,
            'parent_model_id': self.parent_model_id,
            'training_commit_hash': self.training_commit_hash
        }

class EmbeddingModelRegistry:
    """
    Central registry for embedding models across training and serving

    Functions:
    - Store trained models with metadata
    - Version management (semantic versioning)
    - Model lineage tracking
    - Deployment state management
    - Model discovery for inference pipelines

    In production: Use MLflow, Weights & Biases, or custom S3/GCS-based registry
    """

    def __init__(self, storage_backend: str = "local"):
        """
        Args:
            storage_backend: 's3', 'gcs', 'mlflow', or 'local'
        """
        self.storage_backend = storage_backend
        self.models: Dict[str, EmbeddingModelMetadata] = {}

    def register_model(
        self,
        model: nn.Module,
        metadata: EmbeddingModelMetadata,
        artifacts: Optional[Dict] = None
    ) -> str:
        """
        Register a trained embedding model

        Args:
            model: Trained PyTorch model
            metadata: Model metadata
            artifacts: Additional artifacts (preprocessing objects, config files)

        Returns:
            model_id: Unique identifier for registered model
        """
        # Validate model
        self._validate_model(model, metadata)

        # Store model weights
        model_path = self._store_model_weights(model, metadata.model_id)

        # Store artifacts (preprocessing pipelines, vocabularies, etc.)
        if artifacts:
            artifact_path = self._store_artifacts(artifacts, metadata.model_id)

        # Register in metadata store
        self.models[metadata.model_id] = metadata
        self._persist_metadata(metadata)

        print(f"✓ Registered model: {metadata.model_id}")
        print(f"  Version: {metadata.model_version}")
        print(f"  Embedding dim: {metadata.embedding_dimension}")
        print(f"  Training date: {metadata.training_date}")

        return metadata.model_id

    def load_model(
        self,
        model_id: str,
        device: str = 'cpu'
    ) -> tuple[nn.Module, EmbeddingModelMetadata]:
        """
        Load model and metadata from registry

        Args:
            model_id: Model identifier
            device: 'cpu', 'cuda', or specific device

        Returns:
            (model, metadata): Loaded model and metadata
        """
        if model_id not in self.models:
            # Try loading from persistent storage
            metadata = self._load_metadata(model_id)
            if metadata is None:
                raise ValueError(f"Model {model_id} not found in registry")
            self.models[model_id] = metadata
        else:
            metadata = self.models[model_id]

        # Load model weights
        model = self._load_model_weights(model_id, metadata, device)

        print(f"✓ Loaded model: {model_id}")
        print(f"  Version: {metadata.model_version}")

        return model, metadata

    def promote_to_production(
        self,
        model_id: str,
        validation_results: Dict
    ) -> bool:
        """
        Promote model from staging to production

        Validates:
        - Performance metrics meet thresholds
        - Staging testing completed
        - Rollback plan exists

        Args:
            model_id: Model to promote
            validation_results: Results from staging validation

        Returns:
            success: True if promotion succeeded
        """
        metadata = self.models.get(model_id)
        if metadata is None:
            raise ValueError(f"Model {model_id} not found")

        # Validation checks
        if metadata.deployed_to_staging is None:
            raise ValueError("Model must be deployed to staging before production")

        # Check performance thresholds
        required_metrics = ['retrieval_recall@10', 'embedding_quality_score']
        for metric in required_metrics:
            if metric not in validation_results:
                raise ValueError(f"Missing required validation metric: {metric}")

        # Example thresholds (customize per use case)
        if validation_results['retrieval_recall@10'] < 0.85:
            print(f"✗ Promotion failed: recall@10 ({validation_results['retrieval_recall@10']}) < 0.85")
            return False

        # Find current production model for rollback
        current_prod_model = self._get_current_production_model()
        if current_prod_model:
            metadata.rollback_model_id = current_prod_model.model_id

        # Promote
        metadata.deployed_to_production = datetime.now()
        self._persist_metadata(metadata)

        # Update production pointer
        self._update_production_pointer(model_id)

        print(f"✓ Promoted {model_id} to production")
        if metadata.rollback_model_id:
            print(f"  Rollback target: {metadata.rollback_model_id}")

        return True

    def rollback(self) -> str:
        """
        Rollback to previous production model

        Returns:
            model_id: ID of model rolled back to
        """
        current_prod = self._get_current_production_model()
        if current_prod is None:
            raise ValueError("No production model to rollback from")

        if current_prod.rollback_model_id is None:
            raise ValueError("No rollback target defined")

        rollback_model_id = current_prod.rollback_model_id

        # Update production pointer
        self._update_production_pointer(rollback_model_id)

        print(f"✓ Rolled back to {rollback_model_id}")
        print(f"  From: {current_prod.model_id}")

        return rollback_model_id

    def _validate_model(self, model: nn.Module, metadata: EmbeddingModelMetadata):
        """Validate model matches metadata"""
        # Check model produces correct embedding dimension
        dummy_input = torch.randn(1, len(metadata.input_features))
        try:
            output = model(dummy_input)
            if output.shape[-1] != metadata.embedding_dimension:
                raise ValueError(
                    f"Model output dim ({output.shape[-1]}) != "
                    f"metadata dim ({metadata.embedding_dimension})"
                )
        except Exception as e:
            raise ValueError(f"Model validation failed: {e}")

    def _store_model_weights(self, model: nn.Module, model_id: str) -> str:
        """Store model weights to backend storage"""
        # In production: Upload to S3/GCS
        # For now: Local storage
        path = f"models/{model_id}/model.pt"
        torch.save(model.state_dict(), path)
        return path

    def _store_artifacts(self, artifacts: Dict, model_id: str) -> str:
        """Store additional artifacts"""
        path = f"models/{model_id}/artifacts.json"
        with open(path, 'w') as f:
            json.dump(artifacts, f)
        return path

    def _persist_metadata(self, metadata: EmbeddingModelMetadata):
        """Persist metadata to storage"""
        path = f"models/{metadata.model_id}/metadata.json"
        with open(path, 'w') as f:
            json.dump(metadata.to_dict(), f, indent=2)

    def _load_metadata(self, model_id: str) -> Optional[EmbeddingModelMetadata]:
        """Load metadata from storage"""
        # Implementation depends on storage backend
        return None

    def _load_model_weights(
        self,
        model_id: str,
        metadata: EmbeddingModelMetadata,
        device: str
    ) -> nn.Module:
        """Load model weights from storage"""
        # In production: Download from S3/GCS
        path = f"models/{model_id}/model.pt"

        # Reconstruct model architecture (stored in metadata.hyperparameters)
        model = self._reconstruct_model_architecture(metadata)
        model.load_state_dict(torch.load(path, map_location=device))
        model.to(device)
        model.eval()

        return model

    def _reconstruct_model_architecture(self, metadata: EmbeddingModelMetadata) -> nn.Module:
        """Reconstruct model architecture from metadata"""
        # Simplified example - real implementation would use config files
        class SimpleEmbedding(nn.Module):
            def __init__(self, input_dim, output_dim):
                super().__init__()
                self.encoder = nn.Sequential(
                    nn.Linear(input_dim, 512),
                    nn.ReLU(),
                    nn.Linear(512, output_dim)
                )

            def forward(self, x):
                return self.encoder(x)

        return SimpleEmbedding(
            len(metadata.input_features),
            metadata.embedding_dimension
        )

    def _get_current_production_model(self) -> Optional[EmbeddingModelMetadata]:
        """Get current production model"""
        # In production: Read from production pointer file/database
        prod_models = [
            m for m in self.models.values()
            if m.deployed_to_production is not None
        ]
        if not prod_models:
            return None
        return max(prod_models, key=lambda m: m.deployed_to_production)

    def _update_production_pointer(self, model_id: str):
        """Update production pointer to new model"""
        # In production: Update pointer file in S3, or database entry
        # This is what serving layer reads to determine which model to use
        pointer_path = "production/current_model.txt"
        with open(pointer_path, 'w') as f:
            f.write(model_id)

class EmbeddingInferencePipeline:
    """
    Batch inference pipeline for generating embeddings at scale

    Handles:
    - Large-scale batch processing (billions of items)
    - Checkpointing for fault tolerance
    - Distributed processing across workers
    - Resource optimization (GPU utilization, batching)
    - Progress tracking and monitoring

    Typical throughput:
    - Single GPU: 10K-100K embeddings/second
    - 8 GPU node: 100K-1M embeddings/second
    - 100 GPU cluster: 10M-100M embeddings/second
    """

    def __init__(
        self,
        model_registry: EmbeddingModelRegistry,
        model_id: str,
        batch_size: int = 1024,
        num_workers: int = 4,
        device: str = 'cuda' if torch.cuda.is_available() else 'cpu'
    ):
        """
        Args:
            model_registry: Registry to load model from
            model_id: Model to use for inference
            batch_size: Batch size for inference
            num_workers: Data loading workers
            device: Inference device
        """
        self.model_registry = model_registry
        self.model_id = model_id
        self.batch_size = batch_size
        self.num_workers = num_workers
        self.device = device

        # Load model
        self.model, self.metadata = model_registry.load_model(model_id, device)
        self.model.eval()

        # Metrics
        self.processed_count = 0
        self.start_time = None

    def process_batch(
        self,
        data_iterator,
        output_writer,
        checkpoint_every: int = 100000
    ):
        """
        Process large dataset in batches

        Args:
            data_iterator: Iterator yielding batches of data
            output_writer: Writer for generated embeddings
            checkpoint_every: Save checkpoint every N items
        """
        self.start_time = datetime.now()
        batch_buffer = []

        print(f"Starting batch inference with model {self.model_id}")
        print(f"  Batch size: {self.batch_size}")
        print(f"  Device: {self.device}")

        with torch.no_grad():
            for batch_idx, batch_data in enumerate(data_iterator):
                # Move to device
                if isinstance(batch_data, torch.Tensor):
                    batch_data = batch_data.to(self.device)

                # Generate embeddings
                embeddings = self.model(batch_data)

                # Write to output
                batch_buffer.extend(embeddings.cpu().numpy())
                self.processed_count += len(embeddings)

                # Flush buffer periodically
                if len(batch_buffer) >= checkpoint_every:
                    output_writer.write(batch_buffer)
                    batch_buffer = []
                    self._save_checkpoint(batch_idx)
                    self._log_progress()

                # Memory management
                if batch_idx % 100 == 0:
                    torch.cuda.empty_cache() if torch.cuda.is_available() else None

        # Final flush
        if batch_buffer:
            output_writer.write(batch_buffer)

        self._log_progress(final=True)

    def _save_checkpoint(self, batch_idx: int):
        """Save checkpoint for fault tolerance"""
        checkpoint = {
            'model_id': self.model_id,
            'processed_count': self.processed_count,
            'batch_idx': batch_idx,
            'timestamp': datetime.now().isoformat()
        }
        # In production: Save to persistent storage
        with open('checkpoint.json', 'w') as f:
            json.dump(checkpoint, f)

    def _log_progress(self, final: bool = False):
        """Log progress metrics"""
        elapsed = (datetime.now() - self.start_time).total_seconds()
        throughput = self.processed_count / elapsed if elapsed > 0 else 0

        if final:
            print(f"\n✓ Batch inference complete")
            print(f"  Total processed: {self.processed_count:,}")
            print(f"  Elapsed time: {elapsed:.1f}s")
            print(f"  Throughput: {throughput:,.0f} embeddings/second")
        else:
            print(f"  Progress: {self.processed_count:,} embeddings "
                  f"({throughput:,.0f}/sec)", end='\r')

# Example: Complete MLOps workflow
def embedding_mlops_example():
    """
    End-to-end MLOps workflow for embeddings

    Steps:
    1. Train model
    2. Register in model registry
    3. Deploy to staging
    4. Validate in staging
    5. Promote to production
    6. Run batch inference
    7. Monitor in production
    """

    # 1. Train model (simplified)
    class SimpleEmbeddingModel(nn.Module):
        def __init__(self, input_dim, embedding_dim):
            super().__init__()
            self.encoder = nn.Sequential(
                nn.Linear(input_dim, 512),
                nn.ReLU(),
                nn.Linear(512, embedding_dim)
            )

        def forward(self, x):
            return self.encoder(x)

    model = SimpleEmbeddingModel(input_dim=100, embedding_dim=256)

    # 2. Create metadata
    metadata = EmbeddingModelMetadata(
        model_id="product-embeddings-v1.0.0",
        model_version="1.0.0",
        training_date=datetime.now(),
        training_data_hash="abc123",
        hyperparameters={
            'embedding_dim': 256,
            'learning_rate': 0.001,
            'batch_size': 512
        },
        performance_metrics={
            'retrieval_recall@10': 0.89,
            'embedding_quality_score': 0.92
        },
        embedding_dimension=256,
        input_features=['feature_' + str(i) for i in range(100)],
        preprocessing_config={'normalization': 'standard'},
        training_commit_hash="def456"
    )

    # 3. Register model
    registry = EmbeddingModelRegistry(storage_backend='local')
    model_id = registry.register_model(model, metadata)

    # 4. Deploy to staging
    metadata.deployed_to_staging = datetime.now()
    registry._persist_metadata(metadata)
    print(f"\n✓ Deployed to staging: {model_id}")

    # 5. Validate in staging
    staging_validation_results = {
        'retrieval_recall@10': 0.89,
        'embedding_quality_score': 0.92,
        'latency_p99_ms': 15
    }

    # 6. Promote to production
    success = registry.promote_to_production(model_id, staging_validation_results)

    if success:
        # 7. Run batch inference
        pipeline = EmbeddingInferencePipeline(
            model_registry=registry,
            model_id=model_id,
            batch_size=1024
        )

        print(f"\n✓ MLOps workflow complete")
        print(f"  Model in production: {model_id}")

# Uncomment to run:
# embedding_mlops_example()
```

:::{.callout-tip}
## Model Registry Best Practices
1. **Semantic versioning**: Use MAJOR.MINOR.PATCH for model versions
2. **Immutable models**: Never modify registered models; create new versions
3. **Metadata completeness**: Track training data, hyperparameters, and performance metrics
4. **Rollback plan**: Always maintain reference to previous production model
5. **Audit trail**: Log all deployments, rollbacks, and configuration changes
:::

## Real-Time vs. Batch Embedding Generation

One of the most critical architectural decisions for embedding systems is **when and how to generate embeddings**. Batch processing offers throughput and cost efficiency, while real-time generation provides freshness and personalization. Most production systems use a hybrid approach, optimizing for different use cases within the same platform.

### The Batch vs. Real-Time Trade-off

**Batch Processing** generates embeddings offline in large batches:
- **Advantages**: High throughput (10-100x faster), cost-efficient (cheaper compute), optimized resource utilization, quality control before serving
- **Disadvantages**: Staleness (hours to days old), no personalization, large storage requirements, delayed updates
- **Best for**: Product catalogs, document collections, static content, historical data

**Real-Time Processing** generates embeddings on-demand:
- **Advantages**: Fresh embeddings (milliseconds old), personalized to context, storage efficient (compute on-demand), immediate updates
- **Disadvantages**: High latency (10-100ms), expensive (online GPU inference), variable load patterns, harder to monitor quality
- **Best for**: User queries, personalized feeds, dynamic content, real-time sessions

```python
import torch
import torch.nn as nn
from typing import Optional, List, Dict
import asyncio
from datetime import datetime, timedelta
import numpy as np
from collections import deque

class HybridEmbeddingSystem:
    """
    Hybrid system combining batch and real-time embedding generation

    Architecture:
    - Batch layer: Pre-compute embeddings for stable entities (products, documents)
    - Real-time layer: Generate embeddings for dynamic entities (user queries, sessions)
    - Caching layer: Cache frequently accessed real-time embeddings
    - Routing logic: Determine batch vs. real-time for each request

    Decision matrix:
    | Entity Type | Update Frequency | Access Pattern | Strategy |
    |-------------|-----------------|----------------|----------|
    | Products    | Daily           | Random         | Batch    |
    | Documents   | Hourly          | Zipfian        | Batch + Cache |
    | User Queries| Per request     | Unique         | Real-time |
    | User Profiles| Weekly         | Personalized   | Batch + Real-time |
    """

    def __init__(
        self,
        model_registry,
        model_id: str,
        cache_size: int = 100000,
        real_time_threshold_ms: int = 50
    ):
        """
        Args:
            model_registry: Registry to load embedding model
            model_id: Model to use
            cache_size: Number of embeddings to cache
            real_time_threshold_ms: Max latency for real-time generation
        """
        self.model, self.metadata = model_registry.load_model(model_id, 'cuda')
        self.model.eval()

        # Batch embedding storage (in production: vector database)
        self.batch_embeddings: Dict[str, np.ndarray] = {}
        self.batch_embedding_timestamps: Dict[str, datetime] = {}

        # Real-time embedding cache (LRU)
        self.embedding_cache: Dict[str, np.ndarray] = {}
        self.cache_access_times = deque(maxlen=cache_size)

        # Metrics
        self.cache_hits = 0
        self.cache_misses = 0
        self.batch_lookups = 0
        self.realtime_generations = 0

    def get_embedding(
        self,
        entity_id: str,
        entity_type: str,
        features: Optional[torch.Tensor] = None,
        max_staleness: Optional[timedelta] = None
    ) -> np.ndarray:
        """
        Get embedding using appropriate strategy

        Args:
            entity_id: Unique identifier for entity
            entity_type: 'product', 'document', 'query', 'user'
            features: Features for real-time generation
            max_staleness: Maximum acceptable age for batch embeddings

        Returns:
            embedding: Vector representation
        """
        # Route based on entity type
        if entity_type in ['query', 'session']:
            # Always generate real-time for transient entities
            return self._generate_realtime(entity_id, features)

        elif entity_type in ['product', 'document']:
            # Prefer batch, fallback to real-time
            batch_emb = self._lookup_batch(entity_id, max_staleness)
            if batch_emb is not None:
                return batch_emb
            else:
                return self._generate_realtime(entity_id, features)

        elif entity_type == 'user':
            # Hybrid: batch base + real-time personalization
            base_emb = self._lookup_batch(entity_id, max_staleness)
            if base_emb is None:
                base_emb = self._generate_realtime(entity_id, features)

            # Personalize based on current session
            personalized_emb = self._personalize_embedding(base_emb, features)
            return personalized_emb

        else:
            raise ValueError(f"Unknown entity type: {entity_type}")

    def _lookup_batch(
        self,
        entity_id: str,
        max_staleness: Optional[timedelta]
    ) -> Optional[np.ndarray]:
        """
        Lookup pre-computed batch embedding

        Returns None if:
        - Embedding doesn't exist
        - Embedding is too stale
        """
        if entity_id not in self.batch_embeddings:
            return None

        # Check staleness
        if max_staleness is not None:
            embedding_time = self.batch_embedding_timestamps[entity_id]
            age = datetime.now() - embedding_time
            if age > max_staleness:
                return None  # Too stale, need fresh embedding

        self.batch_lookups += 1
        return self.batch_embeddings[entity_id]

    def _generate_realtime(
        self,
        entity_id: str,
        features: torch.Tensor
    ) -> np.ndarray:
        """
        Generate embedding in real-time

        With caching for frequently accessed entities
        """
        # Check cache first
        if entity_id in self.embedding_cache:
            self.cache_hits += 1
            return self.embedding_cache[entity_id]

        self.cache_misses += 1
        self.realtime_generations += 1

        # Generate embedding
        start_time = datetime.now()

        with torch.no_grad():
            features = features.to('cuda')
            embedding = self.model(features).cpu().numpy()

        latency_ms = (datetime.now() - start_time).total_seconds() * 1000

        # Cache if latency-sensitive
        if latency_ms > 10:  # Cache expensive computations
            self._add_to_cache(entity_id, embedding)

        return embedding

    def _personalize_embedding(
        self,
        base_embedding: np.ndarray,
        session_features: torch.Tensor
    ) -> np.ndarray:
        """
        Personalize base embedding with real-time session context

        Techniques:
        - Additive: base + alpha * session_vector
        - Attention: weighted combination based on context
        - Learned: small neural network for personalization
        """
        # Simple additive personalization
        with torch.no_grad():
            session_features = session_features.to('cuda')
            session_vector = self.model(session_features).cpu().numpy()

        # Weighted combination (80% base, 20% session)
        personalized = 0.8 * base_embedding + 0.2 * session_vector

        return personalized

    def _add_to_cache(self, entity_id: str, embedding: np.ndarray):
        """
        Add to LRU cache

        Evict oldest if cache full
        """
        if len(self.embedding_cache) >= len(self.cache_access_times):
            # Evict oldest
            oldest_id = self.cache_access_times.popleft()
            if oldest_id in self.embedding_cache:
                del self.embedding_cache[oldest_id]

        self.embedding_cache[entity_id] = embedding
        self.cache_access_times.append(entity_id)

    def batch_update(
        self,
        entity_ids: List[str],
        embeddings: List[np.ndarray]
    ):
        """
        Update batch embeddings (called by batch processing job)

        Args:
            entity_ids: IDs of entities
            embeddings: Pre-computed embeddings
        """
        timestamp = datetime.now()

        for entity_id, embedding in zip(entity_ids, embeddings):
            self.batch_embeddings[entity_id] = embedding
            self.batch_embedding_timestamps[entity_id] = timestamp

        print(f"✓ Updated {len(entity_ids)} batch embeddings")

        # Invalidate cache for updated entities
        for entity_id in entity_ids:
            if entity_id in self.embedding_cache:
                del self.embedding_cache[entity_id]

    def get_metrics(self) -> Dict:
        """
        Get system metrics for monitoring

        Key metrics:
        - Cache hit rate
        - Batch vs. real-time ratio
        - Average latency per strategy
        """
        total_requests = self.batch_lookups + self.realtime_generations
        cache_hit_rate = self.cache_hits / (self.cache_hits + self.cache_misses) if (self.cache_hits + self.cache_misses) > 0 else 0

        return {
            'total_requests': total_requests,
            'batch_lookups': self.batch_lookups,
            'batch_ratio': self.batch_lookups / total_requests if total_requests > 0 else 0,
            'realtime_generations': self.realtime_generations,
            'realtime_ratio': self.realtime_generations / total_requests if total_requests > 0 else 0,
            'cache_hit_rate': cache_hit_rate,
            'cache_size': len(self.embedding_cache)
        }

class StreamingEmbeddingPipeline:
    """
    Streaming pipeline for near-real-time embedding updates

    Architecture:
    - Consume events from stream (Kafka, Kinesis, Pub/Sub)
    - Micro-batch embeddings (100-1000 items every 10-60 seconds)
    - Update vector index incrementally
    - Balance freshness vs. throughput

    Use cases:
    - News articles (publish → embed → searchable in <1 minute)
    - E-commerce products (inventory update → embed → discoverable)
    - Social media posts (post → embed → recommendable)
    - Log events (event → embed → queryable)
    """

    def __init__(
        self,
        model,
        batch_window_seconds: int = 30,
        max_batch_size: int = 500
    ):
        """
        Args:
            model: Embedding model
            batch_window_seconds: Time window for micro-batching
            max_batch_size: Max items per micro-batch
        """
        self.model = model
        self.model.eval()

        self.batch_window = timedelta(seconds=batch_window_seconds)
        self.max_batch_size = max_batch_size

        # Accumulation buffer
        self.buffer: List[Dict] = []
        self.last_flush = datetime.now()

        # Metrics
        self.items_processed = 0
        self.batches_processed = 0

    async def process_stream(self, event_stream):
        """
        Process streaming events in micro-batches

        Args:
            event_stream: Async iterator of events
        """
        async for event in event_stream:
            # Add to buffer
            self.buffer.append(event)

            # Flush if buffer full or time window expired
            should_flush = (
                len(self.buffer) >= self.max_batch_size or
                (datetime.now() - self.last_flush) >= self.batch_window
            )

            if should_flush:
                await self._flush_buffer()

    async def _flush_buffer(self):
        """
        Process accumulated buffer as micro-batch

        Benefits of micro-batching:
        - Amortize model overhead
        - Better GPU utilization
        - Reduce index update frequency
        """
        if not self.buffer:
            return

        print(f"Flushing buffer with {len(self.buffer)} items...")

        # Extract features from events
        features = torch.stack([self._extract_features(event) for event in self.buffer])

        # Generate embeddings (batched)
        with torch.no_grad():
            embeddings = self.model(features.to('cuda')).cpu().numpy()

        # Update vector index
        entity_ids = [event['entity_id'] for event in self.buffer]
        await self._update_index(entity_ids, embeddings)

        # Metrics
        self.items_processed += len(self.buffer)
        self.batches_processed += 1

        # Clear buffer
        self.buffer = []
        self.last_flush = datetime.now()

        print(f"✓ Processed {len(entity_ids)} embeddings (total: {self.items_processed})")

    def _extract_features(self, event: Dict) -> torch.Tensor:
        """Extract features from streaming event"""
        # Implementation depends on event schema
        return torch.randn(100)  # Placeholder

    async def _update_index(self, entity_ids: List[str], embeddings: np.ndarray):
        """
        Update vector index with new embeddings

        In production: Call vector database API (Pinecone, Weaviate, etc.)
        """
        # Asynchronous index update
        await asyncio.sleep(0.01)  # Simulate async update
        pass

# Example: E-commerce product embeddings
def ecommerce_hybrid_example():
    """
    E-commerce scenario with hybrid approach

    Products (batch):
    - 10M products
    - Update daily (new products, price changes, inventory)
    - Batch process overnight

    User Queries (real-time):
    - 100M queries/day
    - Generate on-demand
    - Cache popular queries

    User Profiles (hybrid):
    - 50M users
    - Base profile updated weekly (batch)
    - Personalized with current session (real-time)
    """
    from datetime import timedelta

    # Initialize hybrid system
    registry = EmbeddingModelRegistry()
    # Assume model is registered
    hybrid_system = HybridEmbeddingSystem(
        model_registry=registry,
        model_id="ecommerce-v1.0.0",
        cache_size=100000
    )

    # Scenario 1: Product lookup (batch)
    product_embedding = hybrid_system.get_embedding(
        entity_id="product_12345",
        entity_type="product",
        max_staleness=timedelta(days=1)
    )
    print(f"Product embedding: {product_embedding.shape}")

    # Scenario 2: User query (real-time)
    query_features = torch.randn(1, 100)
    query_embedding = hybrid_system.get_embedding(
        entity_id="query_unique_789",
        entity_type="query",
        features=query_features
    )
    print(f"Query embedding: {query_embedding.shape}")

    # Scenario 3: Personalized user (hybrid)
    user_session_features = torch.randn(1, 100)
    user_embedding = hybrid_system.get_embedding(
        entity_id="user_456",
        entity_type="user",
        features=user_session_features,
        max_staleness=timedelta(days=7)
    )
    print(f"User embedding: {user_embedding.shape}")

    # Metrics
    metrics = hybrid_system.get_metrics()
    print(f"\nSystem metrics:")
    print(f"  Batch ratio: {metrics['batch_ratio']:.2%}")
    print(f"  Real-time ratio: {metrics['realtime_ratio']:.2%}")
    print(f"  Cache hit rate: {metrics['cache_hit_rate']:.2%}")

# Uncomment to run:
# ecommerce_hybrid_example()
```

:::{.callout-tip}
## Choosing the Right Strategy

**Use batch processing when:**
- Entity changes are infrequent (daily/weekly updates)
- Dataset is large but manageable (millions to billions)
- Latency requirements are relaxed (seconds acceptable)
- Cost optimization is critical

**Use real-time generation when:**
- Freshness is critical (sub-second requirements)
- Entities are transient (search queries, sessions)
- Personalization is required (user-specific embeddings)
- Dataset is small (thousands to millions)

**Use hybrid approach when:**
- Mixed entity types with different requirements
- Need both cost efficiency and freshness
- Serving 100M+ requests/day across diverse use cases
:::

:::{.callout-warning}
## Cold Start Problem
Real-time generation can fail during cold starts (model not loaded, GPU unavailable). Always maintain:
1. **Warm standby**: Pre-warmed models ready to serve
2. **Fallback to batch**: Serve slightly stale batch embeddings if real-time fails
3. **Graceful degradation**: Return approximate results rather than errors
:::

## Embedding Versioning and Rollback Strategies

Embeddings generated by different model versions are **incompatible**—you cannot mix vectors from v1.0 and v2.0 in the same similarity search. This creates unique versioning challenges that require careful coordination across the entire embedding pipeline.

### The Versioning Challenge

When you deploy a new embedding model:
1. All existing embeddings become incompatible with new queries
2. Must re-generate embeddings for entire corpus (billions of vectors)
3. Must coordinate index updates with model deployment
4. Must support rollback if new model underperforms

The core challenge: **How do you deploy a new embedding model without downtime or inconsistency?**

```python
from enum import Enum
from typing import Dict, List, Optional
import torch
import numpy as np
from datetime import datetime

class DeploymentStrategy(Enum):
    """
    Strategies for deploying new embedding models

    1. BLUE_GREEN: Maintain two complete indices, switch traffic instantly
    2. INCREMENTAL: Gradually re-embed corpus, serve from multiple indices
    3. SHADOW: Run new model in shadow mode, compare before switching
    4. CANARY: Route small % of traffic to new model, monitor metrics
    """
    BLUE_GREEN = "blue_green"
    INCREMENTAL = "incremental"
    SHADOW = "shadow"
    CANARY = "canary"

class EmbeddingVersionCoordinator:
    """
    Coordinate embedding model versions across pipeline stages

    Responsibilities:
    - Track active model versions (training, staging, production)
    - Coordinate re-embedding campaigns for new versions
    - Manage multiple vector indices during transitions
    - Enable safe rollbacks with version pinning
    - Monitor version-specific metrics

    Architecture:
    - Version Registry: Maps model_version → index_version
    - Index Router: Routes queries to appropriate index based on version
    - Re-embedding Orchestrator: Manages gradual corpus re-embedding
    - Rollback Controller: Handles instant rollbacks to previous version
    """

    def __init__(self, model_registry):
        self.model_registry = model_registry

        # Track deployed versions
        self.active_versions: Dict[str, VersionDeployment] = {}

        # Index mapping: version → index_name
        self.version_to_index: Dict[str, str] = {}

        # Traffic routing: version → traffic_percentage
        self.traffic_routing: Dict[str, float] = {}

    def deploy_new_version(
        self,
        new_model_id: str,
        strategy: DeploymentStrategy,
        corpus_iterator=None
    ):
        """
        Deploy new embedding model version

        Args:
            new_model_id: New model to deploy
            strategy: Deployment strategy
            corpus_iterator: Iterator over corpus for re-embedding
        """
        new_model, metadata = self.model_registry.load_model(new_model_id)

        print(f"Deploying {new_model_id} using {strategy.value} strategy...")

        if strategy == DeploymentStrategy.BLUE_GREEN:
            self._deploy_blue_green(new_model_id, new_model, corpus_iterator)

        elif strategy == DeploymentStrategy.INCREMENTAL:
            self._deploy_incremental(new_model_id, new_model, corpus_iterator)

        elif strategy == DeploymentStrategy.SHADOW:
            self._deploy_shadow(new_model_id, new_model)

        elif strategy == DeploymentStrategy.CANARY:
            self._deploy_canary(new_model_id, new_model)

    def _deploy_blue_green(
        self,
        new_model_id: str,
        new_model,
        corpus_iterator
    ):
        """
        Blue-Green Deployment

        Steps:
        1. Build complete new index (GREEN) while serving from old (BLUE)
        2. Validate new index quality
        3. Switch traffic from BLUE → GREEN instantly
        4. Keep BLUE as rollback target

        Pros:
        - Instant switchover (no partial state)
        - Easy rollback (flip traffic back)
        - No version mixing

        Cons:
        - Expensive (2x storage during transition)
        - Long preparation time (full re-embedding)
        - All-or-nothing switch
        """
        print("Building GREEN index (new version)...")

        # Create new index
        green_index_name = f"embeddings_{new_model_id.replace('.', '_')}"
        self._create_new_index(green_index_name)

        # Re-embed entire corpus into GREEN
        self._reembed_corpus(
            new_model,
            corpus_iterator,
            target_index=green_index_name
        )

        # Validate GREEN index
        validation_passed = self._validate_index_quality(green_index_name)

        if not validation_passed:
            print("✗ Validation failed, aborting deployment")
            self._delete_index(green_index_name)
            return

        # Get current BLUE index for rollback
        current_prod = self.model_registry._get_current_production_model()
        blue_index_name = self.version_to_index.get(current_prod.model_id)

        # Switch traffic: BLUE → GREEN
        print("Switching traffic from BLUE → GREEN...")
        self._switch_traffic(
            from_index=blue_index_name,
            to_index=green_index_name
        )

        # Register new version
        self.version_to_index[new_model_id] = green_index_name
        self.traffic_routing[new_model_id] = 1.0  # 100% traffic

        # Keep BLUE as rollback target (don't delete yet)
        print(f"✓ Deployment complete. GREEN active, BLUE retained for rollback.")

    def _deploy_incremental(
        self,
        new_model_id: str,
        new_model,
        corpus_iterator
    ):
        """
        Incremental Deployment

        Steps:
        1. Create new index (initially empty)
        2. Re-embed corpus gradually (over hours/days)
        3. Route queries across OLD + NEW indices during transition
        4. Switch to NEW once re-embedding complete

        Pros:
        - Lower resource spike (gradual re-embedding)
        - Faster time-to-production (start using partially)
        - More control over transition

        Cons:
        - Complex routing logic (query both indices)
        - Duplicate storage during transition
        - Potential inconsistency (different items in different versions)
        """
        print("Starting incremental deployment...")

        # Create new index
        new_index_name = f"embeddings_{new_model_id.replace('.', '_')}"
        self._create_new_index(new_index_name)

        # Start re-embedding in background
        self._start_background_reembedding(
            new_model,
            corpus_iterator,
            target_index=new_index_name,
            rate_limit_items_per_sec=1000
        )

        # Route queries to BOTH old and new indices
        # (merge results, prefer new embeddings when available)
        current_prod = self.model_registry._get_current_production_model()
        old_index = self.version_to_index.get(current_prod.model_id)

        self._enable_dual_index_routing(old_index, new_index_name)

        print(f"✓ Incremental deployment started")
        print(f"  Re-embedding progress tracked in background")
        print(f"  Queries served from both OLD and NEW indices during transition")

    def _deploy_shadow(
        self,
        new_model_id: str,
        new_model
    ):
        """
        Shadow Deployment

        Steps:
        1. Deploy new model in "shadow mode"
        2. Serve all queries with OLD model (production)
        3. Simultaneously generate embeddings with NEW model (shadow)
        4. Compare OLD vs NEW results, collect metrics
        5. Switch to NEW if metrics better

        Pros:
        - Safe validation (no user impact)
        - Comprehensive metric collection
        - Detects issues before full deployment

        Cons:
        - 2x compute cost (running both models)
        - Longer validation period
        - Still need full re-embedding after shadow validation
        """
        print("Deploying in shadow mode...")

        # Register shadow model (doesn't serve production traffic)
        self.active_versions[new_model_id] = VersionDeployment(
            model_id=new_model_id,
            status=DeploymentStatus.SHADOW,
            traffic_percentage=0.0
        )

        # Shadow traffic logs comparisons but doesn't serve
        self._enable_shadow_mode(new_model_id)

        print(f"✓ Shadow deployment active")
        print(f"  NEW model: Running in shadow (no production traffic)")
        print(f"  OLD model: Serving 100% production traffic")
        print(f"  Comparison metrics being collected")

    def _deploy_canary(
        self,
        new_model_id: str,
        new_model
    ):
        """
        Canary Deployment

        Steps:
        1. Route 1% of traffic to NEW model
        2. Monitor metrics (latency, quality, errors)
        3. Gradually increase traffic: 1% → 5% → 10% → 50% → 100%
        4. Rollback if any stage shows degradation

        Pros:
        - Gradual validation with real traffic
        - Early detection of issues
        - Limits blast radius of problems

        Cons:
        - Requires mixing versions (compatibility issues)
        - Longer rollout timeline
        - Complex traffic routing
        """
        print("Starting canary deployment...")

        # Start with 1% traffic
        initial_canary_percentage = 0.01

        self.active_versions[new_model_id] = VersionDeployment(
            model_id=new_model_id,
            status=DeploymentStatus.CANARY,
            traffic_percentage=initial_canary_percentage
        )

        self.traffic_routing[new_model_id] = initial_canary_percentage

        print(f"✓ Canary deployment started")
        print(f"  NEW model: {initial_canary_percentage:.1%} traffic")
        print(f"  OLD model: {1 - initial_canary_percentage:.1%} traffic")
        print(f"  Monitor metrics, then gradually increase canary traffic")

    def rollback(self, target_model_id: Optional[str] = None):
        """
        Rollback to previous model version

        Critical for:
        - Quality degradation detected in production
        - Performance regression (latency spike)
        - Bugs in new model
        - Incompatibility issues

        Instant rollback: Switch traffic to previous index (Blue-Green)
        Gradual rollback: Reduce traffic to new version (Canary)
        """
        if target_model_id is None:
            # Rollback to previous production model
            current_prod = self.model_registry._get_current_production_model()
            if current_prod.rollback_model_id is None:
                raise ValueError("No rollback target defined")
            target_model_id = current_prod.rollback_model_id

        print(f"Rolling back to {target_model_id}...")

        # Get target index
        target_index = self.version_to_index.get(target_model_id)
        if target_index is None:
            raise ValueError(f"No index found for {target_model_id}")

        # Switch traffic
        self._switch_traffic(from_index=None, to_index=target_index)

        # Update traffic routing
        self.traffic_routing = {target_model_id: 1.0}

        print(f"✓ Rollback complete to {target_model_id}")

    def _reembed_corpus(self, model, corpus_iterator, target_index: str):
        """
        Re-embed entire corpus with new model

        For trillion-row scale:
        - Distribute across 100-1000 workers
        - Process 1M-10M items per worker
        - Checkpoint every 10M items for fault tolerance
        - Takes hours to days depending on scale
        """
        print(f"Re-embedding corpus into {target_index}...")

        processed = 0
        batch_size = 1024

        with torch.no_grad():
            for batch in corpus_iterator:
                # Generate embeddings
                embeddings = model(batch).cpu().numpy()

                # Write to new index
                self._write_to_index(target_index, embeddings)

                processed += len(batch)
                if processed % 1000000 == 0:
                    print(f"  Re-embedded {processed:,} items...")

        print(f"✓ Re-embedding complete: {processed:,} items")

    def _validate_index_quality(self, index_name: str) -> bool:
        """
        Validate new index meets quality thresholds

        Checks:
        - Retrieval quality (recall@k on test set)
        - Latency (p50, p95, p99)
        - Index size (compression ratio)
        - Coverage (% of corpus embedded)
        """
        print(f"Validating index {index_name}...")

        # Run test queries
        test_recall = 0.89  # Placeholder
        test_latency_p99_ms = 15  # Placeholder

        # Thresholds
        min_recall = 0.85
        max_latency_ms = 20

        passed = test_recall >= min_recall and test_latency_p99_ms <= max_latency_ms

        if passed:
            print(f"✓ Validation passed (recall={test_recall:.3f}, latency={test_latency_p99_ms}ms)")
        else:
            print(f"✗ Validation failed (recall={test_recall:.3f}, latency={test_latency_p99_ms}ms)")

        return passed

    # Placeholder helper methods
    def _create_new_index(self, index_name: str):
        pass

    def _delete_index(self, index_name: str):
        pass

    def _switch_traffic(self, from_index: Optional[str], to_index: str):
        pass

    def _write_to_index(self, index_name: str, embeddings: np.ndarray):
        pass

    def _start_background_reembedding(self, model, corpus_iterator, target_index: str, rate_limit_items_per_sec: int):
        pass

    def _enable_dual_index_routing(self, old_index: str, new_index: str):
        pass

    def _enable_shadow_mode(self, model_id: str):
        pass

@dataclass
class VersionDeployment:
    model_id: str
    status: 'DeploymentStatus'
    traffic_percentage: float

class DeploymentStatus(Enum):
    STAGING = "staging"
    SHADOW = "shadow"
    CANARY = "canary"
    PRODUCTION = "production"
    ROLLBACK_TARGET = "rollback_target"
```

:::{.callout-tip}
## Version Pinning for Reproducibility

For debugging and compliance, support **version pinning** in queries:
```python
# Allow clients to specify model version explicitly
query_embedding = embedding_service.get_embedding(
    query="...",
    model_version="v1.2.3"  # Pin to specific version
)
```

This enables:
- Reproducing historical results for debugging
- A/B testing different model versions
- Gradual migration for sensitive applications
:::

## A/B Testing Embedding Models

Embedding quality is difficult to evaluate offline. **A/B testing** measures real-world impact on business metrics: click-through rate, conversion rate, user satisfaction, revenue. This section covers experimental design for embedding systems at scale.

### Unique Challenges of Embedding A/B Tests

Unlike testing UI changes or ranking algorithms, embedding A/B tests require:
1. **Consistency**: Same user must see results from same model version throughout session
2. **Index versioning**: Maintain separate indices for treatment and control
3. **Longer ramp-up**: New embeddings need time to "stabilize" in caches
4. **Interaction effects**: Embeddings affect multiple surfaces (search, recommendations, related items)

```python
import hashlib
import numpy as np
from typing import Dict, List, Optional
from dataclasses import dataclass
from datetime import datetime, timedelta

@dataclass
class ExperimentConfig:
    """
    Configuration for embedding A/B test

    Key parameters:
    - experiment_id: Unique identifier
    - control_model_id: Baseline model
    - treatment_model_id: New model being tested
    - traffic_allocation: % of users in treatment
    - primary_metric: Main success metric
    - secondary_metrics: Additional metrics to track
    - minimum_sample_size: Min users before stat sig test
    - maximum_duration_days: Auto-conclude after this period
    """
    experiment_id: str
    control_model_id: str
    treatment_model_id: str
    traffic_allocation: float  # 0.0 to 1.0
    primary_metric: str
    secondary_metrics: List[str]
    minimum_sample_size: int
    maximum_duration_days: int
    start_time: datetime

class EmbeddingExperimentFramework:
    """
    Framework for A/B testing embedding models

    Responsibilities:
    - User assignment (control vs. treatment)
    - Consistent routing throughout user session
    - Metric collection and aggregation
    - Statistical significance testing
    - Automatic ramp-up and conclusion

    Example experiment:
    - Control: Current product embeddings (v1.0)
    - Treatment: New contrastive learning model (v2.0)
    - Primary metric: Add-to-cart rate
    - Secondary metrics: Click-through rate, session length, revenue
    - Allocation: 95% control, 5% treatment (initial canary)
    """

    def __init__(self):
        self.active_experiments: Dict[str, ExperimentConfig] = {}
        self.user_assignments: Dict[str, Dict[str, str]] = {}  # user_id → {exp_id → variant}

        # Metric storage (in production: data warehouse)
        self.metrics: Dict[str, List[Dict]] = {}  # exp_id → [metric_events]

    def create_experiment(
        self,
        config: ExperimentConfig
    ) -> str:
        """
        Create new A/B test experiment

        Validations:
        - Control and treatment models exist
        - Metrics are measurable
        - Traffic allocation is valid
        """
        # Validate models
        # (In production: Check model registry)

        # Register experiment
        self.active_experiments[config.experiment_id] = config
        self.metrics[config.experiment_id] = []

        print(f"✓ Created experiment: {config.experiment_id}")
        print(f"  Control: {config.control_model_id}")
        print(f"  Treatment: {config.treatment_model_id}")
        print(f"  Traffic to treatment: {config.traffic_allocation:.1%}")
        print(f"  Primary metric: {config.primary_metric}")

        return config.experiment_id

    def assign_user(
        self,
        user_id: str,
        experiment_id: str
    ) -> str:
        """
        Assign user to control or treatment

        Requirements:
        - Deterministic (same user always gets same variant)
        - Consistent (throughout entire experiment)
        - Randomized (but stable hash, not random seed)

        Implementation:
        - Hash user_id + experiment_id
        - Use hash to determine control vs. treatment
        - Store assignment for consistency
        """
        # Check if already assigned
        if user_id in self.user_assignments:
            if experiment_id in self.user_assignments[user_id]:
                return self.user_assignments[user_id][experiment_id]

        # Assign based on hash
        config = self.active_experiments[experiment_id]
        variant = self._hash_based_assignment(
            user_id,
            experiment_id,
            config.traffic_allocation
        )

        # Store assignment
        if user_id not in self.user_assignments:
            self.user_assignments[user_id] = {}
        self.user_assignments[user_id][experiment_id] = variant

        return variant

    def _hash_based_assignment(
        self,
        user_id: str,
        experiment_id: str,
        treatment_allocation: float
    ) -> str:
        """
        Deterministic assignment using hash function

        Hash(user_id + experiment_id) → [0, 1]
        If hash < treatment_allocation: treatment
        Else: control
        """
        hash_input = f"{user_id}:{experiment_id}".encode('utf-8')
        hash_output = hashlib.md5(hash_input).hexdigest()

        # Convert hex to float in [0, 1]
        hash_value = int(hash_output[:8], 16) / (2**32)

        if hash_value < treatment_allocation:
            return "treatment"
        else:
            return "control"

    def get_model_for_user(
        self,
        user_id: str,
        experiment_id: str
    ) -> str:
        """
        Get appropriate model version for user

        Returns:
            model_id: Either control or treatment model ID
        """
        variant = self.assign_user(user_id, experiment_id)
        config = self.active_experiments[experiment_id]

        if variant == "treatment":
            return config.treatment_model_id
        else:
            return config.control_model_id

    def log_metric(
        self,
        experiment_id: str,
        user_id: str,
        metric_name: str,
        metric_value: float,
        timestamp: Optional[datetime] = None
    ):
        """
        Log metric event for analysis

        Args:
            experiment_id: Experiment this metric belongs to
            user_id: User who generated this metric
            metric_name: Name of metric (e.g., 'click_through_rate')
            metric_value: Value (e.g., 0.0 or 1.0 for binary, revenue for continuous)
            timestamp: When event occurred
        """
        if experiment_id not in self.active_experiments:
            raise ValueError(f"Unknown experiment: {experiment_id}")

        variant = self.user_assignments.get(user_id, {}).get(experiment_id)
        if variant is None:
            # User not assigned yet - assign now
            variant = self.assign_user(user_id, experiment_id)

        metric_event = {
            'user_id': user_id,
            'variant': variant,
            'metric_name': metric_name,
            'metric_value': metric_value,
            'timestamp': timestamp or datetime.now()
        }

        self.metrics[experiment_id].append(metric_event)

    def analyze_experiment(
        self,
        experiment_id: str
    ) -> Dict:
        """
        Analyze experiment results

        Statistical tests:
        - T-test for continuous metrics (revenue, session_length)
        - Chi-square for binary metrics (click_through, conversion)
        - Multiple testing correction (Bonferroni) for secondary metrics

        Returns:
            results: {
                'primary_metric': {lift: X%, p_value: Y, sig: True/False},
                'secondary_metrics': {...},
                'recommendation': 'ship' / 'iterate' / 'abandon'
            }
        """
        if experiment_id not in self.active_experiments:
            raise ValueError(f"Unknown experiment: {experiment_id}")

        config = self.active_experiments[experiment_id]
        events = self.metrics[experiment_id]

        # Separate control and treatment metrics
        control_metrics = [e for e in events if e['variant'] == 'control']
        treatment_metrics = [e for e in events if e['variant'] == 'treatment']

        # Analyze primary metric
        primary_result = self._analyze_metric(
            config.primary_metric,
            control_metrics,
            treatment_metrics
        )

        # Analyze secondary metrics
        secondary_results = {}
        for metric in config.secondary_metrics:
            secondary_results[metric] = self._analyze_metric(
                metric,
                control_metrics,
                treatment_metrics
            )

        # Statistical power and sample size check
        adequate_sample_size = len(set([e['user_id'] for e in events])) >= config.minimum_sample_size

        # Generate recommendation
        recommendation = self._generate_recommendation(
            primary_result,
            secondary_results,
            adequate_sample_size
        )

        results = {
            'experiment_id': experiment_id,
            'primary_metric': primary_result,
            'secondary_metrics': secondary_results,
            'sample_size': {
                'control': len(set([e['user_id'] for e in control_metrics])),
                'treatment': len(set([e['user_id'] for e in treatment_metrics]))
            },
            'adequate_sample_size': adequate_sample_size,
            'recommendation': recommendation
        }

        return results

    def _analyze_metric(
        self,
        metric_name: str,
        control_events: List[Dict],
        treatment_events: List[Dict]
    ) -> Dict:
        """
        Statistical analysis for single metric

        Returns lift, p-value, confidence interval
        """
        # Extract metric values
        control_values = [e['metric_value'] for e in control_events if e['metric_name'] == metric_name]
        treatment_values = [e['metric_value'] for e in treatment_events if e['metric_name'] == metric_name]

        if not control_values or not treatment_values:
            return {
                'control_mean': None,
                'treatment_mean': None,
                'lift': None,
                'p_value': None,
                'significant': False
            }

        # Compute statistics
        control_mean = np.mean(control_values)
        treatment_mean = np.mean(treatment_values)
        lift = (treatment_mean - control_mean) / control_mean if control_mean > 0 else 0

        # T-test for significance (simplified)
        # In production: Use proper statistical test (t-test, chi-square, bootstrap)
        control_std = np.std(control_values)
        treatment_std = np.std(treatment_values)
        pooled_std = np.sqrt((control_std**2 + treatment_std**2) / 2)

        if pooled_std > 0:
            t_stat = (treatment_mean - control_mean) / (pooled_std * np.sqrt(2 / min(len(control_values), len(treatment_values))))
            p_value = 2 * (1 - 0.975) if abs(t_stat) > 1.96 else 0.5  # Simplified
        else:
            p_value = 1.0

        significant = p_value < 0.05

        return {
            'metric_name': metric_name,
            'control_mean': control_mean,
            'treatment_mean': treatment_mean,
            'lift': lift,
            'p_value': p_value,
            'significant': significant
        }

    def _generate_recommendation(
        self,
        primary_result: Dict,
        secondary_results: Dict,
        adequate_sample: bool
    ) -> str:
        """
        Generate recommendation based on results

        Decision tree:
        - Inadequate sample → 'continue'
        - Primary metric positive and significant → 'ship'
        - Primary metric negative and significant → 'abandon'
        - Primary metric neutral, but secondaries positive → 'iterate'
        - All metrics neutral → 'abandon' (not worth the change)
        """
        if not adequate_sample:
            return "continue"

        if primary_result['significant']:
            if primary_result['lift'] > 0.02:  # >2% lift
                return "ship"
            elif primary_result['lift'] < -0.01:  # >1% degradation
                return "abandon"

        # Check secondary metrics for positive signals
        positive_secondaries = sum(
            1 for r in secondary_results.values()
            if r.get('significant') and r.get('lift', 0) > 0
        )

        if positive_secondaries >= 2:
            return "iterate"  # Some positive signals, needs more work

        return "abandon"  # No significant improvement

# Example: A/B test for product recommendation embeddings
def product_recommendation_ab_test():
    """
    A/B test new product embedding model

    Scenario:
    - E-commerce with 10M daily active users
    - Testing new embedding model for product recommendations
    - Primary metric: Add-to-cart rate
    - Secondary metrics: Click-through rate, session length, revenue per session

    Experiment design:
    - 95% control (current model)
    - 5% treatment (new model)
    - Run for 14 days or until statistical significance
    """

    framework = EmbeddingExperimentFramework()

    # Create experiment
    config = ExperimentConfig(
        experiment_id="product_embeddings_v2_test",
        control_model_id="product-embeddings-v1.0.0",
        treatment_model_id="product-embeddings-v2.0.0",
        traffic_allocation=0.05,  # 5% treatment
        primary_metric="add_to_cart_rate",
        secondary_metrics=["click_through_rate", "session_length_minutes", "revenue_per_session"],
        minimum_sample_size=10000,  # 10K users minimum
        maximum_duration_days=14,
        start_time=datetime.now()
    )

    framework.create_experiment(config)

    # Simulate user sessions
    print("\nSimulating user sessions...")

    for user_id in range(1000):
        uid = f"user_{user_id}"

        # Get assigned variant
        model_id = framework.get_model_for_user(uid, config.experiment_id)

        # Simulate user behavior
        # Treatment model is slightly better
        if "v2" in model_id:
            add_to_cart = np.random.random() < 0.12  # 12% conversion (treatment)
            ctr = np.random.random() < 0.25
        else:
            add_to_cart = np.random.random() < 0.10  # 10% conversion (control)
            ctr = np.random.random() < 0.22

        # Log metrics
        framework.log_metric(config.experiment_id, uid, "add_to_cart_rate", float(add_to_cart))
        framework.log_metric(config.experiment_id, uid, "click_through_rate", float(ctr))

    # Analyze results
    print("\nAnalyzing experiment results...")
    results = framework.analyze_experiment(config.experiment_id)

    print(f"\nExperiment: {results['experiment_id']}")
    print(f"Sample size: Control={results['sample_size']['control']}, Treatment={results['sample_size']['treatment']}")
    print(f"\nPrimary Metric ({results['primary_metric']['metric_name']}):")
    print(f"  Control: {results['primary_metric']['control_mean']:.3f}")
    print(f"  Treatment: {results['primary_metric']['treatment_mean']:.3f}")
    print(f"  Lift: {results['primary_metric']['lift']:.2%}")
    print(f"  P-value: {results['primary_metric']['p_value']:.4f}")
    print(f"  Significant: {results['primary_metric']['significant']}")
    print(f"\nRecommendation: {results['recommendation'].upper()}")

# Uncomment to run:
# product_recommendation_ab_test()
```

:::{.callout-tip}
## A/B Test Best Practices

1. **Pre-register hypothesis**: Define success metrics before starting
2. **Power analysis**: Calculate required sample size upfront
3. **Avoid peeking**: Don't conclude early based on interim results (increases false positive rate)
4. **Monitor guardrail metrics**: Latency, error rate, system health
5. **Document everything**: Experiment design, results, learnings for future reference
:::

:::{.callout-warning}
## Simpson's Paradox in Embedding Tests

Embeddings can show different effects across user segments. A model might improve recommendations for new users but degrade for power users. Always segment analysis by key user characteristics (tenure, engagement level, device type) to detect heterogeneous treatment effects.
:::

## Monitoring Embedding Drift and Degradation

Embedding quality degrades over time even without model changes. Data distribution shifts, user behavior evolves, and the corpus grows. **Continuous monitoring** detects degradation before it impacts users, enabling proactive retraining and updates.

### Sources of Embedding Degradation

1. **Data drift**: Input data distribution changes (new product categories, seasonal trends)
2. **Concept drift**: Relationships between entities change (word meanings shift, user preferences evolve)
3. **Corpus growth**: New items dilute existing embeddings (index becomes less representative)
4. **Model staleness**: Fixed model doesn't adapt to new patterns
5. **Infrastructure changes**: Index configuration, hardware, network latency

```python
import numpy as np
from typing import Dict, List, Optional, Tuple
from datetime import datetime, timedelta
from dataclasses import dataclass
import torch
import torch.nn as nn

@dataclass
class EmbeddingQualityMetrics:
    """
    Metrics for monitoring embedding quality

    Intrinsic metrics (computed from embeddings themselves):
    - Average embedding norm
    - Embedding variance
    - Nearest neighbor distance distribution
    - Cluster cohesion

    Extrinsic metrics (task performance):
    - Retrieval recall@k
    - Ranking metrics (NDCG, MRR)
    - Downstream task accuracy (if available)
    - User engagement metrics (CTR, conversion)
    """
    timestamp: datetime

    # Intrinsic metrics
    avg_norm: float
    norm_std: float
    avg_nn_distance: float  # Average distance to nearest neighbor
    embedding_variance: float

    # Extrinsic metrics
    retrieval_recall_at_10: float
    retrieval_recall_at_100: float
    ndcg_at_10: float
    mrr: float  # Mean reciprocal rank

    # System metrics
    inference_latency_p50_ms: float
    inference_latency_p99_ms: float
    index_size_gb: float
    queries_per_second: float

class EmbeddingMonitoringSystem:
    """
    Continuous monitoring system for embedding quality

    Responsibilities:
    - Periodic quality evaluation (hourly/daily)
    - Drift detection (statistical tests on metric distributions)
    - Alerting on quality degradation
    - Automatic retraining triggers
    - Historical metric tracking

    Alert conditions:
    - Recall drops >5% from baseline
    - Latency increases >20% from baseline
    - Embedding distribution shifts significantly (KL divergence)
    - User engagement metrics decline
    """

    def __init__(
        self,
        model_registry,
        test_dataset,
        alert_thresholds: Optional[Dict] = None
    ):
        """
        Args:
            model_registry: Access to embedding models
            test_dataset: Fixed test set for consistent evaluation
            alert_thresholds: Thresholds for triggering alerts
        """
        self.model_registry = model_registry
        self.test_dataset = test_dataset

        # Default alert thresholds
        self.alert_thresholds = alert_thresholds or {
            'recall_at_10_drop': 0.05,  # Alert if recall drops >5%
            'latency_p99_increase': 0.20,  # Alert if p99 latency increases >20%
            'embedding_norm_change': 0.15,  # Alert if avg norm changes >15%
            'queries_per_second_drop': 0.30  # Alert if QPS drops >30%
        }

        # Historical metrics for baseline comparison
        self.historical_metrics: List[EmbeddingQualityMetrics] = []

        # Baseline metrics (from initial deployment)
        self.baseline_metrics: Optional[EmbeddingQualityMetrics] = None

    def evaluate_current_quality(
        self,
        model_id: str,
        sample_size: int = 10000
    ) -> EmbeddingQualityMetrics:
        """
        Evaluate current embedding quality

        Runs comprehensive evaluation:
        - Sample embeddings from index
        - Compute intrinsic metrics
        - Run retrieval evaluation on test set
        - Measure system performance
        """
        print(f"Evaluating embedding quality for {model_id}...")

        # Load model
        model, metadata = self.model_registry.load_model(model_id, 'cuda')
        model.eval()

        # Compute intrinsic metrics
        intrinsic = self._compute_intrinsic_metrics(model, sample_size)

        # Compute extrinsic metrics (retrieval evaluation)
        extrinsic = self._compute_extrinsic_metrics(model)

        # Measure system performance
        system = self._measure_system_performance(model)

        # Combine into quality metrics
        metrics = EmbeddingQualityMetrics(
            timestamp=datetime.now(),
            avg_norm=intrinsic['avg_norm'],
            norm_std=intrinsic['norm_std'],
            avg_nn_distance=intrinsic['avg_nn_distance'],
            embedding_variance=intrinsic['embedding_variance'],
            retrieval_recall_at_10=extrinsic['recall_at_10'],
            retrieval_recall_at_100=extrinsic['recall_at_100'],
            ndcg_at_10=extrinsic['ndcg_at_10'],
            mrr=extrinsic['mrr'],
            inference_latency_p50_ms=system['latency_p50_ms'],
            inference_latency_p99_ms=system['latency_p99_ms'],
            index_size_gb=system['index_size_gb'],
            queries_per_second=system['queries_per_second']
        )

        return metrics

    def _compute_intrinsic_metrics(
        self,
        model: nn.Module,
        sample_size: int
    ) -> Dict:
        """
        Compute intrinsic embedding metrics

        Intrinsic metrics measure properties of embedding space itself:
        - Norm statistics: Are embeddings well-scaled?
        - Variance: Is embedding space well-utilized?
        - Nearest neighbor distances: Are similar items close?
        """
        # Sample random inputs
        sample_data = torch.randn(sample_size, model.encoder[0].in_features).to('cuda')

        with torch.no_grad():
            embeddings = model(sample_data).cpu().numpy()

        # Compute norms
        norms = np.linalg.norm(embeddings, axis=1)
        avg_norm = np.mean(norms)
        norm_std = np.std(norms)

        # Compute variance (measure of space utilization)
        embedding_variance = np.mean(np.var(embeddings, axis=0))

        # Nearest neighbor distances (sample 1000 points)
        nn_distances = []
        sample_indices = np.random.choice(len(embeddings), size=min(1000, len(embeddings)), replace=False)

        for idx in sample_indices[:100]:  # Limit for efficiency
            query_emb = embeddings[idx]
            distances = np.linalg.norm(embeddings - query_emb, axis=1)
            distances = np.sort(distances)
            nn_distances.append(distances[1])  # Distance to nearest neighbor (exclude self)

        avg_nn_distance = np.mean(nn_distances)

        return {
            'avg_norm': float(avg_norm),
            'norm_std': float(norm_std),
            'embedding_variance': float(embedding_variance),
            'avg_nn_distance': float(avg_nn_distance)
        }

    def _compute_extrinsic_metrics(self, model: nn.Module) -> Dict:
        """
        Compute extrinsic metrics (task performance)

        Extrinsic metrics measure how well embeddings perform on real tasks:
        - Retrieval: Given query, find relevant documents
        - Ranking: Order results by relevance
        - Classification: If embeddings used for downstream task
        """
        # Run retrieval evaluation on test set
        # (Simplified - real implementation uses full evaluation framework)

        recall_at_10 = 0.89  # Placeholder
        recall_at_100 = 0.95  # Placeholder
        ndcg_at_10 = 0.85  # Placeholder
        mrr = 0.78  # Placeholder

        return {
            'recall_at_10': recall_at_10,
            'recall_at_100': recall_at_100,
            'ndcg_at_10': ndcg_at_10,
            'mrr': mrr
        }

    def _measure_system_performance(self, model: nn.Module) -> Dict:
        """
        Measure system performance metrics

        System metrics track operational health:
        - Latency: How fast are embeddings generated?
        - Throughput: How many queries per second?
        - Resource usage: Index size, memory, compute
        """
        # Run latency benchmark
        num_queries = 1000
        latencies_ms = []

        sample_data = torch.randn(1, model.encoder[0].in_features).to('cuda')

        for _ in range(num_queries):
            start = datetime.now()
            with torch.no_grad():
                _ = model(sample_data)
            latency = (datetime.now() - start).total_seconds() * 1000
            latencies_ms.append(latency)

        latency_p50 = np.percentile(latencies_ms, 50)
        latency_p99 = np.percentile(latencies_ms, 99)

        # Measure throughput
        queries_per_second = 1000 / np.mean(latencies_ms) if np.mean(latencies_ms) > 0 else 0

        # Estimate index size (simplified)
        index_size_gb = 10.5  # Placeholder

        return {
            'latency_p50_ms': latency_p50,
            'latency_p99_ms': latency_p99,
            'queries_per_second': queries_per_second,
            'index_size_gb': index_size_gb
        }

    def detect_drift(
        self,
        current_metrics: EmbeddingQualityMetrics
    ) -> Dict[str, bool]:
        """
        Detect drift in embedding quality

        Compares current metrics to:
        - Baseline (from initial deployment)
        - Recent history (rolling window)

        Returns alerts for each metric that exceeded threshold
        """
        if self.baseline_metrics is None:
            # First evaluation - set as baseline
            self.baseline_metrics = current_metrics
            print("✓ Baseline metrics established")
            return {}

        alerts = {}

        # Check recall drift
        recall_drop = (
            self.baseline_metrics.retrieval_recall_at_10 -
            current_metrics.retrieval_recall_at_10
        ) / self.baseline_metrics.retrieval_recall_at_10

        if recall_drop > self.alert_thresholds['recall_at_10_drop']:
            alerts['recall_degradation'] = True
            print(f"⚠️  ALERT: Recall dropped {recall_drop:.1%} from baseline")

        # Check latency drift
        latency_increase = (
            current_metrics.inference_latency_p99_ms -
            self.baseline_metrics.inference_latency_p99_ms
        ) / self.baseline_metrics.inference_latency_p99_ms

        if latency_increase > self.alert_thresholds['latency_p99_increase']:
            alerts['latency_increase'] = True
            print(f"⚠️  ALERT: P99 latency increased {latency_increase:.1%}")

        # Check embedding distribution drift
        norm_change = abs(
            current_metrics.avg_norm - self.baseline_metrics.avg_norm
        ) / self.baseline_metrics.avg_norm

        if norm_change > self.alert_thresholds['embedding_norm_change']:
            alerts['embedding_distribution_shift'] = True
            print(f"⚠️  ALERT: Embedding norm changed {norm_change:.1%}")

        # Check throughput degradation
        qps_drop = (
            self.baseline_metrics.queries_per_second -
            current_metrics.queries_per_second
        ) / self.baseline_metrics.queries_per_second

        if qps_drop > self.alert_thresholds['queries_per_second_drop']:
            alerts['throughput_degradation'] = True
            print(f"⚠️  ALERT: Throughput dropped {qps_drop:.1%}")

        if not alerts:
            print("✓ No drift detected - quality stable")

        return alerts

    def should_retrain(
        self,
        alerts: Dict[str, bool],
        days_since_training: int
    ) -> Tuple[bool, str]:
        """
        Decide whether to trigger model retraining

        Retraining triggers:
        - Quality degradation alerts
        - Staleness (>30 days since last training)
        - Significant corpus growth (>20% new items)

        Returns:
            (should_retrain, reason)
        """
        # Quality-based trigger
        critical_alerts = [
            'recall_degradation',
            'embedding_distribution_shift'
        ]

        if any(alerts.get(alert, False) for alert in critical_alerts):
            return True, "quality_degradation"

        # Time-based trigger
        if days_since_training > 30:
            return True, "model_staleness"

        # No retraining needed
        return False, ""

    def continuous_monitoring_loop(
        self,
        model_id: str,
        check_interval_hours: int = 24
    ):
        """
        Continuous monitoring loop (runs as background service)

        Args:
            model_id: Model to monitor
            check_interval_hours: How often to evaluate
        """
        print(f"Starting continuous monitoring for {model_id}")
        print(f"Check interval: every {check_interval_hours} hours")

        while True:
            # Evaluate quality
            current_metrics = self.evaluate_current_quality(model_id)

            # Store in history
            self.historical_metrics.append(current_metrics)

            # Detect drift
            alerts = self.detect_drift(current_metrics)

            # Check retraining trigger
            days_since_training = 15  # Placeholder (get from model metadata)
            should_retrain, reason = self.should_retrain(alerts, days_since_training)

            if should_retrain:
                print(f"\n🔄 Triggering model retraining: {reason}")
                # In production: Trigger retraining pipeline
                break

            # Wait for next check
            print(f"\nNext check in {check_interval_hours} hours...")
            # In production: time.sleep(check_interval_hours * 3600)
            break  # For example purposes

# Example: Monitor production embeddings
def production_monitoring_example():
    """
    Monitor product embeddings in production

    Checks:
    - Daily quality evaluation
    - Drift detection
    - Automatic retraining trigger
    """
    # Initialize monitoring system
    registry = EmbeddingModelRegistry()
    test_dataset = None  # In production: load test set

    monitor = EmbeddingMonitoringSystem(
        model_registry=registry,
        test_dataset=test_dataset
    )

    # Day 1: Initial deployment
    print("=== Day 1: Initial Deployment ===")
    model_id = "product-embeddings-v1.0.0"
    metrics_day1 = monitor.evaluate_current_quality(model_id)
    alerts_day1 = monitor.detect_drift(metrics_day1)

    # Day 30: Quality check
    print("\n=== Day 30: Regular Quality Check ===")
    metrics_day30 = monitor.evaluate_current_quality(model_id)

    # Simulate quality degradation
    metrics_day30.retrieval_recall_at_10 = 0.82  # Degraded from 0.89

    alerts_day30 = monitor.detect_drift(metrics_day30)

    # Check retraining trigger
    should_retrain, reason = monitor.should_retrain(alerts_day30, days_since_training=30)

    if should_retrain:
        print(f"\n✓ Retraining triggered: {reason}")
    else:
        print("\n✓ No retraining needed")

# Uncomment to run:
# production_monitoring_example()
```

:::{.callout-tip}
## Monitoring Dashboard Essentials

A production embedding monitoring dashboard should display:

**Real-time metrics** (updated every minute):
- Query latency (p50, p95, p99)
- Throughput (queries/second)
- Error rate
- Cache hit rate

**Quality metrics** (updated hourly/daily):
- Retrieval recall@10, recall@100
- NDCG@10
- User engagement metrics (CTR, conversion rate)
- Embedding distribution statistics

**System health** (updated every 5 minutes):
- Index size and growth rate
- Memory usage
- GPU utilization
- Background job status (retraining, re-embedding)
:::

:::{.callout-warning}
## Silent Degradation
Embedding quality can degrade gradually without triggering alerts. Complement threshold-based alerts with:
- **Trend analysis**: Detect slow downward trends even within thresholds
- **Comparative baselines**: Compare against historical best, not just initial baseline
- **Canary queries**: Maintain set of "golden queries" that should always perform well
:::

## Key Takeaways

- **MLOps for embeddings requires specialized infrastructure**: Model registries, batch inference pipelines, and version coordination across training, serving, and indexing stages differentiate embedding systems from traditional ML deployments

- **Hybrid batch/real-time strategies optimize cost and freshness**: Batch processing for stable entities (products, documents), real-time generation for dynamic content (queries, sessions), and caching for popular items balances throughput, latency, and resource utilization at scale

- **Embedding versioning is complex due to incompatibility between model versions**: Blue-green, incremental, shadow, and canary deployment strategies each offer different trade-offs between safety, speed, and resource requirements when updating embedding models

- **A/B testing measures real-world embedding impact**: Hash-based user assignment, consistent routing, separate indices per variant, and statistical analysis of business metrics (CTR, conversion, revenue) validate embedding improvements beyond offline metrics

- **Continuous monitoring detects degradation before user impact**: Track intrinsic metrics (embedding norms, variance, nearest neighbor distances), extrinsic metrics (recall, NDCG, MRR), and system metrics (latency, throughput) with drift detection and automatic retraining triggers

- **Production embedding systems require operational maturity**: Rollback plans, version pinning for reproducibility, graceful degradation, alerting on quality and performance regressions, and documentation of all experiments and deployments

- **Scale demands automation**: Manual embedding pipeline management breaks down at trillion-row scale; invest in automated quality monitoring, deployment orchestration, and retraining workflows early

## Looking Ahead

This chapter covered the operational practices for deploying and maintaining embedding systems in production. Chapter 10 shifts focus to the computational challenges of training embedding models at scale, exploring distributed training architectures, gradient accumulation and mixed precision techniques, memory optimization strategies, and multi-GPU/multi-node training approaches that enable learning from trillion-row datasets.

## Further Reading

### MLOps and Model Management
- Sculley et al. (2015). "Hidden Technical Debt in Machine Learning Systems." NeurIPS.
- Renggli et al. (2021). "A Data Quality-Driven View of MLOps." IEEE Data Engineering Bulletin.
- Paleyes et al. (2022). "Challenges in Deploying Machine Learning: A Survey of Case Studies." ACM Computing Surveys.

### Deployment Strategies
- Kubernetes Documentation. "Blue-Green Deployments and Canary Releases."
- Richardson, C. (2018). "Microservices Patterns: With Examples in Java." Manning Publications.
- Humble & Farley (2010). "Continuous Delivery: Reliable Software Releases through Build, Test, and Deployment Automation." Addison-Wesley.

### A/B Testing
- Kohavi & Longbotham (2017). "Online Controlled Experiments and A/B Testing." Encyclopedia of Machine Learning and Data Mining.
- Deng et al. (2013). "Improving the Sensitivity of Online Controlled Experiments by Utilizing Pre-Experiment Data." WSDM.
- Gupta et al. (2019). "Top Challenges from the First Practical Online Controlled Experiments Summit." SIGKDD.

### Monitoring and Observability
- Schelter et al. (2018). "Automating Large-Scale Data Quality Verification." VLDB.
- Polyzotis et al. (2018). "Data Lifecycle Challenges in Production Machine Learning." SIGMOD.
- Breck et al. (2019). "Data Validation for Machine Learning." MLSys.

### Embedding-Specific Operations
- Grbovic & Cheng (2018). "Real-time Personalization using Embeddings for Search Ranking at Airbnb." KDD.
- Haldar et al. (2019). "Applying Deep Learning To Airbnb Search." KDD.
- Bernhardsson, E. (2015). "Nearest Neighbors and Vector Models." Erik Bernhardsson Blog.
