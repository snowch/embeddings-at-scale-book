# Automated Decision Systems {#sec-automated-decision-systems}

:::{.callout-note}
## Chapter Overview
Automated decision systems drive critical business operations from pricing to lending to maintenance scheduling, traditionally relying on rigid rule-based logic or simple statistical models. This chapter transforms these systems with embeddings: embedding-driven business rules that replace hand-crafted if-then logic with learned patterns from historical decisions, dynamic pricing systems that optimize prices based on product and customer embeddings capturing market positioning and willingness-to-pay, supply chain optimization using facility and route embeddings to minimize costs while meeting service levels, risk scoring and underwriting with entity embeddings that encode creditworthiness and insurance risk from multi-modal data, and predictive maintenance systems using equipment embeddings to forecast failures and schedule interventions before breakdowns. These techniques transform brittle rule engines into adaptive decision systems that learn from outcomes and handle novel scenarios.
:::

After detecting anomalies and threats (@sec-anomaly-detection-security), embeddings enable **automated decision systems** that go beyond detection to take action—approving loans, setting prices, scheduling maintenance, routing shipments. Traditional decision systems use rule-based engines (if credit_score > 700 and income > $50K, approve) or simple models (logistic regression on 10-20 features). **Embedding-based decision systems** represent entities (customers, products, facilities) as vectors encoding rich context, enabling decisions that consider hundreds of implicit factors, transfer learning across domains, and continuous improvement as new data arrives.

## Embedding-Driven Business Rules

Business rules encode domain knowledge: credit policies, pricing strategies, underwriting guidelines. **Embedding-driven business rules** replace rigid if-then logic with learned decision boundaries in embedding space, adapting to patterns that humans can't articulate and updating as business conditions change.

### The Business Rules Challenge

Traditional business rules face limitations:

- **Brittleness**: Rules hardcode thresholds (credit score > 700) that don't generalize
- **Maintenance burden**: Hundreds of rules accumulate, interact unpredictably
- **Cold start**: No rules exist for new products, markets, situations
- **Suboptimality**: Rules encode human intuition, miss non-linear patterns

**Embedding approach**: Learn entity embeddings (customers, products, transactions) and decision boundaries from historical outcomes. New decisions query: "find similar past cases, what happened?" See @sec-custom-embedding-strategies for guidance on building these embeddings, and @sec-siamese-networks for similarity-based learning approaches.

```{python}
#| echo: true
#| code-fold: true
#| code-summary: "Show Case-Based Reasoning System"
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from dataclasses import dataclass
from typing import List, Tuple, Optional


@dataclass
class BusinessCase:
    """Historical business decision case."""
    case_id: str
    entity_id: str
    context: dict
    decision: any
    outcome: Optional[any] = None
    embedding: Optional[np.ndarray] = None


class EntityEncoder(nn.Module):
    """Encode entities for decision making."""
    def __init__(self, embedding_dim: int = 128,
                 num_categorical: int = 10, num_numerical: int = 20):
        super().__init__()
        self.embedding_dim = embedding_dim
        self.categorical_embeddings = nn.ModuleList(
            [nn.Embedding(1000, 16) for _ in range(num_categorical)]
        )
        self.numerical_encoder = nn.Sequential(
            nn.Linear(num_numerical, 64),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(64, 64)
        )
        feature_dim = num_categorical * 16 + 64
        self.feature_encoder = nn.Sequential(
            nn.Linear(feature_dim, 256),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(256, embedding_dim)
        )

    def forward(self, categorical_features, numerical_features):
        """Encode entities to embeddings."""
        cat_embs = [emb_layer(categorical_features[:, i])
                    for i, emb_layer in enumerate(self.categorical_embeddings)]
        cat_emb = torch.cat(cat_embs, dim=1)
        num_emb = self.numerical_encoder(numerical_features)
        combined = torch.cat([cat_emb, num_emb], dim=1)
        entity_emb = self.feature_encoder(combined)
        return F.normalize(entity_emb, p=2, dim=1)

# Usage example
encoder = EntityEncoder(embedding_dim=128, num_categorical=10, num_numerical=20)
cat_features = torch.randint(0, 100, (1, 10))
num_features = torch.randn(1, 20)
embedding = encoder(cat_features, num_features)
print(f"Entity embedding shape: {embedding.shape}")
```

:::{.callout-tip}
## Embedding-Driven Business Rules Best Practices

**Architecture:**

- **Entity encoders**: Learn embeddings that predict outcomes
- **Case-based reasoning**: Retrieve similar historical cases
- **Hybrid systems**: Combine learned patterns + explicit rules
- **Explainability**: Surface similar cases that influenced decision

**Training:**

- **Metric learning**: Entities with similar outcomes close in embedding space (see @sec-siamese-networks)
- **Multi-task**: Predict multiple outcomes jointly (default, LTV, churn)
- **Temporal**: Weight recent cases higher (concept drift)
- **Fairness**: Constrain to prevent disparate impact

**Production:**

- **Low latency**: <100ms for real-time decisions (credit cards, pricing)
- **Confidence thresholds**: Route low-confidence to humans
- **Rule compliance**: Hard constraints for regulations
- **Monitoring**: Track decision quality, fairness metrics
- **Feedback loops**: Continuously add outcomes to case database

**Challenges:**

- **Cold start**: No historical cases for new scenarios
- **Distribution shift**: Decisions change underlying distribution
- **Adversarial**: Bad actors game the system
- **Fairness**: Embeddings can encode bias from historical data
:::

## Dynamic Pricing with Embeddings

Pricing is complex: consider product attributes, customer willingness-to-pay, competitive positioning, inventory levels, time-of-day demand. **Embedding-based dynamic pricing** represents products and customers as vectors, enabling price optimization that considers hundreds of implicit factors.

### The Dynamic Pricing Challenge

Traditional pricing approaches:

- **Cost-plus**: Price = cost × markup (ignores demand)
- **Competitive**: Match competitor prices (race to bottom)
- **Segmented**: Fixed tiers (doesn't capture individual WTP)
- **Regression**: Linear models (misses non-linear patterns)

**Embedding approach**: Learn product embeddings (quality, brand, features) and customer embeddings (purchase history, preferences). Price = f(product_emb, customer_emb, context). See @sec-custom-embedding-strategies for approaches to building these embeddings.

```{python}
#| echo: true
#| code-fold: true
#| code-summary: "Show Dynamic Pricing Engine"
import torch
import torch.nn as nn
import numpy as np
from dataclasses import dataclass
from typing import Dict, Tuple


@dataclass
class Product:
    """Product with pricing attributes."""
    product_id: str
    category: str
    brand: str
    cost: float
    base_price: float
    embedding: np.ndarray = None


class DemandModel(nn.Module):
    """Predict purchase probability as function of price."""
    def __init__(self, embedding_dim: int = 128, context_dim: int = 10):
        super().__init__()
        input_dim = embedding_dim * 2 + 1 + context_dim
        self.demand_predictor = nn.Sequential(
            nn.Linear(input_dim, 256),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(128, 1),
            nn.Sigmoid()
        )

    def forward(self, product_emb, customer_emb, price, context):
        """Predict purchase probability."""
        combined = torch.cat([product_emb, customer_emb, price, context], dim=1)
        purchase_prob = self.demand_predictor(combined)
        return purchase_prob


class DynamicPricingEngine:
    """Dynamic pricing using embeddings."""
    def __init__(self, demand_model, min_margin: float = 0.2):
        self.demand_model = demand_model
        self.min_margin = min_margin

    def optimize_price(self, product_emb, customer_emb, cost: float,
                       base_price: float, num_price_points: int = 20) -> Tuple[float, float]:
        """Optimize price for product-customer pair."""
        min_price = cost * (1 + self.min_margin)
        max_price = base_price * 1.2
        prices = np.linspace(min_price, max_price, num_price_points)

        best_price = None
        best_profit = -float('inf')

        with torch.no_grad():
            for price in prices:
                price_t = torch.tensor([[price]]).float()
                context_t = torch.zeros(1, 10).float()
                purchase_prob = self.demand_model(
                    product_emb, customer_emb, price_t, context_t
                ).item()

                expected_profit = purchase_prob * (price - cost)
                if expected_profit > best_profit:
                    best_profit = expected_profit
                    best_price = price

        return best_price, best_profit

# Usage example
demand_model = DemandModel(embedding_dim=128)
pricing_engine = DynamicPricingEngine(demand_model, min_margin=0.2)

product_emb = torch.randn(1, 128)
customer_emb = torch.randn(1, 128)
optimal_price, expected_profit = pricing_engine.optimize_price(
    product_emb, customer_emb, cost=50.0, base_price=100.0
)
print(f"Optimal price: ${optimal_price:.2f}, Expected profit: ${expected_profit:.2f}")
```

:::{.callout-tip}
## Dynamic Pricing Best Practices

**Demand modeling:**

- **Price elasticity**: Encode in customer embedding (price sensitivity)
- **Competitive response**: Monitor competitor prices, adjust accordingly
- **Temporal patterns**: Time-of-day, day-of-week, seasonality
- **Inventory pressure**: Increase discount as stock ages

**Optimization:**

- **Expected profit**: price × P(purchase | price) × (price - cost)
- **Multi-objective**: Balance revenue, margin, market share
- **Constraints**: Minimum margin, maximum discount, price stability
- **A/B testing**: Randomized experiments to measure elasticity

**Production:**

- **Real-time**: Recompute prices as conditions change (hourly/daily)
- **Personalization**: Different prices for different customer segments
- **Fairness**: Avoid discriminatory pricing (same price for same features)
- **Transparency**: Explain price changes to customers when asked

**Challenges:**

- **Strategic behavior**: Customers learn to wait for discounts
- **Fairness**: Personalized pricing can seem unfair
- **Complexity**: Many factors interact non-linearly
- **Adverse selection**: Low prices attract low-value customers
:::

## Supply Chain Optimization

Supply chain optimization minimizes costs while meeting service levels: which suppliers, which routes, which warehouses. **Embedding-based supply chain optimization** represents facilities, routes, and products as vectors, enabling similarity-based decision-making at scale.

### The Supply Chain Challenge

Traditional approaches:

- **Linear programming**: Optimal but requires perfect models, doesn't scale
- **Heuristics**: Rules-of-thumb (use closest warehouse), suboptimal
- **Simulation**: Slow, can't handle real-time decisions

**Embedding approach**: Learn embeddings of facilities (warehouses, suppliers), products, and routes. Optimize decisions via learned representations that capture complex cost structures. See @sec-custom-embedding-strategies for the decision framework on building domain-specific embeddings.

```{python}
#| echo: true
#| code-fold: true
#| code-summary: "Show Supply Chain Optimizer"
import torch
import torch.nn as nn
import numpy as np
from dataclasses import dataclass
from typing import List, Tuple


@dataclass
class Facility:
    """Warehouse or distribution center."""
    facility_id: str
    location: Tuple[float, float]
    capacity: float
    cost_structure: dict
    embedding: np.ndarray = None


class RouteCostModel(nn.Module):
    """Predict cost of fulfilling order from facility."""
    def __init__(self, embedding_dim: int = 128):
        super().__init__()
        input_dim = embedding_dim * 2 + 10
        self.cost_predictor = nn.Sequential(
            nn.Linear(input_dim, 256),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Linear(128, 1)
        )

    def forward(self, facility_emb, order_emb, context):
        """Predict fulfillment cost."""
        combined = torch.cat([facility_emb, order_emb, context], dim=1)
        cost = self.cost_predictor(combined)
        return cost


class SupplyChainOptimizer:
    """Optimize supply chain decisions using embeddings."""
    def __init__(self, cost_model):
        self.cost_model = cost_model

    def select_facility(self, order_emb, facilities: List[Facility]) -> Tuple[str, float]:
        """Select which facility should fulfill order."""
        best_facility = None
        best_cost = float('inf')

        with torch.no_grad():
            for facility in facilities:
                facility_emb = torch.from_numpy(facility.embedding).unsqueeze(0).float()
                context = torch.zeros(1, 10).float()  # Distance, urgency, etc.

                cost = self.cost_model(facility_emb, order_emb, context).item()

                if cost < best_cost:
                    best_cost = cost
                    best_facility = facility.facility_id

        return best_facility, best_cost

# Usage example
cost_model = RouteCostModel(embedding_dim=128)
optimizer = SupplyChainOptimizer(cost_model)

# Create facilities
facilities = [
    Facility('warehouse_1', (37.7, -122.4), 10000, {}, np.random.randn(128)),
    Facility('warehouse_2', (40.7, -74.0), 15000, {}, np.random.randn(128))
]

order_emb = torch.randn(1, 128)
selected_facility, cost = optimizer.select_facility(order_emb, facilities)
print(f"Selected facility: {selected_facility}, Cost: ${cost:.2f}")
```

:::{.callout-tip}
## Supply Chain Optimization Best Practices

**Embeddings:**

- **Facility embeddings**: Location, capacity, costs, performance history
- **Product embeddings**: Size, weight, fragility, demand patterns
- **Route embeddings**: Distance, reliability, transit time, cost
- **Graph embeddings**: Supply chain as graph, learn relationships

**Optimization:**

- **Multi-objective**: Minimize cost, meet service levels, balance inventory
- **Constraints**: Capacity, inventory, service level agreements
- **Uncertainty**: Demand variability, transit delays, supply disruptions
- **Temporal**: Seasonal patterns, day-of-week effects

**Production:**

- **Real-time**: Optimize as orders arrive (milliseconds to seconds)
- **Batch**: Optimize daily/weekly for inventory allocation
- **Simulation**: Test policies on historical data
- **Continuous learning**: Update embeddings as costs/patterns change

**Challenges:**

- **Complexity**: Many interacting factors (costs, capacity, demand)
- **Uncertainty**: Demand forecasting errors, supply disruptions
- **Strategic behavior**: Bullwhip effect (amplification up supply chain)
- **Cold start**: New facilities, products, routes lack training data
:::

## Risk Scoring and Underwriting

Risk scoring quantifies default probability, fraud risk, insurance claims. **Embedding-based risk scoring** represents entities (customers, transactions, properties) as vectors, enabling risk assessment that considers rich context.

### The Risk Scoring Challenge

Traditional approaches:

- **Credit scores**: Linear models on 10-20 features (income, debt, delinquencies)
- **Actuarial tables**: Tabular risk by category (age, location)
- **Rule-based**: Hard thresholds (if bankruptcy, reject)

**Embedding approach**: Learn embeddings capturing risk from multi-modal data (text, images, graphs), enabling non-linear risk assessment that adapts to new data. See @sec-custom-embedding-strategies for guidance on choosing the right level of customization for your domain.

```{python}
#| echo: true
#| code-fold: true
#| code-summary: "Show Risk Scoring Model"
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np


class RiskEncoder(nn.Module):
    """Encode entities for risk assessment."""
    def __init__(self, embedding_dim: int = 128):
        super().__init__()
        self.encoder = nn.Sequential(
            nn.Linear(50, 256),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(128, embedding_dim)
        )

    def forward(self, features):
        """Encode entity features for risk assessment."""
        risk_emb = self.encoder(features)
        return F.normalize(risk_emb, p=2, dim=1)


class RiskScoringModel(nn.Module):
    """Predict risk score from embedding."""
    def __init__(self, embedding_dim: int = 128):
        super().__init__()
        self.risk_head = nn.Sequential(
            nn.Linear(embedding_dim, 128),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(64, 1),
            nn.Sigmoid()
        )

    def forward(self, embeddings):
        """Predict risk scores (probability of adverse event)."""
        risk_scores = self.risk_head(embeddings)
        return risk_scores

# Usage example
risk_encoder = RiskEncoder(embedding_dim=128)
risk_model = RiskScoringModel(embedding_dim=128)

# Credit application features
features = torch.randn(1, 50)  # Income, debt, credit history, etc.

# Encode and score
embedding = risk_encoder(features)
risk_score = risk_model(embedding)

print(f"Risk score (probability of default): {risk_score.item():.3f}")
print(f"Decision: {'APPROVE' if risk_score.item() < 0.15 else 'REJECT'}")
```

:::{.callout-tip}
## Risk Scoring Best Practices

**Features:**

- **Traditional**: Credit score, income, debt, delinquencies
- **Alternative**: Rent payments, utility bills, education, employment
- **Behavioral**: Transaction patterns, app usage, social connections
- **External**: Economic indicators, industry trends, local market

**Modeling:**

- **Multi-task**: Predict multiple risk types (default, prepayment, fraud)
- **Survival analysis**: Time-to-event modeling (months until default)
- **Calibration**: Ensure predicted probabilities match empirical rates
- **Fairness**: Monitor for disparate impact on protected classes

**Production:**

- **Explainability**: SHAP values, feature importance for lending decisions
- **Monitoring**: Track actual default rates vs predictions
- **Adverse action**: Provide reasons for credit denials (FCRA compliance)
- **Continuous learning**: Retrain as outcomes observed

**Challenges:**

- **Class imbalance**: Defaults are rare (1-5% of loans)
- **Long feedback loops**: Takes months/years to observe outcomes
- **Fairness**: Alternative data may encode demographic proxies
- **Regulation**: Lending laws constrain features and require explainability
:::

## Predictive Maintenance

Predictive maintenance forecasts equipment failures before they occur, enabling proactive interventions. **Embedding-based predictive maintenance** represents equipment state as vectors from sensor data, images, and maintenance history.

### The Predictive Maintenance Challenge

Traditional approaches:

- **Reactive**: Fix after failure (downtime, safety hazards)
- **Preventive**: Fix on schedule (unnecessary maintenance, still miss failures)
- **Threshold-based**: Alert if sensor > threshold (brittle, misses complex patterns)

**Embedding approach**: Learn equipment embeddings from multi-modal data (sensor streams, images, audio). Predict failure by detecting drift from normal operation patterns. See @sec-custom-embedding-strategies for approaches to building these embeddings, and @sec-contrastive-learning for training techniques.

```{python}
#| echo: true
#| code-fold: true
#| code-summary: "Show Predictive Maintenance System"
import torch
import torch.nn as nn
import torch.nn.functional as F


class EquipmentEncoder(nn.Module):
    """Encode equipment state from sensor time series."""
    def __init__(self, embedding_dim: int = 128, num_sensors: int = 10):
        super().__init__()
        # LSTM for sensor time series
        self.sensor_encoder = nn.LSTM(
            input_size=num_sensors,
            hidden_size=64,
            num_layers=2,
            batch_first=True,
            dropout=0.2
        )
        # Fusion layer
        self.fusion = nn.Sequential(
            nn.Linear(64, 128),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(128, embedding_dim)
        )

    def forward(self, sensor_data):
        """Encode equipment state from sensor readings."""
        _, (hidden, _) = self.sensor_encoder(sensor_data)
        sensor_emb = hidden[-1]
        equipment_emb = self.fusion(sensor_emb)
        return F.normalize(equipment_emb, p=2, dim=1)


class FailurePredictionModel(nn.Module):
    """Predict equipment failure from embedding."""
    def __init__(self, embedding_dim: int = 128):
        super().__init__()
        # Failure probability head
        self.failure_prob_head = nn.Sequential(
            nn.Linear(embedding_dim, 128),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(128, 1),
            nn.Sigmoid()
        )
        # Time to failure head
        self.time_to_failure_head = nn.Sequential(
            nn.Linear(embedding_dim, 128),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(128, 1),
            nn.ReLU()
        )

    def forward(self, embeddings):
        """Predict failure probability and time to failure."""
        failure_prob = self.failure_prob_head(embeddings)
        time_to_failure = self.time_to_failure_head(embeddings)
        return failure_prob, time_to_failure

# Usage example
equipment_encoder = EquipmentEncoder(embedding_dim=128, num_sensors=10)
failure_model = FailurePredictionModel(embedding_dim=128)

# Sensor time series (100 timesteps, 10 sensors)
sensor_data = torch.randn(1, 100, 10)

# Encode equipment state
equipment_emb = equipment_encoder(sensor_data)

# Predict failure
failure_prob, time_to_failure = failure_model(equipment_emb)

print(f"Failure probability (next 100 hours): {failure_prob.item():.1%}")
print(f"Estimated time to failure: {time_to_failure.item():.0f} hours")

if failure_prob.item() > 0.8:
    print("→ URGENT: Schedule maintenance immediately")
elif failure_prob.item() > 0.3:
    print("→ MONITOR: Schedule maintenance soon")
else:
    print("→ NORMAL: Continue operation")
```

:::{.callout-tip}
## Predictive Maintenance Best Practices

**Data sources:**

- **Sensors**: Temperature, vibration, pressure, current, acoustics
- **Images**: Visual inspection photos, thermal imaging
- **Audio**: Equipment sounds (bearing noise, motor hum)
- **Maintenance history**: Past failures, repairs, part replacements
- **Operating conditions**: Load, speed, environment

**Modeling:**

- **LSTM**: Sequential sensor data modeling
- **Autoencoder**: Anomaly detection via reconstruction error
- **Survival analysis**: Time-to-failure prediction
- **Multi-modal fusion**: Combine sensors + images + audio
- **Transfer learning**: Pre-train on similar equipment (see @sec-custom-embedding-strategies)

**Production:**

- **Real-time monitoring**: Continuous sensor data processing
- **Alerting**: Notify when failure probability exceeds threshold
- **Scheduling**: Integrate with maintenance management system
- **Feedback loops**: Update model with observed failures
- **Edge deployment**: On-device inference for low latency

**Challenges:**

- **Rare failures**: Limited failure examples for training
- **Heterogeneous equipment**: Many equipment types, hard to generalize
- **Sensor drift**: Sensors degrade over time, need calibration
- **Operating conditions**: Equipment behavior varies with load, environment
- **Cost-benefit**: Balance maintenance cost vs failure cost
:::

## Key Takeaways

- **Embedding-driven business rules replace brittle if-then logic with learned decision boundaries**: Case-based reasoning retrieves similar historical cases and applies their outcomes, adapting automatically as new cases arrive without retraining, while hybrid systems enforce hard regulatory constraints alongside learned patterns

- **Dynamic pricing optimizes revenue by personalizing prices based on customer and product embeddings**: Demand models predict purchase probability as a function of price, enabling expected profit optimization that balances revenue, margin, and market share while respecting minimum margins and price stability constraints

- **Supply chain optimization benefits from facility and route embeddings that encode complex cost structures**: Warehouse selection considers shipping costs, labor costs, inventory pressure, and capacity utilization simultaneously through learned representations, often identifying counterintuitive solutions like using farther facilities with better logistics contracts

- **Embedding-based risk scoring captures non-linear patterns traditional models miss**: Multi-modal embeddings from credit history, transaction patterns, employment data, and alternative data sources (rent, utilities) enable more accurate risk assessment while providing SHAP-based explanations for regulatory compliance and adverse action requirements

- **Predictive maintenance with equipment embeddings detects failures before they occur**: LSTM encoders over sensor time series learn normal operation patterns, with drift from baseline predicting impending failures, enabling proactive scheduling that reduces downtime by 95% and unnecessary maintenance by 60%

- **Explainability is critical for adoption of automated decision systems**: While embeddings enable better decisions, stakeholders require explanations—similar case retrieval for case-based reasoning, SHAP values for model-based decisions, and feature importance for risk scoring satisfy regulatory and business requirements

- **Continuous learning and monitoring are essential for production decision systems**: Decisions change underlying distributions (approved customers behave differently than rejected), attackers adapt to fraud detection, equipment degrades—systems must incorporate feedback, retrain periodically, and monitor for concept drift and fairness violations

## Looking Ahead

Part V (Industry Applications) begins with @sec-financial-services, which applies embeddings to financial services disruption: trading signal generation using embeddings of securities and market conditions to identify opportunities, credit risk assessment with entity embeddings for underwriting decisions, regulatory compliance automation through document embeddings for policy monitoring, customer behavior analysis via embedding-based segmentation, and market sentiment analysis extracting trading signals from news and social media embeddings.

## Further Reading

### Automated Decision Systems
- Brynjolfsson, Erik, and Andrew McAfee (2017). "The Business of Artificial Intelligence." Harvard Business Review.
- Kleinberg, Jon, et al. (2018). "Human Decisions and Machine Predictions." Quarterly Journal of Economics.
- Mullainathan, Sendhil, and Jann Spiess (2017). "Machine Learning: An Applied Econometric Approach." Journal of Economic Perspectives.
- Athey, Susan, and Guido Imbens (2019). "Machine Learning Methods That Economists Should Know About." Annual Review of Economics.

### Dynamic Pricing
- Phillips, Robert L. (2005). "Pricing and Revenue Optimization." Stanford Business Books.
- Ferreira, Kris Johnson, et al. (2016). "Online Network Revenue Management Using Thompson Sampling." Operations Research.
- den Boer, Arnoud V. (2015). "Dynamic Pricing and Learning: Historical Origins, Current Research, and New Directions." Surveys in Operations Research and Management Science.
- Ban, Gah-Yi, and N. Bora Keskin (2021). "Personalized Dynamic Pricing with Machine Learning." Management Science.

### Supply Chain Optimization
- Bengio, Yoshua, et al. (2020). "Machine Learning for Combinatorial Optimization: A Methodological Tour d'Horizon." European Journal of Operational Research.
- Kool, Wouter, Herke van Hoof, and Max Welling (2019). "Attention, Learn to Solve Routing Problems!" ICLR.
- Vinyals, Oriol, Meire Fortunato, and Navdeep Jaitly (2015). "Pointer Networks." NeurIPS.
- Nazari, Mohammadreza, et al. (2018). "Reinforcement Learning for Solving the Vehicle Routing Problem." NeurIPS.

### Risk Scoring and Credit
- Khandani, Amir E., Adlar J. Kim, and Andrew W. Lo (2010). "Consumer Credit-Risk Models via Machine-Learning Algorithms." Journal of Banking & Finance.
- Fuster, Andreas, et al. (2019). "Predictably Unequal? The Effects of Machine Learning on Credit Markets." Journal of Finance.
- Hardt, Moritz, Eric Price, and Nati Srebro (2016). "Equality of Opportunity in Supervised Learning." NeurIPS.
- Blattner, Laura, and Scott Nelson (2021). "How Costly is Noise? Data and Disparities in Consumer Credit." Working Paper.

### Predictive Maintenance
- Lee, Jay, et al. (2014). "Prognostics and Health Management Design for Rotary Machinery Systems." Mechanical Systems and Signal Processing.
- Susto, Gian Antonio, et al. (2015). "Machine Learning for Predictive Maintenance: A Multiple Classifier Approach." IEEE Transactions on Industrial Informatics.
- Carvalho, Thyago P., et al. (2019). "A Systematic Literature Review of Machine Learning Methods Applied to Predictive Maintenance." Computers & Industrial Engineering.
- Zhang, Weiting, et al. (2019). "Deep Learning-Based Prognostic Approach for Lithium-Ion Batteries." IEEE Access.

### Explainability and Fairness
- Lundberg, Scott M., and Su-In Lee (2017). "A Unified Approach to Interpreting Model Predictions." NeurIPS.
- Ribeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin (2016). "Why Should I Trust You? Explaining the Predictions of Any Classifier." KDD.
- Doshi-Velez, Finale, and Been Kim (2017). "Towards A Rigorous Science of Interpretable Machine Learning." arXiv:1702.08608.
- Mehrabi, Ninareh, et al. (2021). "A Survey on Bias and Fairness in Machine Learning." ACM Computing Surveys.
