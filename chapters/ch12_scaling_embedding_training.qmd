# Scaling Embedding Training {#sec-scaling-embedding-training}

:::{.callout-note}
## Chapter Overview
Training embedding models on trillion-row datasets requires computational infrastructure that goes far beyond single-GPU training. This chapter explores the architectures and techniques that enable embedding training at unprecedented scale: distributed training across hundreds of GPUs, gradient accumulation and mixed precision for memory efficiency, advanced memory optimization techniques, multi-GPU and multi-node coordination strategies, and cost optimization approaches that make large-scale training economically viable. These techniques transform embedding training from a multi-day single-machine task to a multi-hour distributed operation, enabling rapid iteration and larger, more powerful models.
:::

Embedding model training faces unique scaling challenges. Unlike image classification models that process fixed-size inputs, embedding models often work with variable-length sequences, sparse features, and massive vocabularies. Contrastive learning requires large batch sizes (4K-32K samples) for effective negative sampling. Self-supervised pre-training demands processing billions of documents. These requirements push standard training infrastructure to its limits, requiring specialized techniques for efficient distributed training.

## Distributed Training Architectures

Distributed training parallelizes model training across multiple devices, reducing training time from weeks to hours. However, embedding training has unique requirements that distinguish it from standard distributed training: **large batch sizes for contrastive learning**, **sparse feature handling**, **vocabulary parallelism for large embedding tables**, and **efficient negative sampling across devices**. This section explores architectures that address these challenges.

### Parallelism Strategies for Embedding Training

Modern distributed training employs multiple parallelism strategies simultaneously:

```python
{{< include /code_examples/ch12_scaling_embedding_training/distributedembeddingtable.py >}}
```bash
    torchrun --nproc_per_node=8 train.py
    ```

    Or for multi-node:
    ```bash
    torchrun --nproc_per_node=8 --nnodes=4 --node_rank=0 \
             --master_addr=node0 --master_port=1234 train.py
    ```

    Args:
        rank: GPU rank (assigned by torchrun)
        world_size: Total GPUs (assigned by torchrun)
        epochs: Number of training epochs
    """

    # Initialize distributed trainer
    model = DistributedContrastiveEmbedding(
        vocab_size=100000,
        embedding_dim=512
    )

    trainer = DistributedTrainer(
        model=model,
        local_rank=rank,
        world_size=world_size
    )

    # Optimizer (scale learning rate by world size)
    base_lr = 0.001
    scaled_lr = base_lr * world_size
    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=scaled_lr
    )

    # Training loop
    for epoch in range(epochs):
        # In practice: Use DistributedSampler for data loading
        # to ensure each GPU sees different data
        for step in range(100):  # Placeholder
            # Create dummy batch
            batch = {
                'anchor_ids': torch.randint(0, 100000, (256,), device=trainer.device),
                'positive_ids': torch.randint(0, 100000, (256,), device=trainer.device)
            }

            loss = trainer.train_step(batch, optimizer)

            if rank == 0 and step % 10 == 0:
                print(f"Epoch {epoch}, Step {step}: Loss = {loss:.4f}")

        # Save checkpoint
        trainer.save_checkpoint(f"checkpoint_epoch_{epoch}.pt", epoch, optimizer)

    trainer.cleanup()
    print(f"Rank {rank}: Training complete")

# Uncomment to run (requires torchrun):
# if __name__ == '__main__':
#     import os
#     rank = int(os.environ['LOCAL_RANK'])
#     world_size = int(os.environ['WORLD_SIZE'])
#     train_distributed_embedding_model(rank, world_size)
```

:::{.callout-tip}
## Choosing the Right Parallelism Strategy

**Use Data Parallelism when:**

- Model fits on single GPU
- Batch size is primary bottleneck
- Most layers are data-parallel friendly (convolutions, transformers)

**Add Model Parallelism when:**

- Embedding tables > GPU memory (100M+ vocabulary)
- Single layer > GPU memory (very wide transformer layers)

**Add Pipeline Parallelism when:**

- Model depth > memory capacity (100+ transformer layers)
- High arithmetic intensity (can hide communication latency)

**For embedding training:**

- Start with Data Parallelism for encoder
- Add Model Parallelism for large embedding tables
- Consider Pipeline Parallelism for deep architectures (BERT-Large, GPT-3 scale)
:::

:::{.callout-warning}
## Communication Bottlenecks

Distributed training speedup is limited by communication:

- **All-reduce** (gradient sync): O(parameters × world_size)
- **All-gather** (activations): O(batch_size × hidden_dim × world_size)
- **Point-to-point** (pipeline): O(hidden_dim × micro_batch_size)

Optimizations:

- **Gradient compression**: Reduce precision (FP32 → FP16 gradients)
- **Overlap communication and computation**: Backward pass while communicating gradients
- **Hierarchical reduction**: Node-local reduction, then cross-node
- **Faster interconnect**: InfiniBand (200 Gbps) vs Ethernet (10-100 Gbps)
:::

## Gradient Accumulation and Mixed Precision

Memory is the primary constraint in deep learning training. A single NVIDIA A100 GPU has 80GB memory, yet training large embedding models with contrastive learning (32K batch size × 512 dims × 4 bytes ≈ 64GB just for embeddings) quickly exceeds capacity. **Gradient accumulation** enables large effective batch sizes by splitting batches into smaller micro-batches, while **mixed precision** reduces memory footprint and accelerates computation by using FP16 for most operations while maintaining FP32 for numerical stability.

### Gradient Accumulation for Large Batch Training

Contrastive learning benefits from large batch sizes—more negatives improve representation quality. But memory limits batch size. Gradient accumulation solves this:

```python
{{< include /code_examples/ch12_scaling_embedding_training/gradientaccumulationtrainer.py >}}
```

### Mixed Precision Training

Modern GPUs (Volta, Turing, Ampere architectures) have specialized Tensor Cores that accelerate FP16 matrix multiplications by 2-8×. **Mixed precision** uses FP16 for computation while maintaining FP32 for numerical stability:

```python
{{< include /code_examples/ch12_scaling_embedding_training/mixedprecisiontrainer.py >}}
```

:::{.callout-tip}
## When to Use Gradient Accumulation vs Larger Hardware

**Use gradient accumulation when:**

- Memory-constrained (batch won't fit on GPU)
- Want to experiment with very large batches (64K+)
- Training on cloud instances with limited GPU memory

**Upgrade hardware when:**

- Wall-clock time is critical (accumulation is slower)
- Training very frequently (hardware cost amortizes)
- Need to scale beyond single node (distributed > accumulation)

**Use mixed precision almost always:**

- Modern GPUs (V100, A100) have Tensor Cores
- 2-3× speedup with minimal code changes
- Rarely causes numerical issues (except very deep networks)
:::

:::{.callout-warning}
## Mixed Precision Gotchas

**Gradient underflow**: Very small gradients (< 1e-7) round to zero in FP16. Gradient scaling addresses this, but extreme cases may need:

- Larger learning rates
- Loss scaling adjustments
- FP32 for sensitive layers (layer norm, softmax)

**Batch normalization**: BatchNorm statistics in FP16 can be unstable. Use FP32 for BatchNorm layers:
```python
model = model.half()  # Convert to FP16
# Keep BatchNorm in FP32
for module in model.modules():
    if isinstance(module, nn.BatchNorm1d):
        module.float()
```
:::

## Memory Optimization Techniques

Beyond mixed precision and gradient accumulation, several techniques reduce memory footprint, enabling larger models and batch sizes:

### Gradient Checkpointing

Trade computation for memory by recomputing activations during backward pass instead of storing them:

```python
{{< include /code_examples/ch12_scaling_embedding_training/checkpointedtransformerlayer.py >}}
```

### Optimizer State Optimization

Adam optimizer stores momentum and variance for each parameter, tripling memory usage. Optimizations:

```python
{{< include /code_examples/ch12_scaling_embedding_training/memoryefficientoptimizer.py >}}
```

:::{.callout-tip}
## Memory Optimization Checklist

When hitting memory limits, apply optimizations in this order:

1. **Mixed precision (FP16)**: 2× memory reduction, 2-3× speedup
2. **Gradient accumulation**: Enables larger effective batch sizes
3. **Gradient checkpointing**: 10-50× activation memory reduction
4. **Optimizer state optimization**: 8-bit Adam or SGD
5. **Model parallelism**: Split model across GPUs
6. **Batch size reduction**: Last resort (hurts contrastive learning)

Typical savings:

- FP16: 40GB → 20GB
- + Checkpointing: 20GB → 8GB
- + 8-bit optimizer: 8GB → 5GB
- Result: Fit on single A100 (80GB) with large batch
:::

## Multi-GPU and Multi-Node Strategies

Scaling beyond single GPU requires coordination across devices. This section covers practical strategies for multi-GPU (single node) and multi-node (multiple machines) training.

### Multi-GPU Training on Single Node

Single-node multi-GPU is the most common setup (8× A100 or V100 GPUs on one machine):

```python
{{< include /code_examples/ch12_scaling_embedding_training/embeddingdataset.py >}}
```

### Multi-Node Training

Multi-node training scales to hundreds of GPUs across dozens of machines:

```python
{{< include /code_examples/ch12_scaling_embedding_training/setup_multi_node.py >}}
```bash
    sbatch --nodes=4 --gres=gpu:8 train_multi_node.sh
    ```

    Or with torchrun:
    ```bash
    # On each node, run:
    torchrun --nproc_per_node=8 \
             --nnodes=4 \
             --node_rank=$NODE_RANK \
             --master_addr=$MASTER_ADDR \
             --master_port=1234 \
             train_script.py
    ```
    """

    # Get distributed parameters from environment
    rank = int(os.environ['RANK'])
    world_size = int(os.environ['WORLD_SIZE'])
    master_addr = os.environ['MASTER_ADDR']
    master_port = os.environ['MASTER_PORT']

    # Setup multi-node training
    setup_multi_node(rank, world_size, master_addr, master_port)

    # Rest of training code (same as single-node)
    # ...

    dist.destroy_process_group()
```

:::{.callout-tip}
## Multi-GPU Best Practices

**Data loading:**

- Use `DistributedSampler` to partition data across GPUs
- Set `num_workers=4` per GPU for async data loading
- Use `pin_memory=True` for faster CPU→GPU transfer

**Learning rate scaling:**

- Scale learning rate linearly with batch size
- 1 GPU (batch 512, lr 0.001) → 8 GPUs (batch 4096, lr 0.008)
- May need warmup for large learning rates

**Synchronization:**

- Minimize `dist.barrier()` calls (blocks all GPUs)
- Overlap communication with computation
- Use `find_unused_parameters=False` in DDP when possible

**Checkpointing:**

- Only save from rank 0 to avoid duplicate writes
- Use `dist.barrier()` after saving to synchronize
- Consider sharded checkpointing for very large models
:::

:::{.callout-warning}
## Multi-Node Challenges

**Network bottlenecks:**

- Cross-node communication 10-100× slower than NVLink
- Use gradient compression or ZeRO optimizer
- Consider hierarchical all-reduce (node-local first)

**Fault tolerance:**

- Single node failure kills entire job
- Implement checkpointing every N steps
- Use elastic training frameworks (TorchElastic)

**Load imbalance:**

- Stragglers slow down entire cluster
- Monitor per-GPU utilization
- Use dynamic batch sizing if variability high
:::

## Training Cost Optimization

Large-scale training is expensive. A 100-GPU training run can cost $10K-$100K. This section covers strategies to minimize cost while maintaining quality.

### Cloud Cost Optimization

```python
{{< include /code_examples/ch12_scaling_embedding_training/from.py >}}
```

### Spot Instance Training

Spot instances offer 50-90% discounts but can be preempted. Strategies for resilient training:

```python
{{< include /code_examples/ch12_scaling_embedding_training/spotinstancetrainer.py >}}
```

:::{.callout-tip}
## Cost Optimization Strategies

**Immediate savings (no quality impact):**
1. **Spot instances**: 50-90% discount (with checkpointing)
2. **Mixed precision**: 2-3× speedup → 50-70% cost reduction
3. **Reserved instances**: 30-50% discount for long-term projects
4. **Multi-cloud**: Compare prices across AWS/GCP/Azure

**Advanced optimizations:**
1. **Early stopping**: Halt when validation loss plateaus
2. **Hyperparameter search efficiency**: Use Bayesian optimization, not grid search
3. **Model distillation**: Train large model, deploy small model
4. **Sparse training**: Train only subset of parameters

**Typical cost breakdown (100-GPU training):**

- Hardware: 70% (can optimize with spot instances)
- Storage: 10% (use cheaper object storage)
- Network: 10% (minimize cross-region transfer)
- Other: 10% (monitoring, logging, etc.)
:::

## Key Takeaways

- **Distributed training is essential at scale**: Data parallelism for throughput, model parallelism for large embedding tables, and pipeline parallelism for deep architectures combine to enable trillion-row training in reasonable time

- **Gradient accumulation enables large effective batch sizes**: Split large batches into micro-batches to fit memory constraints while maintaining the benefits of large-batch contrastive learning (16K-32K samples)

- **Mixed precision training provides 2-3× speedup**: FP16 computation on Tensor Cores with FP32 master weights maintains numerical stability while dramatically reducing memory usage and accelerating training

- **Memory optimization unlocks larger models**: Gradient checkpointing, optimizer state quantization (8-bit Adam), and efficient activation management reduce memory footprint by 10-50×, enabling BERT-scale models on single GPUs

- **Multi-node training scales to hundreds of GPUs**: Proper configuration of distributed samplers, learning rate scaling, and network topology awareness enable near-linear scaling to 64+ GPUs with 40-50× speedup

- **Cost optimization is critical for sustainable training**: Spot instances (50-90% savings), mixed precision speedup, and efficient checkpointing reduce training costs from $100K to $10K-$30K for large models

- **Communication is the bottleneck at scale**: Gradient synchronization, activation gathering, and cross-node communication limit speedup; overlap computation with communication and use gradient compression to mitigate

## Looking Ahead

This chapter covered the computational techniques for training embedding models at scale. Chapter 11 shifts focus from training to inference, exploring high-performance vector operations for serving embeddings in production: optimized similarity search algorithms, approximate nearest neighbor (ANN) methods, GPU acceleration for vector operations, memory-mapped storage strategies, and parallel query processing that enables sub-millisecond similarity search across billion-vector indices.

## Further Reading

### Distributed Training
- Li, Shen, et al. (2020). "PyTorch Distributed: Experiences on Accelerating Data Parallel Training." VLDB.
- Shoeybi, Mohammad, et al. (2019). "Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism." arXiv:1909.08053.
- Rajbhandari, Samyam, et al. (2020). "ZeRO: Memory Optimizations Toward Training Trillion Parameter Models." SC20.

### Mixed Precision Training
- Micikevicius, Paulius, et al. (2018). "Mixed Precision Training." ICLR.
- Narang, Sharan, et al. (2018). "Mixed Precision Training With 8-bit Floating Point." arXiv:1905.12334.

### Memory Optimization
- Chen, Tianqi, et al. (2016). "Training Deep Nets with Sublinear Memory Cost." arXiv:1604.06174.
- Sohoni, Nimit, et al. (2019). "Low-Memory Neural Network Training." arXiv:1904.10631.
- Dettmers, Tim, et al. (2022). "8-bit Optimizers via Block-wise Quantization." arXiv:2110.02861.

### Large-Scale Training Systems
- Jia, Xianyan, et al. (2018). "Highly Scalable Deep Learning Training System with Mixed-Precision." arXiv:1807.11205.
- Sergeev, Alexander, and Mike Del Balso (2018). "Horovod: Fast and Easy Distributed Deep Learning in TensorFlow." arXiv:1802.05799.
- Paszke, Adam, et al. (2019). "PyTorch: An Imperative Style, High-Performance Deep Learning Library." NeurIPS.

### Cost Optimization
- Chaudhary, Vinay, et al. (2020). "Balancing Efficiency and Flexibility for DNN Acceleration via Temporal GPU-Systolic Array Integration." DAC.
- Yang, Tianyi, et al. (2021). "Toward Efficient Deep Learning in the Cloud: Resource Provisioning and Workload Scheduling." IEEE Cloud Computing.
