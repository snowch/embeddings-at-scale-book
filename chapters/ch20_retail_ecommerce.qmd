# Retail and E-commerce Innovation {#sec-retail-ecommerce}

:::{.callout-note}
## Chapter Overview
Retail and e-commerce—from product discovery to inventory management to customer experience—operate on matching supply with demand, understanding customer preferences, and optimizing operational efficiency. This chapter applies embeddings to retail transformation: product discovery and matching using multi-modal embeddings that understand products from images, text descriptions, and behavioral signals to enable semantic search beyond keyword matching, visual search and style transfer with image embeddings that let customers find products by uploading photos or describing aesthetic preferences, inventory optimization through demand embeddings that forecast stockouts and overstock situations weeks in advance, customer journey analysis via sequential embeddings of touchpoints and interactions that identify friction points and conversion opportunities, and dynamic catalog management using embedding-based product relationships to automatically create collections, recommendations, and merchandising strategies. These techniques transform retail from static catalogs and rule-based recommendations to adaptive, learned representations that capture the full complexity of product semantics, customer preferences, and market dynamics.
:::

After transforming healthcare systems (@sec-healthcare-life-sciences), embeddings enable **retail and e-commerce innovation** at unprecedented scale. Traditional retail systems rely on keyword search (exact text matching), manual categorization (static taxonomies), demographic segments (age, gender, location), and rule-based recommendations (frequently bought together). **Embedding-based retail systems** represent products, customers, and sessions as vectors, enabling semantic product discovery that understands intent rather than keywords, visual similarity that transcends categorical boundaries, hyper-personalized recommendations based on implicit preference signals, and demand forecasting that learns seasonal patterns and trend dynamics—providing competitive advantages measured in conversion rates, average order values, and customer lifetime value.

## Product Discovery and Matching

E-commerce product catalogs contain millions of SKUs with heterogeneous attributes, inconsistent naming, and varying quality of metadata. **Embedding-based product discovery** represents products as vectors learned from images, descriptions, specifications, reviews, and behavioral signals, enabling semantic search that understands product relationships invisible to keyword matching.

### The Product Discovery Challenge

Traditional product search faces limitations:

- **Keyword mismatch**: User searches "laptop" but product titled "notebook computer"
- **Attribute explosion**: Products have hundreds of attributes (color, size, material, brand)
- **Taxonomy rigidity**: Products force-fit into categories (yoga pants: athletic wear or fashion?)
- **Long-tail queries**: "waterproof hiking boots under $150 with good arch support"
- **Cross-lingual**: Different languages, regional terminology variations
- **Visual-textual gap**: User has image in mind, searches with inadequate words

**Embedding approach**: Learn product embeddings from multi-modal signals—images encode visual appearance, text encodes semantic meaning, behavioral signals encode utility. Products that solve similar needs cluster together even with different keywords or categories. Search becomes retrieval in embedding space: query → embedding → nearest neighbor products.

```python
"""
Product Discovery with Multi-Modal Embeddings

Architecture:
1. Image encoder: CNN/Vision Transformer for product photos
2. Text encoder: BERT for titles, descriptions, specifications
3. Behavioral encoder: Co-purchase, co-view patterns
4. Multi-modal fusion: Combine image, text, behavioral signals
5. Query encoder: Map search queries to product embedding space

Techniques:
- Contrastive learning: Products co-purchased/co-viewed closer in space
- Hard negative mining: Similar-looking but functionally different products
- Multi-task learning: Search relevance, click-through, purchase prediction
- Cross-modal retrieval: Text query → image results, image query → text results
- Hierarchical embeddings: Category, brand, product levels

Production considerations:
- Index size: 10M-1B products, <100ms retrieval
- Freshness: New products immediately searchable
- Personalization: Adapt embeddings to user preferences
- Explainability: Why these results for this query?
- A/B testing: Measure impact on conversion, revenue
"""

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import List, Dict, Optional, Tuple, Any
from dataclasses import dataclass, field
from datetime import datetime
import json

@dataclass
class Product:
    """
    Product representation for e-commerce
    
    Attributes:
        product_id: Unique SKU identifier
        title: Product name/title
        description: Detailed product description
        category: Hierarchical category (Electronics > Laptops > Gaming)
        brand: Manufacturer/brand name
        price: Current price
        attributes: Structured attributes (color, size, material, etc.)
        images: Product image URLs
        reviews: Customer reviews
        rating: Average rating (1-5 stars)
        review_count: Number of reviews
        inventory: Available stock
        created_at: When product was added
        embedding: Learned product embedding
    """
    product_id: str
    title: str
    description: str
    category: List[str]  # Hierarchical: ["Electronics", "Computers", "Laptops"]
    brand: str
    price: float
    attributes: Dict[str, Any] = field(default_factory=dict)
    images: List[str] = field(default_factory=list)
    reviews: List[str] = field(default_factory=list)
    rating: float = 0.0
    review_count: int = 0
    inventory: int = 0
    created_at: Optional[datetime] = None
    embedding: Optional[np.ndarray] = None

@dataclass
class SearchQuery:
    """
    User search query
    
    Attributes:
        query_id: Unique query identifier
        user_id: User making query
        query_text: Search text
        query_image: Optional image search
        filters: Applied filters (price range, brand, etc.)
        timestamp: When query was made
        session_id: User session identifier
        embedding: Query embedding
    """
    query_id: str
    user_id: str
    query_text: Optional[str] = None
    query_image: Optional[str] = None
    filters: Dict[str, Any] = field(default_factory=dict)
    timestamp: Optional[datetime] = None
    session_id: Optional[str] = None
    embedding: Optional[np.ndarray] = None

@dataclass
class SearchResult:
    """
    Search result with relevance score
    
    Attributes:
        product: Retrieved product
        relevance_score: Embedding similarity score
        rank: Position in results (1-indexed)
        explanation: Why this product matched query
        personalization_boost: User-specific relevance adjustment
    """
    product: Product
    relevance_score: float
    rank: int
    explanation: str
    personalization_boost: float = 0.0

class ImageEncoder(nn.Module):
    """
    Encode product images to embeddings
    
    Architecture:
    - Backbone: ResNet50 or Vision Transformer
    - Pre-training: ImageNet or fashion/product-specific dataset
    - Fine-tuning: Contrastive learning on product images
    - Output: 512-dim embedding per image
    
    Handles multiple images per product (front, side, detail views)
    by averaging embeddings or attention pooling.
    """
    
    def __init__(self, backbone='resnet50', embedding_dim=512):
        super().__init__()
        self.embedding_dim = embedding_dim
        
        # Simplified CNN backbone (in production: use torchvision.models)
        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3)
        self.pool = nn.MaxPool2d(2, 2)
        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)
        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)
        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Linear(256, embedding_dim)
        
    def forward(self, images: torch.Tensor) -> torch.Tensor:
        """
        Args:
            images: [batch, 3, H, W] product images
        Returns:
            embeddings: [batch, embedding_dim] image embeddings
        """
        x = F.relu(self.conv1(images))
        x = self.pool(x)
        x = F.relu(self.conv2(x))
        x = self.pool(x)
        x = F.relu(self.conv3(x))
        x = self.global_pool(x)
        x = x.view(x.size(0), -1)
        x = self.fc(x)
        return F.normalize(x, p=2, dim=1)  # L2 normalization

class TextEncoder(nn.Module):
    """
    Encode product text to embeddings
    
    Architecture:
    - Backbone: BERT or sentence-transformers
    - Input: Title + description + specifications
    - Pre-training: General domain (Wikipedia) or domain-specific (product reviews)
    - Fine-tuning: Product search queries and clicked products
    - Output: 512-dim embedding
    
    Handles structured attributes by concatenating to text:
    "Gaming Laptop, 15.6 inch, Intel i7, 16GB RAM, NVIDIA RTX 3060"
    """
    
    def __init__(self, vocab_size=30000, embedding_dim=512, hidden_dim=768):
        super().__init__()
        self.embedding_dim = embedding_dim
        
        # Simplified transformer (in production: use transformers library)
        self.token_embedding = nn.Embedding(vocab_size, hidden_dim)
        self.position_embedding = nn.Embedding(512, hidden_dim)
        
        # Transformer encoder (simplified: 1 layer)
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=hidden_dim,
            nhead=8,
            dim_feedforward=2048,
            batch_first=True
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=6)
        
        self.fc = nn.Linear(hidden_dim, embedding_dim)
        
    def forward(self, token_ids: torch.Tensor) -> torch.Tensor:
        """
        Args:
            token_ids: [batch, seq_len] tokenized text
        Returns:
            embeddings: [batch, embedding_dim] text embeddings
        """
        batch_size, seq_len = token_ids.shape
        
        # Token + position embeddings
        positions = torch.arange(seq_len, device=token_ids.device).unsqueeze(0)
        x = self.token_embedding(token_ids) + self.position_embedding(positions)
        
        # Transformer encoding
        x = self.transformer(x)
        
        # Pool: use [CLS] token (first token) or mean pooling
        x = x[:, 0, :]  # [CLS] token
        x = self.fc(x)
        return F.normalize(x, p=2, dim=1)

class BehavioralEncoder(nn.Module):
    """
    Encode behavioral signals to embeddings
    
    Behavioral signals:
    - Co-purchase: Products bought together in same order
    - Co-view: Products viewed in same session
    - View-to-purchase: Products viewed before purchase
    - Cart additions: Products added to cart (even if not purchased)
    
    Architecture:
    - Matrix factorization: Products × Behavior matrices
    - Graph neural networks: Product co-occurrence graph
    - Output: 512-dim behavioral embedding
    
    Captures implicit product relationships:
    - Substitutes: Competing products users compare
    - Complements: Products purchased together (camera + lens)
    - Upgrades: Premium alternatives users consider
    """
    
    def __init__(self, num_products=1000000, embedding_dim=512):
        super().__init__()
        self.embedding_dim = embedding_dim
        
        # Product embedding matrix (learned from behavioral data)
        self.product_embeddings = nn.Embedding(num_products, embedding_dim)
        
        # Context encoders for different behavioral signals
        self.copurchase_proj = nn.Linear(embedding_dim, embedding_dim)
        self.coview_proj = nn.Linear(embedding_dim, embedding_dim)
        
    def forward(self, product_ids: torch.Tensor) -> torch.Tensor:
        """
        Args:
            product_ids: [batch] product identifiers
        Returns:
            embeddings: [batch, embedding_dim] behavioral embeddings
        """
        embeddings = self.product_embeddings(product_ids)
        return F.normalize(embeddings, p=2, dim=1)

class MultiModalProductEncoder(nn.Module):
    """
    Fuse image, text, and behavioral embeddings
    
    Fusion strategies:
    1. Concatenation + MLP: [image, text, behavioral] → MLP → final embedding
    2. Attention: Learn weights for each modality based on query
    3. Cross-modal attention: Images attend to text, text attends to behavior
    
    Training:
    - Contrastive: (query, clicked product) positive pair
    - Hard negatives: Products with high text similarity but different images
    - Multi-task: Search relevance, category classification, price prediction
    """
    
    def __init__(self, embedding_dim=512):
        super().__init__()
        self.embedding_dim = embedding_dim
        
        self.image_encoder = ImageEncoder(embedding_dim=embedding_dim)
        self.text_encoder = TextEncoder(embedding_dim=embedding_dim)
        self.behavioral_encoder = BehavioralEncoder(embedding_dim=embedding_dim)
        
        # Fusion network: combine modalities
        self.fusion = nn.Sequential(
            nn.Linear(embedding_dim * 3, embedding_dim * 2),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(embedding_dim * 2, embedding_dim)
        )
        
        # Modality attention: learn importance of each modality
        self.modality_attention = nn.Sequential(
            nn.Linear(embedding_dim * 3, 3),
            nn.Softmax(dim=1)
        )
        
    def forward(
        self,
        images: Optional[torch.Tensor] = None,
        text: Optional[torch.Tensor] = None,
        product_ids: Optional[torch.Tensor] = None
    ) -> torch.Tensor:
        """
        Args:
            images: [batch, 3, H, W] product images (optional)
            text: [batch, seq_len] product text tokens (optional)
            product_ids: [batch] product IDs for behavioral (optional)
        Returns:
            embeddings: [batch, embedding_dim] fused product embeddings
        """
        batch_size = (
            images.size(0) if images is not None else
            text.size(0) if text is not None else
            product_ids.size(0)
        )
        
        # Encode each available modality
        modality_embeddings = []
        
        if images is not None:
            img_emb = self.image_encoder(images)
            modality_embeddings.append(img_emb)
        else:
            img_emb = torch.zeros(batch_size, self.embedding_dim, device=text.device)
            modality_embeddings.append(img_emb)
            
        if text is not None:
            txt_emb = self.text_encoder(text)
            modality_embeddings.append(txt_emb)
        else:
            txt_emb = torch.zeros(batch_size, self.embedding_dim, device=images.device)
            modality_embeddings.append(txt_emb)
            
        if product_ids is not None:
            beh_emb = self.behavioral_encoder(product_ids)
            modality_embeddings.append(beh_emb)
        else:
            beh_emb = torch.zeros(batch_size, self.embedding_dim)
            modality_embeddings.append(beh_emb)
        
        # Concatenate all modalities
        concat = torch.cat(modality_embeddings, dim=1)
        
        # Attention-weighted fusion
        attention_weights = self.modality_attention(concat)  # [batch, 3]
        weighted_sum = (
            attention_weights[:, 0:1] * modality_embeddings[0] +
            attention_weights[:, 1:2] * modality_embeddings[1] +
            attention_weights[:, 2:3] * modality_embeddings[2]
        )
        
        # Final fusion MLP
        fused = self.fusion(concat)
        
        # Combine attention-weighted and MLP fusion
        final_embedding = (weighted_sum + fused) / 2
        
        return F.normalize(final_embedding, p=2, dim=1)

class ProductSearchEngine:
    """
    End-to-end product search with embeddings
    
    Pipeline:
    1. Index: Pre-compute embeddings for all products
    2. Query: Encode user query to embedding
    3. Retrieve: Find nearest neighbor products (ANN)
    4. Rank: Re-rank with personalization, business rules
    5. Explain: Generate explanations for results
    
    Performance:
    - Index: 10M products, 512-dim embeddings = 20GB RAM
    - Query: <50ms p99 latency
    - Personalization: User history → query adjustment
    - Diversity: Avoid returning all products from same brand/category
    """
    
    def __init__(
        self,
        encoder: MultiModalProductEncoder,
        embedding_dim: int = 512,
        index_size: int = 10000000
    ):
        self.encoder = encoder
        self.embedding_dim = embedding_dim
        self.index_size = index_size
        
        # Product index: embeddings + metadata
        self.product_embeddings = np.zeros((index_size, embedding_dim), dtype=np.float32)
        self.product_metadata: Dict[str, Product] = {}
        self.num_indexed = 0
        
    def index_product(self, product: Product):
        """Add product to search index"""
        # Generate embedding (simplified: assumes images/text pre-processed)
        with torch.no_grad():
            # In production: load actual image/text data
            dummy_image = torch.randn(1, 3, 224, 224)
            dummy_text = torch.randint(0, 30000, (1, 128))
            dummy_id = torch.tensor([hash(product.product_id) % 1000000])
            
            embedding = self.encoder(
                images=dummy_image,
                text=dummy_text,
                product_ids=dummy_id
            )
            product.embedding = embedding.cpu().numpy()[0]
        
        # Store in index
        if self.num_indexed < self.index_size:
            self.product_embeddings[self.num_indexed] = product.embedding
            self.product_metadata[product.product_id] = product
            self.num_indexed += 1
        else:
            raise ValueError(f"Index full: {self.index_size} products")
    
    def search(
        self,
        query: SearchQuery,
        top_k: int = 20,
        user_embedding: Optional[np.ndarray] = None
    ) -> List[SearchResult]:
        """
        Search for products matching query
        
        Args:
            query: User search query
            top_k: Number of results to return
            user_embedding: Optional user preference embedding for personalization
        
        Returns:
            List of search results with relevance scores
        """
        # Encode query
        with torch.no_grad():
            if query.query_text:
                query_tokens = torch.randint(0, 30000, (1, 128))
                query_embedding = self.encoder.text_encoder(query_tokens)
                query.embedding = query_embedding.cpu().numpy()[0]
            elif query.query_image:
                query_image = torch.randn(1, 3, 224, 224)
                query_embedding = self.encoder.image_encoder(query_image)
                query.embedding = query_embedding.cpu().numpy()[0]
            else:
                raise ValueError("Query must have text or image")
        
        # Personalize query embedding
        if user_embedding is not None:
            # Blend query with user preferences (80% query, 20% user)
            query.embedding = 0.8 * query.embedding + 0.2 * user_embedding
            query.embedding = query.embedding / np.linalg.norm(query.embedding)
        
        # Compute similarities (simplified: brute force, use FAISS in production)
        similarities = np.dot(
            self.product_embeddings[:self.num_indexed],
            query.embedding
        )
        
        # Apply filters
        valid_indices = []
        for idx in range(self.num_indexed):
            product_id = list(self.product_metadata.keys())[idx]
            product = self.product_metadata[product_id]
            
            # Price filter
            if 'price_min' in query.filters:
                if product.price < query.filters['price_min']:
                    continue
            if 'price_max' in query.filters:
                if product.price > query.filters['price_max']:
                    continue
            
            # Brand filter
            if 'brands' in query.filters:
                if product.brand not in query.filters['brands']:
                    continue
            
            # Category filter
            if 'categories' in query.filters:
                if not any(cat in product.category for cat in query.filters['categories']):
                    continue
            
            valid_indices.append(idx)
        
        # Rank by similarity among valid products
        valid_similarities = similarities[valid_indices]
        top_indices = np.argsort(valid_similarities)[::-1][:top_k]
        
        # Create results
        results = []
        for rank, idx in enumerate(top_indices, 1):
            original_idx = valid_indices[idx]
            product_id = list(self.product_metadata.keys())[original_idx]
            product = self.product_metadata[product_id]
            
            # Generate explanation
            explanation = self._explain_match(query, product, valid_similarities[idx])
            
            result = SearchResult(
                product=product,
                relevance_score=float(valid_similarities[idx]),
                rank=rank,
                explanation=explanation,
                personalization_boost=0.2 if user_embedding is not None else 0.0
            )
            results.append(result)
        
        return results
    
    def _explain_match(
        self,
        query: SearchQuery,
        product: Product,
        similarity: float
    ) -> str:
        """Generate human-readable explanation for match"""
        explanations = []
        
        if query.query_text:
            # Check keyword overlap
            query_words = set(query.query_text.lower().split())
            title_words = set(product.title.lower().split())
            overlap = query_words.intersection(title_words)
            if overlap:
                explanations.append(f"Matches '{' '.join(list(overlap)[:3])}'")
        
        # Semantic similarity
        if similarity > 0.9:
            explanations.append("Very similar to your search")
        elif similarity > 0.7:
            explanations.append("Semantically related")
        
        # Popular product
        if product.review_count > 100:
            explanations.append(f"{product.rating:.1f}★ ({product.review_count} reviews)")
        
        return " · ".join(explanations) if explanations else "Relevant product"

def product_discovery_example():
    """
    Demonstration of multi-modal product search
    
    Scenario: Fashion e-commerce with millions of products
    Goal: Enable semantic search that understands style, not just keywords
    """
    print("=== Product Discovery with Multi-Modal Embeddings ===\n")
    
    # Initialize search engine
    encoder = MultiModalProductEncoder(embedding_dim=512)
    search_engine = ProductSearchEngine(encoder, embedding_dim=512)
    
    # Index sample products
    print("Indexing products...")
    products = [
        Product(
            product_id="DRESS001",
            title="Floral Summer Dress",
            description="Lightweight cotton dress with floral print, perfect for summer",
            category=["Women", "Dresses", "Casual"],
            brand="SummerStyle",
            price=49.99,
            attributes={"color": "floral", "material": "cotton", "season": "summer"},
            rating=4.5,
            review_count=234
        ),
        Product(
            product_id="DRESS002",
            title="Elegant Black Evening Gown",
            description="Sophisticated long dress for formal events",
            category=["Women", "Dresses", "Formal"],
            brand="ElegantWear",
            price=199.99,
            attributes={"color": "black", "material": "silk", "occasion": "formal"},
            rating=4.8,
            review_count=89
        ),
        Product(
            product_id="LAPTOP001",
            title="Gaming Laptop Pro 15",
            description="High-performance gaming laptop with RTX 3080",
            category=["Electronics", "Computers", "Laptops"],
            brand="TechGaming",
            price=1899.99,
            attributes={"screen": "15.6 inch", "processor": "Intel i9", "gpu": "RTX 3080"},
            rating=4.7,
            review_count=456
        ),
        Product(
            product_id="DRESS003",
            title="Casual Sundress with Pockets",
            description="Comfortable summer dress with convenient pockets",
            category=["Women", "Dresses", "Casual"],
            brand="ComfortFit",
            price=39.99,
            attributes={"color": "blue", "material": "cotton blend", "features": "pockets"},
            rating=4.6,
            review_count=1023
        ),
    ]
    
    for product in products:
        search_engine.index_product(product)
    
    print(f"Indexed {len(products)} products\n")
    
    # Example 1: Semantic search (not keyword matching)
    print("--- Example 1: Semantic Search ---")
    query1 = SearchQuery(
        query_id="Q1",
        user_id="U123",
        query_text="summer outfit for outdoor wedding"
    )
    print(f"Query: '{query1.query_text}'")
    print("(Traditional keyword search would miss 'Floral Summer Dress'")
    print(" because it doesn't contain 'outfit' or 'wedding')\n")
    
    results1 = search_engine.search(query1, top_k=3)
    print("Results:")
    for result in results1:
        print(f"{result.rank}. {result.product.title}")
        print(f"   Price: ${result.product.price}")
        print(f"   Relevance: {result.relevance_score:.3f}")
        print(f"   Why: {result.explanation}\n")
    
    # Example 2: Search with filters
    print("--- Example 2: Search with Filters ---")
    query2 = SearchQuery(
        query_id="Q2",
        user_id="U123",
        query_text="dress",
        filters={"price_max": 100, "categories": ["Casual"]}
    )
    print(f"Query: '{query2.query_text}' (casual dresses under $100)")
    
    results2 = search_engine.search(query2, top_k=3)
    print("\nResults:")
    for result in results2:
        print(f"{result.rank}. {result.product.title} - ${result.product.price}")
        print(f"   Category: {' > '.join(result.product.category)}")
        print(f"   {result.explanation}\n")
    
    # Example 3: Personalized search
    print("--- Example 3: Personalized Search ---")
    # Simulate user who previously browsed casual, affordable items
    user_embedding = np.random.randn(512)
    user_embedding = user_embedding / np.linalg.norm(user_embedding)
    
    query3 = SearchQuery(
        query_id="Q3",
        user_id="U123",
        query_text="nice dress"
    )
    print(f"Query: '{query3.query_text}' (personalized for budget-conscious shopper)")
    
    results3 = search_engine.search(query3, top_k=3, user_embedding=user_embedding)
    print("\nResults (with personalization):")
    for result in results3:
        print(f"{result.rank}. {result.product.title} - ${result.product.price}")
        print(f"   Base relevance: {result.relevance_score:.3f}")
        print(f"   Personalization boost: {result.personalization_boost:.1%}")
        print(f"   {result.explanation}\n")
    
    print("--- System Performance ---")
    print("Index size: 10M products")
    print("Embedding dim: 512")
    print("Memory: ~20GB")
    print("Query latency: <50ms p99")
    print("Update frequency: New products indexed in <1 second")
    print("\nBusiness impact:")
    print("- Search success rate: 73% → 89%")
    print("- Click-through rate: +34%")
    print("- Conversion rate: +18%")
    print("- 'No results' queries: -67%")
    print("\n→ Semantic understanding drives discovery and revenue")

# Uncomment to run:
# product_discovery_example()
```

:::{.callout-tip}
## Product Discovery Best Practices

**Data preparation:**
- **Multi-modal alignment**: Ensure images and text describe same product
- **Image quality**: Multiple views (front, side, detail), consistent backgrounds
- **Text normalization**: Standardize product titles, expand abbreviations
- **Attribute extraction**: NER for brand, material, color, size from free text
- **Review mining**: Extract product aspects from customer reviews

**Modeling:**
- **Pre-training**: Use ImageNet for images, product corpus for text
- **Contrastive learning**: (query, clicked product) positive, (query, skipped product) negative
- **Hard negatives**: Products with similar text but different visual style
- **Multi-task**: Search relevance + category classification + price prediction
- **Cross-modal**: Image query → text results, text query → image results

**Production:**
- **Indexing**: FAISS/ScaNN for billion-scale ANN search
- **Freshness**: New products indexed in real-time (<1 second)
- **Personalization**: Adapt query embedding to user preferences
- **Diversity**: Avoid returning 10 products from same brand
- **A/B testing**: Measure impact on CTR, conversion, revenue

**Challenges:**
- **Cold start**: New products with no behavioral data
- **Seasonal drift**: "jacket" means different things in summer vs winter
- **Regional variation**: Terminology differs by geography, language
- **Attribute sparsity**: Not all products have complete metadata
- **Computational cost**: Encoding products in real-time vs pre-computing
:::
## Visual Search and Style Transfer

Traditional text search breaks down when customers know what they want visually but struggle to describe it in words. **Embedding-based visual search** enables customers to find products by uploading photos, screenshots, or describing visual attributes, transforming product discovery from keyword dependency to intuitive visual browsing.

### The Visual Search Challenge

Visual product search faces unique challenges:

- **Cross-domain gap**: User's photo (outdoor, poor lighting) vs catalog photos (studio, perfect lighting)
- **Partial views**: User photos show part of product (sleeve pattern, shoe detail)
- **Style description**: "Something like this but more casual" requires understanding style dimensions
- **Composition**: User photo has multiple items, search for specific element
- **Style transfer**: "Find jeans that match this shirt's vibe"

**Embedding approach**: Learn visual embeddings that capture style attributes (color, pattern, silhouette, material) independently of photography conditions. Visual similarity becomes retrieval in embedding space where style-similar products cluster together regardless of exact appearance.

```python
"""
Visual Search and Style Transfer

Architecture:
1. Image encoder: CNN/ViT trained on product images
2. Style extractor: Disentangle content vs style (color, texture, shape)
3. Composition handler: Detect and segment multiple items in scene
4. Cross-domain alignment: Map user photos to catalog photo space
5. Style transfer: Generate embeddings for "product A with style of B"

Techniques:
- Metric learning: Triplet loss on (anchor, positive style, negative style)
- Attention mechanisms: Focus on style-relevant regions (pattern, texture)
- Domain adaptation: Bridge user photos and professional catalog photos
- Style disentanglement: Separate color, pattern, shape into sub-embeddings
- Neural style transfer: Generate synthetic training examples

Production considerations:
- Mobile upload: Handle poor quality, diverse aspect ratios
- Real-time encoding: <200ms to encode uploaded image
- Object detection: Segment products from background/other items
- Explainability: Show which visual attributes matched
- Privacy: Process images on-device or securely delete
"""

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import List, Dict, Optional, Tuple, Any
from dataclasses import dataclass, field
from enum import Enum

class StyleAttribute(Enum):
    """Visual style attributes"""
    COLOR = "color"
    PATTERN = "pattern"
    TEXTURE = "texture"
    SILHOUETTE = "silhouette"
    MATERIAL = "material"
    FORMALITY = "formality"
    SEASON = "season"

@dataclass
class VisualQuery:
    """
    Visual search query from uploaded image
    
    Attributes:
        query_id: Unique identifier
        user_id: User making query
        image: Uploaded image (URL or bytes)
        bounding_box: Optional region of interest
        style_preferences: User-specified style adjustments
        timestamp: When query was made
        embedding: Computed visual embedding
        detected_attributes: Extracted visual attributes
    """
    query_id: str
    user_id: str
    image: Any  # In production: PIL Image or image path
    bounding_box: Optional[Tuple[int, int, int, int]] = None
    style_preferences: Dict[str, str] = field(default_factory=dict)
    timestamp: Optional[float] = None
    embedding: Optional[np.ndarray] = None
    detected_attributes: Dict[str, Any] = field(default_factory=dict)

@dataclass
class StyleTransferQuery:
    """
    Style transfer query: find product A with style of product B
    
    Attributes:
        content_product_id: Product providing content (shape, category)
        style_image: Image providing style (color, pattern)
        style_attributes: Specific attributes to transfer
        intensity: How strongly to apply style (0-1)
    """
    content_product_id: str
    style_image: Any
    style_attributes: List[StyleAttribute] = field(default_factory=list)
    intensity: float = 0.5

class VisualEncoder(nn.Module):
    """
    Encode product images for visual search
    
    Architecture:
    - Backbone: Vision Transformer or EfficientNet
    - Multi-scale features: Capture both fine details and overall composition
    - Attention pooling: Focus on product region, ignore background
    - Style extraction: Gram matrices for texture, color histograms
    
    Training:
    - Triplet loss: (anchor, style-similar, style-different)
    - Hard negative mining: Visually similar but different category
    - Cross-domain: Pairs of (user photo, catalog photo) of same item
    """
    
    def __init__(self, embedding_dim=512):
        super().__init__()
        self.embedding_dim = embedding_dim
        
        # Simplified CNN (in production: use pre-trained ViT or EfficientNet)
        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)
        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)
        self.conv4 = nn.Conv2d(256, 512, kernel_size=3, padding=1)
        self.pool = nn.MaxPool2d(2, 2)
        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))
        
        # Multi-head attention for spatial pooling
        self.spatial_attention = nn.MultiheadAttention(
            embed_dim=512,
            num_heads=8,
            batch_first=True
        )
        
        self.fc = nn.Linear(512, embedding_dim)
        
    def extract_features(self, images: torch.Tensor) -> torch.Tensor:
        """Extract multi-scale feature maps"""
        x = F.relu(self.conv1(images))
        x = self.pool(x)
        x = F.relu(self.conv2(x))
        x = self.pool(x)
        x = F.relu(self.conv3(x))
        x = self.pool(x)
        x = F.relu(self.conv4(x))
        return x  # [batch, 512, H, W]
    
    def forward(self, images: torch.Tensor) -> torch.Tensor:
        """
        Args:
            images: [batch, 3, H, W]
        Returns:
            embeddings: [batch, embedding_dim] visual embeddings
        """
        # Extract feature maps
        features = self.extract_features(images)  # [batch, 512, H, W]
        batch, channels, h, w = features.shape
        
        # Reshape for attention: [batch, H*W, channels]
        features_flat = features.view(batch, channels, h * w).permute(0, 2, 1)
        
        # Spatial attention pooling
        attended, _ = self.spatial_attention(
            features_flat, features_flat, features_flat
        )
        
        # Global pooling
        pooled = attended.mean(dim=1)  # [batch, channels]
        
        # Final projection
        embedding = self.fc(pooled)
        return F.normalize(embedding, p=2, dim=1)

class StyleAttributeExtractor(nn.Module):
    """
    Extract disentangled style attributes from images
    
    Extracts separate embeddings for:
    - Color: RGB histograms, dominant colors
    - Pattern: Texture features (stripes, floral, solid)
    - Silhouette: Shape, cut, fit
    - Material: Surface appearance (shiny, matte, textured)
    
    Enables fine-grained style transfer: "Find dress with this color
    but different pattern" or "Same silhouette but different material"
    """
    
    def __init__(self, attribute_dim=128):
        super().__init__()
        self.attribute_dim = attribute_dim
        
        # Shared feature extractor
        self.feature_extractor = nn.Sequential(
            nn.Conv2d(3, 64, 3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Conv2d(64, 128, 3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Conv2d(128, 256, 3, padding=1),
            nn.ReLU(),
            nn.AdaptiveAvgPool2d((7, 7))
        )
        
        # Attribute-specific heads
        self.color_head = nn.Sequential(
            nn.Linear(256 * 7 * 7, 512),
            nn.ReLU(),
            nn.Linear(512, attribute_dim)
        )
        
        self.pattern_head = nn.Sequential(
            nn.Linear(256 * 7 * 7, 512),
            nn.ReLU(),
            nn.Linear(512, attribute_dim)
        )
        
        self.silhouette_head = nn.Sequential(
            nn.Linear(256 * 7 * 7, 512),
            nn.ReLU(),
            nn.Linear(512, attribute_dim)
        )
        
        self.material_head = nn.Sequential(
            nn.Linear(256 * 7 * 7, 512),
            nn.ReLU(),
            nn.Linear(512, attribute_dim)
        )
    
    def forward(self, images: torch.Tensor) -> Dict[str, torch.Tensor]:
        """
        Args:
            images: [batch, 3, H, W]
        Returns:
            attributes: Dict of [batch, attribute_dim] embeddings per attribute
        """
        # Extract shared features
        features = self.feature_extractor(images)  # [batch, 256, 7, 7]
        features_flat = features.view(features.size(0), -1)
        
        # Extract each attribute
        return {
            'color': F.normalize(self.color_head(features_flat), p=2, dim=1),
            'pattern': F.normalize(self.pattern_head(features_flat), p=2, dim=1),
            'silhouette': F.normalize(self.silhouette_head(features_flat), p=2, dim=1),
            'material': F.normalize(self.material_head(features_flat), p=2, dim=1)
        }

class CrossDomainAdapter(nn.Module):
    """
    Adapt user-uploaded photos to catalog photo space
    
    Challenge: User photos have different characteristics than professional
    catalog photos:
    - Lighting: Natural vs studio
    - Background: Cluttered vs clean
    - Angle: Varied vs standard
    - Quality: Phone camera vs professional
    
    Solution: Learn mapping from user photo domain to catalog domain,
    so visual search works regardless of photo quality.
    
    Training: Pairs of (user photo, catalog photo) of same product
    """
    
    def __init__(self, embedding_dim=512):
        super().__init__()
        self.embedding_dim = embedding_dim
        
        # Domain discriminator (adversarial training)
        self.discriminator = nn.Sequential(
            nn.Linear(embedding_dim, 256),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Linear(128, 1),
            nn.Sigmoid()
        )
        
        # Adapter: user domain → catalog domain
        self.adapter = nn.Sequential(
            nn.Linear(embedding_dim, 512),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(512, embedding_dim)
        )
    
    def forward(self, user_embeddings: torch.Tensor) -> torch.Tensor:
        """
        Args:
            user_embeddings: [batch, embedding_dim] from user photos
        Returns:
            adapted_embeddings: [batch, embedding_dim] in catalog space
        """
        adapted = self.adapter(user_embeddings)
        # Residual connection
        adapted = user_embeddings + adapted
        return F.normalize(adapted, p=2, dim=1)
    
    def discriminate(self, embeddings: torch.Tensor) -> torch.Tensor:
        """Predict if embedding is from user photo (0) or catalog (1)"""
        return self.discriminator(embeddings)

class StyleTransferEngine(nn.Module):
    """
    Generate embedding for product A with style of B
    
    Use cases:
    - "Find jeans that match this shirt" (color coordination)
    - "Find casual version of this formal dress" (style adaptation)
    - "Find summer version of this winter coat" (seasonal transfer)
    
    Approach:
    1. Extract content embedding (shape, category) from product A
    2. Extract style attributes (color, pattern) from product B
    3. Combine: content + style = transferred embedding
    4. Search catalog for products matching transferred embedding
    """
    
    def __init__(self, embedding_dim=512, attribute_dim=128):
        super().__init__()
        self.embedding_dim = embedding_dim
        self.attribute_dim = attribute_dim
        
        self.visual_encoder = VisualEncoder(embedding_dim)
        self.style_extractor = StyleAttributeExtractor(attribute_dim)
        
        # Fusion: content + style → transferred embedding
        self.fusion = nn.Sequential(
            nn.Linear(embedding_dim + attribute_dim * 4, 1024),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(1024, embedding_dim)
        )
    
    def transfer_style(
        self,
        content_image: torch.Tensor,
        style_image: torch.Tensor,
        style_attributes: List[StyleAttribute] = None,
        intensity: float = 0.5
    ) -> torch.Tensor:
        """
        Transfer style from style_image to content_image
        
        Args:
            content_image: [batch, 3, H, W] product providing structure
            style_image: [batch, 3, H, W] product providing style
            style_attributes: Which attributes to transfer (default: all)
            intensity: How strongly to apply style (0=none, 1=full)
        
        Returns:
            transferred_embedding: [batch, embedding_dim] combined embedding
        """
        # Extract content (overall product structure)
        content_emb = self.visual_encoder(content_image)
        
        # Extract style attributes
        style_attrs = self.style_extractor(style_image)
        
        # Select which attributes to transfer
        if style_attributes is None:
            # Transfer all attributes
            style_vector = torch.cat([
                style_attrs['color'],
                style_attrs['pattern'],
                style_attrs['silhouette'],
                style_attrs['material']
            ], dim=1)
        else:
            # Transfer only specified attributes
            attr_vectors = []
            for attr in style_attributes:
                if attr == StyleAttribute.COLOR:
                    attr_vectors.append(style_attrs['color'])
                elif attr == StyleAttribute.PATTERN:
                    attr_vectors.append(style_attrs['pattern'])
                elif attr == StyleAttribute.SILHOUETTE:
                    attr_vectors.append(style_attrs['silhouette'])
                elif attr == StyleAttribute.MATERIAL:
                    attr_vectors.append(style_attrs['material'])
            style_vector = torch.cat(attr_vectors, dim=1)
        
        # Combine content + style
        combined = torch.cat([content_emb, style_vector], dim=1)
        transferred = self.fusion(combined)
        
        # Blend with original content based on intensity
        transferred = intensity * transferred + (1 - intensity) * content_emb
        
        return F.normalize(transferred, p=2, dim=1)

class VisualSearchSystem:
    """
    End-to-end visual search system
    
    Features:
    1. Image upload: Search by uploading photo
    2. Object detection: Isolate product from background
    3. Style attributes: Extract color, pattern, material
    4. Similar products: Find visually similar items
    5. Style transfer: "Find X with style of Y"
    """
    
    def __init__(self):
        self.visual_encoder = VisualEncoder(embedding_dim=512)
        self.style_extractor = StyleAttributeExtractor(attribute_dim=128)
        self.domain_adapter = CrossDomainAdapter(embedding_dim=512)
        self.style_transfer = StyleTransferEngine(embedding_dim=512)
        
        # Product index (simplified)
        self.product_embeddings = {}
        self.product_metadata = {}
    
    def search_by_image(
        self,
        query_image: torch.Tensor,
        top_k: int = 20,
        is_user_photo: bool = True
    ) -> List[Dict[str, Any]]:
        """
        Search for products matching uploaded image
        
        Args:
            query_image: [1, 3, H, W] uploaded image
            top_k: Number of results
            is_user_photo: Whether to apply domain adaptation
        
        Returns:
            List of matching products with similarity scores
        """
        with torch.no_grad():
            # Encode query image
            query_emb = self.visual_encoder(query_image)
            
            # Adapt to catalog domain if user photo
            if is_user_photo:
                query_emb = self.domain_adapter(query_emb)
            
            query_emb = query_emb.cpu().numpy()[0]
        
        # Find similar products (simplified: brute force)
        results = []
        for product_id, product_emb in self.product_embeddings.items():
            similarity = np.dot(query_emb, product_emb)
            results.append({
                'product_id': product_id,
                'similarity': float(similarity),
                'product': self.product_metadata[product_id]
            })
        
        # Sort by similarity
        results.sort(key=lambda x: x['similarity'], reverse=True)
        return results[:top_k]
    
    def search_with_style_transfer(
        self,
        content_product_id: str,
        style_image: torch.Tensor,
        style_attributes: List[StyleAttribute] = None,
        top_k: int = 20
    ) -> List[Dict[str, Any]]:
        """
        Find products like content_product with style from style_image
        
        Example: "Find jeans (content) that match this shirt (style)"
        """
        # Get content product image (simplified: assumes stored)
        content_image = torch.randn(1, 3, 224, 224)
        
        with torch.no_grad():
            # Generate transferred embedding
            transferred_emb = self.style_transfer.transfer_style(
                content_image,
                style_image,
                style_attributes=style_attributes,
                intensity=0.7
            )
            transferred_emb = transferred_emb.cpu().numpy()[0]
        
        # Search for products matching transferred embedding
        results = []
        for product_id, product_emb in self.product_embeddings.items():
            similarity = np.dot(transferred_emb, product_emb)
            results.append({
                'product_id': product_id,
                'similarity': float(similarity),
                'product': self.product_metadata[product_id]
            })
        
        results.sort(key=lambda x: x['similarity'], reverse=True)
        return results[:top_k]

def visual_search_example():
    """
    Demonstration of visual search and style transfer
    
    Scenarios:
    1. Upload photo: Find exact or similar products
    2. Style transfer: "Find jeans that match this shirt"
    3. Attribute search: "Find dress with this pattern but different color"
    """
    print("=== Visual Search and Style Transfer ===\n")
    
    system = VisualSearchSystem()
    
    # Scenario 1: Search by uploaded photo
    print("--- Scenario 1: Search by Uploaded Photo ---")
    print("User uploads photo of friend's dress from Instagram")
    print("Photo details:")
    print("  - Outdoor lighting, casual snapshot")
    print("  - Dress visible but not perfect angle")
    print("  - Background: coffee shop")
    print()
    
    user_photo = torch.randn(1, 3, 224, 224)
    results = system.search_by_image(user_photo, top_k=5, is_user_photo=True)
    
    print("Search results:")
    print("1. Floral Summer Dress (similarity: 0.89)")
    print("   Why: Similar pattern, color palette, casual style")
    print("   Price: $49.99")
    print()
    print("2. Garden Party Midi Dress (similarity: 0.84)")
    print("   Why: Matching floral print, similar silhouette")
    print("   Price: $59.99")
    print()
    print("3. Botanical Print Sundress (similarity: 0.81)")
    print("   Why: Similar colors and casual vibe")
    print("   Price: $44.99")
    print()
    
    # Scenario 2: Style transfer
    print("--- Scenario 2: Style Transfer Search ---")
    print("User wants: 'Find jeans that match this blue shirt'")
    print("Shirt details:")
    print("  - Navy blue")
    print("  - Casual button-down")
    print("  - Cotton material")
    print()
    
    style_image = torch.randn(1, 3, 224, 224)
    results = system.search_with_style_transfer(
        content_product_id="JEANS_BASE",
        style_image=style_image,
        style_attributes=[StyleAttribute.COLOR],
        top_k=5
    )
    
    print("Matching jeans:")
    print("1. Dark Wash Slim Fit Jeans (match: 0.92)")
    print("   Why: Navy-toned denim complements shirt blue")
    print("   Price: $79.99")
    print()
    print("2. Indigo Straight Leg Jeans (match: 0.88)")
    print("   Why: Similar blue tone, casual style")
    print("   Price: $69.99")
    print()
    print("3. Classic Blue Jeans (match: 0.85)")
    print("   Why: Color coordination, versatile fit")
    print("   Price: $59.99")
    print()
    
    # Scenario 3: Attribute-specific search
    print("--- Scenario 3: Attribute-Specific Search ---")
    print("User says: 'I love this dress pattern but want it in red'")
    print()
    
    print("Original dress: White with floral print")
    print()
    print("Results with same pattern, different color:")
    print("1. Red Floral Maxi Dress (match: 0.91)")
    print("   Why: Identical pattern, red base as requested")
    print("   Price: $54.99")
    print()
    print("2. Burgundy Floral Sundress (match: 0.87)")
    print("   Why: Similar pattern, darker red tone")
    print("   Price: $49.99")
    print()
    print("3. Rose Floral Midi Dress (match: 0.84)")
    print("   Why: Matching print, lighter red/pink")
    print("   Price: $59.99")
    print()
    
    print("--- System Performance ---")
    print("Image encoding: 45ms avg")
    print("Search latency: <100ms")
    print("Index: 5M products")
    print("Accuracy (user study):")
    print("  - Exact match found: 67%")
    print("  - Satisfactory alternative: 89%")
    print("  - No good match: 11%")
    print()
    print("Business impact:")
    print("- Visual search users convert 2.3x higher")
    print("- Average order value: +$23")
    print("- Browse time: +5 minutes")
    print("- Returns: -15% (better expectations)")
    print()
    print("→ Visual search transforms product discovery")

# Uncomment to run:
# visual_search_example()
```

:::{.callout-tip}
## Visual Search Best Practices

**Data preparation:**
- **Multi-view images**: Front, side, back, detail shots for each product
- **Consistent quality**: Standardize catalog photos (lighting, background, resolution)
- **User photo collection**: Gather real user-uploaded images for training
- **Data augmentation**: Vary lighting, angle, background for robustness
- **Object detection**: Annotate bounding boxes to focus on product

**Modeling:**
- **Pre-training**: ImageNet, fashion-specific datasets (DeepFashion)
- **Metric learning**: Triplet loss with hard negative mining
- **Multi-task**: Visual similarity + category + attributes
- **Domain adaptation**: Bridge user photos and catalog photos
- **Style disentanglement**: Separate color, pattern, shape, material

**Production:**
- **Mobile optimization**: Support various aspect ratios, low-resolution
- **Real-time encoding**: <200ms for uploaded images
- **Object detection**: Segment products from backgrounds
- **Privacy**: Process images securely, delete after encoding
- **Explainability**: Show matched attributes (color, pattern, style)

**Challenges:**
- **Lighting invariance**: Same product looks different in different lighting
- **Pose variation**: Products at different angles
- **Occlusion**: Partial views, items blocking each other
- **Background clutter**: User photos have distracting backgrounds
- **Cross-domain gap**: User photos vs professional catalog photos
:::
## Inventory Optimization

Retail inventory management faces the classic trade-off: overstock ties up capital and leads to markdowns, while stockouts lose sales and frustrate customers. **Embedding-based inventory optimization** learns demand patterns from product features, temporal signals, and market dynamics to forecast demand at SKU-region-week granularity, enabling optimal stock levels that balance holding costs and lost sales.

### The Inventory Challenge

Traditional inventory management faces limitations:

- **Cold start**: New products have no sales history
- **Seasonal patterns**: Complex seasonality (holidays, weather, trends)
- **Substitution effects**: Stockouts of product A drive sales of product B
- **Regional variation**: Same product, different demand by location
- **Promotion response**: How do discounts affect demand?
- **Long-tail**: 80% of SKUs have sparse, noisy demand signals

**Embedding approach**: Represent products as embeddings that encode attributes (category, brand, price, style), learn temporal embeddings of demand patterns, and model regional preferences. Similar products have similar demand curves; new products inherit forecast from similar items; promotion effects transfer across comparable SKUs.

```python
"""
Inventory Optimization with Demand Embeddings

Architecture:
1. Product encoder: SKU → embedding (attributes, historical demand)
2. Temporal encoder: Time series embedding (seasonality, trends)
3. Regional encoder: Location-specific demand patterns
4. External signals: Weather, events, competitor pricing
5. Demand forecaster: Product + time + region → demand prediction

Techniques:
- Transfer learning: Similar products share demand patterns
- Hierarchical forecasting: Category → brand → SKU
- Multi-task learning: Demand + stockout probability + markdowns
- Uncertainty quantification: Not just forecast, but confidence intervals
- Counterfactual reasoning: "What if we stock 20% more?"

Production considerations:
- Granularity: SKU × location × week = billions of forecasts
- Freshness: Update forecasts daily with latest sales
- Cold start: Immediate forecasts for new products
- Explainability: Why forecast changed, which factors matter
- Optimization: Forecast → stocking decision → order quantities
"""

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import List, Dict, Optional, Tuple, Any
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from enum import Enum

class DemandRegime(Enum):
    """Demand pattern categories"""
    STEADY = "steady"  # Consistent demand
    SEASONAL = "seasonal"  # Seasonal spikes
    TRENDING_UP = "trending_up"  # Growing demand
    TRENDING_DOWN = "trending_down"  # Declining demand
    VOLATILE = "volatile"  # Unpredictable
    LONG_TAIL = "long_tail"  # Sparse, intermittent

@dataclass
class InventoryItem:
    """
    SKU with inventory details
    
    Attributes:
        sku_id: Stock keeping unit identifier
        product_name: Product name
        category: Product category
        brand: Brand name
        price: Retail price
        cost: Acquisition cost
        attributes: Product features (size, color, material, etc.)
        historical_demand: Past sales [units per week]
        current_stock: Units in stock
        lead_time_weeks: Replenishment lead time
        holding_cost_per_week: Storage cost per unit per week
        stockout_cost: Lost profit per stockout
        markdown_risk: Probability of requiring discount
        supplier_reliability: Delivery reliability (0-1)
        embedding: Learned product embedding
    """
    sku_id: str
    product_name: str
    category: str
    brand: str
    price: float
    cost: float
    attributes: Dict[str, Any] = field(default_factory=dict)
    historical_demand: Optional[np.ndarray] = None
    current_stock: int = 0
    lead_time_weeks: int = 2
    holding_cost_per_week: float = 0.5
    stockout_cost: float = 10.0
    markdown_risk: float = 0.1
    supplier_reliability: float = 0.95
    embedding: Optional[np.ndarray] = None

@dataclass
class Location:
    """
    Store or warehouse location
    
    Attributes:
        location_id: Unique identifier
        location_type: Store, warehouse, distribution center
        region: Geographic region
        demographics: Customer demographics nearby
        weather: Typical weather patterns
        nearby_competitors: Competition level
        traffic: Foot traffic or web traffic
        embedding: Location embedding (regional preferences)
    """
    location_id: str
    location_type: str  # "store", "warehouse", "dc"
    region: str
    demographics: Dict[str, Any] = field(default_factory=dict)
    weather: Dict[str, Any] = field(default_factory=dict)
    nearby_competitors: int = 0
    traffic: float = 1000.0
    embedding: Optional[np.ndarray] = None

@dataclass
class DemandForecast:
    """
    Demand prediction for SKU-location-time
    
    Attributes:
        sku_id: Product identifier
        location_id: Location identifier
        forecast_date: Date being forecasted
        predicted_demand: Expected units sold
        confidence_interval: (lower, upper) bounds
        demand_regime: Categorized demand pattern
        contributing_factors: What drives this forecast
        stockout_probability: P(demand > current_stock)
        optimal_order_quantity: Recommended replenishment
    """
    sku_id: str
    location_id: str
    forecast_date: datetime
    predicted_demand: float
    confidence_interval: Tuple[float, float]
    demand_regime: DemandRegime
    contributing_factors: Dict[str, float]
    stockout_probability: float
    optimal_order_quantity: int

class ProductEncoder(nn.Module):
    """
    Encode products for demand forecasting
    
    Architecture:
    - Attribute encoder: Categorical + numerical features
    - Historical demand encoder: LSTM over past sales
    - Price sensitivity: Embedding includes price elasticity
    - Category hierarchy: Leverage category relationships
    
    Output: Product embedding capturing demand drivers
    """
    
    def __init__(
        self,
        num_categories=1000,
        num_brands=5000,
        embedding_dim=256
    ):
        super().__init__()
        self.embedding_dim = embedding_dim
        
        # Categorical embeddings
        self.category_emb = nn.Embedding(num_categories, 64)
        self.brand_emb = nn.Embedding(num_brands, 64)
        
        # Numerical features: price, cost, etc.
        self.numerical_proj = nn.Linear(10, 64)
        
        # Historical demand encoder (LSTM)
        self.demand_lstm = nn.LSTM(
            input_size=1,
            hidden_size=128,
            num_layers=2,
            batch_first=True,
            dropout=0.2
        )
        
        # Fusion
        self.fusion = nn.Sequential(
            nn.Linear(64 + 64 + 64 + 128, 512),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(512, embedding_dim)
        )
    
    def forward(
        self,
        category_ids: torch.Tensor,
        brand_ids: torch.Tensor,
        numerical_features: torch.Tensor,
        demand_history: torch.Tensor
    ) -> torch.Tensor:
        """
        Args:
            category_ids: [batch] category indices
            brand_ids: [batch] brand indices
            numerical_features: [batch, 10] price, cost, etc.
            demand_history: [batch, seq_len] historical sales
        
        Returns:
            embeddings: [batch, embedding_dim] product embeddings
        """
        # Categorical embeddings
        cat_emb = self.category_emb(category_ids)
        brand_emb = self.brand_emb(brand_ids)
        
        # Numerical features
        num_emb = self.numerical_proj(numerical_features)
        
        # Historical demand pattern
        demand_history = demand_history.unsqueeze(-1)  # [batch, seq_len, 1]
        _, (h_n, _) = self.demand_lstm(demand_history)
        demand_emb = h_n[-1]  # Last hidden state
        
        # Combine all
        combined = torch.cat([cat_emb, brand_emb, num_emb, demand_emb], dim=1)
        embedding = self.fusion(combined)
        
        return F.normalize(embedding, p=2, dim=1)

class TemporalEncoder(nn.Module):
    """
    Encode time-dependent patterns
    
    Captures:
    - Seasonality: Day of week, month, holidays
    - Trends: Long-term growth/decline
    - Events: Promotions, weather events, competitor actions
    - Regime changes: Sudden shifts in demand patterns
    
    Architecture:
    - Cyclical encoding: sin/cos for periodic patterns
    - Trend encoder: Linear/polynomial trends
    - Event embeddings: Holiday flags, promotion indicators
    """
    
    def __init__(self, embedding_dim=128):
        super().__init__()
        self.embedding_dim = embedding_dim
        
        # Cyclical features: day, week, month, year
        self.cyclical_proj = nn.Linear(8, 64)  # sin/cos pairs
        
        # Trend features
        self.trend_proj = nn.Linear(3, 32)  # Linear, quadratic, cubic
        
        # Event embeddings
        self.event_emb = nn.Embedding(100, 32)  # Various events
        
        # Fusion
        self.fusion = nn.Sequential(
            nn.Linear(64 + 32 + 32, embedding_dim),
            nn.ReLU()
        )
    
    def encode_cyclical(self, timestamps: torch.Tensor) -> torch.Tensor:
        """
        Encode periodic patterns with sin/cos
        
        Args:
            timestamps: [batch] unix timestamps
        Returns:
            cyclical: [batch, 8] sin/cos encoding
        """
        # Convert to various cycles (simplified)
        day_of_week = (timestamps % (7 * 24 * 3600)) / (7 * 24 * 3600)
        week_of_year = (timestamps % (52 * 7 * 24 * 3600)) / (52 * 7 * 24 * 3600)
        month = (timestamps % (12 * 30 * 24 * 3600)) / (12 * 30 * 24 * 3600)
        quarter = (timestamps % (4 * 90 * 24 * 3600)) / (4 * 90 * 24 * 3600)
        
        cyclical = torch.stack([
            torch.sin(2 * np.pi * day_of_week),
            torch.cos(2 * np.pi * day_of_week),
            torch.sin(2 * np.pi * week_of_year),
            torch.cos(2 * np.pi * week_of_year),
            torch.sin(2 * np.pi * month),
            torch.cos(2 * np.pi * month),
            torch.sin(2 * np.pi * quarter),
            torch.cos(2 * np.pi * quarter)
        ], dim=1)
        
        return cyclical
    
    def forward(
        self,
        timestamps: torch.Tensor,
        trends: torch.Tensor,
        event_ids: torch.Tensor
    ) -> torch.Tensor:
        """
        Args:
            timestamps: [batch] timestamps
            trends: [batch, 3] linear, quadratic, cubic trends
            event_ids: [batch] event indicators
        Returns:
            embeddings: [batch, embedding_dim] temporal embeddings
        """
        # Cyclical patterns
        cyclical = self.encode_cyclical(timestamps)
        cyclical_emb = self.cyclical_proj(cyclical)
        
        # Trends
        trend_emb = self.trend_proj(trends)
        
        # Events
        event_emb = self.event_emb(event_ids)
        
        # Combine
        combined = torch.cat([cyclical_emb, trend_emb, event_emb], dim=1)
        return self.fusion(combined)

class RegionalEncoder(nn.Module):
    """
    Encode location-specific demand patterns
    
    Different locations have different:
    - Demographics: Age, income, preferences
    - Climate: Weather affects product demand
    - Competition: Market saturation
    - Traffic: Foot/web traffic patterns
    
    Output: Location embedding capturing regional preferences
    """
    
    def __init__(self, num_locations=10000, embedding_dim=128):
        super().__init__()
        self.embedding_dim = embedding_dim
        
        # Location embedding (learned)
        self.location_emb = nn.Embedding(num_locations, 64)
        
        # Demographics encoder
        self.demo_proj = nn.Linear(10, 64)
        
        # Weather/climate encoder
        self.climate_proj = nn.Linear(5, 32)
        
        # Fusion
        self.fusion = nn.Sequential(
            nn.Linear(64 + 64 + 32, embedding_dim),
            nn.ReLU()
        )
    
    def forward(
        self,
        location_ids: torch.Tensor,
        demographics: torch.Tensor,
        climate: torch.Tensor
    ) -> torch.Tensor:
        """
        Args:
            location_ids: [batch] location indices
            demographics: [batch, 10] demographic features
            climate: [batch, 5] weather features
        Returns:
            embeddings: [batch, embedding_dim] location embeddings
        """
        loc_emb = self.location_emb(location_ids)
        demo_emb = self.demo_proj(demographics)
        climate_emb = self.climate_proj(climate)
        
        combined = torch.cat([loc_emb, demo_emb, climate_emb], dim=1)
        return self.fusion(combined)

class DemandForecaster(nn.Module):
    """
    Forecast demand from product, time, and location embeddings
    
    Architecture:
    - Multi-modal fusion: Product × time × location
    - Attention mechanism: Learn which factors matter when
    - Uncertainty: Predict mean and variance
    - Multi-horizon: Forecast 1-week, 4-week, 13-week simultaneously
    
    Output:
    - Point forecast: Expected demand
    - Confidence interval: Uncertainty bounds
    - Regime classification: Demand pattern type
    """
    
    def __init__(self, embedding_dim=256):
        super().__init__()
        self.embedding_dim = embedding_dim
        
        self.product_encoder = ProductEncoder(embedding_dim=embedding_dim)
        self.temporal_encoder = TemporalEncoder(embedding_dim=128)
        self.regional_encoder = RegionalEncoder(embedding_dim=128)
        
        # Attention fusion: which factors matter for this forecast?
        self.attention = nn.MultiheadAttention(
            embed_dim=embedding_dim + 128 + 128,
            num_heads=8,
            batch_first=True
        )
        
        # Forecast heads
        self.demand_head = nn.Sequential(
            nn.Linear(embedding_dim + 128 + 128, 512),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Linear(256, 1)
        )
        
        # Uncertainty head (predict log variance)
        self.uncertainty_head = nn.Sequential(
            nn.Linear(embedding_dim + 128 + 128, 256),
            nn.ReLU(),
            nn.Linear(256, 1)
        )
        
        # Regime classifier
        self.regime_head = nn.Sequential(
            nn.Linear(embedding_dim + 128 + 128, 256),
            nn.ReLU(),
            nn.Linear(256, len(DemandRegime))
        )
    
    def forward(
        self,
        # Product inputs
        category_ids: torch.Tensor,
        brand_ids: torch.Tensor,
        numerical_features: torch.Tensor,
        demand_history: torch.Tensor,
        # Temporal inputs
        timestamps: torch.Tensor,
        trends: torch.Tensor,
        event_ids: torch.Tensor,
        # Regional inputs
        location_ids: torch.Tensor,
        demographics: torch.Tensor,
        climate: torch.Tensor
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        Returns:
            demand_forecast: [batch, 1] predicted demand
            uncertainty: [batch, 1] prediction std deviation
            regime_logits: [batch, num_regimes] regime classification
        """
        # Encode each modality
        product_emb = self.product_encoder(
            category_ids, brand_ids, numerical_features, demand_history
        )
        temporal_emb = self.temporal_encoder(timestamps, trends, event_ids)
        regional_emb = self.regional_encoder(location_ids, demographics, climate)
        
        # Concatenate embeddings
        combined = torch.cat([product_emb, temporal_emb, regional_emb], dim=1)
        combined = combined.unsqueeze(1)  # [batch, 1, total_dim]
        
        # Self-attention (learn importance of each factor)
        attended, _ = self.attention(combined, combined, combined)
        attended = attended.squeeze(1)  # [batch, total_dim]
        
        # Predictions
        demand = self.demand_head(attended)
        demand = F.relu(demand)  # Demand is non-negative
        
        log_variance = self.uncertainty_head(attended)
        uncertainty = torch.exp(0.5 * log_variance)  # Convert to std dev
        
        regime_logits = self.regime_head(attended)
        
        return demand, uncertainty, regime_logits

class InventoryOptimizer:
    """
    Optimize inventory decisions using demand forecasts
    
    Optimization:
    - Minimize: holding costs + stockout costs + markdown costs
    - Subject to: storage constraints, budget constraints
    - Consider: lead times, supplier reliability, seasonality
    
    Decision variables:
    - Order quantity: How much to order now
    - Reorder point: When to place next order
    - Safety stock: Buffer against uncertainty
    """
    
    def __init__(self, forecaster: DemandForecaster):
        self.forecaster = forecaster
    
    def compute_optimal_order(
        self,
        item: InventoryItem,
        location: Location,
        forecast_horizon_weeks: int = 8
    ) -> DemandForecast:
        """
        Compute optimal order quantity for SKU at location
        
        Uses forecast demand, lead time, holding costs, and stockout costs
        to determine order quantity minimizing expected total cost.
        """
        # Generate forecast (simplified: dummy data)
        with torch.no_grad():
            category_id = torch.tensor([hash(item.category) % 1000])
            brand_id = torch.tensor([hash(item.brand) % 5000])
            numerical = torch.randn(1, 10)
            demand_hist = torch.tensor([item.historical_demand[:52]], dtype=torch.float32)
            
            timestamp = torch.tensor([datetime.now().timestamp()])
            trends = torch.randn(1, 3)
            event_id = torch.zeros(1, dtype=torch.long)
            
            location_id = torch.tensor([hash(location.location_id) % 10000])
            demographics = torch.randn(1, 10)
            climate = torch.randn(1, 5)
            
            demand, uncertainty, regime_logits = self.forecaster(
                category_id, brand_id, numerical, demand_hist,
                timestamp, trends, event_id,
                location_id, demographics, climate
            )
            
            predicted_demand = float(demand[0, 0])
            predicted_std = float(uncertainty[0, 0])
            regime_probs = F.softmax(regime_logits[0], dim=0)
            regime = DemandRegime(list(DemandRegime)[torch.argmax(regime_probs).item()].value)
        
        # Confidence interval (95%)
        confidence_interval = (
            max(0, predicted_demand - 1.96 * predicted_std),
            predicted_demand + 1.96 * predicted_std
        )
        
        # Stockout probability
        # P(demand > current_stock) = P(Z > (stock - mean) / std)
        if predicted_std > 0:
            z_score = (item.current_stock - predicted_demand) / predicted_std
            stockout_prob = 1 - 0.5 * (1 + np.tanh(z_score / np.sqrt(2)))
        else:
            stockout_prob = 0.0 if item.current_stock >= predicted_demand else 1.0
        
        # Optimal order quantity (Economic Order Quantity + safety stock)
        # Account for lead time, demand uncertainty, holding costs
        lead_time_demand = predicted_demand * item.lead_time_weeks
        safety_stock = 1.96 * predicted_std * np.sqrt(item.lead_time_weeks)
        
        target_stock = lead_time_demand + safety_stock
        optimal_order = max(0, int(target_stock - item.current_stock))
        
        # Contributing factors
        factors = {
            'base_demand': 0.6,
            'seasonality': 0.2,
            'trend': 0.1,
            'regional_preference': 0.1
        }
        
        return DemandForecast(
            sku_id=item.sku_id,
            location_id=location.location_id,
            forecast_date=datetime.now() + timedelta(weeks=1),
            predicted_demand=predicted_demand,
            confidence_interval=confidence_interval,
            demand_regime=regime,
            contributing_factors=factors,
            stockout_probability=stockout_prob,
            optimal_order_quantity=optimal_order
        )

def inventory_optimization_example():
    """
    Demonstration of embedding-based inventory optimization
    
    Scenarios:
    1. Cold start: New product with no history
    2. Seasonal spike: Anticipate holiday demand
    3. Regional variation: Same product, different locations
    """
    print("=== Inventory Optimization with Demand Embeddings ===\n")
    
    # Initialize system
    forecaster = DemandForecaster(embedding_dim=256)
    optimizer = InventoryOptimizer(forecaster)
    
    # Scenario 1: Cold start product
    print("--- Scenario 1: Cold Start Product ---")
    new_item = InventoryItem(
        sku_id="NEW001",
        product_name="Trendy Summer Sandals",
        category="Footwear",
        brand="FashionForward",
        price=79.99,
        cost=35.00,
        attributes={"color": "coral", "style": "casual", "season": "summer"},
        historical_demand=np.zeros(52),  # No history!
        current_stock=100,
        lead_time_weeks=3,
        holding_cost_per_week=0.50,
        stockout_cost=15.00
    )
    
    location1 = Location(
        location_id="STORE_MIAMI",
        location_type="store",
        region="Southeast",
        demographics={"avg_age": 32, "income": "medium-high"},
        weather={"avg_temp": 85, "rainfall": "low"}
    )
    
    print(f"New product: {new_item.product_name}")
    print(f"No sales history, but similar products available:")
    print(f"  - Category: {new_item.category} (thousands of SKUs)")
    print(f"  - Brand: {new_item.brand} (established brand with history)")
    print(f"  - Attributes: Summer, casual, trendy (similar items exist)")
    print()
    
    forecast1 = optimizer.compute_optimal_order(new_item, location1)
    print(f"Demand forecast (week 1):")
    print(f"  Predicted: {forecast1.predicted_demand:.1f} units")
    print(f"  95% CI: [{forecast1.confidence_interval[0]:.1f}, {forecast1.confidence_interval[1]:.1f}]")
    print(f"  Regime: {forecast1.demand_regime.value}")
    print(f"  Stockout risk: {forecast1.stockout_probability:.1%}")
    print()
    print(f"Inventory decision:")
    print(f"  Current stock: {new_item.current_stock} units")
    print(f"  Recommended order: {forecast1.optimal_order_quantity} units")
    print(f"  Rationale:")
    print(f"    - Transfer learning from similar sandals (coral trend popular)")
    print(f"    - Miami location: high summer footwear demand")
    print(f"    - Safety stock: Account for new product uncertainty")
    print()
    
    # Scenario 2: Seasonal spike
    print("--- Scenario 2: Seasonal Spike Prediction ---")
    seasonal_item = InventoryItem(
        sku_id="WINTER001",
        product_name="Wool Blend Winter Coat",
        category="Outerwear",
        brand="WarmWear",
        price=199.99,
        cost=85.00,
        attributes={"material": "wool", "season": "winter", "weight": "heavy"},
        historical_demand=np.array([
            # Weekly sales: low in summer, spike in fall/winter
            2, 1, 1, 2, 1, 1, 2, 2,  # Summer
            3, 4, 5, 8, 12, 18, 25, 35,  # Fall ramp-up
            45, 52, 48, 50, 55, 60, 58, 62,  # Winter peak
            54, 48, 40, 32, 25, 18, 12, 8,  # Spring decline
            5, 3, 2, 2, 1, 1, 1, 2,  # Early summer
        ] + [1] * 16),  # Continuation
        current_stock=150,
        lead_time_weeks=4,
        holding_cost_per_week=1.50,
        stockout_cost=50.00
    )
    
    location2 = Location(
        location_id="STORE_BOSTON",
        location_type="store",
        region="Northeast",
        demographics={"avg_age": 38, "income": "high"},
        weather={"avg_temp": 42, "rainfall": "medium"}
    )
    
    print(f"Product: {seasonal_item.product_name}")
    print(f"Historical pattern: Strong winter seasonality")
    print(f"Current date: Early November (pre-winter spike)")
    print()
    
    forecast2 = optimizer.compute_optimal_order(seasonal_item, location2)
    print(f"Demand forecast (next 4 weeks):")
    print(f"  Week 1: ~35 units (initial ramp-up)")
    print(f"  Week 2: ~48 units (accelerating)")
    print(f"  Week 3: ~55 units (peak approaching)")
    print(f"  Week 4: ~60 units (peak season)")
    print(f"  95% CI: [{forecast2.confidence_interval[0]:.1f}, {forecast2.confidence_interval[1]:.1f}]")
    print()
    print(f"Inventory decision:")
    print(f"  Current stock: {seasonal_item.current_stock} units")
    print(f"  Forecasted demand (4 weeks): ~200 units")
    print(f"  Recommended order: 250 units")
    print(f"  Rationale:")
    print(f"    - Historical: November-January = 70% of annual sales")
    print(f"    - Lead time: 4 weeks (must order now for peak)")
    print(f"    - Safety stock: High demand volatility, stockout very costly")
    print(f"    - Risk: Better to have 10% overstock than 1% stockout")
    print()
    
    # Scenario 3: Regional variation
    print("--- Scenario 3: Regional Demand Variation ---")
    item = InventoryItem(
        sku_id="UNIVERSAL001",
        product_name="Classic White T-Shirt",
        category="Basics",
        brand="Essentials",
        price=19.99,
        cost=5.00,
        attributes={"color": "white", "style": "basic", "fit": "regular"},
        historical_demand=np.random.poisson(25, 52),
        current_stock=200,
        lead_time_weeks=2
    )
    
    locations = [
        Location("NYC_MANHATTAN", "store", "Northeast", {"density": "very_high"}),
        Location("SUBURBAN_TX", "store", "South", {"density": "low"}),
        Location("LA_VENICE", "store", "West", {"density": "high"})
    ]
    
    print(f"Product: {item.product_name}")
    print(f"Same SKU, three different locations:")
    print()
    
    for loc in locations:
        forecast = optimizer.compute_optimal_order(item, loc)
        print(f"{loc.location_id}:")
        print(f"  Forecast: {forecast.predicted_demand:.1f} units/week")
        print(f"  Order: {forecast.optimal_order_quantity} units")
        print(f"  Factors:")
        for factor, weight in forecast.contributing_factors.items():
            print(f"    - {factor}: {weight:.1%}")
        print()
    
    print("Regional insights:")
    print("  - NYC: High density → higher baseline demand")
    print("  - Texas: Lower density but larger area → moderate demand")
    print("  - LA: Beach location → slightly higher (casual style)")
    print("  → Same product, different inventory strategies")
    print()
    
    print("--- System Performance ---")
    print("Forecast granularity: SKU × location × week")
    print("Number of forecasts: 500K SKUs × 2K locations × 12 weeks = 12B")
    print("Update frequency: Daily (overnight batch)")
    print("Accuracy:")
    print("  - MAPE (mean absolute % error): 18.5%")
    print("  - Bias: -2.3% (slight under-forecast)")
    print("  - Cold start accuracy: 28% vs 45% traditional")
    print()
    print("Business impact:")
    print("  - Stockouts: -35% (from 8% to 5.2%)")
    print("  - Overstock: -28% (from $50M to $36M)")
    print("  - Working capital: -$14M tied up in inventory")
    print("  - Lost sales recovered: +$8M annually")
    print("  - Markdown rate: -4.2 percentage points")
    print()
    print("→ Better forecasts = optimal inventory = higher profitability")

# Uncomment to run:
# inventory_optimization_example()
```

:::{.callout-tip}
## Inventory Optimization Best Practices

**Data preparation:**
- **Historical demand**: Clean sales data (remove stockouts, promotions)
- **Product hierarchy**: Category → subcategory → brand → SKU
- **External factors**: Weather, events, competitor pricing, trends
- **Regional data**: Demographics, store traffic, local preferences
- **Supply chain**: Lead times, supplier reliability, minimum order quantities

**Modeling:**
- **Transfer learning**: Similar products share demand patterns
- **Hierarchical forecasting**: Top-down (category) + bottom-up (SKU)
- **Multi-task**: Demand + stockout probability + markdown risk
- **Uncertainty quantification**: Prediction intervals, not just point estimates
- **Regime detection**: Identify demand pattern changes (trending, seasonal)

**Production:**
- **Scale**: Millions of SKUs × thousands of locations
- **Freshness**: Daily forecast updates with latest sales
- **Cold start**: Immediate forecasts for new products
- **Explainability**: Why forecast changed, which factors matter
- **Integration**: Forecasts → ordering systems → fulfillment

**Challenges:**
- **Sparse demand**: Long-tail SKUs have intermittent sales
- **Promotion effects**: Discounts create demand spikes
- **Substitution**: Stockouts shift demand to alternatives
- **Cannibalization**: New products steal sales from existing
- **Bullwhip effect**: Demand variability amplifies upstream
:::
## Customer Journey Analysis

E-commerce customer journeys involve dozens of touchpoints across channels (web, mobile, email, ads) before conversion. **Embedding-based customer journey analysis** represents sessions, user actions, and customer states as vectors, enabling identification of conversion patterns, friction points, and optimal intervention moments for hyper-personalized experiences.

### The Customer Journey Challenge

Traditional journey analytics face limitations:

- **High dimensionality**: Thousands of possible page sequences, product views, interactions
- **Variable length**: Journeys range from single visit to months of browsing
- **Multi-channel**: Users switch between devices, channels mid-journey
- **Individual variation**: No two customers follow same path
- **Causality**: Did email cause purchase or coincide with intent?
- **Real-time personalization**: Must predict next action in <50ms

**Embedding approach**: Learn sequential embeddings where customer states evolve through session history, similar journey patterns cluster together, and distance to conversion embedding predicts purchase probability. Enables real-time journey stage detection and micro-moment personalization based on implicit signals.

```python
"""
Customer Journey Analysis with Sequential Embeddings

Architecture:
1. Session encoder: LSTM/Transformer over user actions
2. Product interaction encoder: View, cart, purchase embeddings
3. Journey stage classifier: Browse, consider, decide, convert
4. Friction detector: Identify abandonment risk signals
5. Next action predictor: Recommend optimal intervention

Techniques:
- Sequential modeling: RNN/Transformer over timestamped events
- Contrastive learning: Converting vs abandoning journeys
- Attention: Which past actions predict future behavior
- Multi-task: Conversion + time-to-convert + next action
- Transfer learning: Similar products = similar journey patterns

Production considerations:
- Real-time: <50ms to encode session and predict
- Streaming: Update embeddings as actions arrive
- Personalization: Journey stage → content/offers
- Attribution: Which touchpoints contributed to conversion
- Privacy: Handle PII, GDPR compliance
"""

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import List, Dict, Optional, Tuple, Any, Set
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from enum import Enum
import json

class ActionType(Enum):
    """User action types"""
    PAGE_VIEW = "page_view"
    PRODUCT_VIEW = "product_view"
    CATEGORY_BROWSE = "category_browse"
    SEARCH = "search"
    ADD_TO_CART = "add_to_cart"
    REMOVE_FROM_CART = "remove_from_cart"
    ADD_TO_WISHLIST = "add_to_wishlist"
    REVIEW_READ = "review_read"
    IMAGE_ZOOM = "image_zoom"
    SIZE_GUIDE_VIEW = "size_guide_view"
    CHECKOUT_START = "checkout_start"
    PAYMENT_INFO = "payment_info"
    PURCHASE = "purchase"
    EMAIL_OPEN = "email_open"
    EMAIL_CLICK = "email_click"
    AD_CLICK = "ad_click"

class JourneyStage(Enum):
    """Customer journey stages"""
    AWARENESS = "awareness"  # Just discovered site/products
    CONSIDERATION = "consideration"  # Browsing, comparing
    INTENT = "intent"  # Added to cart, high interest
    PURCHASE = "purchase"  # Completed transaction
    LOYALTY = "loyalty"  # Repeat customer

@dataclass
class UserAction:
    """
    Single user action/event
    
    Attributes:
        action_id: Unique action identifier
        user_id: User performing action
        session_id: Session identifier
        action_type: Type of action
        timestamp: When action occurred
        product_id: Product involved (if applicable)
        metadata: Additional context (page URL, search query, etc.)
        duration: Time spent (seconds)
        device: Mobile, desktop, tablet
        channel: Web, app, email, ad
    """
    action_id: str
    user_id: str
    session_id: str
    action_type: ActionType
    timestamp: datetime
    product_id: Optional[str] = None
    metadata: Dict[str, Any] = field(default_factory=dict)
    duration: float = 0.0
    device: str = "desktop"
    channel: str = "web"

@dataclass
class CustomerSession:
    """
    User session with action sequence
    
    Attributes:
        session_id: Unique session identifier
        user_id: User identifier
        actions: Sequence of user actions
        start_time: Session start
        end_time: Session end (or None if ongoing)
        converted: Whether session resulted in purchase
        revenue: Revenue if converted
        cart_value: Current cart value
        viewed_products: Set of viewed products
        journey_stage: Classified journey stage
        embedding: Learned session embedding
    """
    session_id: str
    user_id: str
    actions: List[UserAction] = field(default_factory=list)
    start_time: Optional[datetime] = None
    end_time: Optional[datetime] = None
    converted: bool = False
    revenue: float = 0.0
    cart_value: float = 0.0
    viewed_products: Set[str] = field(default_factory=set)
    journey_stage: Optional[JourneyStage] = None
    embedding: Optional[np.ndarray] = None

@dataclass
class JourneyInsight:
    """
    Insights from journey analysis
    
    Attributes:
        user_id: User identifier
        session_id: Current session
        journey_stage: Current stage
        conversion_probability: Likelihood to convert (0-1)
        time_to_conversion: Expected time until purchase (seconds)
        friction_points: Detected abandonment risks
        recommended_actions: Suggested interventions
        similar_journeys: Comparable customer paths
        next_likely_action: Predicted next action
    """
    user_id: str
    session_id: str
    journey_stage: JourneyStage
    conversion_probability: float
    time_to_conversion: float
    friction_points: List[str]
    recommended_actions: List[str]
    similar_journeys: List[str]
    next_likely_action: Optional[ActionType] = None

class ActionEncoder(nn.Module):
    """
    Encode user actions to embeddings
    
    Each action embedded as:
    - Action type (view, cart, purchase)
    - Product involved (if any)
    - Temporal context (time since last action)
    - Channel/device context
    """
    
    def __init__(
        self,
        num_action_types=20,
        num_products=1000000,
        embedding_dim=128
    ):
        super().__init__()
        self.embedding_dim = embedding_dim
        
        # Action type embedding
        self.action_type_emb = nn.Embedding(num_action_types, 64)
        
        # Product embedding (shared with product search)
        self.product_emb = nn.Embedding(num_products, 64)
        
        # Temporal features
        self.temporal_proj = nn.Linear(5, 32)  # Time features
        
        # Context features (device, channel)
        self.context_proj = nn.Linear(10, 32)
        
        # Fusion
        self.fusion = nn.Sequential(
            nn.Linear(64 + 64 + 32 + 32, embedding_dim),
            nn.ReLU()
        )
    
    def forward(
        self,
        action_types: torch.Tensor,
        product_ids: torch.Tensor,
        temporal_features: torch.Tensor,
        context_features: torch.Tensor
    ) -> torch.Tensor:
        """
        Args:
            action_types: [batch] action type indices
            product_ids: [batch] product indices (0 if no product)
            temporal_features: [batch, 5] time since last action, etc.
            context_features: [batch, 10] device, channel, etc.
        
        Returns:
            embeddings: [batch, embedding_dim] action embeddings
        """
        action_emb = self.action_type_emb(action_types)
        product_emb = self.product_emb(product_ids)
        temporal_emb = self.temporal_proj(temporal_features)
        context_emb = self.context_proj(context_features)
        
        combined = torch.cat([
            action_emb, product_emb, temporal_emb, context_emb
        ], dim=1)
        
        return self.fusion(combined)

class SessionEncoder(nn.Module):
    """
    Encode session history to embedding
    
    Architecture:
    - Action encoder: Map each action to embedding
    - Sequential model: LSTM/Transformer over action sequence
    - Attention: Learn which past actions matter most
    - Output: Session embedding capturing current state
    
    Training:
    - Contrastive: Converting sessions closer than abandoning
    - Predictive: Embedding predicts next action, conversion
    - Multi-task: Journey stage, conversion, time-to-convert
    """
    
    def __init__(self, action_encoder: ActionEncoder, embedding_dim=256):
        super().__init__()
        self.action_encoder = action_encoder
        self.embedding_dim = embedding_dim
        
        # Sequential encoder (LSTM)
        self.lstm = nn.LSTM(
            input_size=action_encoder.embedding_dim,
            hidden_size=embedding_dim,
            num_layers=2,
            batch_first=True,
            dropout=0.2
        )
        
        # Self-attention over actions
        self.attention = nn.MultiheadAttention(
            embed_dim=embedding_dim,
            num_heads=8,
            batch_first=True
        )
        
    def forward(
        self,
        action_embeddings: torch.Tensor,
        sequence_lengths: Optional[torch.Tensor] = None
    ) -> torch.Tensor:
        """
        Args:
            action_embeddings: [batch, max_seq_len, action_dim]
            sequence_lengths: [batch] actual sequence lengths
        
        Returns:
            session_embeddings: [batch, embedding_dim]
        """
        # LSTM encoding
        if sequence_lengths is not None:
            # Pack padded sequences
            packed = nn.utils.rnn.pack_padded_sequence(
                action_embeddings,
                sequence_lengths.cpu(),
                batch_first=True,
                enforce_sorted=False
            )
            lstm_out, (h_n, c_n) = self.lstm(packed)
            lstm_out, _ = nn.utils.rnn.pad_packed_sequence(lstm_out, batch_first=True)
        else:
            lstm_out, (h_n, c_n) = self.lstm(action_embeddings)
        
        # Attention pooling
        attended, _ = self.attention(lstm_out, lstm_out, lstm_out)
        
        # Combine last hidden state and attention
        session_emb = (h_n[-1] + attended.mean(dim=1)) / 2
        
        return F.normalize(session_emb, p=2, dim=1)

class JourneyAnalyzer(nn.Module):
    """
    Analyze customer journey and predict outcomes
    
    Predictions:
    1. Journey stage: Which stage is customer in?
    2. Conversion probability: Will customer purchase?
    3. Time to conversion: How long until purchase?
    4. Next action: What will customer do next?
    5. Friction detection: Is customer about to abandon?
    
    Architecture:
    - Session encoder: History → current state embedding
    - Multi-head prediction: Multiple outcomes from embedding
    """
    
    def __init__(self, embedding_dim=256):
        super().__init__()
        self.embedding_dim = embedding_dim
        
        self.action_encoder = ActionEncoder(embedding_dim=128)
        self.session_encoder = SessionEncoder(
            self.action_encoder,
            embedding_dim=embedding_dim
        )
        
        # Journey stage classifier
        self.stage_classifier = nn.Sequential(
            nn.Linear(embedding_dim, 128),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(128, len(JourneyStage))
        )
        
        # Conversion probability predictor
        self.conversion_predictor = nn.Sequential(
            nn.Linear(embedding_dim, 128),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(128, 1),
            nn.Sigmoid()
        )
        
        # Time-to-conversion predictor
        self.time_predictor = nn.Sequential(
            nn.Linear(embedding_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 1),
            nn.Softplus()  # Non-negative
        )
        
        # Next action predictor
        self.next_action_predictor = nn.Sequential(
            nn.Linear(embedding_dim, 128),
            nn.ReLU(),
            nn.Linear(128, len(ActionType))
        )
        
        # Friction detector (binary: at risk or not)
        self.friction_detector = nn.Sequential(
            nn.Linear(embedding_dim, 128),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(128, 1),
            nn.Sigmoid()
        )
    
    def forward(
        self,
        action_embeddings: torch.Tensor,
        sequence_lengths: Optional[torch.Tensor] = None
    ) -> Dict[str, torch.Tensor]:
        """
        Returns:
            predictions: Dict with journey stage, conversion prob, etc.
        """
        # Encode session
        session_emb = self.session_encoder(action_embeddings, sequence_lengths)
        
        # Multiple predictions
        return {
            'stage_logits': self.stage_classifier(session_emb),
            'conversion_prob': self.conversion_predictor(session_emb),
            'time_to_convert': self.time_predictor(session_emb),
            'next_action_logits': self.next_action_predictor(session_emb),
            'friction_score': self.friction_detector(session_emb),
            'embedding': session_emb
        }

class HyperPersonalizationEngine:
    """
    Real-time hyper-personalization based on journey analysis
    
    Personalization dimensions:
    1. Content: Products, banners, copy
    2. Layout: Page structure, element prominence
    3. Pricing: Discounts, promotions
    4. Timing: When to show interventions
    5. Channel: Email, push, on-site
    
    Hyper-personalization: Individual-level real-time adaptation
    based on current session state, not segment averages.
    """
    
    def __init__(self, analyzer: JourneyAnalyzer):
        self.analyzer = analyzer
        
        # Intervention strategies
        self.interventions = {
            'high_intent_low_conversion': [
                'Show limited-time discount',
                'Display trust badges',
                'Free shipping offer',
                'Live chat prompt'
            ],
            'browsing_high_engagement': [
                'Recommend similar products',
                'Show trending items in category',
                'Curate collection based on views'
            ],
            'cart_abandonment_risk': [
                'Save cart reminder',
                'Price drop alert',
                'Stock scarcity notification',
                'Free returns emphasized'
            ],
            'first_time_visitor': [
                'Welcome discount',
                'Product tour',
                'Popular items showcase'
            ],
            'return_customer': [
                'Welcome back message',
                'New arrivals since last visit',
                'Replenishment suggestions',
                'Loyalty points reminder'
            ]
        }
    
    def analyze_realtime(
        self,
        session: CustomerSession
    ) -> JourneyInsight:
        """
        Analyze session in real-time and generate insights
        
        Args:
            session: Current customer session with action history
        
        Returns:
            insights: Journey insights with recommendations
        """
        # Encode session (simplified: using dummy data)
        with torch.no_grad():
            # In production: encode actual action sequence
            dummy_actions = torch.randn(1, len(session.actions), 128)
            seq_lengths = torch.tensor([len(session.actions)])
            
            predictions = self.analyzer(dummy_actions, seq_lengths)
            
            # Extract predictions
            stage_probs = F.softmax(predictions['stage_logits'][0], dim=0)
            journey_stage = JourneyStage(
                list(JourneyStage)[torch.argmax(stage_probs).item()].value
            )
            
            conversion_prob = float(predictions['conversion_prob'][0, 0])
            time_to_convert = float(predictions['time_to_convert'][0, 0])
            friction_score = float(predictions['friction_score'][0, 0])
            
            next_action_probs = F.softmax(predictions['next_action_logits'][0], dim=0)
            next_action = ActionType(
                list(ActionType)[torch.argmax(next_action_probs).item()].value
            )
        
        # Detect friction points
        friction_points = []
        if friction_score > 0.7:
            friction_points.append("High abandonment risk detected")
        if session.cart_value > 0 and len(session.actions) > 10:
            last_actions = [a.action_type for a in session.actions[-5:]]
            if ActionType.ADD_TO_CART not in last_actions:
                friction_points.append("Cart sitting idle")
        if len(session.viewed_products) > 5 and session.cart_value == 0:
            friction_points.append("Browsing without commitment")
        
        # Recommend actions based on state
        recommended_actions = self._recommend_interventions(
            journey_stage, conversion_prob, friction_score, session
        )
        
        # Find similar journeys (simplified)
        similar_journeys = [
            "Session_12345 (converted, similar product interest)",
            "Session_67890 (browsing pattern match)",
            "Session_11223 (same journey stage)"
        ]
        
        return JourneyInsight(
            user_id=session.user_id,
            session_id=session.session_id,
            journey_stage=journey_stage,
            conversion_probability=conversion_prob,
            time_to_conversion=time_to_convert,
            friction_points=friction_points,
            recommended_actions=recommended_actions,
            similar_journeys=similar_journeys,
            next_likely_action=next_action
        )
    
    def _recommend_interventions(
        self,
        stage: JourneyStage,
        conversion_prob: float,
        friction_score: float,
        session: CustomerSession
    ) -> List[str]:
        """Recommend personalized interventions"""
        recommendations = []
        
        # High intent but not converting
        if stage == JourneyStage.INTENT and conversion_prob < 0.5:
            recommendations.extend(self.interventions['high_intent_low_conversion'])
        
        # Cart abandonment risk
        if session.cart_value > 0 and friction_score > 0.6:
            recommendations.extend(self.interventions['cart_abandonment_risk'])
        
        # High engagement browsing
        if stage == JourneyStage.CONSIDERATION and len(session.viewed_products) > 3:
            recommendations.extend(self.interventions['browsing_high_engagement'])
        
        # First time vs returning
        # (Simplified: check number of previous sessions)
        if len(session.actions) < 5:
            recommendations.extend(self.interventions['first_time_visitor'][:1])
        else:
            recommendations.extend(self.interventions['return_customer'][:1])
        
        return recommendations[:3]  # Top 3 recommendations
    
    def personalize_experience(
        self,
        session: CustomerSession,
        context: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Generate hyper-personalized experience
        
        Returns: Personalized content, layout, offers
        """
        insight = self.analyze_realtime(session)
        
        personalization = {
            'hero_banner': self._select_hero_banner(insight, context),
            'product_recommendations': self._recommend_products(insight, session),
            'discount_offer': self._determine_offer(insight, session),
            'urgency_messages': self._generate_urgency(insight),
            'layout_priority': self._adjust_layout(insight),
            'next_best_action': insight.recommended_actions
        }
        
        return personalization
    
    def _select_hero_banner(
        self,
        insight: JourneyInsight,
        context: Dict[str, Any]
    ) -> str:
        """Select personalized hero banner"""
        if insight.journey_stage == JourneyStage.AWARENESS:
            return "New Arrival Showcase"
        elif insight.journey_stage == JourneyStage.CONSIDERATION:
            return "Curated Collection Based on Your Browsing"
        elif insight.journey_stage == JourneyStage.INTENT:
            return "Complete Your Look - Cart Suggestions"
        else:
            return "Welcome Back - New Items You'll Love"
    
    def _recommend_products(
        self,
        insight: JourneyInsight,
        session: CustomerSession
    ) -> List[str]:
        """Generate hyper-personalized product recommendations"""
        # Based on journey stage and viewed products
        if session.cart_value > 0:
            return [
                "Complementary items for cart contents",
                "Complete the outfit",
                "Customers also bought"
            ]
        elif len(session.viewed_products) > 0:
            return [
                "Similar to what you viewed",
                "Alternative options",
                "In the same style"
            ]
        else:
            return [
                "Trending now",
                "Popular in your region",
                "Based on your profile"
            ]
    
    def _determine_offer(
        self,
        insight: JourneyInsight,
        session: CustomerSession
    ) -> Optional[Dict[str, Any]]:
        """Determine personalized discount offer"""
        # High intent but friction = offer discount
        if (insight.journey_stage == JourneyStage.INTENT and
            insight.conversion_probability < 0.4):
            return {
                'type': '10% discount',
                'code': 'SAVE10',
                'expires_in': 3600,  # 1 hour
                'message': 'Special offer just for you!'
            }
        
        # Cart abandonment risk
        if session.cart_value > 100 and any('abandon' in fp for fp in insight.friction_points):
            return {
                'type': 'Free shipping',
                'threshold': None,
                'message': 'Free shipping on your order today!'
            }
        
        return None
    
    def _generate_urgency(self, insight: JourneyInsight) -> List[str]:
        """Generate urgency messages"""
        messages = []
        
        if insight.journey_stage == JourneyStage.INTENT:
            messages.append("Only 3 left in stock!")
            messages.append("15 people viewing this item")
        
        if insight.conversion_probability > 0.6:
            messages.append("Complete purchase in next 30 min for same-day shipping")
        
        return messages
    
    def _adjust_layout(self, insight: JourneyInsight) -> Dict[str, str]:
        """Adjust page layout based on journey stage"""
        if insight.journey_stage == JourneyStage.AWARENESS:
            return {
                'priority': 'discovery',
                'highlight': 'categories_and_trends'
            }
        elif insight.journey_stage == JourneyStage.CONSIDERATION:
            return {
                'priority': 'comparison',
                'highlight': 'product_details_and_reviews'
            }
        elif insight.journey_stage == JourneyStage.INTENT:
            return {
                'priority': 'conversion',
                'highlight': 'trust_badges_and_checkout'
            }
        else:
            return {
                'priority': 'retention',
                'highlight': 'loyalty_and_new_arrivals'
            }

def customer_journey_example():
    """
    Demonstration of journey analysis and hyper-personalization
    """
    print("=== Customer Journey Analysis & Hyper-Personalization ===\n")
    
    analyzer = JourneyAnalyzer(embedding_dim=256)
    personalization_engine = HyperPersonalizationEngine(analyzer)
    
    # Scenario 1: High-intent customer with friction
    print("--- Scenario 1: Cart Abandonment Risk ---")
    session1 = CustomerSession(
        session_id="SESS_001",
        user_id="USER_12345",
        actions=[
            UserAction("A1", "USER_12345", "SESS_001", ActionType.SEARCH, datetime.now(), metadata={'query': 'winter coat'}),
            UserAction("A2", "USER_12345", "SESS_001", ActionType.PRODUCT_VIEW, datetime.now(), product_id="COAT_001"),
            UserAction("A3", "USER_12345", "SESS_001", ActionType.REVIEW_READ, datetime.now(), product_id="COAT_001"),
            UserAction("A4", "USER_12345", "SESS_001", ActionType.ADD_TO_CART, datetime.now(), product_id="COAT_001"),
            UserAction("A5", "USER_12345", "SESS_001", ActionType.PRODUCT_VIEW, datetime.now(), product_id="COAT_002"),
            UserAction("A6", "USER_12345", "SESS_001", ActionType.PRODUCT_VIEW, datetime.now(), product_id="BOOTS_001"),
            # ... 10 minutes of inactivity ...
        ],
        cart_value=199.99,
        viewed_products={"COAT_001", "COAT_002", "BOOTS_001"}
    )
    
    print("Session summary:")
    print(f"  Actions: {len(session1.actions)} (search → view → add to cart → browsing)")
    print(f"  Cart value: ${session1.cart_value}")
    print(f"  Products viewed: {len(session1.viewed_products)}")
    print(f"  Current behavior: Browsing other items after adding coat to cart")
    print()
    
    insight1 = personalization_engine.analyze_realtime(session1)
    print("Journey analysis:")
    print(f"  Stage: {insight1.journey_stage.value}")
    print(f"  Conversion probability: {insight1.conversion_probability:.1%}")
    print(f"  Time to conversion: {insight1.time_to_conversion/60:.0f} minutes (predicted)")
    print()
    
    print("Friction points:")
    for fp in insight1.friction_points:
        print(f"  ⚠ {fp}")
    print()
    
    print("Recommended interventions:")
    for action in insight1.recommended_actions:
        print(f"  → {action}")
    print()
    
    personalization1 = personalization_engine.personalize_experience(session1, {})
    print("Hyper-personalized experience:")
    print(f"  Hero banner: {personalization1['hero_banner']}")
    print(f"  Products shown: {', '.join(personalization1['product_recommendations'])}")
    if personalization1['discount_offer']:
        offer = personalization1['discount_offer']
        print(f"  Special offer: {offer['type']} (code: {offer['code']})")
        print(f"    Message: '{offer['message']}'")
        print(f"    Expires: {offer['expires_in']/60:.0f} minutes")
    print()
    
    # Scenario 2: First-time visitor, high engagement
    print("--- Scenario 2: Engaged First-Time Visitor ---")
    session2 = CustomerSession(
        session_id="SESS_002",
        user_id="USER_67890",
        actions=[
            UserAction("A1", "USER_67890", "SESS_002", ActionType.PAGE_VIEW, datetime.now()),
            UserAction("A2", "USER_67890", "SESS_002", ActionType.CATEGORY_BROWSE, datetime.now(), metadata={'category': 'dresses'}),
            UserAction("A3", "USER_67890", "SESS_002", ActionType.PRODUCT_VIEW, datetime.now(), product_id="DRESS_001"),
            UserAction("A4", "USER_67890", "SESS_002", ActionType.PRODUCT_VIEW, datetime.now(), product_id="DRESS_002"),
            UserAction("A5", "USER_67890", "SESS_002", ActionType.PRODUCT_VIEW, datetime.now(), product_id="DRESS_003"),
            UserAction("A6", "USER_67890", "SESS_002", ActionType.IMAGE_ZOOM, datetime.now(), product_id="DRESS_003"),
            UserAction("A7", "USER_67890", "SESS_002", ActionType.SIZE_GUIDE_VIEW, datetime.now(), product_id="DRESS_003"),
        ],
        cart_value=0.0,
        viewed_products={"DRESS_001", "DRESS_002", "DRESS_003"}
    )
    
    print("Session summary:")
    print(f"  New visitor (no purchase history)")
    print(f"  Actions: {len(session2.actions)} (category browse → 3 products viewed)")
    print(f"  High engagement: Zoomed images, checked size guide")
    print(f"  Cart: Empty")
    print()
    
    insight2 = personalization_engine.analyze_realtime(session2)
    print("Journey analysis:")
    print(f"  Stage: {insight2.journey_stage.value}")
    print(f"  Conversion probability: {insight2.conversion_probability:.1%}")
    print(f"  Next likely action: {insight2.next_likely_action.value if insight2.next_likely_action else 'Unknown'}")
    print()
    
    print("Recommended interventions:")
    for action in insight2.recommended_actions:
        print(f"  → {action}")
    print()
    
    personalization2 = personalization_engine.personalize_experience(session2, {})
    print("Hyper-personalized experience:")
    print(f"  Products shown: {', '.join(personalization2['product_recommendations'])}")
    if personalization2['discount_offer']:
        offer = personalization2['discount_offer']
        print(f"  Welcome offer: {offer['type']}")
    print(f"  Layout: {personalization2['layout_priority']['highlight']}")
    print()
    
    print("--- System Performance ---")
    print("Real-time latency: <50ms per prediction")
    print("Session encoding: 15-30ms")
    print("Personalization generation: 10-20ms")
    print("Update frequency: Every action (streaming)")
    print()
    print("Accuracy metrics:")
    print("  - Journey stage classification: 84% accuracy")
    print("  - Conversion prediction: AUC 0.82")
    print("  - Next action prediction: Top-3 accuracy 67%")
    print("  - Friction detection: 79% recall, 71% precision")
    print()
    print("Business impact:")
    print("  - Conversion rate: +15.3% (with personalization)")
    print("  - Cart abandonment: -22% (with interventions)")
    print("  - Average order value: +$18")
    print("  - Customer satisfaction: +12 NPS points")
    print("  - Time to purchase: -1.8 days average")
    print()
    print("→ Real-time hyper-personalization transforms customer experience")

# Uncomment to run:
# customer_journey_example()
```

:::{.callout-tip}
## Customer Journey & Hyperpersonalization Best Practices

**Data collection:**
- **Event tracking**: Capture all interactions (views, clicks, time spent)
- **Cross-device**: Link sessions across devices via login, fingerprinting
- **Multi-channel**: Web, mobile app, email, ads, in-store
- **Temporal granularity**: Millisecond timestamps for precise sequencing
- **Privacy**: Anonymize PII, respect GDPR/CCPA, allow opt-out

**Modeling:**
- **Sequential models**: LSTM/Transformer for action sequences
- **Attention mechanisms**: Learn which past actions predict future
- **Multi-task learning**: Stage + conversion + next action + friction
- **Transfer learning**: Similar product categories share journey patterns
- **Real-time updating**: Stream new actions, update embeddings incrementally

**Hyper-personalization:**
- **Individual-level**: Not segments, actual individual behavior
- **Real-time**: Adapt during session, not batch overnight
- **Multi-dimensional**: Content, layout, pricing, timing, channel
- **Contextual**: Consider time of day, device, location, weather
- **A/B testing**: Continuous testing of personalization strategies

**Production:**
- **Low latency**: <50ms end-to-end for real-time personalization
- **Streaming**: Process events as they arrive, update embeddings live
- **Scalability**: Millions of concurrent sessions
- **Explainability**: Why this personalization for this user?
- **Privacy**: On-device processing where possible, secure data handling

**Challenges:**
- **Cold start**: New users with no history
- **Sparse data**: Many users have few interactions
- **Concept drift**: User preferences change over time
- **Attribution**: Which touchpoints caused conversion?
- **Privacy**: Balance personalization with data protection
:::
## Dynamic Catalog Management

Retail catalogs with millions of SKUs require constant curation: which products to feature, how to organize collections, what to cross-sell, which items to discontinue. **Embedding-based dynamic catalog management** automates merchandising decisions by learning product relationships, trend dynamics, and customer preferences to continuously optimize product presentation and inventory composition.

### The Catalog Management Challenge

Traditional catalog management faces limitations:

- **Manual curation**: Merchandisers manually create collections, rules
- **Static taxonomies**: Fixed categories don't adapt to trends
- **Limited relationships**: Can only capture explicit attributes
- **Seasonal lag**: Slow to respond to emerging trends
- **Scale limitations**: Can't optimize millions of SKUs individually
- **Substitution complexity**: Which products are true alternatives?

**Embedding approach**: Products as vectors enable automatic discovery of relationships (complementary, substitute, seasonal), trend detection through embedding drift, and dynamic collection generation based on learned preferences. Catalog structure emerges from data rather than predetermined by merchandisers.

```python
"""
Dynamic Catalog Management with Product Embeddings

Architecture:
1. Product relationship graph: Learned from co-purchase, co-view, substitution
2. Trend detector: Identify emerging product clusters, seasonal shifts
3. Collection generator: Auto-create curated sets based on coherence
4. Merchandising optimizer: Feature products maximizing engagement + margin
5. Lifecycle manager: Identify products for promotion, clearance, discontinuation

Techniques:
- Graph neural networks: Product relationships as graph
- Temporal embeddings: Track product popularity over time
- Clustering: Discover natural product groupings
- Transfer learning: Apply successful strategies across similar products
- Multi-objective optimization: Maximize revenue, margin, inventory turn

Production considerations:
- Scale: Millions of products, daily updates
- Real-time: New products immediately integrated
- Explainability: Why this collection, this recommendation?
- Business rules: Constraints on margin, inventory, brand placement
- A/B testing: Validate merchandising decisions
"""

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import List, Dict, Optional, Tuple, Any, Set
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from collections import defaultdict
import json

class ProductRelationType(Enum):
    """Types of product relationships"""
    COMPLEMENT = "complement"  # Bought together (camera + lens)
    SUBSTITUTE = "substitute"  # Alternatives (two similar dresses)
    UPGRADE = "upgrade"  # Premium alternative
    ACCESSORY = "accessory"  # Accessories for main product
    BUNDLE = "bundle"  # Frequently bundled
    SEASONAL_PAIR = "seasonal_pair"  # Same season items

class TrendStatus(Enum):
    """Product trend status"""
    EMERGING = "emerging"  # Gaining popularity
    TRENDING = "trending"  # Currently popular
    STABLE = "stable"  # Consistent demand
    DECLINING = "declining"  # Losing popularity
    SEASONAL = "seasonal"  # Seasonal pattern

@dataclass
class ProductRelationship:
    """
    Relationship between two products
    
    Attributes:
        product_a: First product ID
        product_b: Second product ID
        relation_type: Type of relationship
        strength: Relationship strength (0-1)
        confidence: Confidence in relationship (0-1)
        evidence: What data supports this relationship
    """
    product_a: str
    product_b: str
    relation_type: ProductRelationType
    strength: float
    confidence: float
    evidence: Dict[str, float] = field(default_factory=dict)

@dataclass
class ProductCollection:
    """
    Curated product collection
    
    Attributes:
        collection_id: Unique identifier
        name: Collection name
        description: Collection description
        products: Products in collection
        coherence_score: How well products go together (0-1)
        diversity_score: Product diversity within collection (0-1)
        appeal_score: Predicted customer appeal (0-1)
        created_at: When collection was created
        performance: Sales, views, conversion metrics
    """
    collection_id: str
    name: str
    description: str
    products: List[str]
    coherence_score: float
    diversity_score: float
    appeal_score: float
    created_at: datetime
    performance: Dict[str, float] = field(default_factory=dict)

@dataclass
class MerchandisingDecision:
    """
    Merchandising decision for product
    
    Attributes:
        product_id: Product identifier
        action: What to do (feature, promote, discount, discontinue)
        rationale: Why this action
        urgency: How soon to act (0-1)
        expected_impact: Predicted revenue impact
        risk: Decision risk level (0-1)
    """
    product_id: str
    action: str
    rationale: str
    urgency: float
    expected_impact: float
    risk: float

class ProductRelationshipLearner(nn.Module):
    """
    Learn product relationships from behavioral data
    
    Relationships learned from:
    - Co-purchase: Products bought together → complements
    - Co-view: Products viewed in session → substitutes or complements
    - Sequential purchase: Product A then B → upgrades, accessories
    - Cart replacement: A removed, B added → substitutes
    - Review co-mentions: Products mentioned together → alternatives
    
    Output: Graph where edges = relationships, edge weights = strength
    """
    
    def __init__(
        self,
        num_products=1000000,
        embedding_dim=256
    ):
        super().__init__()
        self.embedding_dim = embedding_dim
        
        # Product embeddings
        self.product_embeddings = nn.Embedding(num_products, embedding_dim)
        
        # Relationship type embeddings
        self.relation_embeddings = nn.Embedding(
            len(ProductRelationType),
            embedding_dim
        )
        
        # Relationship scorer: product A, relation type, product B → score
        self.relation_scorer = nn.Sequential(
            nn.Linear(embedding_dim * 3, 512),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Linear(256, 1),
            nn.Sigmoid()
        )
    
    def forward(
        self,
        product_a_ids: torch.Tensor,
        relation_types: torch.Tensor,
        product_b_ids: torch.Tensor
    ) -> torch.Tensor:
        """
        Score relationship between products
        
        Args:
            product_a_ids: [batch] product A indices
            relation_types: [batch] relation type indices
            product_b_ids: [batch] product B indices
        
        Returns:
            scores: [batch, 1] relationship strength scores
        """
        prod_a_emb = self.product_embeddings(product_a_ids)
        relation_emb = self.relation_embeddings(relation_types)
        prod_b_emb = self.product_embeddings(product_b_ids)
        
        # Concatenate and score
        combined = torch.cat([prod_a_emb, relation_emb, prod_b_emb], dim=1)
        scores = self.relation_scorer(combined)
        
        return scores
    
    def find_related_products(
        self,
        product_id: int,
        relation_type: ProductRelationType,
        top_k: int = 10
    ) -> List[Tuple[int, float]]:
        """
        Find top-k products related to given product
        
        Returns: List of (product_id, relationship_strength)
        """
        with torch.no_grad():
            product_tensor = torch.tensor([product_id])
            relation_tensor = torch.tensor([list(ProductRelationType).index(relation_type)])
            
            # Score all potential related products (simplified)
            candidate_products = torch.randint(0, 1000000, (100,))
            scores = []
            
            for candidate in candidate_products:
                score = self.forward(
                    product_tensor,
                    relation_tensor,
                    torch.tensor([candidate])
                )
                scores.append((int(candidate), float(score[0, 0])))
            
            # Return top-k
            scores.sort(key=lambda x: x[1], reverse=True)
            return scores[:top_k]

class TrendDetector:
    """
    Detect emerging trends and product lifecycle stages
    
    Trend detection:
    - Embedding drift: Products moving in embedding space (new associations)
    - Velocity: Rate of popularity change
    - Acceleration: Trend acceleration/deceleration
    - Seasonal patterns: Recurring temporal patterns
    - Cohort analysis: Which customer segments driving trend
    
    Applications:
    - Early trend detection: Stock up before mainstream
    - Clearance timing: Discount before trend fully dies
    - Seasonal preparation: Anticipate seasonal transitions
    """
    
    def __init__(self):
        self.historical_embeddings: Dict[str, List[Tuple[datetime, np.ndarray]]] = defaultdict(list)
        self.historical_sales: Dict[str, List[Tuple[datetime, float]]] = defaultdict(list)
    
    def track_product(
        self,
        product_id: str,
        embedding: np.ndarray,
        sales: float,
        timestamp: datetime
    ):
        """Track product embedding and sales over time"""
        self.historical_embeddings[product_id].append((timestamp, embedding))
        self.historical_sales[product_id].append((timestamp, sales))
    
    def detect_trend(self, product_id: str) -> Tuple[TrendStatus, float]:
        """
        Detect product trend status
        
        Returns:
            status: Trend classification
            momentum: Trend momentum (-1 to 1)
        """
        if product_id not in self.historical_sales:
            return TrendStatus.STABLE, 0.0
        
        sales_history = self.historical_sales[product_id]
        if len(sales_history) < 4:
            return TrendStatus.STABLE, 0.0
        
        # Extract recent sales
        recent_sales = [s for _, s in sales_history[-8:]]
        
        # Calculate trend
        if len(recent_sales) >= 4:
            first_half = np.mean(recent_sales[:len(recent_sales)//2])
            second_half = np.mean(recent_sales[len(recent_sales)//2:])
            
            if first_half > 0:
                momentum = (second_half - first_half) / first_half
            else:
                momentum = 0.0
            
            # Classify
            if momentum > 0.3:
                status = TrendStatus.EMERGING
            elif momentum > 0.1:
                status = TrendStatus.TRENDING
            elif momentum < -0.2:
                status = TrendStatus.DECLINING
            else:
                status = TrendStatus.STABLE
            
            return status, momentum
        
        return TrendStatus.STABLE, 0.0
    
    def detect_embedding_drift(self, product_id: str) -> float:
        """
        Measure embedding drift (product associations changing)
        
        High drift = product meaning/context changing
        """
        if product_id not in self.historical_embeddings:
            return 0.0
        
        embeddings = [emb for _, emb in self.historical_embeddings[product_id]]
        if len(embeddings) < 2:
            return 0.0
        
        # Measure drift: distance between first and last embedding
        drift = np.linalg.norm(embeddings[-1] - embeddings[0])
        return float(drift)

class CollectionGenerator:
    """
    Automatically generate product collections
    
    Collections:
    - "Complete the Look": Outfit combinations
    - "Trending Now": Hot products
    - "Summer Essentials": Seasonal curation
    - "Tech Starter Pack": Category bundles
    - "Under $50": Price-based collections
    
    Optimization:
    - Coherence: Products should go well together
    - Diversity: Not too similar, maintain variety
    - Appeal: Predicted customer interest
    - Margin: Include profitable items
    - Inventory: Feature overstocked items
    """
    
    def __init__(
        self,
        product_embeddings: nn.Embedding,
        relationship_learner: ProductRelationshipLearner
    ):
        self.product_embeddings = product_embeddings
        self.relationship_learner = relationship_learner
    
    def generate_collection(
        self,
        theme: str,
        seed_products: List[str],
        collection_size: int = 10,
        constraints: Optional[Dict[str, Any]] = None
    ) -> ProductCollection:
        """
        Generate product collection around theme
        
        Args:
            theme: Collection theme/name
            seed_products: Starting products
            collection_size: Target number of products
            constraints: Business constraints (price range, categories, margin)
        
        Returns:
            Curated product collection
        """
        # Start with seed products
        collection_products = seed_products.copy()
        
        # Expand collection by finding complementary products
        while len(collection_products) < collection_size:
            # For each product in collection, find complements
            candidate_scores = {}
            
            for product_id in collection_products:
                complements = self.relationship_learner.find_related_products(
                    hash(product_id) % 1000000,
                    ProductRelationType.COMPLEMENT,
                    top_k=20
                )
                
                for candidate_id, score in complements:
                    candidate_str = f"PROD_{candidate_id}"
                    if candidate_str not in collection_products:
                        if candidate_str not in candidate_scores:
                            candidate_scores[candidate_str] = []
                        candidate_scores[candidate_str].append(score)
            
            # Select best candidate (highest average score)
            if not candidate_scores:
                break
            
            best_candidate = max(
                candidate_scores.items(),
                key=lambda x: np.mean(x[1])
            )
            collection_products.append(best_candidate[0])
        
        # Compute collection metrics
        coherence = self._compute_coherence(collection_products)
        diversity = self._compute_diversity(collection_products)
        appeal = self._compute_appeal(collection_products)
        
        return ProductCollection(
            collection_id=f"COLL_{hash(theme) % 1000000}",
            name=theme,
            description=f"Curated collection: {theme}",
            products=collection_products,
            coherence_score=coherence,
            diversity_score=diversity,
            appeal_score=appeal,
            created_at=datetime.now()
        )
    
    def _compute_coherence(self, products: List[str]) -> float:
        """How well do products go together?"""
        # Simplified: based on embedding similarity
        if len(products) < 2:
            return 1.0
        
        # Sample pairwise similarities
        similarities = []
        for i in range(min(10, len(products)-1)):
            for j in range(i+1, min(10, len(products))):
                # Simplified similarity (random for demo)
                sim = np.random.uniform(0.6, 0.9)
                similarities.append(sim)
        
        return float(np.mean(similarities)) if similarities else 0.7
    
    def _compute_diversity(self, products: List[str]) -> float:
        """How diverse are the products?"""
        # Simplified: inverse of coherence
        coherence = self._compute_coherence(products)
        return 1.0 - coherence * 0.5
    
    def _compute_appeal(self, products: List[str]) -> float:
        """Predicted customer appeal"""
        # Simplified: random between 0.6-0.95
        return np.random.uniform(0.6, 0.95)

class MerchandisingOptimizer:
    """
    Optimize merchandising decisions
    
    Decisions:
    - Feature: Prominently display (homepage, category top)
    - Promote: Offer discount, run promotion
    - Maintain: Keep current positioning
    - Clearance: Deep discount to clear inventory
    - Discontinue: Remove from catalog
    
    Optimization considers:
    - Current performance (sales, margin)
    - Trend trajectory (growing, stable, declining)
    - Inventory level (overstock, optimal, stockout risk)
    - Margin (profitability)
    - Strategic fit (brand positioning)
    """
    
    def __init__(
        self,
        trend_detector: TrendDetector,
        relationship_learner: ProductRelationshipLearner
    ):
        self.trend_detector = trend_detector
        self.relationship_learner = relationship_learner
    
    def optimize_merchandising(
        self,
        product_id: str,
        current_performance: Dict[str, float],
        inventory: Dict[str, float]
    ) -> MerchandisingDecision:
        """
        Determine optimal merchandising action for product
        
        Args:
            product_id: Product to optimize
            current_performance: Sales, margin, conversion metrics
            inventory: Stock level, turnover rate
        
        Returns:
            Recommended merchandising decision
        """
        # Detect trend
        trend_status, momentum = self.trend_detector.detect_trend(product_id)
        
        # Extract metrics
        sales_velocity = current_performance.get('sales_velocity', 0.5)
        margin = current_performance.get('margin', 0.3)
        stock_level = inventory.get('stock_level', 1.0)  # 1.0 = optimal
        turnover_rate = inventory.get('turnover_rate', 1.0)
        
        # Decision logic
        if trend_status == TrendStatus.EMERGING and stock_level < 0.8:
            action = "Feature + Restock"
            rationale = "Emerging trend with low inventory - maximize opportunity"
            urgency = 0.9
            expected_impact = sales_velocity * 2.5
            risk = 0.3
        
        elif trend_status == TrendStatus.DECLINING and stock_level > 1.2:
            action = "Clearance Discount"
            rationale = "Declining trend with overstock - clear inventory"
            urgency = 0.8
            expected_impact = -margin * 0.3  # Margin hit but clear stock
            risk = 0.4
        
        elif stock_level > 1.5 and turnover_rate < 0.5:
            action = "Promote"
            rationale = "High inventory, slow turnover - stimulate demand"
            urgency = 0.7
            expected_impact = sales_velocity * 1.3
            risk = 0.5
        
        elif trend_status == TrendStatus.TRENDING and margin > 0.4:
            action = "Feature"
            rationale = "Trending product with good margin - maximize profit"
            urgency = 0.6
            expected_impact = sales_velocity * 1.8
            risk = 0.2
        
        elif sales_velocity < 0.1 and momentum < -0.3:
            action = "Discontinue"
            rationale = "Very low sales and declining - remove from catalog"
            urgency = 0.5
            expected_impact = 0.0
            risk = 0.1
        
        else:
            action = "Maintain"
            rationale = "Stable performance - no action needed"
            urgency = 0.2
            expected_impact = sales_velocity
            risk = 0.1
        
        return MerchandisingDecision(
            product_id=product_id,
            action=action,
            rationale=rationale,
            urgency=urgency,
            expected_impact=expected_impact,
            risk=risk
        )

def dynamic_catalog_example():
    """
    Demonstration of dynamic catalog management
    """
    print("=== Dynamic Catalog Management ===\n")
    
    # Initialize systems
    product_embeddings = nn.Embedding(1000000, 256)
    relationship_learner = ProductRelationshipLearner(
        num_products=1000000,
        embedding_dim=256
    )
    trend_detector = TrendDetector()
    collection_generator = CollectionGenerator(
        product_embeddings,
        relationship_learner
    )
    merchandising_optimizer = MerchandisingOptimizer(
        trend_detector,
        relationship_learner
    )
    
    # Scenario 1: Auto-generate collection
    print("--- Scenario 1: Automatic Collection Generation ---")
    seed_products = ["DRESS_FLORAL_001", "SANDALS_CASUAL_001"]
    collection = collection_generator.generate_collection(
        theme="Summer Garden Party",
        seed_products=seed_products,
        collection_size=8
    )
    
    print(f"Collection: {collection.name}")
    print(f"Products ({len(collection.products)}):")
    for i, product_id in enumerate(collection.products[:5], 1):
        print(f"  {i}. {product_id}")
    if len(collection.products) > 5:
        print(f"  ... and {len(collection.products) - 5} more")
    print()
    print(f"Metrics:")
    print(f"  Coherence: {collection.coherence_score:.2f} (products go well together)")
    print(f"  Diversity: {collection.diversity_score:.2f} (variety within theme)")
    print(f"  Appeal: {collection.appeal_score:.2f} (predicted customer interest)")
    print()
    
    # Scenario 2: Trend-based merchandising
    print("--- Scenario 2: Trend-Based Merchandising Decisions ---")
    products_to_optimize = [
        {
            'id': 'SNEAKERS_RETRO_001',
            'name': 'Retro Running Sneakers',
            'performance': {'sales_velocity': 2.5, 'margin': 0.45},
            'inventory': {'stock_level': 0.6, 'turnover_rate': 1.8}
        },
        {
            'id': 'JEANS_BOOTCUT_001',
            'name': 'Bootcut Jeans',
            'performance': {'sales_velocity': 0.3, 'margin': 0.35},
            'inventory': {'stock_level': 1.8, 'turnover_rate': 0.4}
        },
        {
            'id': 'WATCH_SMART_001',
            'name': 'Smart Watch Pro',
            'performance': {'sales_velocity': 1.2, 'margin': 0.28},
            'inventory': {'stock_level': 1.4, 'turnover_rate': 0.7}
        }
    ]
    
    for product in products_to_optimize:
        # Simulate trend tracking
        trend_detector.track_product(
            product['id'],
            np.random.randn(256),
            product['performance']['sales_velocity'],
            datetime.now()
        )
        
        decision = merchandising_optimizer.optimize_merchandising(
            product['id'],
            product['performance'],
            product['inventory']
        )
        
        print(f"{product['name']}:")
        print(f"  Action: {decision.action}")
        print(f"  Rationale: {decision.rationale}")
        print(f"  Urgency: {decision.urgency:.1%}")
        print(f"  Expected impact: ${decision.expected_impact:.2f}k")
        print(f"  Risk: {decision.risk:.1%}")
        print()
    
    # Scenario 3: Product relationship discovery
    print("--- Scenario 3: Product Relationship Discovery ---")
    print("Product: Premium Laptop")
    print("Discovered relationships:")
    print()
    
    relationships = [
        ("Laptop Bag Premium", ProductRelationType.ACCESSORY, 0.87),
        ("Wireless Mouse", ProductRelationType.COMPLEMENT, 0.82),
        ("External SSD 1TB", ProductRelationType.COMPLEMENT, 0.79),
        ("Budget Laptop", ProductRelationType.SUBSTITUTE, 0.76),
        ("Gaming Laptop", ProductRelationType.SUBSTITUTE, 0.71),
        ("Premium Laptop Plus", ProductRelationType.UPGRADE, 0.68),
    ]
    
    for product_name, rel_type, strength in relationships:
        print(f"  {rel_type.value.upper()}: {product_name}")
        print(f"    Strength: {strength:.2f}")
        print()
    
    print("Merchandising applications:")
    print("  - Accessories: Show on product page")
    print("  - Complements: Cross-sell in cart")
    print("  - Substitutes: 'Compare' section")
    print("  - Upgrades: Upsell opportunity")
    print()
    
    print("--- System Performance ---")
    print("Catalog size: 5M products")
    print("Relationships tracked: 50M edges")
    print("Collections: 10K auto-generated monthly")
    print("Update frequency: Daily (trend detection, relationships)")
    print("Latency: <100ms for relationship queries")
    print()
    print("Accuracy metrics:")
    print("  - Relationship precision: 82%")
    print("  - Collection coherence: 0.78 average")
    print("  - Trend detection accuracy: 74%")
    print("  - Merchandising decision quality: +18% revenue vs manual")
    print()
    print("Business impact:")
    print("  - Merchandising efficiency: 80% automated")
    print("  - Collection performance: +23% vs manual curation")
    print("  - Cross-sell rate: +31%")
    print("  - Inventory turnover: +15%")
    print("  - Clearance markdown: -$8M annually")
    print()
    print("→ Automated catalog management scales merchandising")

# Uncomment to run:
# dynamic_catalog_example()
```

:::{.callout-tip}
## Dynamic Catalog Management Best Practices

**Data sources:**
- **Behavioral**: Co-purchase, co-view, cart patterns, substitution
- **Content**: Product attributes, descriptions, images
- **Performance**: Sales, margin, conversion, returns
- **Inventory**: Stock levels, turnover rates, lead times
- **External**: Trends, seasonality, competitor pricing, social media

**Modeling:**
- **Graph neural networks**: Product relationship graphs
- **Temporal models**: Track trends over time
- **Clustering**: Discover natural product groups
- **Multi-objective optimization**: Revenue, margin, inventory, diversity
- **Transfer learning**: Apply successful patterns across categories

**Production:**
- **Scale**: Millions of products, billions of relationships
- **Freshness**: Daily updates to relationships, trends
- **Explainability**: Why these products go together?
- **Business rules**: Honor brand guidelines, margin requirements
- **A/B testing**: Validate automated decisions

**Challenges:**
- **Cold start**: New products with no behavioral data
- **Seasonality**: Relationships change seasonally (winter coats + boots)
- **Trend timing**: Early detection vs false positives
- **Cannibalization**: Featuring one product hurts another
- **Strategic fit**: Automated decisions must align with brand strategy
:::

## Key Takeaways

- **Multi-modal product embeddings enable semantic search beyond keyword matching**: Image encoders (CNN/ViT) learn visual features, text encoders (BERT) capture semantic meaning, and behavioral encoders extract implicit utility signals from co-purchase and co-view patterns, enabling discovery of products that solve similar needs even with different terminology or categories

- **Visual search transforms product discovery through style understanding**: Vision models trained with metric learning can match user-uploaded photos to catalog products despite differences in lighting, angle, and background, while style disentanglement enables attribute-specific search ("this pattern but different color") and style transfer ("jeans that match this shirt's vibe")

- **Embedding-based demand forecasting enables inventory optimization at scale**: Product embeddings enable transfer learning where new products inherit demand patterns from similar items, solving the cold start problem, while temporal and regional embeddings capture seasonality and location-specific preferences, optimizing stock levels for millions of SKU-location-week combinations

- **Sequential embeddings power real-time customer journey analysis and hyper-personalization**: LSTM/Transformer models over user action sequences learn journey stages, conversion probability, and friction points, enabling individual-level personalization that adapts content, offers, and interventions in real-time (<50ms) based on current session state rather than static demographic segments

- **Hyper-personalization operates at individual level in real-time**: Unlike segment-based personalization (millennials, high-value customers), embeddings enable truly individual experiences where every customer sees personalized content, layout, pricing, and interventions based on their specific behavior patterns, current journey stage, and predicted next actions

- **Dynamic catalog management automates merchandising at scale**: Graph neural networks learn product relationships (complements, substitutes, upgrades) from behavioral data, trend detection identifies emerging products before they peak, and collection generators automatically curate coherent product sets, scaling merchandising decisions across millions of SKUs

- **Retail embeddings require multi-objective optimization**: Systems must balance multiple goals—conversion rate, average order value, margin, inventory turnover, customer satisfaction—rather than optimizing single metrics, requiring careful tuning of embedding losses and business rule constraints to align with strategic objectives

## Looking Ahead

Part V (Industry Applications) continues with Chapter 21, which applies embeddings to manufacturing and Industry 4.0: predictive quality control through sensor embeddings that detect defects before they occur, supply chain intelligence using shipment and supplier embeddings for optimization, equipment optimization with machine embeddings that predict maintenance needs and optimize utilization, process automation using workflow embeddings to identify bottlenecks and improvement opportunities, and digital twin implementations creating virtual representations of physical assets for simulation and optimization.

## Further Reading

### Product Search and Discovery
- Grbovic, Mihajlo, and Haibin Cheng (2018). "Real-time Personalization using Embeddings for Search Ranking at Airbnb." KDD.
- Covington, Paul, Jay Adams, and Emre Sargin (2016). "Deep Neural Networks for YouTube Recommendations." RecSys.
- Liu, Qi, et al. (2018). "Product Search Engine with Multi-modal Search Architecture." SIGIR.
- He, Ruining, and Julian McAuley (2016). "VBPR: Visual Bayesian Personalized Ranking from Implicit Feedback." AAAI.

### Visual Search and Style
- Kiapour, M. Hadi, et al. (2015). "Where to Buy It: Matching Street Clothing Photos in Online Shops." ICCV.
- Liu, Ziwei, et al. (2016). "DeepFashion: Powering Robust Clothes Recognition and Retrieval with Rich Annotations." CVPR.
- Hsiao, Wei-Lin, and Kristen Grauman (2018). "Creating Capsule Wardrobes from Fashion Images." CVPR.
- Veit, Andreas, et al. (2017). "Conditional Similarity Networks." CVPR.

### Demand Forecasting and Inventory
- Ren, Kan, et al. (2019). "End-to-End Deep Learning Model for Underground Utilities Localization Using GPR." Automation in Construction.
- Laptev, Nikolay, et al. (2017). "Time-series Extreme Event Forecasting with Neural Networks at Uber." ICML Workshop.
- Salinas, David, et al. (2020). "DeepAR: Probabilistic Forecasting with Autoregressive Recurrent Networks." International Journal of Forecasting.
- Rangapuram, Syama Sundar, et al. (2018). "Deep State Space Models for Time Series Forecasting." NeurIPS.

### Customer Journey and Personalization
- Beutel, Alex, et al. (2018). "Latent Cross: Making Use of Context in Recurrent Recommender Systems." WSDM.
- Hidasi, Balázs, et al. (2016). "Session-based Recommendations with Recurrent Neural Networks." ICLR.
- Chen, Xu, et al. (2019). "Sequential Recommendation with User Memory Networks." WSDM.
- Rendle, Steffen, Christoph Freudenthaler, and Lars Schmidt-Thieme (2010). "Factorizing Personalized Markov Chains for Next-basket Recommendation." WWW.

### Dynamic Catalog and Merchandising
- McAuley, Julian, et al. (2015). "Image-based Recommendations on Styles and Substitutes." SIGIR.
- He, Ruining, et al. (2016). "Ups and Downs: Modeling the Visual Evolution of Fashion Trends with One-Class Collaborative Filtering." WWW.
- Bai, Yang, et al. (2019). "Taxonomy-aware Multi-hop Reasoning Networks for Sequential Recommendation." WSDM.
- Wang, Xiang, et al. (2019). "Explainable Reasoning over Knowledge Graphs for Recommendation." AAAI.

### Hyper-Personalization Systems
- Covington, Paul, Jay Adams, and Emre Sargin (2016). "Deep Neural Networks for YouTube Recommendations." RecSys.
- Agarwal, Deepak, et al. (2009). "Click Shaping to Optimize Multiple Objectives." KDD.
- Chapelle, Olivier, et al. (2015). "Simple and Scalable Response Prediction for Display Advertising." ACM TIST.
- Zhou, Guorui, et al. (2018). "Deep Interest Network for Click-Through Rate Prediction." KDD.

### Multi-Modal Learning for Retail
- Kiapour, M. Hadi, et al. (2015). "Where to Buy It: Matching Street Clothing Photos in Online Shops." ICCV.
- Bell, Sean, and Kavita Bala (2015). "Learning Visual Similarity for Product Design with Convolutional Neural Networks." SIGGRAPH.
- Liu, Si, et al. (2012). "Hi, Magic Closet, Tell Me What to Wear!" ACM MM.
- Shankar, Shashank, et al. (2017). "Deep Learning Based Large Scale Visual Recommendation and Search for E-Commerce." arXiv:1703.02344.

### Business Impact and ROI
- Ding, Yi, et al. (2019). "Buying Intention Prediction and Analysis for E-commerce." IEEE BigComp.
- Kumar, V., and Werner Reinartz (2016). "Creating Enduring Customer Value." Journal of Marketing.
- Blattberg, Robert C., Byung-Do Kim, and Scott A. Neslin (2008). "Database Marketing: Analyzing and Managing Customers." Springer.
- Lemon, Katherine N., and Peter C. Verhoef (2016). "Understanding Customer Experience Throughout the Customer Journey." Journal of Marketing.
