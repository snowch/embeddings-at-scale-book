# Multi-Modal Embeddings {#sec-multimodal-embeddings}

::: callout-note
## Chapter Overview

This chapter covers multi-modal embeddings—representations that map different data types (text, images, audio) into a shared vector space where they can be directly compared. We explore how these unified spaces enable powerful cross-modal capabilities like searching images with text queries, zero-shot classification, and multi-sensor fusion.
:::

## Multi-Modal Embeddings {#sec-multimodal-embedding-types}

Multi-modal embeddings map different data types—text, images, audio—into a shared vector space where they can be directly compared. This enables powerful cross-modal capabilities: searching images with text queries, finding text descriptions for images, or categorizing images without any training examples.

::: {.callout-note}
## Zero-Shot Categorization via Embeddings
**Zero-shot categorization** means assigning categories the model was never explicitly trained on—and it works through embedding similarity, not a traditional classifier. Instead of training a "sunset vs ocean vs forest" classifier, you describe categories in text ("a photo of a sunset"), embed both the text and image, and find the closest match. The model generalizes from its pre-training on millions of image-text pairs to recognize new concepts. This is sometimes called "zero-shot classification," but the mechanism is pure embedding similarity.
:::

The key insight is training two encoders (e.g., one for text, one for images) so that matching pairs produce similar vectors. CLIP, trained on 400 million image-text pairs from the internet, learns that "a photo of a cat" and an actual cat photo should have nearby embeddings:

```{python}
#| echo: false
#| label: fig-multimodal-concept
#| fig-cap: "Multi-modal embeddings: separate encoders map text and images into a shared space where matching concepts are close together."
import matplotlib.pyplot as plt
from matplotlib.patches import FancyBboxPatch

fig, ax = plt.subplots(figsize=(10, 5))
ax.set_xlim(0, 10)
ax.set_ylim(0, 5)
ax.axis('off')

# Text input + encoder (top row)
ax.text(0.5, 4.0, '"a sunset"', fontsize=11, ha='center', va='center', style='italic')
ax.annotate('', xy=(1.4, 4.0), xytext=(1.0, 4.0), arrowprops=dict(arrowstyle='->', color='gray', lw=1.5))
text_box = FancyBboxPatch((1.5, 3.6), 1.8, 0.8, boxstyle="round,pad=0.05",
                           facecolor='#E3F2FD', edgecolor='#1976D2', linewidth=2)
ax.add_patch(text_box)
ax.text(2.4, 4.0, 'Text\nEncoder', ha='center', va='center', fontsize=10, fontweight='bold')

# Image input + encoder (bottom row)
ax.add_patch(plt.Rectangle((0.25, 0.75), 0.5, 0.5, facecolor='#FF6B35', edgecolor='black', lw=1))
ax.annotate('', xy=(1.4, 1.0), xytext=(0.9, 1.0), arrowprops=dict(arrowstyle='->', color='gray', lw=1.5))
img_box = FancyBboxPatch((1.5, 0.6), 1.8, 0.8, boxstyle="round,pad=0.05",
                          facecolor='#FFF3E0', edgecolor='#F57C00', linewidth=2)
ax.add_patch(img_box)
ax.text(2.4, 1.0, 'Image\nEncoder', ha='center', va='center', fontsize=10, fontweight='bold')

# Arrows to shared space
ax.annotate('', xy=(4.3, 2.5), xytext=(3.4, 4.0), arrowprops=dict(arrowstyle='->', color='#1976D2', lw=2))
ax.annotate('', xy=(4.3, 2.5), xytext=(3.4, 1.0), arrowprops=dict(arrowstyle='->', color='#F57C00', lw=2))

# Shared embedding space
shared_box = FancyBboxPatch((4.5, 0.5), 5.2, 4.0, boxstyle="round,pad=0.1",
                             facecolor='#F5F5F5', edgecolor='#666', linewidth=2, linestyle='--')
ax.add_patch(shared_box)
ax.text(7.1, 4.2, 'Shared Embedding Space', ha='center', va='center', fontsize=11, fontweight='bold')

# Sunset cluster (upper left in space) - text and image close together
ax.scatter([5.5], [3.2], marker='o', s=108, c='#1976D2', edgecolors='black', zorder=5)
ax.scatter([5.9], [3.0], marker='s', s=108, c='#F57C00', edgecolors='black', zorder=5)
ax.text(5.7, 3.55, 'sunset', fontsize=9, ha='center', fontweight='bold')

# Ocean cluster (lower right in space) - text and image close together
ax.scatter([8.2], [1.5], marker='o', s=108, c='#1976D2', edgecolors='black', zorder=5)
ax.scatter([8.6], [1.3], marker='s', s=108, c='#F57C00', edgecolors='black', zorder=5)
ax.text(8.4, 1.85, 'ocean', fontsize=9, ha='center', fontweight='bold')

# Legend
ax.scatter([], [], marker='o', s=80, c='#1976D2', edgecolors='black', label='Text embedding')
ax.scatter([], [], marker='s', s=80, c='#F57C00', edgecolors='black', label='Image embedding')
ax.legend(loc='lower right', fontsize=9, framealpha=0.9)

plt.subplots_adjust(left=0.02, right=0.98, top=0.95, bottom=0.05)
plt.show()
```

```{python}
#| code-fold: false

"""
Multi-Modal Embeddings with CLIP: Zero-Shot Image Categorization

CLIP embeds text and images into the same 512-dimensional space.
We can categorize images by comparing them to text descriptions—
no training on the target categories required.
"""

import torch
import numpy as np
from PIL import Image
from transformers import CLIPProcessor, CLIPModel
from sklearn.metrics.pairwise import cosine_similarity
import warnings
import logging

# Suppress download progress and warnings
logging.getLogger("transformers").setLevel(logging.ERROR)
warnings.filterwarnings("ignore")

# Load CLIP model (downloads ~600MB on first run)
model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32", use_fast=True)

# Create test images with distinct characteristics
np.random.seed(42)
images = {
    'sunset': Image.fromarray(
        np.random.randint([200, 100, 50], [255, 150, 100], (224, 224, 3), dtype=np.uint8)
    ),
    'ocean': Image.fromarray(
        np.random.randint([30, 80, 150], [80, 150, 220], (224, 224, 3), dtype=np.uint8)
    ),
    'forest': Image.fromarray(
        np.random.randint([20, 80, 20], [80, 150, 80], (224, 224, 3), dtype=np.uint8)
    ),
}

# Text descriptions to match against
text_labels = [
    "a photo of a sunset with warm orange colors",
    "a photo of the ocean with blue water",
    "a photo of a green forest",
]

# Preprocess: resize images and tokenize text into tensors the model expects
image_inputs = processor(images=list(images.values()), return_tensors="pt", padding=True)
text_inputs = processor(text=text_labels, return_tensors="pt", padding=True)

# torch.no_grad() disables gradient computation since we're only generating
# embeddings, not training. This saves memory and speeds up computation.
with torch.no_grad():
    # Each encoder maps its input to a 512-dim vector in the shared space
    image_embeds = model.get_image_features(**image_inputs).numpy()
    text_embeds = model.get_text_features(**text_inputs).numpy()

# Compare each image embedding to each text embedding using cosine similarity.
# High similarity = the image and text describe the same concept.
similarities = cosine_similarity(image_embeds, text_embeds)

# Zero-shot = categorize without training on these specific categories
# We compare image embeddings to text embeddings and pick the closest match
print("Zero-shot categorization: matching images to text descriptions\n")
print(f"Embedding dimension: {image_embeds.shape[1]}\n")

for i, img_name in enumerate(images.keys()):
    best_match = text_labels[similarities[i].argmax()]
    print(f"{img_name:8} → {best_match}")
    print(f"          scores: {[f'{s:.2f}' for s in similarities[i]]}\n")
```

```{python}
#| echo: true
#| code-fold: true
#| code-summary: "Show image and embedding visualization code"
#| label: fig-multimodal-space
#| fig-cap: "CLIP zero-shot categorization: images match text descriptions via embedding similarity. Each image's embedding is closest to the semantically matching text."
import matplotlib.pyplot as plt
import matplotlib.gridspec as gridspec

fig = plt.figure(figsize=(10, 3.5))
gs = gridspec.GridSpec(1, 3, wspace=0.3)

for idx, (name, img) in enumerate(images.items()):
    ax = fig.add_subplot(gs[0, idx])
    ax.imshow(img)
    ax.set_title(f'{name.title()}', fontsize=11, fontweight='bold')

    # Show similarity scores below
    scores_text = '\n'.join([f'{text_labels[j].split("of ")[-1][:20]}: {similarities[idx][j]:.2f}'
                             for j in range(len(text_labels))])
    ax.set_xlabel(scores_text, fontsize=9)
    ax.set_xticks([])
    ax.set_yticks([])

plt.suptitle('CLIP matches images to text descriptions via shared embeddings', fontsize=11, fontweight='bold')
plt.subplots_adjust(top=0.85)
plt.show()
```

The key insight is that CLIP learns a shared space where semantically matching content—regardless of modality—has similar embeddings. The "sunset" image has warm orange pixels, and CLIP places it near the text "sunset with warm orange colors" because it learned this association from millions of image-text pairs. This enables zero-shot categorization: to categorize an image, compare it against text embeddings and pick the highest similarity—no training on your specific categories required.

**When to use multi-modal embeddings:** Cross-modal search (text→image, image→text), zero-shot image categorization, image captioning, visual question answering, and product search with text and images.

This book covers multi-modal search in @sec-semantic-search. If you'd like to see other multi-modal applications covered in future editions, reach out to the author.

**Popular architectures:**

| Architecture | Type | Strengths | Use Cases |
|-------------|------|-----------|-----------|
| [CLIP](https://github.com/openai/CLIP) | Text-image | Fast, versatile | Search, classification |
| [BLIP-2](https://github.com/salesforce/LAVIS) | Text-image | Captioning + retrieval | Image understanding |
| [ImageBind](https://github.com/facebookresearch/ImageBind) | 6 modalities | Audio, depth, thermal | Multi-sensor fusion |
| [LLaVA](https://github.com/haotian-liu/LLaVA) | Vision-language | Conversational | Visual Q&A |

: Multi-modal embedding architectures {.striped}

## Advanced: Multi-Modal Fusion Strategies {.unnumbered}

::: {.callout-note}
## Optional Section
This section covers advanced techniques for combining modalities. Skip if you just need basic multi-modal capabilities.
:::

When working with multiple modalities, you can combine embeddings in several ways:

### Early Fusion

Combine embeddings before indexing:

```python
def early_fusion(text_emb, image_emb, weights=(0.5, 0.5)):
    """Combine modalities into a single embedding."""
    fused = weights[0] * text_emb + weights[1] * image_emb
    return fused / np.linalg.norm(fused)  # Normalize
```

**Best for**: Static entities where all modalities are always available (products with descriptions and images).

### Late Fusion

Combine similarity scores after separate retrieval:

```python
def late_fusion(query_embs, candidate_embs, weights):
    """Combine similarity scores from multiple modalities."""
    total_score = 0
    for modality, weight in weights.items():
        if modality in query_embs:
            score = cosine_similarity(query_embs[modality], candidate_embs[modality])
            total_score += weight * score
    return total_score
```

**Best for**: Queries where available modalities vary (user might search with text only, or text + image).

### Attention Fusion

Learn which modalities to emphasize for each query:

```python
def attention_fusion(modality_embeddings):
    """Dynamically weight modalities using attention."""
    stacked = torch.stack(modality_embeddings)
    attention_weights = torch.softmax(
        torch.matmul(stacked, stacked.transpose(0, 1)), dim=-1
    )
    attended = torch.matmul(attention_weights, stacked)
    return attended.mean(dim=0)
```

**Best for**: Complex queries where modality importance varies by context.

## Key Takeaways

- **Multi-modal embeddings** create a shared space where different data types (text, images, audio) can be directly compared

- **Zero-shot classification** works by comparing embeddings to text descriptions—no training on specific categories required

- **CLIP** is the most popular approach, trained on 400M image-text pairs to align visual and textual concepts

- **Fusion strategies** determine how to combine modalities: early (before indexing), late (after retrieval), or attention-based (learned weighting)

## Looking Ahead

Now that you understand multi-modal embeddings, @sec-graph-embeddings explores graph embeddings—representations that capture network structure and relationships.

## Further Reading

- Radford, A., et al. (2021). "Learning Transferable Visual Models From Natural Language Supervision." *arXiv:2103.00020*
- Li, J., et al. (2023). "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models." *ICML*
- Girdhar, R., et al. (2023). "ImageBind: One Embedding Space To Bind Them All." *CVPR*
