# Chapter 7: Self Supervised Learning

Self-Supervised Learning - Masked language models, autoencoders, domain-specific pre-training

## Code Examples (8 files)

### Main Classes & Systems

- `advancedmlm.py` - Advancedmlm
- `domainspecificmlm.py` - Domainspecificmlm
- `enterpriseselfsupervisedpipeline.py` - Enterpriseselfsupervisedpipeline
- `industrialdefectdetection.py` - Industrialdefectdetection
- `maskedautoencodervit.py` - Maskedautoencodervit
- `multimodalselfsupervised.py` - Multimodalselfsupervised
- `selfsupervisedembeddingframework.py` - Selfsupervisedembeddingframework
- `timeseriesselfsupervised.py` - Timeseriesselfsupervised

## Dependencies

Key Python packages used in this chapter:

- `datasets` → datasets>=2.14.0
- `numpy` → numpy>=1.24.0
- `tokenizers` → tokenizers>=0.13.0
- `torch` → torch>=2.0.0
- `torchvision` → torchvision>=0.15.0
- `transformers` → transformers>=4.30.0

## Usage Notes

Most code examples in this chapter are **illustrative** and designed to demonstrate concepts. Some examples may require:

- Synthetic or sample data (not included)
- Pre-trained models from HuggingFace
- GPU for efficient execution
- Additional setup or configuration

Refer to the book chapter for full context and explanations.
